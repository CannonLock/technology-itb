{
    "docs": [
        {
            "location": "/", 
            "text": "OSG Technology Area\n\n\nWelcome to the home page of the OSG Technology Team documentation area!\n\n\nIf you are looking for site administrator documentation, please visit the \nOSG Documentation page\n.\n\n\nThe Team\n\n\n\n\n\n\n\n\nSoftware and Release\n\n\nTechnology\n\n\n\n\n\n\n\n\n\n\nBrian Lin (software manager)\n\n\nBrian Bockelman (manager) (50%)\n\n\n\n\n\n\nCarl Edquist\n\n\nDerek Weitzel (50%)\n\n\n\n\n\n\nEdgar Fajardo (50%)\n\n\nEdgar Fajardo (50%)\n\n\n\n\n\n\nMat Selmeci\n\n\nJeff Dost (50%)\n\n\n\n\n\n\nSuchandra Thapa (50%)\n\n\nMarian Zvada (25%)\n\n\n\n\n\n\nTim Cartwright (35%)\n\n\n\n\n\n\n\n\nTim Theisen (release manager) (50%)\n\n\n\n\n\n\n\n\n\n\nContact Us\n\n\n\n\nosg-software@opensciencegrid.org\n - General discussion amongst team members\n\n\nSlack channel\n - if you can't create an account, send an e-mail to \nosg-software@opensciencegrid.org\n\n\nosg-commits@cs.wisc.edu\n - Broadcast of all source code repo commits\n\n\n\n\nMeeting Notes\n\n\nWhen:\n Every Monday, 11:00 a.m. (U.S. Central)  \n\n\nWhere:\n +1 719-284-5267, PIN #57363; \nUber Conference\n\n\nRecent Notes\n\n\n\n\n23 April 2018\n\n\n16 April 2018\n\n\n9 April 2018\n\n\n2 April 2018\n\n\n26 March 2018 (Canceled)\n\n\n19 March 2018 (Canceled - OSG All Hands)\n\n\n12 March 2018\n\n\n5 March 2018\n\n\n26 February 2018\n\n\n19 February 2018\n\n\n12 February 2018\n\n\n5 February 2018\n\n\n29 January 2018\n\n\n22 January 2018\n\n\n15 January 2018 (Canceled - MLK day)\n\n\n8 January 2018\n\n\n2 January 2018\n\n\n18 December 2017\n\n\n11 December 2017\n\n\n4 December 2017\n\n\n27 November 2017\n\n\n20 November 2017\n\n\n13 November 2017\n\n\n6 November 2017\n\n\n30 October 2017\n\n\n23 October 2017\n\n\n16 October 2017\n\n\n9 October 2017\n\n\n2 October 2017\n\n\n25 September 2017\n\n\n18 September 2017\n\n\n11 September 2017\n\n\n5 September 2017\n\n\n28 August 2017\n\n\n21 August 2017 - Canceled\n\n\n14 August 2017\n\n\n7 August 2017\n\n\n31 July 2017\n\n\n24 July 2017\n\n\n17 July 2017\n\n\n10 July 2017\n\n\n3 July 2017\n\n\n26 June 2017\n\n\n19 June 2017\n\n\n12 June 2017\n\n\n5 June 2017\n\n\n30 May 2017\n - Memorial Day\n\n\n22 May 2017\n\n\n15 May 2017\n\n\n8 May 2017\n\n\n1 May 2017\n\n\n24 April 2017\n\n\n17 April 2017\n\n\n10 April 2017\n\n\n3 April 2017\n\n\n27 March 2017\n\n\n20 March 2017\n\n\n13 March 2017\n\n\n27 February 2017\n\n\n\n\nMeeting note archives older than February 27, 2017 can be found \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#osg-technology-area", 
            "text": "Welcome to the home page of the OSG Technology Team documentation area!  If you are looking for site administrator documentation, please visit the  OSG Documentation page .", 
            "title": "OSG Technology Area"
        }, 
        {
            "location": "/#the-team", 
            "text": "Software and Release  Technology      Brian Lin (software manager)  Brian Bockelman (manager) (50%)    Carl Edquist  Derek Weitzel (50%)    Edgar Fajardo (50%)  Edgar Fajardo (50%)    Mat Selmeci  Jeff Dost (50%)    Suchandra Thapa (50%)  Marian Zvada (25%)    Tim Cartwright (35%)     Tim Theisen (release manager) (50%)", 
            "title": "The Team"
        }, 
        {
            "location": "/#contact-us", 
            "text": "osg-software@opensciencegrid.org  - General discussion amongst team members  Slack channel  - if you can't create an account, send an e-mail to  osg-software@opensciencegrid.org  osg-commits@cs.wisc.edu  - Broadcast of all source code repo commits", 
            "title": "Contact Us"
        }, 
        {
            "location": "/#meeting-notes", 
            "text": "When:  Every Monday, 11:00 a.m. (U.S. Central)    Where:  +1 719-284-5267, PIN #57363;  Uber Conference", 
            "title": "Meeting Notes"
        }, 
        {
            "location": "/#recent-notes", 
            "text": "23 April 2018  16 April 2018  9 April 2018  2 April 2018  26 March 2018 (Canceled)  19 March 2018 (Canceled - OSG All Hands)  12 March 2018  5 March 2018  26 February 2018  19 February 2018  12 February 2018  5 February 2018  29 January 2018  22 January 2018  15 January 2018 (Canceled - MLK day)  8 January 2018  2 January 2018  18 December 2017  11 December 2017  4 December 2017  27 November 2017  20 November 2017  13 November 2017  6 November 2017  30 October 2017  23 October 2017  16 October 2017  9 October 2017  2 October 2017  25 September 2017  18 September 2017  11 September 2017  5 September 2017  28 August 2017  21 August 2017 - Canceled  14 August 2017  7 August 2017  31 July 2017  24 July 2017  17 July 2017  10 July 2017  3 July 2017  26 June 2017  19 June 2017  12 June 2017  5 June 2017  30 May 2017  - Memorial Day  22 May 2017  15 May 2017  8 May 2017  1 May 2017  24 April 2017  17 April 2017  10 April 2017  3 April 2017  27 March 2017  20 March 2017  13 March 2017  27 February 2017   Meeting note archives older than February 27, 2017 can be found  here .", 
            "title": "Recent Notes"
        }, 
        {
            "location": "/software/rpm-development-guide/", 
            "text": "RPM Development Guide\n\n\nThis page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack.\n\n\nPrinciples\n\n\nThe principles below guide the design and implementation of the technical details that follow.\n\n\n\n\nPackages should adhere to community standards (e.g., \nFedora Packaging Guidelines\n) when possible, and significant deviations must be documented\n\n\nEvery released package must be reproducible from data stored in our system\n\n\nSource code for software should be clearly separable from the packaging of that software\n\n\nUpstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team\n\n\nBuilding source and binary packages from our system should be easy and efficient\n\n\nExternal developers should have a clear and effective system for developing and contributing packages\n\n\nWe should use standard tools from relevant packaging and development communities when appropriate\n\n\n\n\nContributing Packages\n\n\nWe encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors.\n\n\n\n\nTo participate in the packaging community: You must subscribe to the \n email list. Subscribing to an OSG email list is \ndescribed here\n.\n\n\nTo create and edit packages: \nObtain access to VDT SVN\n.\n\n\nTo upload new source tarballs: You must have a cs.wisc.edu account with write access to the VDT source tarball directory. Email the osg-software list and request permission.\n\n\nTo build using the OSG's Koji build system: You must have a valid grid certificate and a Koji account. Email the osg-software list with your cert's DN and request permission.\n\n\n\n\nDevelopment Infrastructure\n\n\nThis section documents most of what a developer needs to know about our RPM infrastructure:\n\n\n\n\nUpstream Source Cache \u2014 a filesystem scheme for caching upstream source files\n\n\nRevision Control System \u2014 where to get and store development files, and how they are organized\n\n\nBuild System \u2014 how to build packages from the revision control system\n\n\nYum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it\n\n\n\n\nUpstream Source Cache\n\n\nOne of our principles (every released package must be reproducible from data stored in our system) creates a potential issue: If we keep all historical source data, especially upstream files like source tarballs and source RPMs, in our revision control system, we may face large checkouts and consequently long checkout and update times.\n\n\nOur solution is to cache all upstream source files in a separate filesystem area, retaining historical files indefinitely. To avoid tainting upstream files, our policy is to leave them unmodified after download.\n\n\nLocating Files in the Cache\n\n\nUpstream source files are stored in the filesystem as follows:\n\n\n\n\n/p/vdt/public/html/upstream/\nPACKAGE\n/\nVERSION\n/\nFILE\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPACKAGE\n\n\nUpstream name of the source package, or some widely accepted form thereof\n\n\nndt\n\n\n\n\n\n\nVERSION\n\n\nUpstream version string used to identify the release\n\n\n3.6.4\n\n\n\n\n\n\nFILE\n\n\nUpstream filename itself\n\n\nndt-3.6.4.tar.gz\n\n\n\n\n\n\n\n\nThe authoritative cache is the VDT webserver, which is fully backed up. The Koji build system uses this cache.\n\n\nUpstream source files are referenced from within the revision control system; see below for details.\n\n\nContributing Upstream Files\n\n\nYou must make sure that any new upstream source files are cached on the VDT webserver before building the package via Koji. You have two options:\n\n\n\n\nIf you have access to a UW\u2013Madison CSL machine, you can scp the source files directly into the AFS locations using that machine\n\n\nIf you do not have such access, write to the osg-software list to find someone who will post the files for you\n\n\n\n\nGit Hosted Upstream Files\n\n\nAs of OSG-Build 1.11.2, it is possible to pull sources and spec files from a remote Git repo instead of our source cache.\nSee the \nupstream dir info\n for more information.\n\n\nRevision Control System\n\n\nAll packages that the OSG Software Team releases are checked into our revision control system (currently Subversion but with the option to change at a later date).\n\n\nSubversion Access\n\n\nOur Subversion repository is located at:\n\n\n\n\nhttps://vdt.cs.wisc.edu/svn\n\n\n\n\n\n\n\nProcedure for offsite users obtaining access to Subversion\n\n\nOr, from a UW\u2013Madison Computer Sciences machine:\n\n\n\n\nfile:///p/condor/workspaces/vdt/svn\n\n\n\n\n\n\n\nThe current SVN directory housing our native package work is \n$SVN/native/redhat\n (where \n$SVN\n is one of the ways of accessing our SVN repository above). For example, to check out the current package repository via HTTPS, do:\n\n\n[you@host]$\n svn co https://vdt.cs.wisc.edu/svn/native/redhat\n\n\n\n\n\nOSG-Owned Software\n\n\nOSG-owned software goes into GitHub under the \nopensciencegrid\n organization. Files are organized as the developer sees fit.\n\n\nIt is strongly recommended that each software package include a top-level Makefile with at least the following targets:\n\n\n\n\n\n\n\n\nSymbol\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\ninstall\n\n\nInstall the software into final FHS locations rooted at \nDESTDIR\n\n\n\n\n\n\ndist\n\n\nCreate a distribution source tarball (in the current section directory) for a release\n\n\n\n\n\n\nupstream\n\n\nInstall the distribution source tarball into the upstream source cache\n\n\n\n\n\n\n\n\nPackaging Top-Level Directory Organization\n\n\nThe top levels of our Subversion directory hierarchy for packaging are as follows:\n\n\n\n\nnative/redhat/\nSECTION\n/\nPACKAGE\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nSECTION\n\n\nDevelopment section\n\n\nStandard Subversion sections like \ntrunk\n and \nbranches/*\n\n\n\n\n\n\nPACKAGE\n\n\nOur standardized name for a source package\n\n\nndt\n\n\n\n\n\n\n\n\nPackage Directory Organization\n\n\nWithin a source package directory, the following files (detailed in separate sections below) may exist:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nREADME\n\n\ntext file\n\n\npackage notes, by and for developers\n\n\n\n\n\n\nupstream/\n\n\ndirectory\n\n\nreferences to the upstream source cache and other kinds of upstream files\n\n\n\n\n\n\nosg/\n\n\ndirectory\n\n\noverrides and patches of upstream files, plus new files, which contribute to the final OSG source package\n\n\n\n\n\n\n\n\nREADME\n\n\nThis is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the \nwhy\n, not just the \nwhat\n.\n\n\nupstream\n\n\nWithin the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents.\n\n\nA reference file is named:\n\n\n\n\nDESCRIPTION\n.\nTYPE\n.source\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nDESCRIPTION\n\n\nDescriptive label of the source of the referenced file\n\n\ndeveloper\n, \nepel\n, \nemi\n\n\n\n\n\n\nTYPE\n\n\nType of referenced file\n\n\ntarball\n, \nsrpm\n\n\n\n\n\n\n\n\nand contain references to cached files, Git repos, and comments.\nwhich start with \n#\n and continue until the end of the line.\nIt is useful to add the source of the upstream file into a comment.\n\n\nCached files\n\n\nTo reference files in the upstream source cache, use the upstream source cache path defined above, without the prefix component:\n\n\n\n\nPACKAGE\n/\nVERSION\n/\nFILE\n\n\n\n\n\n\nExample\n\n\nThe reference file for \nglobus-common\n's source tarball is named \nepel.srpm.source\n and contains:\n\n\nglobus-common/16.4/globus-common-16.4-1.el6.src.rpm\n# Downloaded from \nhttp://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm\n\n\n\n\n\n\n\n\nGit repos (OSG-Build 1.11.2+)\n\n\n\n\nWarning\n\n\nOSG software policy requires that all Git repos used for building software have mirrors at the UW.\nMany software repos under the \nopensciencegrid GitHub organization\n are already mirrored.\nIf you are uncertain, or have a new project that you want mirrored, send email to \n.\n\n\n\n\nTo reference tags in Git repos, use the following syntax (all on one line):\n\n\n\n\ntype=git url=\nURL\n name=\nNAME\n tag=\nTAG\n hash=\nHASH\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nLocation of the Git repo\n\n\nhttps://github.com/opensciencegrid/osg-build.git\n\n\n\n\n\n\nNAME\n\n\nName of the software (optional)\n\n\nosg-build\n\n\n\n\n\n\nTAG\n\n\nGit tag to use\n\n\nv1.11.2\n\n\n\n\n\n\nHASH\n\n\nFull 40-char Git hash of the tag\n\n\n5bcf48c442d21b1e8c93a468d884f84122f7cc9e\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNAME\n is optional; if not present, OSG-Build will use the last component of the URL, without the \n.git\n suffix.\n\n\nThe tarball will be called \nNAME\n-\nVERSION\n.tar.gz\n where \nVERSION\n is \nTAG\n without the \nv\n prefix (if there is one).\n\n\n\n\n\n\nExample\n\n\nThe reference file for \nosg-build\n's repo is named \nosg.github.source\n and contains:\n\n\ntype=git url=https://github.com/opensciencegrid/osg-build.git name=osg-build tag=v1.11.2 hash=5bcf48c442d21b1e8c93a468d884f84122f7cc9e\n\n\n\n\n\nThis results in a tarball named \nosg-build-1.11.2.tar.gz\n.\n\n\n\n\nIn addition, if the repository contains a file called \nrpm/\nNAME\n.spec\n, it will be used as the spec file for the build\n(unless overridden in the \nosg\n directory).\n\n\nOSG-Build 1.11.2 or later is required to use this feature.\n\n\nosg\n\n\nThe \nosg\n directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files:\n\n\n\n\nAn RPM \n.spec\n file, which overrides any spec file from a referenced source\n\n\nPatch (\n.patch\n) or replacement files, which override any same-named file from the top-level directory of a referenced source\n\n\nOther files, which must be explicitly placed into the package by the spec file\n\n\n\n\nGenerated directories\n\n\nThe following directories may be generated by our build tool, \nOSG-Build\n. They are not under revision control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_upstream_srpm_contents/\n\n\nexpanded contents of a cached upstream source package\n\n\n\n\n\n\n_upstream_tarball_contents/\n\n\nexpanded contents of all cached upstream source tarballs\n\n\n\n\n\n\n_final_srpm_contents/\n\n\nthe final contents of the OSG source package\n\n\n\n\n\n\n_build_results/\n\n\nOSG source and binary packages resulting from a build\n\n\n\n\n\n\n_quilt/\n\n\nexpanded, patched contents of the upstream sources, as generated by the \nquilt\n tool\n\n\n\n\n\n\n\n\n_upstream_srpm_contents\n\n\nThe \n_upstream_srpm_contents\n directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use.\n\n\n_upstream_tarball_contents\n\n\nThe \n_upstream_tarball_contents\n directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the \n--full-extract\n option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.).\n\n\n_final_srpm_contents\n\n\nThe \n_final_srpm_contents\n directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use.\n\n\n_build_results\n\n\nThe \n_build_results\n directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use.\n\n\n_quilt\n\n\nThe \n_quilt\n directory contains the unpacked sources after they have been patched using the \nquilt\n utility. This allows easier patch development.\n\n\nPackaging Organization Examples\n\n\nUse Case 1: Packaging an Upstream Source Tarball\n\n\nWhen the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains:\n\n\n\n\nrsv/\n    osg/\n        rsv.spec\n    upstream/\n        developer.tarball.source\n\n\n\n\n\n\n\nUse Case 2: Passing Through a Source RPM\n\n\nWhen the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the \nglobus-common\n source RPM from EPEL. Its package directory contains:\n\n\n\n\nglobus-common/\n    upstream/\n        epel.srpm.source\n\n\n\n\n\n\n\nUse Case 3: Modifying a Source RPM\n\n\nWhen the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the \nosg\n directory. For example, we use this mechanism for the \nglobus-ftp-client\n package, originally obtained from EPEL. Its package directory contains:\n\n\n\n\nglobus-ftp-client/\n    osg/\n        globus-ftp-client.spec\n        1853-ssh-bin.patch\n    upstream/\n        epel.srpm.source\n\n\n\n\n\n\n\nBuild Process\n\n\n\n\nAll necessary information to create the package will be committed to the VDT source code repository (see below)\n\n\nThe \nOSG build tools\n will take those files, create a source RPM, and submit it to our Koji build system\n\n\n\n\nDevelopers may use \nrpmbuild\n and \nmock\n for faster iterative development before submitting the package to Koji. \nosg-build\n may be used as a wrapper script around \nrpmbuild\n and \nmock\n.\n\n\nOSG Software Repository\n\n\nOSG Operations maintains the Yum repositories that contain our source and binary RPMs at \nhttps://repo.opensciencegrid.org/osg/\n and are mirrored at other institutions as well.\n\n\nRelease Levels\n\n\nEvery package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level (\nosg-development\n). The members of the OSG Software and Release teams may promote packages through the release levels, as per our \nRelease Policy page\n.\n\n\nPackaging Conventions\n\n\nIn addition to adhering to the \nFedora Packaging Guidelines\n (FPG), we have a few rules and guidelines of our own:\n\n\n\n\nWhen we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows:\n\n\nThe version of the original RPM remains unchanged\n\n\nThe release is composed of three parts: ORIGINALRELEASE.OSGRELEASE\n\n\nWe add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use \n%{?dist}\n in the Release field)\n\n\n\n\n\n\n\n\nExample: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes \nfoobar-3.0.5-1.1.osg33.el6\n Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like:\n\n\nRelease\n:\n 1.2\n%{?dist}\n\n\n\n\n\n\nPackaging for Multiple Distro Versions\n\n\nConditionalizing spec files\n\n\nSome packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this.\n\n\nThe following macros are defined:\n\n\n\n\n\n\n\n\nName\n\n\nValue (EL6)\n\n\nValue (EL7)\n\n\n\n\n\n\n\n\n\n\n%rhel\n\n\n6\n\n\n7\n\n\n\n\n\n\n%el6\n\n\n1\n\n\nundefined\n or \n0\n\n\n\n\n\n\n%el7\n\n\nundefined\n or \n0\n\n\n1\n\n\n\n\n\n\n\n\nHere's how to use them:\n\n\n%if\n 0%{?el6}\n\n# this code will be executed on EL 6 only\n\n\n%endif\n\n\n\n%if\n 0%{?el7}\n\n# this code will be executed on EL 7 only\n\n\n%endif\n\n\n\n%if\n 0%{?rhel} \n= 7\n\n# this code will be executed on EL 7 and newer\n\n\n%endif\n\n\n\n\n\n\n(There does not seem to be an \n%elseif\n).\n\n\nThe syntax \n%{?el6}\n expands to the value of the \n%el6\n macro if it is defined, and to the empty string if not; the \n0\n is there to keep the condition from being empty in the \n%if\n statement if the macro is not defined.\n\n\nRenaming or Removing Packages\n\n\nOccasionally we want to cause a package to be removed on update, or replaced by a package with a different name.\n\n\nFor the most part, the \nFedora Packaging Guidelines page on renames\n shows how to do that.\nThe exception is that we do not have the equivalent of a \nfedora-obsolete-packages\n package, so in order to force the removal of an entire package (not a subpackage), we have to dummy out the package instead -- see below.\n(This should be a rare situation.)\n\n\n\n\nNote\n\n\nAfter doing a rename or a removal, you must update all the packages and subpackages that require the package being removed or renamed, and change or remove the requirements as appropriate.\n\n\n\n\nTo find packages that require the old package at run time, set up a host with the OSG repos and install the \nyum-utils\n RPM.\nThen, run:\n\n\n$\n repoquery --plugins --whatrequires \n$OLDPACKAGE\n\n\n\n\n\n\nTo find packages that require the old package at build time, install \nosg-build\n, and do this from a checkout of the OSG repos:\n\n\n$\n osg-build prebuild *\n\n$\n \nfor\n srpm in */_final_srpm_contents/*.src.rpm\n;\n \ndo\n \n\\\n\n    \necho\n \n***** \n$srpm\n *****\n;\n \n\\\n\n    rpm -q --requires -p \n$srpm\n \n|\n grep -w \n$OLDPACKAGE\n;\n \n\\\n\n  \ndone\n\n\n\n\n\n\n(examine the output to avoid false matches)\n\n\n\n\nNote\n\n\nCarefully test these changes, including places where the old package may be brought in indirectly.\n\n\n\n\nDummying out a package\n\n\nIn order to forcibly remove an entire package with no replacement, you have to replace the package with one that does nothing.\nThis is because there is no package that will \"obsolete\" the old package.\n\n\nDo the following for the main package and any subpackages it may have:\n\n\n\n\nChange the Summary to \"Dummy package\"\n\n\n\n\nChange the %description to:\n\n\n\n\nThis is an empty package created for \n$\n.\nIt may safely be removed.\n\n\n\n\n\n\n\n\nRemove all Requires and Obsoletes lines\n\n\n\n\nDo not remove Provides lines\n\n\nRemove %pre and %post scriptlets\n\n\nUnless there is a good reason not to, remove %preun and %postun scriptlets\n\n\nEmpty the %files section", 
            "title": "RPM Development Guide"
        }, 
        {
            "location": "/software/rpm-development-guide/#rpm-development-guide", 
            "text": "This page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack.", 
            "title": "RPM Development Guide"
        }, 
        {
            "location": "/software/rpm-development-guide/#principles", 
            "text": "The principles below guide the design and implementation of the technical details that follow.   Packages should adhere to community standards (e.g.,  Fedora Packaging Guidelines ) when possible, and significant deviations must be documented  Every released package must be reproducible from data stored in our system  Source code for software should be clearly separable from the packaging of that software  Upstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team  Building source and binary packages from our system should be easy and efficient  External developers should have a clear and effective system for developing and contributing packages  We should use standard tools from relevant packaging and development communities when appropriate", 
            "title": "Principles"
        }, 
        {
            "location": "/software/rpm-development-guide/#contributing-packages", 
            "text": "We encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors.   To participate in the packaging community: You must subscribe to the   email list. Subscribing to an OSG email list is  described here .  To create and edit packages:  Obtain access to VDT SVN .  To upload new source tarballs: You must have a cs.wisc.edu account with write access to the VDT source tarball directory. Email the osg-software list and request permission.  To build using the OSG's Koji build system: You must have a valid grid certificate and a Koji account. Email the osg-software list with your cert's DN and request permission.", 
            "title": "Contributing Packages"
        }, 
        {
            "location": "/software/rpm-development-guide/#development-infrastructure", 
            "text": "This section documents most of what a developer needs to know about our RPM infrastructure:   Upstream Source Cache \u2014 a filesystem scheme for caching upstream source files  Revision Control System \u2014 where to get and store development files, and how they are organized  Build System \u2014 how to build packages from the revision control system  Yum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it", 
            "title": "Development Infrastructure"
        }, 
        {
            "location": "/software/rpm-development-guide/#upstream-source-cache", 
            "text": "One of our principles (every released package must be reproducible from data stored in our system) creates a potential issue: If we keep all historical source data, especially upstream files like source tarballs and source RPMs, in our revision control system, we may face large checkouts and consequently long checkout and update times.  Our solution is to cache all upstream source files in a separate filesystem area, retaining historical files indefinitely. To avoid tainting upstream files, our policy is to leave them unmodified after download.", 
            "title": "Upstream Source Cache"
        }, 
        {
            "location": "/software/rpm-development-guide/#locating-files-in-the-cache", 
            "text": "Upstream source files are stored in the filesystem as follows:   /p/vdt/public/html/upstream/ PACKAGE / VERSION / FILE   where:     Symbol  Definition  Example      PACKAGE  Upstream name of the source package, or some widely accepted form thereof  ndt    VERSION  Upstream version string used to identify the release  3.6.4    FILE  Upstream filename itself  ndt-3.6.4.tar.gz     The authoritative cache is the VDT webserver, which is fully backed up. The Koji build system uses this cache.  Upstream source files are referenced from within the revision control system; see below for details.", 
            "title": "Locating Files in the Cache"
        }, 
        {
            "location": "/software/rpm-development-guide/#contributing-upstream-files", 
            "text": "You must make sure that any new upstream source files are cached on the VDT webserver before building the package via Koji. You have two options:   If you have access to a UW\u2013Madison CSL machine, you can scp the source files directly into the AFS locations using that machine  If you do not have such access, write to the osg-software list to find someone who will post the files for you", 
            "title": "Contributing Upstream Files"
        }, 
        {
            "location": "/software/rpm-development-guide/#git-hosted-upstream-files", 
            "text": "As of OSG-Build 1.11.2, it is possible to pull sources and spec files from a remote Git repo instead of our source cache.\nSee the  upstream dir info  for more information.", 
            "title": "Git Hosted Upstream Files"
        }, 
        {
            "location": "/software/rpm-development-guide/#revision-control-system", 
            "text": "All packages that the OSG Software Team releases are checked into our revision control system (currently Subversion but with the option to change at a later date).", 
            "title": "Revision Control System"
        }, 
        {
            "location": "/software/rpm-development-guide/#subversion-access", 
            "text": "Our Subversion repository is located at:   https://vdt.cs.wisc.edu/svn   Procedure for offsite users obtaining access to Subversion  Or, from a UW\u2013Madison Computer Sciences machine:   file:///p/condor/workspaces/vdt/svn   The current SVN directory housing our native package work is  $SVN/native/redhat  (where  $SVN  is one of the ways of accessing our SVN repository above). For example, to check out the current package repository via HTTPS, do:  [you@host]$  svn co https://vdt.cs.wisc.edu/svn/native/redhat", 
            "title": "Subversion Access"
        }, 
        {
            "location": "/software/rpm-development-guide/#osg-owned-software", 
            "text": "OSG-owned software goes into GitHub under the  opensciencegrid  organization. Files are organized as the developer sees fit.  It is strongly recommended that each software package include a top-level Makefile with at least the following targets:     Symbol  Purpose      install  Install the software into final FHS locations rooted at  DESTDIR    dist  Create a distribution source tarball (in the current section directory) for a release    upstream  Install the distribution source tarball into the upstream source cache", 
            "title": "OSG-Owned Software"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-top-level-directory-organization", 
            "text": "The top levels of our Subversion directory hierarchy for packaging are as follows:   native/redhat/ SECTION / PACKAGE   where:     Symbol  Definition  Example      SECTION  Development section  Standard Subversion sections like  trunk  and  branches/*    PACKAGE  Our standardized name for a source package  ndt", 
            "title": "Packaging Top-Level Directory Organization"
        }, 
        {
            "location": "/software/rpm-development-guide/#package-directory-organization", 
            "text": "Within a source package directory, the following files (detailed in separate sections below) may exist:            README  text file  package notes, by and for developers    upstream/  directory  references to the upstream source cache and other kinds of upstream files    osg/  directory  overrides and patches of upstream files, plus new files, which contribute to the final OSG source package", 
            "title": "Package Directory Organization"
        }, 
        {
            "location": "/software/rpm-development-guide/#readme", 
            "text": "This is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the  why , not just the  what .", 
            "title": "README"
        }, 
        {
            "location": "/software/rpm-development-guide/#upstream", 
            "text": "Within the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents.  A reference file is named:   DESCRIPTION . TYPE .source   where:     Symbol  Definition  Example      DESCRIPTION  Descriptive label of the source of the referenced file  developer ,  epel ,  emi    TYPE  Type of referenced file  tarball ,  srpm     and contain references to cached files, Git repos, and comments.\nwhich start with  #  and continue until the end of the line.\nIt is useful to add the source of the upstream file into a comment.", 
            "title": "upstream"
        }, 
        {
            "location": "/software/rpm-development-guide/#cached-files", 
            "text": "To reference files in the upstream source cache, use the upstream source cache path defined above, without the prefix component:   PACKAGE / VERSION / FILE    Example  The reference file for  globus-common 's source tarball is named  epel.srpm.source  and contains:  globus-common/16.4/globus-common-16.4-1.el6.src.rpm\n# Downloaded from  http://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm", 
            "title": "Cached files"
        }, 
        {
            "location": "/software/rpm-development-guide/#git-repos-osg-build-1112", 
            "text": "Warning  OSG software policy requires that all Git repos used for building software have mirrors at the UW.\nMany software repos under the  opensciencegrid GitHub organization  are already mirrored.\nIf you are uncertain, or have a new project that you want mirrored, send email to  .   To reference tags in Git repos, use the following syntax (all on one line):   type=git url= URL  name= NAME  tag= TAG  hash= HASH   where:     Symbol  Definition  Example      URL  Location of the Git repo  https://github.com/opensciencegrid/osg-build.git    NAME  Name of the software (optional)  osg-build    TAG  Git tag to use  v1.11.2    HASH  Full 40-char Git hash of the tag  5bcf48c442d21b1e8c93a468d884f84122f7cc9e      Note  NAME  is optional; if not present, OSG-Build will use the last component of the URL, without the  .git  suffix.  The tarball will be called  NAME - VERSION .tar.gz  where  VERSION  is  TAG  without the  v  prefix (if there is one).    Example  The reference file for  osg-build 's repo is named  osg.github.source  and contains:  type=git url=https://github.com/opensciencegrid/osg-build.git name=osg-build tag=v1.11.2 hash=5bcf48c442d21b1e8c93a468d884f84122f7cc9e  This results in a tarball named  osg-build-1.11.2.tar.gz .   In addition, if the repository contains a file called  rpm/ NAME .spec , it will be used as the spec file for the build\n(unless overridden in the  osg  directory).  OSG-Build 1.11.2 or later is required to use this feature.", 
            "title": "Git repos (OSG-Build 1.11.2+)"
        }, 
        {
            "location": "/software/rpm-development-guide/#osg", 
            "text": "The  osg  directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files:   An RPM  .spec  file, which overrides any spec file from a referenced source  Patch ( .patch ) or replacement files, which override any same-named file from the top-level directory of a referenced source  Other files, which must be explicitly placed into the package by the spec file", 
            "title": "osg"
        }, 
        {
            "location": "/software/rpm-development-guide/#generated-directories", 
            "text": "The following directories may be generated by our build tool,  OSG-Build . They are not under revision control.           _upstream_srpm_contents/  expanded contents of a cached upstream source package    _upstream_tarball_contents/  expanded contents of all cached upstream source tarballs    _final_srpm_contents/  the final contents of the OSG source package    _build_results/  OSG source and binary packages resulting from a build    _quilt/  expanded, patched contents of the upstream sources, as generated by the  quilt  tool", 
            "title": "Generated directories"
        }, 
        {
            "location": "/software/rpm-development-guide/#95upstream95srpm95contents", 
            "text": "The  _upstream_srpm_contents  directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use.", 
            "title": "_upstream_srpm_contents"
        }, 
        {
            "location": "/software/rpm-development-guide/#95upstream95tarball95contents", 
            "text": "The  _upstream_tarball_contents  directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the  --full-extract  option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.).", 
            "title": "_upstream_tarball_contents"
        }, 
        {
            "location": "/software/rpm-development-guide/#95final95srpm95contents", 
            "text": "The  _final_srpm_contents  directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use.", 
            "title": "_final_srpm_contents"
        }, 
        {
            "location": "/software/rpm-development-guide/#95build95results", 
            "text": "The  _build_results  directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use.", 
            "title": "_build_results"
        }, 
        {
            "location": "/software/rpm-development-guide/#95quilt", 
            "text": "The  _quilt  directory contains the unpacked sources after they have been patched using the  quilt  utility. This allows easier patch development.", 
            "title": "_quilt"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-organization-examples", 
            "text": "", 
            "title": "Packaging Organization Examples"
        }, 
        {
            "location": "/software/rpm-development-guide/#use-case-1-packaging-an-upstream-source-tarball", 
            "text": "When the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains:   rsv/\n    osg/\n        rsv.spec\n    upstream/\n        developer.tarball.source", 
            "title": "Use Case 1: Packaging an Upstream Source Tarball"
        }, 
        {
            "location": "/software/rpm-development-guide/#use-case-2-passing-through-a-source-rpm", 
            "text": "When the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the  globus-common  source RPM from EPEL. Its package directory contains:   globus-common/\n    upstream/\n        epel.srpm.source", 
            "title": "Use Case 2: Passing Through a Source RPM"
        }, 
        {
            "location": "/software/rpm-development-guide/#use-case-3-modifying-a-source-rpm", 
            "text": "When the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the  osg  directory. For example, we use this mechanism for the  globus-ftp-client  package, originally obtained from EPEL. Its package directory contains:   globus-ftp-client/\n    osg/\n        globus-ftp-client.spec\n        1853-ssh-bin.patch\n    upstream/\n        epel.srpm.source", 
            "title": "Use Case 3: Modifying a Source RPM"
        }, 
        {
            "location": "/software/rpm-development-guide/#build-process", 
            "text": "All necessary information to create the package will be committed to the VDT source code repository (see below)  The  OSG build tools  will take those files, create a source RPM, and submit it to our Koji build system   Developers may use  rpmbuild  and  mock  for faster iterative development before submitting the package to Koji.  osg-build  may be used as a wrapper script around  rpmbuild  and  mock .", 
            "title": "Build Process"
        }, 
        {
            "location": "/software/rpm-development-guide/#osg-software-repository", 
            "text": "OSG Operations maintains the Yum repositories that contain our source and binary RPMs at  https://repo.opensciencegrid.org/osg/  and are mirrored at other institutions as well.", 
            "title": "OSG Software Repository"
        }, 
        {
            "location": "/software/rpm-development-guide/#release-levels", 
            "text": "Every package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level ( osg-development ). The members of the OSG Software and Release teams may promote packages through the release levels, as per our  Release Policy page .", 
            "title": "Release Levels"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-conventions", 
            "text": "In addition to adhering to the  Fedora Packaging Guidelines  (FPG), we have a few rules and guidelines of our own:   When we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows:  The version of the original RPM remains unchanged  The release is composed of three parts: ORIGINALRELEASE.OSGRELEASE  We add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use  %{?dist}  in the Release field)     Example: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes  foobar-3.0.5-1.1.osg33.el6  Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like:  Release :  1.2 %{?dist}", 
            "title": "Packaging Conventions"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-for-multiple-distro-versions", 
            "text": "", 
            "title": "Packaging for Multiple Distro Versions"
        }, 
        {
            "location": "/software/rpm-development-guide/#conditionalizing-spec-files", 
            "text": "Some packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this.  The following macros are defined:     Name  Value (EL6)  Value (EL7)      %rhel  6  7    %el6  1  undefined  or  0    %el7  undefined  or  0  1     Here's how to use them:  %if  0%{?el6} # this code will be executed on EL 6 only  %endif  %if  0%{?el7} # this code will be executed on EL 7 only  %endif  %if  0%{?rhel}  = 7 # this code will be executed on EL 7 and newer  %endif   (There does not seem to be an  %elseif ).  The syntax  %{?el6}  expands to the value of the  %el6  macro if it is defined, and to the empty string if not; the  0  is there to keep the condition from being empty in the  %if  statement if the macro is not defined.", 
            "title": "Conditionalizing spec files"
        }, 
        {
            "location": "/software/rpm-development-guide/#renaming-or-removing-packages", 
            "text": "Occasionally we want to cause a package to be removed on update, or replaced by a package with a different name.  For the most part, the  Fedora Packaging Guidelines page on renames  shows how to do that.\nThe exception is that we do not have the equivalent of a  fedora-obsolete-packages  package, so in order to force the removal of an entire package (not a subpackage), we have to dummy out the package instead -- see below.\n(This should be a rare situation.)   Note  After doing a rename or a removal, you must update all the packages and subpackages that require the package being removed or renamed, and change or remove the requirements as appropriate.   To find packages that require the old package at run time, set up a host with the OSG repos and install the  yum-utils  RPM.\nThen, run:  $  repoquery --plugins --whatrequires  $OLDPACKAGE   To find packages that require the old package at build time, install  osg-build , and do this from a checkout of the OSG repos:  $  osg-build prebuild * $   for  srpm in */_final_srpm_contents/*.src.rpm ;   do   \\ \n     echo   *****  $srpm  ***** ;   \\ \n    rpm -q --requires -p  $srpm   |  grep -w  $OLDPACKAGE ;   \\ \n   done   (examine the output to avoid false matches)   Note  Carefully test these changes, including places where the old package may be brought in indirectly.", 
            "title": "Renaming or Removing Packages"
        }, 
        {
            "location": "/software/rpm-development-guide/#dummying-out-a-package", 
            "text": "In order to forcibly remove an entire package with no replacement, you have to replace the package with one that does nothing.\nThis is because there is no package that will \"obsolete\" the old package.  Do the following for the main package and any subpackages it may have:   Change the Summary to \"Dummy package\"   Change the %description to:   This is an empty package created for  $ .\nIt may safely be removed.     Remove all Requires and Obsoletes lines   Do not remove Provides lines  Remove %pre and %post scriptlets  Unless there is a good reason not to, remove %preun and %postun scriptlets  Empty the %files section", 
            "title": "Dummying out a package"
        }, 
        {
            "location": "/software/osg-build-tools/", 
            "text": "OSG Build Tools\n\n\nThis page documents the tools used for RPM development for the OSG Software Stack. See \nthe RPM development guide\n for the principles on which these tools are based.\n\n\nThe tools are distributed in the \nosg-build\n RPM in our repositories, but can also be used from a Git clone of \nopensciencegrid/osg-build on GitHub\n.\n\n\nThis page is up-to-date as of \nosg-build\n version 1.10.1.\n\n\nThe tools\n\n\nosg-build\n\n\nOverview\n\n\nThis is the primary tool used in building source and binary RPMs.\n\n\n\n\nosg-build \nTASK\n [options] \nPACKAGE DIRECTORY\n [...]\n\n\n\n\npackage_directory\n is a directory containing an \nosg/\n and/or an \nupstream/\n subdirectory. See \nthe RPM development guide\n for how these directories are organized.\n\n\nTasks\n\n\nkoji\n\n\nPrebuilds the final source package, then builds it remotely using the Koji instance hosted at UW-Madison. \nhttps://koji.chtc.wisc.edu\n By default, the resulting RPMs will end up in the osg-minefield repositories based on the most recent OSG major version (e.g. 3.4). You may specify a different set of repos with \n--repo\n, described later. RPMs from the osg-minefield repositories are regularly pulled to the osg-development repositories hosted by the GOC at \nhttp://repo.opensciencegrid.org\n Unless you specify otherwise (by passing \n--el6\n, \n--el7\n or specifying a different koji tag/target), the package will be built for both el6 and el7. This is the method used to build final versions of packages you expect to ship.\n\n\nlint\n\n\nPrebuilds the final source package, then runs \nrpmlint\n on it to check for various problems. You will need to have \nrpmlint\n installed. People on UW CSL machines should add \n/p/vdt/workspace/rpmlint\n to their $PATH.\n\n\nmock\n\n\nPrebuilds the final source package, then builds it locally using \nmock\n, and stores the resulting source and binary RPMs in the package-specific \n_build_results\n directory.\n\n\nprebuild\n\n\nPrebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the \n_upstream_srpm_contents\n and \n_final_srpm_contents\n directories.\n\n\nprepare\n\n\nPrebuilds the final source package, then calls \nrpmbuild -bp\n on the result, extracting and patching the source files (and performing any other steps defined in the \n%prep\n section of the spec file. The resulting sources will be under \n_final_srpm_contents\n.\n\n\nrpmbuild\n\n\nPrebuilds the final source package, then builds it locally using \nrpmbuild\n, and stores the resulting source and binary RPMs in the package-specific \n_build_results\n directory.\n\n\nquilt\n\n\nCollects the upstream local sources and spec file, then calls \nquilt setup\n on the spec file, extracting the source files and adding the patches to a quilt series file. See \nQuilt documentation (PDF link)\n for more information on quilt; also look at the example in the Usage Patterns section below. Similar to \nprepare\n (in fact, \nquilt\n calls \nrpmbuild -bp\n behind the scenes), but the source tree is in pre-patch state, and various quilt commands can be used to apply and modify patches. Unpacks into \n_quilt\n as of \nosg-build-1.2.2\n or \n_final_srpm_contents\n in previous versions. Requires \nquilt\n. People on UW CSL machines should add \n/p/vdt/workspace/quilt/bin\n to their \n$PATH\n, and \n/p/vdt/workspace/quilt/share/man\n to their \n$MANPATH\n.\n\n\nOptions\n\n\nThis section lists the command-line options.\n\n\n--help\n\n\nPrints the built-in usage information and exits without doing anything else.\n\n\n--version\n\n\nPrints the version of \nosg-build\n and exits without doing anything else.\n\n\nCommon Options\n\n\n-a, --autoclean, --no-autoclean\n\n\nBefore each build, clean out the contents of the underscore directories (_build_results, _final_srpm_contents, _upstream_srpm_contents, _upstream_tarball_contents). If the directories are not cleaned up, earlier builds of a package may interfere with later ones. \n--no-autoclean\n will disable this.\n\n\nDefault is \ntrue\n.\n\n\nHas no effect with the \n--vcs\n flag.\n\n\n-c, --cache-prefix \nprefix\n\n\nSets the \nprefix\n for upstream source cache references. The prefix must be a valid URI starting with either \nhttp\n, \nhttps\n, or \nfile\n, or one of the following special values:\n\n\n\n\nAFS (corresponds to \nfile:///p/vdt/public/html/upstream\n, which is the location of the VDT cache using AFS from a UW CS machine).\n\n\nVDT (corresponds to \nhttp://vdt.cs.wisc.edu/upstream\n, which is the location of the VDT cache from off-site).\n\n\nAUTO (AFS if available, VDT if not)\n\n\n\n\nThe upstream source cache must be organized as described above. All files referenced by \n.source\n files in the affected packages must exist in the cache, or a runtime error will occur.\n\n\nDefault is \nAUTO\n.\n\n\nHas no effect with the \n--vcs\n flag.\n\n\n--el6, --el7, --redhat-release \nversion\n (Config: redhat_release)\n\n\nSets the distro version to build for. This affects the %dist tag, the mock config, and the default koji tag and target (unless otherwise specified).\n\n\n--el6\n is equivalent to \n--redhat-release 6\n\n\n--el7\n is equivalent to \n--redhat-release 7\n\n\n--loglevel \nloglevel\n\n\nSets the verbosity of the script. Valid values are: \ndebug\n, \ninfo\n, \nwarning\n, \nerror\n and \ncritical\n.\n\n\nDefault is \ninfo\n.\n\n\n-q, --quiet\n\n\nDo not display as much information. Equivalent to \n--loglevel warning\n\n\n-v, --verbose\n\n\nDisplay more information. Equivalent to \n--loglevel debug\n\n\n-w, --working-directory \npath\n\n\nUse \npath\n as the root directory of the files created by the script. For example, if \npath\n is \n$HOME/working\n, and the package being built is \nndt\n, the following tree will be created:\n\n\n\n\n$HOME/working/ndt/_upstream_srpm_contents\n\n\n$HOME/working/ndt/_upstream_tarball_contents\n\n\n$HOME/working/ndt/_final_srpm_contents\n\n\n$HOME/working/ndt/_build_results\n\n\n\n\nIf \npath\n is \nTEMP\n, a randomly named directory under \n/tmp\n is used as the working directory.\n\n\nThe default setting is to use the package directory as the working directory.\n\n\nHas no effect with the \n--vcs\n flag.\n\n\nOptions specific to prebuild task\n\n\n--full-extract\n\n\nIf set, all upstream tarballs will be extracted into \n_upstream_tarball_contents/\n during the prebuild step. This flag is now mostly redundant with the \nprepare\n and \nquilt\n tasks.\n\n\nOptions specific to rpmbuild and mock tasks\n\n\n--distro-tag \ndist\n\n\nSets the distribution tag added on to the end of the release in the RPM ( \nrpmbuild\n and \nmock\n tasks only ).\n\n\nDefault is \n.osg.el6\n or \n.osg.el7\n\n\n-t, --target-arch \narch\n\n\nSpecify an architecture to build packages for ( \nrpmbuild\n and \nmock\n tasks only ).\n\n\nDefault is unspecified, which builds for the current machine architecture.\n\n\nOptions specific to mock task\n\n\n--mock-clean, --no-mock-clean\n\n\nEnable/disable deletion of the mock buildroot after a successful build.\n\n\nDefault is \ntrue\n.\n\n\n-m, --mock-config \npath\n\n\nSpecifies the \nmock\n configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag \n%dist\n).\n\n\nSee also \n--mock-config-from-koji\n.\n\n\n--mock-config-from-koji \nbuild tag\n\n\nCreates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is: \nosg-3.4-el6-build\n Also requires the Koji command-line tools (package \nkoji\n), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely: \n--koji-backend\n, \n--koji-login\n, and \n--koji-wrapper\n.\n\n\nOptions specific to koji task\n\n\n--dry-run\n\n\nDo not actually run koji, merely show the command(s) that will be run. For debugging purposes.\n\n\n--getfiles, --get-files\n\n\nFor scratch builds without \n--vcs\n only. Download the resulting RPMs and logs from the build into the \n_build_results\n directory.\n\n\n-k, --kojilogin, --koji-login \nlogin\n\n\nSets the login to use for the koji task. This should most likely be your CN. If not specified, will extract it from your client cert (\n~/.osg-koji/client.crt\n or \n~/.koji/client.crt\n).\n\n\n--koji-target \ntarget\n\n\nThe koji target to use for building.\n\n\nDefault is \nosg-el6\n for el6 and \nosg-el7\n for el7.\n\n\n--koji-tag \ntag\n\n\nThe koji tag to add packages to. See the \nKoji Workflow guide\n for more information on the terminology. The special value \nTARGET\n uses the destination tag defined in the koji target.\n\n\nDefault is \nosg-el6\n or \nosg-el7\n.\n\n\n--ktt, --koji-tag-and-target \narg\n\n\nShorthand for setting both --koji-tag and --koji-target to \narg\n.\n\n\n--koji-wrapper, --no-koji-wrapper\n\n\nEnable/disable use of the \nosg-koji\n wrapper script around koji. See below for a description of \nosg-koji\n.\n\n\nDefault is \ntrue\n.\n\n\n--koji-backend \nbackend\n\n\nSpecifies the method osg-build will use to interface with Koji. This can be \nshell\n or \nkojilib\n.\n\n\n--wait, --no-wait, --nowait\n\n\nWait for koji tasks to finish. Bad for running multiple builds in a single command, since you will have to type in your passphrase for the first one, wait for it to complete, then type in your passphrase for the second one, wait for it to complete, etc. If you want to wait for multiple tasks to finish, use the \nkoji watch-task\n command or look at the website \nhttps://koji.chtc.wisc.edu\n.\n\n\n--wait\n used to be the default until \nosg-build-1.1.3\n\n\n--regen-repos\n\n\nStart a \nregen-repo\n koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with \n--no-wait\n, since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete.\n\n\n--scratch, --no-scratch\n\n\nPerform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient.\n\n\n--vcs, --no-vcs, --svn, --no-svn\n\n\nHave Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. Currently, SVN and Git are supported. If this flag is specified, you may use SVN URLs or URL@Revision pairs to specify the packages to build. You may continue specify package directories from an SVN checkout, in which case osg-build will use \nsvn info\n to find the right URL@Revision pair to use and warn you about uncommitted changes. osg-build will also warn you about an outdated working directory.\n\n\n--vcs\n defaults to \ntrue\n for non-scratch builds, and \nfalse\n for scratch builds.\n\n\n--repo=\ndestination repository\n, --upcoming\n\n\nSelects the repositories (osg-3.3, upcoming, etc.) to build packages for. Currently valid repositories are:\n\n\n\n\n\n\n\n\nRepository\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nosg\n\n\nOSG Software development repos for trunk (this is the default)\n\n\n\n\n\n\nosg-3.3 (or just 3.3)\n\n\nOSG Software development repos for 3.2 branch\n\n\n\n\n\n\nupcoming\n\n\nOSG Software development repos for upcoming branch\n\n\n\n\n\n\ninternal\n\n\nOSG Software internal branch\n\n\n\n\n\n\nhcc\n\n\nHolland Computing Center (Nebraska) testing repos\n\n\n\n\n\n\n\n\n--upcoming\n is an alias for \n--repo=upcoming\n\n\nNote that the repo selection affects which VCS paths you are allowed to build from. For example, you are not allowed to build from branches/osg-3.3 (from the OSG SVN) into the 'osg' repo, or from HCC's git repositories into the 'upcoming' repo.\n\n\nkoji-tag-diff\n\n\nThis script displays the differences between the latest packages in two koji tags.\n\n\nExample invocation: \nkoji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing\n\n\nThis prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa.\n\n\nosg-build-test\n\n\nThis script runs automated tests for \nosg-build\n. Only a few tests have been implemented so far.\n\n\nosg-import-srpm\n\n\nThis is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an \nupstream/*.source\n file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass \n--upstream-root=\nUPSTREAM DIR\n to put them somewhere else. If called with the \n--extract-spec\n or \n-e\n argument, it will extract the spec file from the SRPM and place it into the \nosg\n subdir in SVN. If called with the \n--diff-spec\n or \n-d\n argument, it will extract the spec file and compare it to the existing spec file in the \nosg\n subdir. \nThe script hasn't been touched in a while and needs a good deal of cleanup.\n A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications.\n\n\nosg-koji\n\n\nThis is a wrapper script around the \nkoji\n command line tool. It automatically specifies parameters to access the OSG's koji instance, and forces SSL authentication. It takes the same parameters as \nkoji\n and passes them on.\n\n\nAn additional command, \nosg-koji setup\n exists, which performs the following tasks:\n\n\n\n\nCreate a koji configuration in \n~/.osg-koji\n\n\nCreate a CA bundle for verifying the server.\n    Use either files in \n/etc/grid-security/certificates\n, or (if those are not found), from files downloaded from the DOEGrids and DigiCert sites.\n\n\nCreate a client cert file. This can be a symlink to your grid proxy, or it can be a file created from your grid public and private key files.\n    The location of those files can be specified by the \n--usercert\n and \n--userkey\n arguments.\n    If unspecified, \nusercert\n defaults to \n~/.globus/usercert.pem\n, and \nuserkey\n defaults to \n~/.globus/userkey.pem\n.\n\n\n\n\nosg-promote\n\n\nOverview\n\n\nRun this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines.\n\n\nOnce the packages are promoted, the script will generate code to cut and paste into a JIRA comment.\n\n\nSynopsis\n\n\n\n\nosg-promote [-r|--route \nROUTE\n]... [options] \nPACKAGE OR BUILD\n [...]\n\n\n\n\nExamples\n\n\n\n\n\n\nPromote the latest build of \nosg-version\n to testing for the current release series  \n\n\nosg-promote -r testing osg-version\n\n\n\n\n\n\n\n\n\nPromote the latest builds of \nosg-ce\n to testing for the 3.3 and 3.4 release series  \n\n\nosg-promote -r 3.3-testing -r 3.4-testing osg-ce\n\n\n\n\n\n\n\n\n\nPromote \nosg-build-1.5.0-1\n to testing for the current release series  \n\n\nosg-promote -r testing osg-build-1.5.0-1\n\n\n\n\n\n\n\n\n\nArguments\n\n\n-h\n\n\nDisplay help and a list of valid routes.\n\n\npackage or build\n\n\nA package (e.g. \nosg-version\n) or build (e.g. \nosg-version-3.3.0-1.osg33.el6\n) to promote. You may omit the dist tag (the \n.osg33.el6\n part).\n\n\nIf a package is specified, the most recent version of that package will be promoted.\n\n\nIf a build is specified, that build and the build that has the same \nversion\n-\nrelease\n for the other distro version(s) will be promoted. That is, if you specify the route \n3.3-testing\n and the build \nfoo-1-1\n, then \nfoo-1-1.osg33.el6\n and \nfoo-1-1.osg33.el7\n will be promoted.\n\n\nThis may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package \nmust\n match.\n\n\n-r \nROUTE\n, --route \nROUTE\n\n\nThe promotion route to use. Use \nosg-promote -h\n to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass \n-r 3.4-testing -r 3.3-testing\n.\n\n\nIf not specified, the \ntesting\n route is used, which corresponds to the testing route for the latest release series.\n\n\n-n, --dry-run\n\n\nDo not promote, just show what would be done.\n\n\n--el6-only / --el7-only\n\n\nOnly promote packages for el6 / el7.\n\n\n--no-el6 / --no-el7\n\n\nDo not promote packages for el6 / el7.\n\n\n--ignore-rejects\n\n\nIgnore rejections due to version mismatch between dvers or missing package for one dver.\n\n\n--regen\n\n\nRegenerate the destination repos after promoting.\n\n\n-y, --assume-yes\n\n\nDo not prompt before promotion.\n\n\nCommon Usage Patterns\n\n\nVerify that all files necessary to build the package are in the right place\n\n\nRun \nosg-build prebuild \nPACKAGEDIR\n.\n\n\nFetch and extract all source files for examination\n\n\nRun \nosg-build prebuild --full-extract \nPACKAGEDIR\n. Look inside the \n_upstream_tarball_contents\n directory.\n\n\nGet a post-patch version of the upstream sources for examination\n\n\nRun \nosg-build prepare \nPACKAGEDIR\n. Look inside the \n_build_results\n directory.\n\n\nSee which patches work with a new version of a package, update or remove them\n\n\n\n\nPlace the new source tarball into the upstream cache, edit the version in the spec file and *.sources files as necessary\n\n\nRun \nosg-build quilt \nPACKAGEDIR\n.\n\n\nEnter the extracted sources inside the \n_final_srpm_contents\n directory. You should see a file called \nseries\n and a symlink called \npatches\n.\n\n\nType \nquilt series\n to get a list of patches in order of application.\n\n\nType \nquilt push\n to apply the next patch.\n\n\nIf the patch applies cleanly, continue.\n\n\nIf the patch applies with some fuzz, type \nquilt refresh\n to update the offsets in the patch.\n\n\nIf the patch does not apply and you wish to remove it, type \nquilt delete \nPATCH NAME\n (delete only removes it from the series file, not the disk)\n\n\nIf the patch does not apply and you wish to fix it, either type \nquilt push -f\n to interactively apply the patch, or \nquilt delete \nPATCH NAME\n the patch and use \nquilt new\n / \nquilt edit\n / \nquilt refresh\n to edit files and make a new patch from your changes. Consult the \nquilt(1)\n manpage for more info.\n\n\n\n\n\n\nIf you have a new patch, run \nquilt import \nPATCHFILE\n to add the patch to the series file, and run \nquilt push\n to apply it.\n\n\nIf you have changes to make to the source code that you want to save as a patch, type \nquilt new \nPATCHNAME\n, edit the files, type \nquilt add \nFILE\n on each file you edited, then type \nquilt refresh\n to recreate the patch.\n\n\nOnce you're all done, copy the patches in the \npatches/\n directory to the \nosg/\n dir in SVN, run \nquilt series\n to get the application order and update the spec file accordingly.\n\n\n\n\nSee if a package builds successfully for OSG 3.4\n\n\n\n\nIf you have all the build dependencies of the package installed, run \nosg-build rpmbuild \nPACKAGEDIR\n. The resulting RPMs will be in the \n_build_results\n directory.\n\n\nIf you do not have all the build dependencies installed, or want to make sure you specified all of the necessary ones and the package builds from a clean environment, run \nosg-build mock --mock-config-from-koji osg-3.4-el6-build \nPACKAGEDIR\n. The resulting RPMs will be in the \n_build_results\n directory.\n\n\nIf you do not have mock installed, or want to exactly replicate the build environment in Koji, run \nosg-build koji --scratch \nPACKAGEDIR\n. You may download the resulting RPMs from kojiweb \nhttps://koji.chtc.wisc.edu/koji\n or pass \n--getfiles\n to \nosg-build koji\n and they will get downloaded to the \n_build_results\n directory.\n\n\n\n\nCheck for potential errors in a package\n\n\nRun \nosg-build lint \nPACKAGEDIR\n.\n\n\nCreate and test a final build of a package for all platforms for upcoming\n\n\n\n\nsvn commit\n your changes in \nbranches/upcoming\n.\n\n\nType \nosg-build koji --repo=upcoming \nPACKAGEDIR\n\n\nWait for the \nosg-upcoming-minefield\n repos to be regenerated containing the new version of your package. You can run \nosg-koji wait-repo osg-upcoming-el\nX\n-development --build=\nPACKAGENAME-VERSION-RELEASE\n and wait for that process to finish (substitute \n6\n or \n7\n for \nX\n). Or, you can just check kojiweb \nhttps://koji.chtc.wisc.edu/koji/tasks\n.\n\n\nOn your test machine, make sure the \nosg-upcoming-minefield\n repo is enabled (edit \n/etc/yum.repos.d/osg-upcoming-minefield.repo\n or \n/etc/yum.repos.d/osg-el6-upcoming-minefield.repo\n). Clean your cache (\nyum clean all; yum clean expire-cache\n).\n\n\nInstall your software, see if it works.\n\n\n\n\nPromote the latest build of a package to testing for the current OSG release series\n\n\nRun \nosg-promote -r testing \nPACKAGE\n\n\nPromote the latest build of a package to testing for the 3.3 and 3.4 release series\n\n\nRun \nosg-promote -r 3.3-testing -r 3.4-testing \nPACKAGE", 
            "title": "OSG Build Tools"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-build-tools", 
            "text": "This page documents the tools used for RPM development for the OSG Software Stack. See  the RPM development guide  for the principles on which these tools are based.  The tools are distributed in the  osg-build  RPM in our repositories, but can also be used from a Git clone of  opensciencegrid/osg-build on GitHub .  This page is up-to-date as of  osg-build  version 1.10.1.", 
            "title": "OSG Build Tools"
        }, 
        {
            "location": "/software/osg-build-tools/#the-tools", 
            "text": "", 
            "title": "The tools"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-build", 
            "text": "", 
            "title": "osg-build"
        }, 
        {
            "location": "/software/osg-build-tools/#overview", 
            "text": "This is the primary tool used in building source and binary RPMs.   osg-build  TASK  [options]  PACKAGE DIRECTORY  [...]   package_directory  is a directory containing an  osg/  and/or an  upstream/  subdirectory. See  the RPM development guide  for how these directories are organized.", 
            "title": "Overview"
        }, 
        {
            "location": "/software/osg-build-tools/#tasks", 
            "text": "", 
            "title": "Tasks"
        }, 
        {
            "location": "/software/osg-build-tools/#koji", 
            "text": "Prebuilds the final source package, then builds it remotely using the Koji instance hosted at UW-Madison.  https://koji.chtc.wisc.edu  By default, the resulting RPMs will end up in the osg-minefield repositories based on the most recent OSG major version (e.g. 3.4). You may specify a different set of repos with  --repo , described later. RPMs from the osg-minefield repositories are regularly pulled to the osg-development repositories hosted by the GOC at  http://repo.opensciencegrid.org  Unless you specify otherwise (by passing  --el6 ,  --el7  or specifying a different koji tag/target), the package will be built for both el6 and el7. This is the method used to build final versions of packages you expect to ship.", 
            "title": "koji"
        }, 
        {
            "location": "/software/osg-build-tools/#lint", 
            "text": "Prebuilds the final source package, then runs  rpmlint  on it to check for various problems. You will need to have  rpmlint  installed. People on UW CSL machines should add  /p/vdt/workspace/rpmlint  to their $PATH.", 
            "title": "lint"
        }, 
        {
            "location": "/software/osg-build-tools/#mock", 
            "text": "Prebuilds the final source package, then builds it locally using  mock , and stores the resulting source and binary RPMs in the package-specific  _build_results  directory.", 
            "title": "mock"
        }, 
        {
            "location": "/software/osg-build-tools/#prebuild", 
            "text": "Prebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the  _upstream_srpm_contents  and  _final_srpm_contents  directories.", 
            "title": "prebuild"
        }, 
        {
            "location": "/software/osg-build-tools/#prepare", 
            "text": "Prebuilds the final source package, then calls  rpmbuild -bp  on the result, extracting and patching the source files (and performing any other steps defined in the  %prep  section of the spec file. The resulting sources will be under  _final_srpm_contents .", 
            "title": "prepare"
        }, 
        {
            "location": "/software/osg-build-tools/#rpmbuild", 
            "text": "Prebuilds the final source package, then builds it locally using  rpmbuild , and stores the resulting source and binary RPMs in the package-specific  _build_results  directory.", 
            "title": "rpmbuild"
        }, 
        {
            "location": "/software/osg-build-tools/#quilt", 
            "text": "Collects the upstream local sources and spec file, then calls  quilt setup  on the spec file, extracting the source files and adding the patches to a quilt series file. See  Quilt documentation (PDF link)  for more information on quilt; also look at the example in the Usage Patterns section below. Similar to  prepare  (in fact,  quilt  calls  rpmbuild -bp  behind the scenes), but the source tree is in pre-patch state, and various quilt commands can be used to apply and modify patches. Unpacks into  _quilt  as of  osg-build-1.2.2  or  _final_srpm_contents  in previous versions. Requires  quilt . People on UW CSL machines should add  /p/vdt/workspace/quilt/bin  to their  $PATH , and  /p/vdt/workspace/quilt/share/man  to their  $MANPATH .", 
            "title": "quilt"
        }, 
        {
            "location": "/software/osg-build-tools/#options", 
            "text": "This section lists the command-line options.", 
            "title": "Options"
        }, 
        {
            "location": "/software/osg-build-tools/#-help", 
            "text": "Prints the built-in usage information and exits without doing anything else.", 
            "title": "--help"
        }, 
        {
            "location": "/software/osg-build-tools/#-version", 
            "text": "Prints the version of  osg-build  and exits without doing anything else.", 
            "title": "--version"
        }, 
        {
            "location": "/software/osg-build-tools/#common-options", 
            "text": "", 
            "title": "Common Options"
        }, 
        {
            "location": "/software/osg-build-tools/#-a-autoclean-no-autoclean", 
            "text": "Before each build, clean out the contents of the underscore directories (_build_results, _final_srpm_contents, _upstream_srpm_contents, _upstream_tarball_contents). If the directories are not cleaned up, earlier builds of a package may interfere with later ones.  --no-autoclean  will disable this.  Default is  true .  Has no effect with the  --vcs  flag.", 
            "title": "-a, --autoclean, --no-autoclean"
        }, 
        {
            "location": "/software/osg-build-tools/#-c-cache-prefix-prefix", 
            "text": "Sets the  prefix  for upstream source cache references. The prefix must be a valid URI starting with either  http ,  https , or  file , or one of the following special values:   AFS (corresponds to  file:///p/vdt/public/html/upstream , which is the location of the VDT cache using AFS from a UW CS machine).  VDT (corresponds to  http://vdt.cs.wisc.edu/upstream , which is the location of the VDT cache from off-site).  AUTO (AFS if available, VDT if not)   The upstream source cache must be organized as described above. All files referenced by  .source  files in the affected packages must exist in the cache, or a runtime error will occur.  Default is  AUTO .  Has no effect with the  --vcs  flag.", 
            "title": "-c, --cache-prefix prefix"
        }, 
        {
            "location": "/software/osg-build-tools/#-el6-el7-redhat-release-version-config-redhat95release", 
            "text": "Sets the distro version to build for. This affects the %dist tag, the mock config, and the default koji tag and target (unless otherwise specified).  --el6  is equivalent to  --redhat-release 6  --el7  is equivalent to  --redhat-release 7", 
            "title": "--el6, --el7, --redhat-release version (Config: redhat_release)"
        }, 
        {
            "location": "/software/osg-build-tools/#-loglevel-loglevel", 
            "text": "Sets the verbosity of the script. Valid values are:  debug ,  info ,  warning ,  error  and  critical .  Default is  info .", 
            "title": "--loglevel loglevel"
        }, 
        {
            "location": "/software/osg-build-tools/#-q-quiet", 
            "text": "Do not display as much information. Equivalent to  --loglevel warning", 
            "title": "-q, --quiet"
        }, 
        {
            "location": "/software/osg-build-tools/#-v-verbose", 
            "text": "Display more information. Equivalent to  --loglevel debug", 
            "title": "-v, --verbose"
        }, 
        {
            "location": "/software/osg-build-tools/#-w-working-directory-path", 
            "text": "Use  path  as the root directory of the files created by the script. For example, if  path  is  $HOME/working , and the package being built is  ndt , the following tree will be created:   $HOME/working/ndt/_upstream_srpm_contents  $HOME/working/ndt/_upstream_tarball_contents  $HOME/working/ndt/_final_srpm_contents  $HOME/working/ndt/_build_results   If  path  is  TEMP , a randomly named directory under  /tmp  is used as the working directory.  The default setting is to use the package directory as the working directory.  Has no effect with the  --vcs  flag.", 
            "title": "-w, --working-directory path"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-prebuild-task", 
            "text": "", 
            "title": "Options specific to prebuild task"
        }, 
        {
            "location": "/software/osg-build-tools/#-full-extract", 
            "text": "If set, all upstream tarballs will be extracted into  _upstream_tarball_contents/  during the prebuild step. This flag is now mostly redundant with the  prepare  and  quilt  tasks.", 
            "title": "--full-extract"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-rpmbuild-and-mock-tasks", 
            "text": "", 
            "title": "Options specific to rpmbuild and mock tasks"
        }, 
        {
            "location": "/software/osg-build-tools/#-distro-tag-dist", 
            "text": "Sets the distribution tag added on to the end of the release in the RPM (  rpmbuild  and  mock  tasks only ).  Default is  .osg.el6  or  .osg.el7", 
            "title": "--distro-tag dist"
        }, 
        {
            "location": "/software/osg-build-tools/#-t-target-arch-arch", 
            "text": "Specify an architecture to build packages for (  rpmbuild  and  mock  tasks only ).  Default is unspecified, which builds for the current machine architecture.", 
            "title": "-t, --target-arch arch"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-mock-task", 
            "text": "", 
            "title": "Options specific to mock task"
        }, 
        {
            "location": "/software/osg-build-tools/#-mock-clean-no-mock-clean", 
            "text": "Enable/disable deletion of the mock buildroot after a successful build.  Default is  true .", 
            "title": "--mock-clean, --no-mock-clean"
        }, 
        {
            "location": "/software/osg-build-tools/#-m-mock-config-path", 
            "text": "Specifies the  mock  configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag  %dist ).  See also  --mock-config-from-koji .", 
            "title": "-m, --mock-config path"
        }, 
        {
            "location": "/software/osg-build-tools/#-mock-config-from-koji-build-tag", 
            "text": "Creates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is:  osg-3.4-el6-build  Also requires the Koji command-line tools (package  koji ), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely:  --koji-backend ,  --koji-login , and  --koji-wrapper .", 
            "title": "--mock-config-from-koji build tag"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-koji-task", 
            "text": "", 
            "title": "Options specific to koji task"
        }, 
        {
            "location": "/software/osg-build-tools/#-dry-run", 
            "text": "Do not actually run koji, merely show the command(s) that will be run. For debugging purposes.", 
            "title": "--dry-run"
        }, 
        {
            "location": "/software/osg-build-tools/#-getfiles-get-files", 
            "text": "For scratch builds without  --vcs  only. Download the resulting RPMs and logs from the build into the  _build_results  directory.", 
            "title": "--getfiles, --get-files"
        }, 
        {
            "location": "/software/osg-build-tools/#-k-kojilogin-koji-login-login", 
            "text": "Sets the login to use for the koji task. This should most likely be your CN. If not specified, will extract it from your client cert ( ~/.osg-koji/client.crt  or  ~/.koji/client.crt ).", 
            "title": "-k, --kojilogin, --koji-login login"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-target-target", 
            "text": "The koji target to use for building.  Default is  osg-el6  for el6 and  osg-el7  for el7.", 
            "title": "--koji-target target"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-tag-tag", 
            "text": "The koji tag to add packages to. See the  Koji Workflow guide  for more information on the terminology. The special value  TARGET  uses the destination tag defined in the koji target.  Default is  osg-el6  or  osg-el7 .", 
            "title": "--koji-tag tag"
        }, 
        {
            "location": "/software/osg-build-tools/#-ktt-koji-tag-and-target-arg", 
            "text": "Shorthand for setting both --koji-tag and --koji-target to  arg .", 
            "title": "--ktt, --koji-tag-and-target arg"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-wrapper-no-koji-wrapper", 
            "text": "Enable/disable use of the  osg-koji  wrapper script around koji. See below for a description of  osg-koji .  Default is  true .", 
            "title": "--koji-wrapper, --no-koji-wrapper"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-backend-backend", 
            "text": "Specifies the method osg-build will use to interface with Koji. This can be  shell  or  kojilib .", 
            "title": "--koji-backend backend"
        }, 
        {
            "location": "/software/osg-build-tools/#-wait-no-wait-nowait", 
            "text": "Wait for koji tasks to finish. Bad for running multiple builds in a single command, since you will have to type in your passphrase for the first one, wait for it to complete, then type in your passphrase for the second one, wait for it to complete, etc. If you want to wait for multiple tasks to finish, use the  koji watch-task  command or look at the website  https://koji.chtc.wisc.edu .  --wait  used to be the default until  osg-build-1.1.3", 
            "title": "--wait, --no-wait, --nowait"
        }, 
        {
            "location": "/software/osg-build-tools/#-regen-repos", 
            "text": "Start a  regen-repo  koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with  --no-wait , since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete.", 
            "title": "--regen-repos"
        }, 
        {
            "location": "/software/osg-build-tools/#-scratch-no-scratch", 
            "text": "Perform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient.", 
            "title": "--scratch, --no-scratch"
        }, 
        {
            "location": "/software/osg-build-tools/#-vcs-no-vcs-svn-no-svn", 
            "text": "Have Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. Currently, SVN and Git are supported. If this flag is specified, you may use SVN URLs or URL@Revision pairs to specify the packages to build. You may continue specify package directories from an SVN checkout, in which case osg-build will use  svn info  to find the right URL@Revision pair to use and warn you about uncommitted changes. osg-build will also warn you about an outdated working directory.  --vcs  defaults to  true  for non-scratch builds, and  false  for scratch builds.", 
            "title": "--vcs, --no-vcs, --svn, --no-svn"
        }, 
        {
            "location": "/software/osg-build-tools/#-repodestination-repository-upcoming", 
            "text": "Selects the repositories (osg-3.3, upcoming, etc.) to build packages for. Currently valid repositories are:     Repository  Description      osg  OSG Software development repos for trunk (this is the default)    osg-3.3 (or just 3.3)  OSG Software development repos for 3.2 branch    upcoming  OSG Software development repos for upcoming branch    internal  OSG Software internal branch    hcc  Holland Computing Center (Nebraska) testing repos     --upcoming  is an alias for  --repo=upcoming  Note that the repo selection affects which VCS paths you are allowed to build from. For example, you are not allowed to build from branches/osg-3.3 (from the OSG SVN) into the 'osg' repo, or from HCC's git repositories into the 'upcoming' repo.", 
            "title": "--repo=destination repository, --upcoming"
        }, 
        {
            "location": "/software/osg-build-tools/#koji-tag-diff", 
            "text": "This script displays the differences between the latest packages in two koji tags.  Example invocation:  koji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing  This prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa.", 
            "title": "koji-tag-diff"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-build-test", 
            "text": "This script runs automated tests for  osg-build . Only a few tests have been implemented so far.", 
            "title": "osg-build-test"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-import-srpm", 
            "text": "This is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an  upstream/*.source  file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass  --upstream-root= UPSTREAM DIR  to put them somewhere else. If called with the  --extract-spec  or  -e  argument, it will extract the spec file from the SRPM and place it into the  osg  subdir in SVN. If called with the  --diff-spec  or  -d  argument, it will extract the spec file and compare it to the existing spec file in the  osg  subdir.  The script hasn't been touched in a while and needs a good deal of cleanup.  A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications.", 
            "title": "osg-import-srpm"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-koji", 
            "text": "This is a wrapper script around the  koji  command line tool. It automatically specifies parameters to access the OSG's koji instance, and forces SSL authentication. It takes the same parameters as  koji  and passes them on.  An additional command,  osg-koji setup  exists, which performs the following tasks:   Create a koji configuration in  ~/.osg-koji  Create a CA bundle for verifying the server.\n    Use either files in  /etc/grid-security/certificates , or (if those are not found), from files downloaded from the DOEGrids and DigiCert sites.  Create a client cert file. This can be a symlink to your grid proxy, or it can be a file created from your grid public and private key files.\n    The location of those files can be specified by the  --usercert  and  --userkey  arguments.\n    If unspecified,  usercert  defaults to  ~/.globus/usercert.pem , and  userkey  defaults to  ~/.globus/userkey.pem .", 
            "title": "osg-koji"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-promote", 
            "text": "", 
            "title": "osg-promote"
        }, 
        {
            "location": "/software/osg-build-tools/#overview_1", 
            "text": "Run this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines.  Once the packages are promoted, the script will generate code to cut and paste into a JIRA comment.", 
            "title": "Overview"
        }, 
        {
            "location": "/software/osg-build-tools/#synopsis", 
            "text": "osg-promote [-r|--route  ROUTE ]... [options]  PACKAGE OR BUILD  [...]", 
            "title": "Synopsis"
        }, 
        {
            "location": "/software/osg-build-tools/#examples", 
            "text": "Promote the latest build of  osg-version  to testing for the current release series    osg-promote -r testing osg-version    Promote the latest builds of  osg-ce  to testing for the 3.3 and 3.4 release series    osg-promote -r 3.3-testing -r 3.4-testing osg-ce    Promote  osg-build-1.5.0-1  to testing for the current release series    osg-promote -r testing osg-build-1.5.0-1", 
            "title": "Examples"
        }, 
        {
            "location": "/software/osg-build-tools/#arguments", 
            "text": "", 
            "title": "Arguments"
        }, 
        {
            "location": "/software/osg-build-tools/#-h", 
            "text": "Display help and a list of valid routes.", 
            "title": "-h"
        }, 
        {
            "location": "/software/osg-build-tools/#package-or-build", 
            "text": "A package (e.g.  osg-version ) or build (e.g.  osg-version-3.3.0-1.osg33.el6 ) to promote. You may omit the dist tag (the  .osg33.el6  part).  If a package is specified, the most recent version of that package will be promoted.  If a build is specified, that build and the build that has the same  version - release  for the other distro version(s) will be promoted. That is, if you specify the route  3.3-testing  and the build  foo-1-1 , then  foo-1-1.osg33.el6  and  foo-1-1.osg33.el7  will be promoted.  This may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package  must  match.", 
            "title": "package or build"
        }, 
        {
            "location": "/software/osg-build-tools/#-r-route-route-route", 
            "text": "The promotion route to use. Use  osg-promote -h  to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass  -r 3.4-testing -r 3.3-testing .  If not specified, the  testing  route is used, which corresponds to the testing route for the latest release series.", 
            "title": "-r ROUTE, --route ROUTE"
        }, 
        {
            "location": "/software/osg-build-tools/#-n-dry-run", 
            "text": "Do not promote, just show what would be done.", 
            "title": "-n, --dry-run"
        }, 
        {
            "location": "/software/osg-build-tools/#-el6-only-el7-only", 
            "text": "Only promote packages for el6 / el7.", 
            "title": "--el6-only / --el7-only"
        }, 
        {
            "location": "/software/osg-build-tools/#-no-el6-no-el7", 
            "text": "Do not promote packages for el6 / el7.", 
            "title": "--no-el6 / --no-el7"
        }, 
        {
            "location": "/software/osg-build-tools/#-ignore-rejects", 
            "text": "Ignore rejections due to version mismatch between dvers or missing package for one dver.", 
            "title": "--ignore-rejects"
        }, 
        {
            "location": "/software/osg-build-tools/#-regen", 
            "text": "Regenerate the destination repos after promoting.", 
            "title": "--regen"
        }, 
        {
            "location": "/software/osg-build-tools/#-y-assume-yes", 
            "text": "Do not prompt before promotion.", 
            "title": "-y, --assume-yes"
        }, 
        {
            "location": "/software/osg-build-tools/#common-usage-patterns", 
            "text": "", 
            "title": "Common Usage Patterns"
        }, 
        {
            "location": "/software/osg-build-tools/#verify-that-all-files-necessary-to-build-the-package-are-in-the-right-place", 
            "text": "Run  osg-build prebuild  PACKAGEDIR .", 
            "title": "Verify that all files necessary to build the package are in the right place"
        }, 
        {
            "location": "/software/osg-build-tools/#fetch-and-extract-all-source-files-for-examination", 
            "text": "Run  osg-build prebuild --full-extract  PACKAGEDIR . Look inside the  _upstream_tarball_contents  directory.", 
            "title": "Fetch and extract all source files for examination"
        }, 
        {
            "location": "/software/osg-build-tools/#get-a-post-patch-version-of-the-upstream-sources-for-examination", 
            "text": "Run  osg-build prepare  PACKAGEDIR . Look inside the  _build_results  directory.", 
            "title": "Get a post-patch version of the upstream sources for examination"
        }, 
        {
            "location": "/software/osg-build-tools/#see-which-patches-work-with-a-new-version-of-a-package-update-or-remove-them", 
            "text": "Place the new source tarball into the upstream cache, edit the version in the spec file and *.sources files as necessary  Run  osg-build quilt  PACKAGEDIR .  Enter the extracted sources inside the  _final_srpm_contents  directory. You should see a file called  series  and a symlink called  patches .  Type  quilt series  to get a list of patches in order of application.  Type  quilt push  to apply the next patch.  If the patch applies cleanly, continue.  If the patch applies with some fuzz, type  quilt refresh  to update the offsets in the patch.  If the patch does not apply and you wish to remove it, type  quilt delete  PATCH NAME  (delete only removes it from the series file, not the disk)  If the patch does not apply and you wish to fix it, either type  quilt push -f  to interactively apply the patch, or  quilt delete  PATCH NAME  the patch and use  quilt new  /  quilt edit  /  quilt refresh  to edit files and make a new patch from your changes. Consult the  quilt(1)  manpage for more info.    If you have a new patch, run  quilt import  PATCHFILE  to add the patch to the series file, and run  quilt push  to apply it.  If you have changes to make to the source code that you want to save as a patch, type  quilt new  PATCHNAME , edit the files, type  quilt add  FILE  on each file you edited, then type  quilt refresh  to recreate the patch.  Once you're all done, copy the patches in the  patches/  directory to the  osg/  dir in SVN, run  quilt series  to get the application order and update the spec file accordingly.", 
            "title": "See which patches work with a new version of a package, update or remove them"
        }, 
        {
            "location": "/software/osg-build-tools/#see-if-a-package-builds-successfully-for-osg-34", 
            "text": "If you have all the build dependencies of the package installed, run  osg-build rpmbuild  PACKAGEDIR . The resulting RPMs will be in the  _build_results  directory.  If you do not have all the build dependencies installed, or want to make sure you specified all of the necessary ones and the package builds from a clean environment, run  osg-build mock --mock-config-from-koji osg-3.4-el6-build  PACKAGEDIR . The resulting RPMs will be in the  _build_results  directory.  If you do not have mock installed, or want to exactly replicate the build environment in Koji, run  osg-build koji --scratch  PACKAGEDIR . You may download the resulting RPMs from kojiweb  https://koji.chtc.wisc.edu/koji  or pass  --getfiles  to  osg-build koji  and they will get downloaded to the  _build_results  directory.", 
            "title": "See if a package builds successfully for OSG 3.4"
        }, 
        {
            "location": "/software/osg-build-tools/#check-for-potential-errors-in-a-package", 
            "text": "Run  osg-build lint  PACKAGEDIR .", 
            "title": "Check for potential errors in a package"
        }, 
        {
            "location": "/software/osg-build-tools/#create-and-test-a-final-build-of-a-package-for-all-platforms-for-upcoming", 
            "text": "svn commit  your changes in  branches/upcoming .  Type  osg-build koji --repo=upcoming  PACKAGEDIR  Wait for the  osg-upcoming-minefield  repos to be regenerated containing the new version of your package. You can run  osg-koji wait-repo osg-upcoming-el X -development --build= PACKAGENAME-VERSION-RELEASE  and wait for that process to finish (substitute  6  or  7  for  X ). Or, you can just check kojiweb  https://koji.chtc.wisc.edu/koji/tasks .  On your test machine, make sure the  osg-upcoming-minefield  repo is enabled (edit  /etc/yum.repos.d/osg-upcoming-minefield.repo  or  /etc/yum.repos.d/osg-el6-upcoming-minefield.repo ). Clean your cache ( yum clean all; yum clean expire-cache ).  Install your software, see if it works.", 
            "title": "Create and test a final build of a package for all platforms for upcoming"
        }, 
        {
            "location": "/software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-current-osg-release-series", 
            "text": "Run  osg-promote -r testing  PACKAGE", 
            "title": "Promote the latest build of a package to testing for the current OSG release series"
        }, 
        {
            "location": "/software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-33-and-34-release-series", 
            "text": "Run  osg-promote -r 3.3-testing -r 3.4-testing  PACKAGE", 
            "title": "Promote the latest build of a package to testing for the 3.3 and 3.4 release series"
        }, 
        {
            "location": "/software/quilt/", 
            "text": "How to Write a Patch\n\n\nYou create one or more \n.patch\n files with diff and stick them in the osg directory. Then you declare the patch files in the header of the spec file with a line like \nPatch0: py24compat.patch\n and in the \n%prep\n section, just after\n%setup\n, you add a \n%patch\n line to actually apply the patch, like this: \n%patch0 -p1\n (where the \n-p1\n indicates that it should strip off the first leading component of the path in each file mentioned in the .patch file) Look at the mash package for an example.\n\n\nThe easiest way to actually create the patch in the first place is to use a utility called quilt. First you run \nosg-build quilt\n on the package directory and it will create a \n_quilt\n subdirectory that has the expanded sources and patches.\n\n\n\n\ncd into \n_quilt/pegasus-source-2.3.0\n, then run \nquilt push -a\n to apply any patches that already exist (there are none for pegasus but there might be for other packages).\n\n\nrun \nquilt new py24compat.patch\n to name your new patch file.\n\n\nrun \nquilt add \nfilename\n for each file you want to make changes to (you \nmust\n run this before making any changes).\n\n\nactually make the changes.\n\n\nrun \nquilt refresh -p1\n to have quilt add those changes into the .patch file. (The -p1 option to quilt refresh must be the same as the -p1 option to %patch0 in your spec file).\n\n\ncopy \npatches/py24compat.patch\n into the \npegasus/osg\n directory and edit the spec file as above.\n\n\n\n\nDon't forget to \ngit add\n your new patch file before committing. Once you've tested your patch successfully, you should make that bug report and send them the patch. A bug report is looked on more favorably if it includes a patch to fix the problem.", 
            "title": "Using Quilt"
        }, 
        {
            "location": "/software/quilt/#how-to-write-a-patch", 
            "text": "You create one or more  .patch  files with diff and stick them in the osg directory. Then you declare the patch files in the header of the spec file with a line like  Patch0: py24compat.patch  and in the  %prep  section, just after %setup , you add a  %patch  line to actually apply the patch, like this:  %patch0 -p1  (where the  -p1  indicates that it should strip off the first leading component of the path in each file mentioned in the .patch file) Look at the mash package for an example.  The easiest way to actually create the patch in the first place is to use a utility called quilt. First you run  osg-build quilt  on the package directory and it will create a  _quilt  subdirectory that has the expanded sources and patches.   cd into  _quilt/pegasus-source-2.3.0 , then run  quilt push -a  to apply any patches that already exist (there are none for pegasus but there might be for other packages).  run  quilt new py24compat.patch  to name your new patch file.  run  quilt add  filename  for each file you want to make changes to (you  must  run this before making any changes).  actually make the changes.  run  quilt refresh -p1  to have quilt add those changes into the .patch file. (The -p1 option to quilt refresh must be the same as the -p1 option to %patch0 in your spec file).  copy  patches/py24compat.patch  into the  pegasus/osg  directory and edit the spec file as above.   Don't forget to  git add  your new patch file before committing. Once you've tested your patch successfully, you should make that bug report and send them the patch. A bug report is looked on more favorably if it includes a patch to fix the problem.", 
            "title": "How to Write a Patch"
        }, 
        {
            "location": "/software/upcoming-to-main/", 
            "text": "Promoting Packages from Upcoming to Main\n\n\nSometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move:\n\n\nIf needed, move the software from upcoming to trunk and release using the usual process:\n\n\n\n\nMerge changes to the package in SVN from branches/upcoming to trunk.\n\n\nBuild the package from trunk.\n\n\nFollow the normal process to prepare a build for release (including development testing, promotion, etc.).\n\n\n\n\nOn release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos:\n\n\n\n\nUntag from \nall upcoming repos\n the version of the package corresponding to the version that was released in main. (Do \nNOT\n untag from the osg-upcoming-elN-release-X.Y.Z tags)\n\n\n\n\nAlso, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos.\n\n\nIf you do not have the privileges to untag from upcoming-release, someone on the Release Team can help.\n\n\n(These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos).\n\n\n\n\nUnless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.", 
            "title": "Upcoming to Main"
        }, 
        {
            "location": "/software/upcoming-to-main/#promoting-packages-from-upcoming-to-main", 
            "text": "Sometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move:  If needed, move the software from upcoming to trunk and release using the usual process:   Merge changes to the package in SVN from branches/upcoming to trunk.  Build the package from trunk.  Follow the normal process to prepare a build for release (including development testing, promotion, etc.).   On release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos:   Untag from  all upcoming repos  the version of the package corresponding to the version that was released in main. (Do  NOT  untag from the osg-upcoming-elN-release-X.Y.Z tags)   Also, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos.  If you do not have the privileges to untag from upcoming-release, someone on the Release Team can help.  (These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos).   Unless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.", 
            "title": "Promoting Packages from Upcoming to Main"
        }, 
        {
            "location": "/software/koji-workflow/", 
            "text": "Koji Workflow\n\n\nThis covers the basics of using and understanding the \nOSG Koji\n instance. It is meant primarily for OSG Software team members who need to interact with the service.\n\n\nTerminology\n\n\nUsing and understanding the following terminology correctly will help in the reading of this document:\n\n\nPackage\n\nThis refers to a named piece of software in the Koji database. An example would be \"lcmaps\".\n\n\nBuild\n\nA specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package.\n\n\nTag\n\nA named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-3.3-el6-development\" tag, which contains (among others) the \"lcmaps\" package and the \"lcmaps-1.6.6-1.1.osg33.el6\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include CentOS base, EPEL, or JPackage.\n\n\n\n\nNote\n\n\nA tag is NOT a yum repository.\n\n\n\n\nTarget\n\nA target consists of a build tag and a destination tag. An example is \"osg-3.3-el6\", where the build tag is \"osg-3.3-el6-build\" and the destination tag is \"osg-3.3-el6\". A target is used by the build task to know what repository to build from and tag to build into.\n\n\nTask\n\nA unit of work for Koji. Several common tasks are:\n\n\n\n\n\n\nbuild\n\n    This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask.\n\n\n\n\nNote\n\n\nIf the build task is marked as \"scratch\", then it won't result in a saved Build.\n\n\n\n\n\n\n\n\nbuildArch\n\n    This task takes a SRPM, architecture name, and a Koji repository as an input, and runs \nmock\n to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful.\n\n\n\n\n\n\ntagBuild\n\n    This adds a successful build to a given tag.\n\n\n\n\n\n\nnewRepo\n\n    This creates a new repository from a given tag.\n\n\n\n\n\n\nBuild artifacts\n\nThe results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs.\n\n\nRepository\n\nA yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag.\n\n\nUsing Koji\n\n\nRequired Software\n\n\nUsing Koji requires:\n\n\n\n\nosg-build\n version 1.6.3 or later.\n\n\nkoji\n 1.6.0-2.osg or later. \nNote\n that you want a koji build from osg; the output of \nrpm -q koji\n should end in \".osg\".\n\n\n\n\nBoth pieces of software are available from the osg repositories. \nosg-build\n may also be obtained from GitHub by cloning out \nhttps://github.com/opensciencegrid/osg-build\n\n\nSpecial instructions for UW-Madison CSL machines:\n\n\n\n\n\n\nClone the osg-build GitHub repo:\n\n\n[you@host]$\n git clone https://github.com/opensciencegrid/osg-build\n\n\n\n\n\n\n\n\n\nAdd this directory to your \n$PATH\n\n\n\n\n\n\nRun\n\n\n[you@host]$\n osg-koji setup\n\n\n\n\n\nto set up the koji configuration and certificates in \n~/.osg-koji\n\n\n\n\n\n\nObtaining a login\n\n\nYou will be using your grid certificate to log in. Email a Koji admin the DN of your certificate, and we will set up a Koji account with the appropriate permissions.\n\n\nIf you are switching certificate providers, you will need to email a Koji admin with your new DN. You will also need to clear your browser cookies and cache for \nhttps://koji.chtc.wisc.edu\n before trying to use the Koji web interface again. If your CN has changed, you will not be able to use your old certificate.\n\n\nCurrent Koji admins are Mat Selmeci and Carl Edquist.\n\n\nConfiguring certificate authentication\n\n\nYou must also configure certificate authentication for the command-line tools on your build host:\n\n\n\n\n\n\nRun\n\n\n[you@host]$\n osg-koji setup\n\n\n\n\n\nto set up the appropriate configuration and certificates in \n~/.osg-koji\n\n\n\n\n\n\nAfter this, you will also be able to run koji commands manually by using the \nosg-koji\n wrapper script. You might need to rerun \nosg-koji setup\n if you renew or change your cert.\n\n\nCreating a new build\n\n\nWe create a new build in Koji from the package's directory in OSG Software subversion.\n\n\nIf a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You \ncan\n do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji.\n\n\nTo do a build, execute the following command from within the OSG Software subversion checkout:\n\n\n[you@host]$\n osg-build koji \nPACKAGE NAME\n\n\n\n\n\n\nTo do a scratch build, simply add the \n--scratch\n command line flag.\n\n\nEach invocation of osg-build will ask for the password once or twice; if you get asked more like 20 times, then you may not be running the OSG-patched version of Koji; try switching to the one from the osg-development repository.\n\n\nWhen you do a non-scratch build, it will build with the \nosg-el6\n and \nosg-el7\n targets. This will assign your build the \nosg-3.4-el6-development\n and \nosg-3.4-el7-development\n tags (and your package will be assigned the \nosg-el6\n and \nosg-el7\n tags). If successful, your build will end up in the Koji \nosg-minefield\n yum repos and will eventually show up in the \nosg-development\n yum repos. This is a high latency process.\n\n\nBuild task Results\n\n\nHow to find build results\n\n\nThe most recent build results are always shown on the home page of Koji:\n\n\nhttps://koji.chtc.wisc.edu/koji/index\n\n\nClicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM.\n\n\nIf your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a wildcard), and it will bring up a list of all builds for that package. You can find your build from there. For example, the \"lcmaps\" package page is here:\n\n\nhttps://koji.chtc.wisc.edu/koji/packageinfo?packageID=56\n\n\nAnd the lcmaps-1.6.6-1.1.osg33.el6 build is here:\n\n\nhttps://koji.chtc.wisc.edu/koji/buildinfo?buildID=7427\n\n\nTrying our your build\n\n\nBecause it takes a while for your build to get into one of the regular repositories, it's simplest to download your RPM directly (see the previous section on How to find build results), and install it with:\n\n\n[root@host]#\n yum localinstall \nRPM\n\n\n\n\n\n\nHow to get the resulting RPM into a repository\n\n\nOnce a package has been built, it is added to a tag. We then must turn the tag into a yum repository. This is normally done automatically and you do not need to deal with it yourself. Three notes:\n\n\n\n\nThe kojira daemon creates a repository automatically post-build on the koji-hub host. Eventually, the development repository will be the one hosted by koji-hub.\n\n\n\n\nThe koji-hub repository can be created manually by running\n\n\n[you@host]$\n osg-koji regen-repo \nTAG NAME\n\n\n\n\n\n\nFor example, the tag name for osg-development in 3.4 on el6 is \"osg-3.4-el6-development\". Likely, you won't need to do this when kojira is working.\n-   Repositories are created on external hosts with the \nmash\n tool. These are usually triggered by cron jobs, but may be run by hand too. Documentation for running mash is on the TODO list.\n-   You can create your own personal repository using \nmash\n.\n\n\n\n\n\n\nDebugging build issues\n\n\n\n\n\n\nFailed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include:\n\n\n\n\n\n\nroot.log\n\n    This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here.\n\n\n\n\n\n\nbuild.log\n\n    The output of the rpmbuild executable. If your package fails to compile, the reason will show up here.\n\n\n\n\n\n\n\n\n\n\nOne input to the buildArch task is a repository, which is based on a Koji tag. If the repository hasn't been recreated for a dependency you need (for example, when kojira isn't working), you may not have the right RPMs available in your build root.\n\n\n\n\n\n\nOne common issue is building a chain of dependencies. For example, suppose build B depends on the results of build A. If you build A then build B immediately, B will likely fail. This is because A is not in the repository that B uses. The proper string of events building A, starting the regeneration of the destination and build repo (which should happen in a few minutes of the build A task completing), then submitting build task B.\n\n\n\n\nNote\n\n\nif you submit build task B while the build repository task is open, it will not start until the build task has finished.\n\n\n\n\n\n\n\n\n\n\n\n\nOther errors\n\n\n\n\npackage \nPACKAGE NAME\n not in list for tag \nTAG\n\n    This happens when the name of the directory your package is in does not match the name of the package.\n    You must rename one or the other and commit your changes before trying again.\n\n\n\n\n\n\n\n\nPromoting Builds from Development -\n Testing\n\n\nSoftware contributors can promote any package to testing. Members of the security team can promote ca-cert packages to testing.\n\n\nTo promote from development to testing:\n\n\nUsing \nosg-promote\n\n\nIf you want to promote the latest version:\n\n\n[you@host]$\n osg-promote -r \nOSGVER\n-testing \nPACKAGE NAME\n\n\n\n\n\n\nPACKAGE NAME\n is the bare package name without version, e.g. \ngratia-probe\n.\n\n\nIf you want to promote a specific version:\n\n\n[you@host]$\n osg-promote -r \nOSGVER\n-testing \nBUILD NAME\n\n\n\n\n\n\nBUILD NAME\n is a full \nname-version-revision.disttag\n such as \ngratia-probe-1.17.0-2.osg33.el6\n.\n\n\nOSGVER\n is the OSG major version that you are promoting for (e.g. \n3.4\n).\n\n\nosg-promote\n will promote both the el6 and el7 builds of a package. After promoting, copy and paste the JIRA code \nosg-promote\n produces into the JIRA ticket that you are working on.\n\n\nFor \nosg-promote\n, you may omit the \n.osg34.el6\n or \n.osg34.el7\n; the script will add the appropriate disttag on.\n\n\nSee \nOSG Building Tools\n for full details on \nosg-promote\n.\n\n\nCreating custom koji areas\n\n\nOccasionally you may want to make builds of a package (or packages) which you\ndo not yet want to go into the main development repos.  In this case, you can\ncreate a set of custom koji tags and build targets for these builds.  We have\na script in our\n\nosg-next-tools\n repo\ncalled\n\nnew-koji-area\n\nthat facilitates this set up.\n\n\nFurther reading\n\n\n\n\nOfficial Koji documentation: \nhttps://docs.pagure.org/koji/\n\n\nFedora's koji documentation: \nhttps://fedoraproject.org/wiki/Koji\n\n\nFedora's \"Using Koji\" page: \nhttps://fedoraproject.org/wiki/Using_the_Koji_build_system\n Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.", 
            "title": "Koji Workflow"
        }, 
        {
            "location": "/software/koji-workflow/#koji-workflow", 
            "text": "This covers the basics of using and understanding the  OSG Koji  instance. It is meant primarily for OSG Software team members who need to interact with the service.", 
            "title": "Koji Workflow"
        }, 
        {
            "location": "/software/koji-workflow/#terminology", 
            "text": "Using and understanding the following terminology correctly will help in the reading of this document:  Package \nThis refers to a named piece of software in the Koji database. An example would be \"lcmaps\".  Build \nA specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package.  Tag \nA named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-3.3-el6-development\" tag, which contains (among others) the \"lcmaps\" package and the \"lcmaps-1.6.6-1.1.osg33.el6\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include CentOS base, EPEL, or JPackage.   Note  A tag is NOT a yum repository.   Target \nA target consists of a build tag and a destination tag. An example is \"osg-3.3-el6\", where the build tag is \"osg-3.3-el6-build\" and the destination tag is \"osg-3.3-el6\". A target is used by the build task to know what repository to build from and tag to build into.  Task \nA unit of work for Koji. Several common tasks are:    build \n    This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask.   Note  If the build task is marked as \"scratch\", then it won't result in a saved Build.     buildArch \n    This task takes a SRPM, architecture name, and a Koji repository as an input, and runs  mock  to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful.    tagBuild \n    This adds a successful build to a given tag.    newRepo \n    This creates a new repository from a given tag.    Build artifacts \nThe results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs.  Repository \nA yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag.", 
            "title": "Terminology"
        }, 
        {
            "location": "/software/koji-workflow/#using-koji", 
            "text": "", 
            "title": "Using Koji"
        }, 
        {
            "location": "/software/koji-workflow/#required-software", 
            "text": "Using Koji requires:   osg-build  version 1.6.3 or later.  koji  1.6.0-2.osg or later.  Note  that you want a koji build from osg; the output of  rpm -q koji  should end in \".osg\".   Both pieces of software are available from the osg repositories.  osg-build  may also be obtained from GitHub by cloning out  https://github.com/opensciencegrid/osg-build", 
            "title": "Required Software"
        }, 
        {
            "location": "/software/koji-workflow/#special-instructions-for-uw-madison-csl-machines", 
            "text": "Clone the osg-build GitHub repo:  [you@host]$  git clone https://github.com/opensciencegrid/osg-build    Add this directory to your  $PATH    Run  [you@host]$  osg-koji setup  to set up the koji configuration and certificates in  ~/.osg-koji", 
            "title": "Special instructions for UW-Madison CSL machines:"
        }, 
        {
            "location": "/software/koji-workflow/#obtaining-a-login", 
            "text": "You will be using your grid certificate to log in. Email a Koji admin the DN of your certificate, and we will set up a Koji account with the appropriate permissions.  If you are switching certificate providers, you will need to email a Koji admin with your new DN. You will also need to clear your browser cookies and cache for  https://koji.chtc.wisc.edu  before trying to use the Koji web interface again. If your CN has changed, you will not be able to use your old certificate.  Current Koji admins are Mat Selmeci and Carl Edquist.", 
            "title": "Obtaining a login"
        }, 
        {
            "location": "/software/koji-workflow/#configuring-certificate-authentication", 
            "text": "You must also configure certificate authentication for the command-line tools on your build host:    Run  [you@host]$  osg-koji setup  to set up the appropriate configuration and certificates in  ~/.osg-koji    After this, you will also be able to run koji commands manually by using the  osg-koji  wrapper script. You might need to rerun  osg-koji setup  if you renew or change your cert.", 
            "title": "Configuring certificate authentication"
        }, 
        {
            "location": "/software/koji-workflow/#creating-a-new-build", 
            "text": "We create a new build in Koji from the package's directory in OSG Software subversion.  If a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You  can  do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji.  To do a build, execute the following command from within the OSG Software subversion checkout:  [you@host]$  osg-build koji  PACKAGE NAME   To do a scratch build, simply add the  --scratch  command line flag.  Each invocation of osg-build will ask for the password once or twice; if you get asked more like 20 times, then you may not be running the OSG-patched version of Koji; try switching to the one from the osg-development repository.  When you do a non-scratch build, it will build with the  osg-el6  and  osg-el7  targets. This will assign your build the  osg-3.4-el6-development  and  osg-3.4-el7-development  tags (and your package will be assigned the  osg-el6  and  osg-el7  tags). If successful, your build will end up in the Koji  osg-minefield  yum repos and will eventually show up in the  osg-development  yum repos. This is a high latency process.", 
            "title": "Creating a new build"
        }, 
        {
            "location": "/software/koji-workflow/#build-task-results", 
            "text": "", 
            "title": "Build task Results"
        }, 
        {
            "location": "/software/koji-workflow/#how-to-find-build-results", 
            "text": "The most recent build results are always shown on the home page of Koji:  https://koji.chtc.wisc.edu/koji/index  Clicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM.  If your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a wildcard), and it will bring up a list of all builds for that package. You can find your build from there. For example, the \"lcmaps\" package page is here:  https://koji.chtc.wisc.edu/koji/packageinfo?packageID=56  And the lcmaps-1.6.6-1.1.osg33.el6 build is here:  https://koji.chtc.wisc.edu/koji/buildinfo?buildID=7427", 
            "title": "How to find build results"
        }, 
        {
            "location": "/software/koji-workflow/#trying-our-your-build", 
            "text": "Because it takes a while for your build to get into one of the regular repositories, it's simplest to download your RPM directly (see the previous section on How to find build results), and install it with:  [root@host]#  yum localinstall  RPM", 
            "title": "Trying our your build"
        }, 
        {
            "location": "/software/koji-workflow/#how-to-get-the-resulting-rpm-into-a-repository", 
            "text": "Once a package has been built, it is added to a tag. We then must turn the tag into a yum repository. This is normally done automatically and you do not need to deal with it yourself. Three notes:   The kojira daemon creates a repository automatically post-build on the koji-hub host. Eventually, the development repository will be the one hosted by koji-hub.   The koji-hub repository can be created manually by running  [you@host]$  osg-koji regen-repo  TAG NAME   For example, the tag name for osg-development in 3.4 on el6 is \"osg-3.4-el6-development\". Likely, you won't need to do this when kojira is working.\n-   Repositories are created on external hosts with the  mash  tool. These are usually triggered by cron jobs, but may be run by hand too. Documentation for running mash is on the TODO list.\n-   You can create your own personal repository using  mash .", 
            "title": "How to get the resulting RPM into a repository"
        }, 
        {
            "location": "/software/koji-workflow/#debugging-build-issues", 
            "text": "Failed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include:    root.log \n    This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here.    build.log \n    The output of the rpmbuild executable. If your package fails to compile, the reason will show up here.      One input to the buildArch task is a repository, which is based on a Koji tag. If the repository hasn't been recreated for a dependency you need (for example, when kojira isn't working), you may not have the right RPMs available in your build root.    One common issue is building a chain of dependencies. For example, suppose build B depends on the results of build A. If you build A then build B immediately, B will likely fail. This is because A is not in the repository that B uses. The proper string of events building A, starting the regeneration of the destination and build repo (which should happen in a few minutes of the build A task completing), then submitting build task B.   Note  if you submit build task B while the build repository task is open, it will not start until the build task has finished.       Other errors   package  PACKAGE NAME  not in list for tag  TAG \n    This happens when the name of the directory your package is in does not match the name of the package.\n    You must rename one or the other and commit your changes before trying again.", 
            "title": "Debugging build issues"
        }, 
        {
            "location": "/software/koji-workflow/#promoting-builds-from-development-testing", 
            "text": "Software contributors can promote any package to testing. Members of the security team can promote ca-cert packages to testing.  To promote from development to testing:", 
            "title": "Promoting Builds from Development -&gt; Testing"
        }, 
        {
            "location": "/software/koji-workflow/#using-osg-promote", 
            "text": "If you want to promote the latest version:  [you@host]$  osg-promote -r  OSGVER -testing  PACKAGE NAME   PACKAGE NAME  is the bare package name without version, e.g.  gratia-probe .  If you want to promote a specific version:  [you@host]$  osg-promote -r  OSGVER -testing  BUILD NAME   BUILD NAME  is a full  name-version-revision.disttag  such as  gratia-probe-1.17.0-2.osg33.el6 .  OSGVER  is the OSG major version that you are promoting for (e.g.  3.4 ).  osg-promote  will promote both the el6 and el7 builds of a package. After promoting, copy and paste the JIRA code  osg-promote  produces into the JIRA ticket that you are working on.  For  osg-promote , you may omit the  .osg34.el6  or  .osg34.el7 ; the script will add the appropriate disttag on.  See  OSG Building Tools  for full details on  osg-promote .", 
            "title": "Using osg-promote"
        }, 
        {
            "location": "/software/koji-workflow/#creating-custom-koji-areas", 
            "text": "Occasionally you may want to make builds of a package (or packages) which you\ndo not yet want to go into the main development repos.  In this case, you can\ncreate a set of custom koji tags and build targets for these builds.  We have\na script in our osg-next-tools  repo\ncalled new-koji-area \nthat facilitates this set up.", 
            "title": "Creating custom koji areas"
        }, 
        {
            "location": "/software/koji-workflow/#further-reading", 
            "text": "Official Koji documentation:  https://docs.pagure.org/koji/  Fedora's koji documentation:  https://fedoraproject.org/wiki/Koji  Fedora's \"Using Koji\" page:  https://fedoraproject.org/wiki/Using_the_Koji_build_system  Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.", 
            "title": "Further reading"
        }, 
        {
            "location": "/software/create-vo-client/", 
            "text": "Creating the VO Client package\n\n\nOverview\n\n\nThe VO Client package is just a conversion of the tarball created by GOC into the RPM format.\n\n\nIn order to build the RPM, one needs:\n\n\n\n\nThe tarball containing the:\n\n\nvomses\n file\n\n\nedg-mkgridmap.conf\n file\n\n\ngums.config.template\n file\n\n\nvomsdir\n directory tree, containing the lsc files.\n\n\n\n\n\n\nThe RPM spec file\n\n\n\n\nMaking the tarball\n\n\nTo make the tarball:\n\n\n\n\nstart with a clean directory\n\n\ncopy in the \nvomses\n, \ngums.template\n, and \nedg-mkgridmap.conf\n files and rename the edg-mkgridmap file:\n\n\n\n\n\n\n\n      wget http://repo.opensciencgrid.org/pacman/tarballs/vo-package/vomses\n      wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/edg-mkgridmap.osg\n      mv edg-mkgridmap.osg edg-mkgridmap.conf\n      wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/gums.template # TODO UNSURE\n\n\n\n\n\n\n\nCreate the vomsdir directory by downloading the .lsc files\n\n\n\n\n\n\n\n     wget --recursive --no-host-directories --cut-dirs=3 -A \n*.lsc\n http://repo.opensciencegrid.org/pacman/tarballs/vo-package/vomsdir\n\n\n\n\n\n\n\nIn a separate directory, unpack the \nold\n vo-client tarball (from the upstream source cache)\n\n\ndiff the two directories, and compare the changes to the expected changes listed in the JIRA ticket for this VO Client package release\n\n\n\n\n\n\n\n\n\nFollow the instructions in the attached \ngums-template-conversion.txt\n file to convert it from GUMS 1.1 (1.2?) format to GUMS 1.3 format. Name the result \ngums.config.template\n. See also the \nAutomated GUMS Conversion\n section below for a scripted version of this step.\n\n\nMove the files into a subdirectory to include in the tarball:\n\n\n\n\n\n\n\n      VERSION=44  # set appropriately\n      mkdir vo-client-$VERSION\n      mv vomses gums.config.template edg-mkgridmap.conf vomsdir vo-client-$VERSION/\n      tar -czf vo-client-$VERSION-osg.tar.gz vo-client-$VERSION/\n\n\n\n\n\nUpload the tarball into the \nupstream source cache\n, in the \nvo-client/VERSION/\n directory.\n\n\nAutomated GUMS Conversion\n\n\nThe above \ninstructions\n outline a procedure for converting the osg gums.config template from GUMS 1.1 format to 1.3 format. Because setting up a GUMS instance for this can be time consuming and tricky to get right, a script was written to automate the procedure on a Fermi VM. The script lives in svn under: \n$SVN/software/tools/convert-osg-gums-template-for-vo-client.sh\n .\n\n\nTo use it:\n\n\n\n\nCreate a new Fermi VM (el5 or el6)\n\n\nCopy the script and the new \ngums.template\n to be converted to the /root homedir on the VM.\n\n\nLog into the VM as root, make sure the script is executable, and run against the gums template:\n\n\n\n\n\n\n\n      $ ssh root@el6-vo-client\n      # wget https://vdt.cs.wisc.edu/svn/software/tools/convert-osg-gums-template-for-vo-client.sh\n      # chmod +x convert-osg-gums-template-for-vo-client.sh\n      # ./convert-osg-gums-template-for-vo-client.sh gums.template\n\n\n\n\n\n\n\nIt takes a little while to install and set up gums and related packages, but if it succeeds, you should see a message that says \"User group has been saved.\", and a file \ngums.config.template\n should be written in the current directory.\n\n\nThe newly converted \ngums.config.template\n should be compared to the old version of that file (from the previous vo-client package) to ensure that the only the differences are the changes for this release. (I have had to manually strip the extra test account stuff.) The 'meld' program is a nice graphical diff tool that I use for comparing them.\n\n\n\n\nRPM spec file maintenance\n\n\nThe OSG RPM spec file is \nmaintained in Subversion\n.\n\n\nThe VO Client package is located in \nnative/redhat/trunk/vo-client\n\n\nThere are two files that need to be maintained:\n\n\n\n\nosg/vo-client.spec\n - This is the RPM spec file proper. One needs to update the version (and/or the release number) every time a new RPM is created.\n\n\nupstream/release_tarball.source\n - This file contains the relative path of the tarball within the \nupstream source cache\n. Since the tarball file name will change with every new RPM version, this file has to be changed accordingly.\n\n\n\n\nRPM building\n\n\nAfter installing the \nosg-build tools\n, check out a clean copy from svn, then:\n\n\n\n\nosg-build prebuild .\n\n\nOnce there are no errors, run \nosg-build koji . --scratch\n This can be done without making any permanent change.\n\n\nOnce that builds successfully, run \nosg-build koji .\n This is permanent, unlike when you ran with \n--scratch\n. You cannot rebuild this version of the RPM again - you must bump the release number and edit the changelog.\n\n\n\n\nThis will push the RPMs into the OSG development repository. Koji requires additional setup compared to rpmbuild; \nsee the documentation here\n.\n\n\nPromotion to testing and release:\n\n\nPolicies\n\n\nRead \nRelease Policy\n.\n\n\nThese should be synchronized internally with other GOC update activities.", 
            "title": "Creating the VO Client Package"
        }, 
        {
            "location": "/software/create-vo-client/#creating-the-vo-client-package", 
            "text": "", 
            "title": "Creating the VO Client package"
        }, 
        {
            "location": "/software/create-vo-client/#overview", 
            "text": "The VO Client package is just a conversion of the tarball created by GOC into the RPM format.  In order to build the RPM, one needs:   The tarball containing the:  vomses  file  edg-mkgridmap.conf  file  gums.config.template  file  vomsdir  directory tree, containing the lsc files.    The RPM spec file", 
            "title": "Overview"
        }, 
        {
            "location": "/software/create-vo-client/#making-the-tarball", 
            "text": "To make the tarball:   start with a clean directory  copy in the  vomses ,  gums.template , and  edg-mkgridmap.conf  files and rename the edg-mkgridmap file:          wget http://repo.opensciencgrid.org/pacman/tarballs/vo-package/vomses\n      wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/edg-mkgridmap.osg\n      mv edg-mkgridmap.osg edg-mkgridmap.conf\n      wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/gums.template # TODO UNSURE   Create the vomsdir directory by downloading the .lsc files         wget --recursive --no-host-directories --cut-dirs=3 -A  *.lsc  http://repo.opensciencegrid.org/pacman/tarballs/vo-package/vomsdir   In a separate directory, unpack the  old  vo-client tarball (from the upstream source cache)  diff the two directories, and compare the changes to the expected changes listed in the JIRA ticket for this VO Client package release     Follow the instructions in the attached  gums-template-conversion.txt  file to convert it from GUMS 1.1 (1.2?) format to GUMS 1.3 format. Name the result  gums.config.template . See also the  Automated GUMS Conversion  section below for a scripted version of this step.  Move the files into a subdirectory to include in the tarball:          VERSION=44  # set appropriately\n      mkdir vo-client-$VERSION\n      mv vomses gums.config.template edg-mkgridmap.conf vomsdir vo-client-$VERSION/\n      tar -czf vo-client-$VERSION-osg.tar.gz vo-client-$VERSION/  Upload the tarball into the  upstream source cache , in the  vo-client/VERSION/  directory.", 
            "title": "Making the tarball"
        }, 
        {
            "location": "/software/create-vo-client/#automated-gums-conversion", 
            "text": "The above  instructions  outline a procedure for converting the osg gums.config template from GUMS 1.1 format to 1.3 format. Because setting up a GUMS instance for this can be time consuming and tricky to get right, a script was written to automate the procedure on a Fermi VM. The script lives in svn under:  $SVN/software/tools/convert-osg-gums-template-for-vo-client.sh  .  To use it:   Create a new Fermi VM (el5 or el6)  Copy the script and the new  gums.template  to be converted to the /root homedir on the VM.  Log into the VM as root, make sure the script is executable, and run against the gums template:          $ ssh root@el6-vo-client\n      # wget https://vdt.cs.wisc.edu/svn/software/tools/convert-osg-gums-template-for-vo-client.sh\n      # chmod +x convert-osg-gums-template-for-vo-client.sh\n      # ./convert-osg-gums-template-for-vo-client.sh gums.template   It takes a little while to install and set up gums and related packages, but if it succeeds, you should see a message that says \"User group has been saved.\", and a file  gums.config.template  should be written in the current directory.  The newly converted  gums.config.template  should be compared to the old version of that file (from the previous vo-client package) to ensure that the only the differences are the changes for this release. (I have had to manually strip the extra test account stuff.) The 'meld' program is a nice graphical diff tool that I use for comparing them.", 
            "title": "Automated GUMS Conversion"
        }, 
        {
            "location": "/software/create-vo-client/#rpm-spec-file-maintenance", 
            "text": "The OSG RPM spec file is  maintained in Subversion .  The VO Client package is located in  native/redhat/trunk/vo-client  There are two files that need to be maintained:   osg/vo-client.spec  - This is the RPM spec file proper. One needs to update the version (and/or the release number) every time a new RPM is created.  upstream/release_tarball.source  - This file contains the relative path of the tarball within the  upstream source cache . Since the tarball file name will change with every new RPM version, this file has to be changed accordingly.", 
            "title": "RPM spec file maintenance"
        }, 
        {
            "location": "/software/create-vo-client/#rpm-building", 
            "text": "After installing the  osg-build tools , check out a clean copy from svn, then:   osg-build prebuild .  Once there are no errors, run  osg-build koji . --scratch  This can be done without making any permanent change.  Once that builds successfully, run  osg-build koji .  This is permanent, unlike when you ran with  --scratch . You cannot rebuild this version of the RPM again - you must bump the release number and edit the changelog.   This will push the RPMs into the OSG development repository. Koji requires additional setup compared to rpmbuild;  see the documentation here .", 
            "title": "RPM building"
        }, 
        {
            "location": "/software/create-vo-client/#promotion-to-testing-and-release", 
            "text": "", 
            "title": "Promotion to testing and release:"
        }, 
        {
            "location": "/software/create-vo-client/#policies", 
            "text": "Read  Release Policy .  These should be synchronized internally with other GOC update activities.", 
            "title": "Policies"
        }, 
        {
            "location": "/software/repository-management/", 
            "text": "Repository Management\n\n\nThis document attempts to record everything there is to know about repository management for the OSG.\n\n\nPublic repositories\n\n\nWe host four public-facing repositories at \nrepo.opensciencegrid.org\n:\n\n\n\n\n\n\ndevelopment\n: This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.\n\n\n\n\n\n\ntesting\n: This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison.\n\n\n\n\n\n\nrelease\n: This repository contains software that we are willing to support and can be used by the general community.\n\n\n\n\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\n\n\n\n\nThese repos are updated by the \nmash\n script running on \nrepo1.grid.iu.edu\n and \nrepo2.grid.iu.edu\n.\n\n\nInternal repositories\n\n\nIn addition to the public repositories above, we host two repositories on \nkoji.chtc.wisc.edu\n. These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them.\n\n\n\n\n\n\nminefield\n: This repository is a copy of development above.\n\n\n\n\n\n\nprerelease\n: This repository is a staging area for software that is slated to be in the next release.\n\n\n\n\n\n\nThese repos are updated by the \nkojira\n daemon running on \nkoji.chtc.wisc.edu\n.\n\n\nBuild repositories\n\n\nThe \nkoji\n task in \nosg-build\n uses the \nosg-3.4-el6-build\n/\nosg-3.4-el7-build\n repo, which is the union of the following repositories:\n\n\n\n\nMinefield a.k.a. \nosg-3.4-el6-development\n / \nosg-3.4-el7-development\n\n\nThe \nosg-el6-internal\n / \nosg-el7-internal\n tag (containing build dependencies we do not want to make public)\n\n\nThe \ndist-el6-build\n / \ndist-el7-build\n tag (consisting of the appropriate macros for %dist)\n\n\nCentOS and EPEL\n\n\n\n\nKoji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies.\n\n\nThe tarball creation scripts use the \nosg-3.4-el6-release-build\n / \nosg-3.4-el7-release-build\n repo, which is the union of the following repositories:\n\n\n\n\nRelease a.k.a. \nosg-3.4-el6-release\n / \nosg-3.4-el7-release\n\n\nThe \ndist-el6-build\n / \ndist-el7-build\n tag (consisting of the appropriate macros for \n%dist\n)\n\n\nCentOS and EPEL", 
            "title": "Repository Management"
        }, 
        {
            "location": "/software/repository-management/#repository-management", 
            "text": "This document attempts to record everything there is to know about repository management for the OSG.", 
            "title": "Repository Management"
        }, 
        {
            "location": "/software/repository-management/#public-repositories", 
            "text": "We host four public-facing repositories at  repo.opensciencegrid.org :    development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.    testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison.    release : This repository contains software that we are willing to support and can be used by the general community.    contrib : RPMs contributed from outside the OSG.    These repos are updated by the  mash  script running on  repo1.grid.iu.edu  and  repo2.grid.iu.edu .", 
            "title": "Public repositories"
        }, 
        {
            "location": "/software/repository-management/#internal-repositories", 
            "text": "In addition to the public repositories above, we host two repositories on  koji.chtc.wisc.edu . These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them.    minefield : This repository is a copy of development above.    prerelease : This repository is a staging area for software that is slated to be in the next release.    These repos are updated by the  kojira  daemon running on  koji.chtc.wisc.edu .", 
            "title": "Internal repositories"
        }, 
        {
            "location": "/software/repository-management/#build-repositories", 
            "text": "The  koji  task in  osg-build  uses the  osg-3.4-el6-build / osg-3.4-el7-build  repo, which is the union of the following repositories:   Minefield a.k.a.  osg-3.4-el6-development  /  osg-3.4-el7-development  The  osg-el6-internal  /  osg-el7-internal  tag (containing build dependencies we do not want to make public)  The  dist-el6-build  /  dist-el7-build  tag (consisting of the appropriate macros for %dist)  CentOS and EPEL   Koji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies.  The tarball creation scripts use the  osg-3.4-el6-release-build  /  osg-3.4-el7-release-build  repo, which is the union of the following repositories:   Release a.k.a.  osg-3.4-el6-release  /  osg-3.4-el7-release  The  dist-el6-build  /  dist-el7-build  tag (consisting of the appropriate macros for  %dist )  CentOS and EPEL", 
            "title": "Build repositories"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/", 
            "text": "Globus mass update procedure\n\n\nGlobus consists of many packages, which we tend to update at the same time. This requires extra work, primarily to prevent dependency issues.\n\n\nPrep work\n\n\nDocs\n\n\nCreate a spreadsheet or table of the builds. Table should have NVR, perhaps URL, status (not started, imported, built, tested), and comments (mostly to record if it was a simple pass-through or not).\n\n\nGet packages to update, using \nosg-outdated-epel-pkgs\n from \nopensciencegrid/tools\n.\n\n\nTo get in N-V-R format:\n\n\n[you@host]$\n ./osg-outdated-epel-pkgs \n|\n \n\\\n\n    egrep \n^(globus|myproxy|gsi)\n \n|\n \n\\\n\n    awk \nBEGIN {OFS=\n} {print $1, \n-\n, $3}\n\n\n\n\n\n\nor to split up N and V-R in a comma-separated way (which you can feed into a Google Sheet to turn it into two columns):\n\n\n[you@host]$\n ./osg-outdated-epel-pkgs \n|\n \n\\\n\n    egrep \n^(globus|myproxy|gsi)\n \n|\n \n\\\n\n    awk \nBEGIN {OFS=\n} {print $1, \n,\n, $3}\n\n\n\n\n\n\nSVN\n\n\nCreate a separate SVN branch and populate it with all the packages you will update. (Get the list from the doc created above).\n\n\n[you@uw]$\n svn mkdir file:///p/vdt/workspace/svn/native/redhat/branches/globus\n\n#\n## From a checkout, in native/redhat\n\n\n[you@uw]$\n \nfor\n x in \nPACKAGES\n;\n \ndo\n \n\\\n\n     svn copy \n$x\n branches/globus/\n${\nx\n#trunk/\n}\n;\n \n\\\n\n   \ndone\n\n\n\n\n\n\nKoji (Mat/Carl)\n\n\nThis requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.\n\n\nEnsure Koji tags exist: a destination tag, and a build tag, one for each dver, e.g.:\n\n\n\n\nel6-globus\n\n\nel6-globus-build\n\n\nel7-globus\n\n\nel7-globus-build\n\n\n\n\nSet up tag inheritence: base the build tags off of the corresponding \ndist-el?-build\n tag. This is because we don't want old osg packages interfering with the new versions we're building. These may already exist -- check the \nel?-globus-build\n tags in the web interface.\n\n\n[you@host]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n        osg-koji add-tag --parent\n=\ndist-\n$el\n-build \n\\\n\n            --arches\n=\nx86_64 \n$el\n-globus-build\n;\n \n\\\n\n    \ndone\n\n\n\n\n\n\nTag \nbuildsys-macros\n for the OSG release into the build tags:\n\n\n[you@host]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n       \nbuildsys_macros_nvr\n=\n$(\nosg-koji -q list-tagged osg-3.4-\n$el\n-development \n\\\n\n         buildsys-macros --latest \n|\n awk \n{print $1}\n)\n;\n \n\\\n\n       osg-koji tag-pkg \n$el\n-globus-build \n$buildsys_macros_nvr\n;\n \n\\\n\n   \ndone\n\n\n\n\n\n\nEnsure Koji targets exist, one for each dver, e.g.:\n\n\n\n\nel6-globus (el6-globus-build \n el6-globus)\n\n\nel7-globus (el7-globus-build \n el7-globus)\n\n\nkojira-fake-el6-globus (el6-globus \n kojira-fake)\n\n\nkojira-fake-el7-globus (el7-globus \n kojira-fake)\n\n\n\n\n[you@host]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n       osg-koji add-target \n$el\n-globus \n$el\n-globus-build \n$el\n-globus\n;\n \n\\\n\n       osg-koji add-target kojira-fake-\n$el\n-globus \n$el\n-globus kojira-fake\n;\n \n\\\n\n   \ndone\n\n\n\n\n\n\nIf basing the packages off of the Globus repos, add the Globus repos as external repos, and add them to the build tags (but not the dest tags).\n\n\nEdit \n/etc/koji-hub/plugins/sign.conf\n and set up the GPG signing for the RPMs. Run \n/etc/koji-hub/plugins/fix-permissions\n after editing the file.\n\n\nPer-package work\n\n\n\n\ncd into branches/globus\n\n\nDownload packages from \nhttp://dl.fedoraproject.org/pub/epel/6/SRPMS/\n\n\n\n\nA useful alias:\n\n\n[you@host]$\n \nalias\n osg-build-globus\n=\nosg-build koji --ktt el6-globus --ktt el7-globus\n\n\n\n\n\n\nStrict pass-through (no osg/ directory)\n\n\n\n\n\n\nRun:\n\n\n[you@uw]$\n osg-import-srpm \nURL\n\n\n[you@uw]$\n osg-build-globus --scratch \nPKG\n\n\n\n\n\n\n\n\n\n\nCommit - use a message like \"Update to 3.12-1 from EPEL (SOFTWARE-2197)\"\n\n\n\n\n\n\nDo a non-scratch build.\n\n\n\n\n\n\nNon-strict pass-through\n\n\n\n\n\n\nRun:\n\n\n[you@uw]$\n osg-import-srpm --diff3 \nURL\n\n\n\n\n\n\n\n\n\n\nFix merge conflicts in the spec file. If not already there, put a .1 after the Release number to mark the changes as ours.\n\n\n\n\n\n\nRun:\n\n\n[you@uw]$\n osg-build quilt \nPKG\n\n\n\n\n\n\n\n\n\n\nFix patches if necessary.\n\n\n\n\n\n\nRun:\n\n\n[you@uw\n]$\n osg-build-globus --scratch \nPKG\n\n\n\n\n\n\n\n\n\n\nCommit - use a message like \"Update to 8.29-1 from EPEL and merge OSG changes (SOFTWARE-2197)\"\n\n\n\n\n\n\nDo a non-scratch build.\n\n\n\n\n\n\nTesting\n\n\nCreate a yum \n.repo\n file similar to \nosg-minefield\n that installs from the \nel?-globus\n repos. Enable this and \nosg-minefield\n.\n\n\nEL7 example:\n\n\n[globus]\n\n\nname\n=\nglobus\n\n\nbaseurl\n=\nhttp://koji.chtc.wisc.edu/mnt/koji/repos/el7-globus/latest/$basearch/\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n0\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nconsider_as_osg\n=\nyes\n\n\n\n\n\n\nMerge\n\n\nKoji (Mat/Carl)\n\n\nThis requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.\n\n\n\n\nUntag broken versions that we don't want to ship.\n\n\nUse \nmove-pkg\n:\n[you@host\n]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n        osg-koji -q list-tagged \n${\nel\n}\n-globus \n|\n \n\\\n\n            awk \n{print $1}\n \n \n${\nel\n}\n-tagged.txt\n;\n \n\\\n\n    \ndone\n\n\n#\n## Check the txts if they look sane\n\n\n[you@host\n]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n        xargs -a \n${\nel\n}\n-tagged.txt \n\\\n\n            osg-koji move-pkg \n${\nel\n}\n-globus \n\\\n\n                osg-3.3-\n${\nel\n}\n-development\n;\n \n\\\n\n    \ndone\n\n\n\n\n\n\n\n\n\n\nSVN\n\n\n\n\nMerge from \ntrunk\n to \nbranches/globus\n first, to pick up any globus changes that may have happened in trunk.\n\n\nMerge from \nbranches/globus\n to \ntrunk\n.\n\n\nMove \nbranches/globus\n to \ntags/globus-\nDATE\n.", 
            "title": "Globus Mass Update Procedure"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#globus-mass-update-procedure", 
            "text": "Globus consists of many packages, which we tend to update at the same time. This requires extra work, primarily to prevent dependency issues.", 
            "title": "Globus mass update procedure"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#prep-work", 
            "text": "", 
            "title": "Prep work"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#docs", 
            "text": "Create a spreadsheet or table of the builds. Table should have NVR, perhaps URL, status (not started, imported, built, tested), and comments (mostly to record if it was a simple pass-through or not).  Get packages to update, using  osg-outdated-epel-pkgs  from  opensciencegrid/tools .  To get in N-V-R format:  [you@host]$  ./osg-outdated-epel-pkgs  |   \\ \n    egrep  ^(globus|myproxy|gsi)   |   \\ \n    awk  BEGIN {OFS= } {print $1,  - , $3}   or to split up N and V-R in a comma-separated way (which you can feed into a Google Sheet to turn it into two columns):  [you@host]$  ./osg-outdated-epel-pkgs  |   \\ \n    egrep  ^(globus|myproxy|gsi)   |   \\ \n    awk  BEGIN {OFS= } {print $1,  , , $3}", 
            "title": "Docs"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#svn", 
            "text": "Create a separate SVN branch and populate it with all the packages you will update. (Get the list from the doc created above).  [you@uw]$  svn mkdir file:///p/vdt/workspace/svn/native/redhat/branches/globus # ## From a checkout, in native/redhat  [you@uw]$   for  x in  PACKAGES ;   do   \\ \n     svn copy  $x  branches/globus/ ${ x #trunk/ } ;   \\ \n    done", 
            "title": "SVN"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#koji-matcarl", 
            "text": "This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.  Ensure Koji tags exist: a destination tag, and a build tag, one for each dver, e.g.:   el6-globus  el6-globus-build  el7-globus  el7-globus-build   Set up tag inheritence: base the build tags off of the corresponding  dist-el?-build  tag. This is because we don't want old osg packages interfering with the new versions we're building. These may already exist -- check the  el?-globus-build  tags in the web interface.  [you@host]$   for  el in el6 el7 ;   do   \\ \n        osg-koji add-tag --parent = dist- $el -build  \\ \n            --arches = x86_64  $el -globus-build ;   \\ \n     done   Tag  buildsys-macros  for the OSG release into the build tags:  [you@host]$   for  el in el6 el7 ;   do   \\ \n        buildsys_macros_nvr = $( osg-koji -q list-tagged osg-3.4- $el -development  \\ \n         buildsys-macros --latest  |  awk  {print $1} ) ;   \\ \n       osg-koji tag-pkg  $el -globus-build  $buildsys_macros_nvr ;   \\ \n    done   Ensure Koji targets exist, one for each dver, e.g.:   el6-globus (el6-globus-build   el6-globus)  el7-globus (el7-globus-build   el7-globus)  kojira-fake-el6-globus (el6-globus   kojira-fake)  kojira-fake-el7-globus (el7-globus   kojira-fake)   [you@host]$   for  el in el6 el7 ;   do   \\ \n       osg-koji add-target  $el -globus  $el -globus-build  $el -globus ;   \\ \n       osg-koji add-target kojira-fake- $el -globus  $el -globus kojira-fake ;   \\ \n    done   If basing the packages off of the Globus repos, add the Globus repos as external repos, and add them to the build tags (but not the dest tags).  Edit  /etc/koji-hub/plugins/sign.conf  and set up the GPG signing for the RPMs. Run  /etc/koji-hub/plugins/fix-permissions  after editing the file.", 
            "title": "Koji (Mat/Carl)"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#per-package-work", 
            "text": "cd into branches/globus  Download packages from  http://dl.fedoraproject.org/pub/epel/6/SRPMS/   A useful alias:  [you@host]$   alias  osg-build-globus = osg-build koji --ktt el6-globus --ktt el7-globus", 
            "title": "Per-package work"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#strict-pass-through-no-osg-directory", 
            "text": "Run:  [you@uw]$  osg-import-srpm  URL  [you@uw]$  osg-build-globus --scratch  PKG     Commit - use a message like \"Update to 3.12-1 from EPEL (SOFTWARE-2197)\"    Do a non-scratch build.", 
            "title": "Strict pass-through (no osg/ directory)"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#non-strict-pass-through", 
            "text": "Run:  [you@uw]$  osg-import-srpm --diff3  URL     Fix merge conflicts in the spec file. If not already there, put a .1 after the Release number to mark the changes as ours.    Run:  [you@uw]$  osg-build quilt  PKG     Fix patches if necessary.    Run:  [you@uw ]$  osg-build-globus --scratch  PKG     Commit - use a message like \"Update to 8.29-1 from EPEL and merge OSG changes (SOFTWARE-2197)\"    Do a non-scratch build.", 
            "title": "Non-strict pass-through"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#testing", 
            "text": "Create a yum  .repo  file similar to  osg-minefield  that installs from the  el?-globus  repos. Enable this and  osg-minefield .  EL7 example:  [globus]  name = globus  baseurl = http://koji.chtc.wisc.edu/mnt/koji/repos/el7-globus/latest/$basearch/  failovermethod = priority  priority = 98  enabled = 1  gpgcheck = 0  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  consider_as_osg = yes", 
            "title": "Testing"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#merge", 
            "text": "", 
            "title": "Merge"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#koji-matcarl_1", 
            "text": "This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.   Untag broken versions that we don't want to ship.  Use  move-pkg : [you@host ]$   for  el in el6 el7 ;   do   \\ \n        osg-koji -q list-tagged  ${ el } -globus  |   \\ \n            awk  {print $1}     ${ el } -tagged.txt ;   \\ \n     done  # ## Check the txts if they look sane  [you@host ]$   for  el in el6 el7 ;   do   \\ \n        xargs -a  ${ el } -tagged.txt  \\ \n            osg-koji move-pkg  ${ el } -globus  \\ \n                osg-3.3- ${ el } -development ;   \\ \n     done", 
            "title": "Koji (Mat/Carl)"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#svn_1", 
            "text": "Merge from  trunk  to  branches/globus  first, to pick up any globus changes that may have happened in trunk.  Merge from  branches/globus  to  trunk .  Move  branches/globus  to  tags/globus- DATE .", 
            "title": "SVN"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/", 
            "text": "Resurrecting EPEL RPMs\n\n\nYou will need to be a Koji admin to do these steps. \n\n\n[user@client ~] $\n osg-koji --list-permissions --mine\n\n\n\n\n\nWill tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman.\n\n\n\n\n\n\n\n\nEPEL version\n\n\nEPEL Koji tag\n\n\nOur Koji tag\n\n\n\n\n\n\n\n\n\n\n5\n\n\ndist-5E-epel\n\n\nepelrescue-el5\n\n\n\n\n\n\n6\n\n\ndist-6E-epel\n\n\nepelrescue-el6\n\n\n\n\n\n\n7\n\n\nepel7\n\n\nepelrescue-el7\n\n\n\n\n\n\n\n\n\n\n\n\nDetermine the NVR of the build containing the RPM of the package you want.\n\n\nUse the Fedora/EPEL Koji web interface (\nhttps://koji.fedoraproject.org\n) to search for it.\n\n\nYou can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards.\n\n\nEPEL builds have .el5, .el6, or .el7 in the dist tag.\n\n\n\n\n\n\nDownload \nall\n RPMs for \nall\n architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms.\n\n\nYou have three options for the downloads:\n\n\n\n\nUse the links in the web interface\n\n\nUse the koji command-line interface against the Fedora koji:\n\n\nDownload \nfedora-koji.conf\n, attached to this page\n\n\nRun \nkoji --noauth -c fedora-koji.conf download-build --debuginfo \nBUILD_NVR\n\n\nDelete RPMs for architectures we do not care about (see list above)\n\n\n\n\n\n\nDig around in \nhttps://kojipkgs.fedoraproject.org/packages/\n\n\n\n\n\n\n\n\nOn your development machine:\n\n\n\n\n\n\nImportant:\n Verify that all of the RPMs are signed:\n\n\n[root@client ~] #\n rpm -K *.rpm \n|\n grep -iv gpg\n\n\n\n\n\nshould be empty\n\n\nIf not, \nSTOP\n and sign them using the OSG RPM key -- talk to Mat\n\n\n\n\n\n\nImport the RPMs themselves into the Koji system\n\n\n[user@client ~] $\n osg-koji import \nRPM_DIRECTORY\n/*.rpm\n\n\n\n\n\nThey will not be in any tags at this point\n\n\n\n\n\n\nAdd the package to the whitelist for our koji tag:\n\n\n[user@client ~] $\n osg-koji add-pkg \nOUR_KOJI_TAG\n \nPACKAGE\n --owner\n=\nYOUR_KOJI_USERNAME\n\n\n\n\n\n\n\n\n\n\nActually tag the builds:\n\n\n[user@client ~] $\n osg-koji tag-pkg \nOUR_KOJI_TAG\n \nBUILD\n\n\n\n\n\n\n\n\n\n\nCheck the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in.\n\n\nIf it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them):\n\n\nfor repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do\n   osg-koji regen-repo --nowait $repo\ndone\n\n\n\n\n\n\n\n\n\nMake a test VM and install the package from minefield to test that it is actually present.\n\n\n\n\nUpdate the epelrescue RPMs table below\n\n\n\n\nRemoving resurrected RPMs\n\n\nIn case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version:\n\n\n\n\n\n\nFind out the NVR of the build:\n\n\n[user@client ~] $\n osg-koji list-tagged \nOUR_KOJI_TAG\n \nPACKAGE\n\n\n\n\n\n\n\n\n\n\nUntag the packages:\n\n\n[user@client ~] $\n osg-koji untag-pkg \nOUR_KOJI_TAG\n \nBUILD\n\n\n\n\n\n\n\n\n\n\nWhy you should not use block-pkg\n\n\nEPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case:\n\n\n\n\nEPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5.\n\n\nLater, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5.\n\n\nI used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead.\n\n\nThe block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build.\n\n\nI unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again.\n\n\n\n\nPolicy for epelrescue tags\n\n\nhttps://jira.opensciencegrid.org/browse/SOFTWARE-2046\n\n\nTable of epelrescue RPMs\n\n\n\n\n\n\n\n\nPackage\n\n\nDistro version\n\n\nDate added\n\n\nReason added\n\n\nDate removed\n\n\n\n\n\n\n\n\n\n\npython-six-1.7.3-1.el6\n\n\n6\n\n\n2015-08-12\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\npython-argparse-1.2.1-2.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-wn-client (via gfal2)\n\n\n2015-10-14\n\n\n\n\n\n\npython-backports-ssl_match_hostname-3.4.0.2-4.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\npython-requests-1.1.0-4.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\npython-urllib3-1.5-7.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\n\n\nFinding out if a package is still needed in epelrescue\n\n\nSet \n$pkg\n to the name of a package to test (e.g. \npython-six\n), and \n$rhel\n set to the RHEL version you're testing for (e.g. \n5\n, \n6\n, or \n7\n).\n\n\nUsing Carl's \ncentos-srpms\n, \nscientific-srpms\n, \nslf-srpms\n scripts:\n\n\nfor script in centos-srpms scientific-srpms slf-srpms; do\n       echo -n $script \n: \n\n       $script -$rhel $pkg | grep . || echo none\ndone\n\n\n\n\n\nA dry run of removing the package:\n\n\nosg-koji untag-pkg -n --all epelrescue-el$rhel $pkg\n\n\n\n\n\nRemove the \n-n\n when the output of that looks fine.", 
            "title": "Resurrecting Epel Packages"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#resurrecting-epel-rpms", 
            "text": "You will need to be a Koji admin to do these steps.   [user@client ~] $  osg-koji --list-permissions --mine  Will tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman.     EPEL version  EPEL Koji tag  Our Koji tag      5  dist-5E-epel  epelrescue-el5    6  dist-6E-epel  epelrescue-el6    7  epel7  epelrescue-el7       Determine the NVR of the build containing the RPM of the package you want.  Use the Fedora/EPEL Koji web interface ( https://koji.fedoraproject.org ) to search for it.  You can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards.  EPEL builds have .el5, .el6, or .el7 in the dist tag.    Download  all  RPMs for  all  architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms.  You have three options for the downloads:   Use the links in the web interface  Use the koji command-line interface against the Fedora koji:  Download  fedora-koji.conf , attached to this page  Run  koji --noauth -c fedora-koji.conf download-build --debuginfo  BUILD_NVR  Delete RPMs for architectures we do not care about (see list above)    Dig around in  https://kojipkgs.fedoraproject.org/packages/     On your development machine:    Important:  Verify that all of the RPMs are signed:  [root@client ~] #  rpm -K *.rpm  |  grep -iv gpg  should be empty  If not,  STOP  and sign them using the OSG RPM key -- talk to Mat    Import the RPMs themselves into the Koji system  [user@client ~] $  osg-koji import  RPM_DIRECTORY /*.rpm  They will not be in any tags at this point    Add the package to the whitelist for our koji tag:  [user@client ~] $  osg-koji add-pkg  OUR_KOJI_TAG   PACKAGE  --owner = YOUR_KOJI_USERNAME     Actually tag the builds:  [user@client ~] $  osg-koji tag-pkg  OUR_KOJI_TAG   BUILD     Check the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in.  If it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them):  for repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do\n   osg-koji regen-repo --nowait $repo\ndone    Make a test VM and install the package from minefield to test that it is actually present.   Update the epelrescue RPMs table below", 
            "title": "Resurrecting EPEL RPMs"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#removing-resurrected-rpms", 
            "text": "In case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version:    Find out the NVR of the build:  [user@client ~] $  osg-koji list-tagged  OUR_KOJI_TAG   PACKAGE     Untag the packages:  [user@client ~] $  osg-koji untag-pkg  OUR_KOJI_TAG   BUILD", 
            "title": "Removing resurrected RPMs"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#why-you-should-not-use-block-pkg", 
            "text": "EPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case:   EPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5.  Later, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5.  I used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead.  The block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build.  I unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again.", 
            "title": "Why you should not use block-pkg"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#policy-for-epelrescue-tags", 
            "text": "https://jira.opensciencegrid.org/browse/SOFTWARE-2046", 
            "title": "Policy for epelrescue tags"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#table-of-epelrescue-rpms", 
            "text": "Package  Distro version  Date added  Reason added  Date removed      python-six-1.7.3-1.el6  6  2015-08-12  Dep of osg-build (via mock)  2015-10-14    python-argparse-1.2.1-2.el6  6  2015-09-23  Dep of osg-wn-client (via gfal2)  2015-10-14    python-backports-ssl_match_hostname-3.4.0.2-4.el6  6  2015-09-23  Dep of osg-build (via mock)  2015-10-14    python-requests-1.1.0-4.el6  6  2015-09-23  Dep of osg-build (via mock)  2015-10-14    python-urllib3-1.5-7.el6  6  2015-09-23  Dep of osg-build (via mock)  2015-10-14", 
            "title": "Table of epelrescue RPMs"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#finding-out-if-a-package-is-still-needed-in-epelrescue", 
            "text": "Set  $pkg  to the name of a package to test (e.g.  python-six ), and  $rhel  set to the RHEL version you're testing for (e.g.  5 ,  6 , or  7 ).  Using Carl's  centos-srpms ,  scientific-srpms ,  slf-srpms  scripts:  for script in centos-srpms scientific-srpms slf-srpms; do\n       echo -n $script  :  \n       $script -$rhel $pkg | grep . || echo none\ndone  A dry run of removing the package:  osg-koji untag-pkg -n --all epelrescue-el$rhel $pkg  Remove the  -n  when the output of that looks fine.", 
            "title": "Finding out if a package is still needed in epelrescue"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/", 
            "text": "Mass RPM Rebuilds for a new Build Target in Koji\n\n\nWhenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner.\n\n\nThis document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds.\n\n\nNew RHEL version vs new OSG series\n\n\nNew RHEL version\n\n\nFor a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later.\n\n\nNew OSG series\n\n\nFor a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series.\n\n\nA prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *).\n\n\nAfter some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort.\n\n\nDoing scratch builds of everything first\n\n\nBefore doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target.\n\n\nDoing mass scratch rebuilds for an existing series is easy, as they can all be done at once.\n\n\nRelatedly, doing a round of scratch builds \nafter\n successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues.\n\n\nOptions for calculating build dependencies\n\n\nWe can get dependency information from a number of places:\n\n\n\n\nscraping .spec files for Requires/BuildRequires/Provides and \n%package\n names\n\n\nquerying existing rpms directly on koji-hub and our OS/EPEL mirrors (\nrpm -q\n)\n\n\nquerying srpms from \nosg-build prebuild\n directly for build requirements\n\n\ninspecting previous buildroots to determine resolved build dependencies\n\n\nuse \nrepoquery\n to determine whatrequires/whatprovides for packages\n\n\nuse \nyum-builddep\n to find packages with all build requirements available\n\n\nusing the repodata (primary+filelists) from rpm repositories, including:\n\n\nupcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR\n\n\nosg-upcoming-elX-build, which includes them all\n\n\n\n\nOne important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build.\n\n\nThat is, \n(A BuildRequires B) and (B Requires C)\n implies \nA BuildRequires C\n.\n\n\nCombined with the fact that runtime requirements are transitive, that is, \n(A Requires B) and (B Requires C)\n implies \nA Requires C\n, computing build requirements is a recursive operation, which can be many levels deep.\n\n\nAnother question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz \n= 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.)\n\n\nNote also that the dependencies/capabilities for a given package often varies between different rhel versions.\n\n\nPre-computing (predictive) vs just-in-time\n\n\nTwo different approaches to determining dependency order for building are:\n\n\n\n\npre compute all dependencies based on an existing series/rhel version, OR\n\n\ncompute which remaining packages have all build reqs satisfied now\n\n\n\n\nThe first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target.\n\n\nThe second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets.\n\n\nIt may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first.\n\n\nPackage list closure, pruning\n\n\nAt some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts).\n\n\nBut if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target.\n\n\nProposal / Recommendations\n\n\nAs mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version.\n\n\nFor a new OSG series:\n\n\n\n\n\n\nupdate native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages\n\n\n\n\n\n\ntag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development)\n\n\n\n\n\n\nbuild all packages in new packaging area into new build target at once\n\n\n\n\n\n\nfor all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development)\n\n\n\n\n\n\nFor a new RHEL version:\n\n\n\n\npull the repodata from the relevant \n*-build\n repo from koji:\n\n\n\n\nfor pre-computing, use a build repo from an existing rhel version:\n\n\nhttps://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/\n\n\nfor just-in-time, use the new build repo:\n\n\nhttps://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/\n\n\nthe primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.)\n\n\n\n\ngenerate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo.\n\n\n\n\nthe primary (sqlite) file can be used to get build-requires.\n\n\n\n\nuse sql to resolve direct dependencies at the package name level:\n\n\nsrc-pkg: bin-pkg (BuildRequires)\n\n\nbin-pkg: bin-pkg (Requires)\n\n\nbin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies)\n\n\n\n\n\n\nresolve this list into a full list of recursive build dependencies.\n\n\n\n\nSince this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python.\n\n\n\n\nbuild ready-to-be-built packages\n\n\nupdate our copy of the repodata from the regen'ed \n*-build\n repo, as often as new versions become available\n\n\nupdate our dependency lists\n\n\nrepeat until all packages are built", 
            "title": "Koji Mass Rebuilds"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#mass-rpm-rebuilds-for-a-new-build-target-in-koji", 
            "text": "Whenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner.  This document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds.", 
            "title": "Mass RPM Rebuilds for a new Build Target in Koji"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#new-rhel-version-vs-new-osg-series", 
            "text": "", 
            "title": "New RHEL version vs new OSG series"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#new-rhel-version", 
            "text": "For a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later.", 
            "title": "New RHEL version"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#new-osg-series", 
            "text": "For a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series.  A prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *).  After some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort.", 
            "title": "New OSG series"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#doing-scratch-builds-of-everything-first", 
            "text": "Before doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target.  Doing mass scratch rebuilds for an existing series is easy, as they can all be done at once.  Relatedly, doing a round of scratch builds  after  successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues.", 
            "title": "Doing scratch builds of everything first"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#options-for-calculating-build-dependencies", 
            "text": "We can get dependency information from a number of places:   scraping .spec files for Requires/BuildRequires/Provides and  %package  names  querying existing rpms directly on koji-hub and our OS/EPEL mirrors ( rpm -q )  querying srpms from  osg-build prebuild  directly for build requirements  inspecting previous buildroots to determine resolved build dependencies  use  repoquery  to determine whatrequires/whatprovides for packages  use  yum-builddep  to find packages with all build requirements available  using the repodata (primary+filelists) from rpm repositories, including:  upcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR  osg-upcoming-elX-build, which includes them all   One important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build.  That is,  (A BuildRequires B) and (B Requires C)  implies  A BuildRequires C .  Combined with the fact that runtime requirements are transitive, that is,  (A Requires B) and (B Requires C)  implies  A Requires C , computing build requirements is a recursive operation, which can be many levels deep.  Another question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz  = 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.)  Note also that the dependencies/capabilities for a given package often varies between different rhel versions.", 
            "title": "Options for calculating build dependencies"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#pre-computing-predictive-vs-just-in-time", 
            "text": "Two different approaches to determining dependency order for building are:   pre compute all dependencies based on an existing series/rhel version, OR  compute which remaining packages have all build reqs satisfied now   The first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target.  The second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets.  It may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first.", 
            "title": "Pre-computing (predictive) vs just-in-time"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#package-list-closure-pruning", 
            "text": "At some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts).  But if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target.", 
            "title": "Package list closure, pruning"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#proposal-recommendations", 
            "text": "As mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version.", 
            "title": "Proposal / Recommendations"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#for-a-new-osg-series", 
            "text": "update native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages    tag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development)    build all packages in new packaging area into new build target at once    for all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development)", 
            "title": "For a new OSG series:"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#for-a-new-rhel-version", 
            "text": "pull the repodata from the relevant  *-build  repo from koji:   for pre-computing, use a build repo from an existing rhel version:  https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/  for just-in-time, use the new build repo:  https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/  the primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.)   generate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo.   the primary (sqlite) file can be used to get build-requires.   use sql to resolve direct dependencies at the package name level:  src-pkg: bin-pkg (BuildRequires)  bin-pkg: bin-pkg (Requires)  bin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies)    resolve this list into a full list of recursive build dependencies.   Since this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python.   build ready-to-be-built packages  update our copy of the repodata from the regen'ed  *-build  repo, as often as new versions become available  update our dependency lists  repeat until all packages are built", 
            "title": "For a new RHEL version:"
        }, 
        {
            "location": "/software/development-process/", 
            "text": "Software Development Process\n\n\nThis page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.)\n\n\nOverall Development Cycle\n\n\nFor a typical update to an existing package, the overall development cycle is roughly as follows:\n\n\n\n\nDownload the new upstream source (tarball, source RPM, checkout) into \nthe UW AFS upstream area\n\n\nIn \na checkout of our packaging code\n, update \nthe reference to the upstream file\n and, as needed, \nthe RPM spec file\n\n\nUse \nosg-build\n to perform a scratch build of the updated package\n\n\nVerify that the build succeeded; if not, redo previous steps until success\n\n\nOptionally, lightly test the new RPM(s); if there are problems, redo previous steps until success\n\n\nUse \nosg-build\n to perform an official build of the updated package (which will go into the development repos)\n\n\nPerform standard developer testing of the new RPM(s) \u2014 see below for details\n\n\nObtain permission from the Software manager to promote the package\n\n\nPromote the package to testing \u2014 see below for details\n\n\n\n\nVersioning Guidelines\n\n\nOSG-owned software should contain three digits, X.Y.Z, where X represents the major version, Y the minor version, and Z the maintenance version. New releases of software should increment one of the major, minor, or maintenance according to the following guidelines:\n\n\n\n\nMajor:\n Major new software, typically (but not limited to) full rewrites, new architectures, major new features; can certainly break backward compatibility (but should provide a smooth upgrade path). Worthy of introduction into Upcoming.\n\n\nMinor:\n Notable changes to the software, including significant feature changes, API changes, etc.; may break compatibility, but must provide an upgrade path from other versions within the same Major series.\n\n\nMaintenance:\n Bug fixes, minor feature tweaks, etc.; must not break compatibility with other versions within the same Major.Minor series.\n\n\n\n\nIf you are unsure about which version number to increment in a software update, consult the Software Manager.\n\n\nBuild Procedures\n\n\nBuilding packages for multiple OSG release series\n\n\nThe OSG Software team supports multiple release series, independent but in parallel to a large degree. In many cases, a single package is the same across release series, and therefore we want to build the package once and share it among the series. The procedure below suggests a way to accomplish this task.\n\n\nCurrent definitions:\n\n\n\n\nmaintenance: OSG 3.3 ( \nbranches/osg-3.3\n )\n\n\ncurrent: OSG 3.4 ( \ntrunk\n )\n\n\n\n\n\n\n\nProcedure:\n\n\n\n\nMake changes to \ntrunk\n\n\nOptionally, make and test a scratch build from \ntrunk\n\n\nCommit the changes\n\n\nMake an official build from \ntrunk\n (e.g.: \nosg-build koji \nPACKAGE\n)\n\n\nPerform the standard 4 tests for the \ncurrent\n series (see below)\n\n\nMerge the relevant commits from \ntrunk\n into the \nmaintenance\n branch (see below for tips)\n\n\nOptionally, make and test a scratch build from the \nmaintenance\n branch\n\n\nCommit the merge\n\n\nMake an official build from the \nmaintenance\n branch (e.g.: \nosg-build koji --repo=3.3 \nPACKAGE\n)\n\n\nPerform the standard 4 tests for the \nmaintenance\n series (see below)\n\n\nAs needed (or directed by the Software manager), perform the cross-series tests (see below)\n\n\n\n\nNote:\n Do not change the RPM Release number in the \nmaintenance\n branch before rebuilding; the %dist tag will differ automatically, and hence the \nmaintenance\n and \ncurrent\n NVRs will not conflict.\n\n\n\n\n\nMerging changes from one release series to another\n\n\nThese instructions assume that you are merging from \ntrunk\n to \nbranches/osg-3.3\n. They also assume that the current directory you are in is a checkout of \nbranches/osg-3.3\n. I will use \n$pkg\n to refer to the name of your package.\n\n\nFirst, you will need the commit numbers for your changes:\n\n\nsvn log \\^/native/redhat/trunk/$pkg | less\n\n\n\n\n\nWrite down the commits you want to merge.\n\n\nIf you only have one commit, merge that commit with -c as follows:\n\n\nsvn merge -c $commit_num \\^/native/redhat/trunk/$pkg $pkg\n\n\n\n\n\nWhere \n$commit_num\n is the SVN revision number of that commit (e.g. 17000). Merging an individual change like this is referred to as \"cherry-picking\".\n\n\nIf you have a range of commits and you wish to merge all commits within that range, then do the following:\n\n\nsvn merge -r $start_num:$end_num \\^/native/redhat/trunk/$pkg $pkg\n\n\n\n\n\nWhere \n$start_num\n is the SVN revision of the commit \nBEFORE\n your first commit, and \n$end_num\n is the SVN revision of your last commit in that range. \nNote:\n Be very careful when merging a range from trunk into the maintenance branch so that you do not introduce more changes to the maintenance branch than are necessary.\n\n\nIf you have multiple commits but they are not contiguous (i.e. there are commits made by you or someone else in that range that you do not want to merge), you will need to cherry-pick each individual commit.\n\n\nsvn merge -c $commit1 \\^/native/redhat/trunk/$pkg $pkg\nsvn merge -c $commit2 \\^/native/redhat/trunk/$pkg $pkg\n...\n\n\n\n\n\nWhere \n$commit1\n, \n$commit2\n are the commit numbers of the individual changes.\n\n\nNote that merge tracking in recent versions of SVN (1.5 or newer) should prevent commits from accidentally being merged multiple times. You should still look out for conflicts and examine the changes via \nsvn diff\n before committing the merge.\n\n\nTesting Procedures\n\n\nBefore promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing.\n\n\nIf you are not doing your own development testing for a package\n, contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released.\n\n\nThe \"Standard 4\" tests, defined\n\n\nIn most cases, the Software manager will ask a developer to perform the \u201cstandard 4\u201d tests on an updated package in a release series before promotion. This is a shorthand description for a standard set of 4 test runs:\n\n\n\n\nFresh install on el6\n\n\nFresh install on el7\n\n\nUpdate install on el6\n\n\nUpdate install on el7\n\n\n\n\nAn \u201cupdate install\u201d is a fresh install of the relevant package (or better yet, metapackage that includes it) \nfrom the production repository\n, followed by an update to the new build \nfrom the development repository\n.\n\n\nFor each test run, the amount of functional testing required will vary.\n\n\n\n\nFor very simple changes, it may be sufficient to verify that each installation succeeds and that the expected files are in place\n\n\nFor some changes, it may be sufficient to run osg-test on the resulting installation\n\n\nFor some changes, it will be necessary to perform careful functional tests of the affected component(s)\n\n\n\n\nIf you have questions, check with the Software Manager to determine the amount of testing that is required per test run.\n\n\nThe \"Cross-Series\" test, defined\n\n\nThe cross-series test may need to be run for packages that have been built for multiple release series of the OSG software stack (i.e. 3.3 and 3.4):\n\n\n\n\nOn el6, install from the 3.3 repositories, then update from the 3.4 repositories\n\n\nOn el7, install from the 3.3 repositories, then update from the 3.4 repositories\n\n\n\n\nViewed another way, this test is similar to the update installs, above, except from 3.3-release to 3.4-development.\n\n\nThe \"Long Tail\" tests, defined\n\n\nThese tests may need to be run when updating a package that's also in the old, unsupported (3.2) branch. They will consist of:\n\n\n\n\nInstall from 3.2-release and update to 3.4-development (on el6 only)\n\n\n\n\nThe \"full set of tests\", defined\n\n\nAll of the tests mentioned above.\n\n\nRunning the tests in VM Universe\n\n\nIn the case that the package you're testing is covered by osg-tested-internal, you can run the full set of tests in a manual VM universe test run. Make sure you meet the \npre-requisites\n required to submit VM Universe jobs on \nosghost.chtc.wisc.edu\n. After that's done, you can prepare the test suite by running:\n\n\nosg-run-tests \nTesting \nchange x\n\n\n\n\n\n\nAfter you \ncd\n into the directory specified in the output of the previous command, you will need to edit the \n*.yaml\n files in \nparameters.d\n to reflect the tests that you will want to run i.e. clean installs, upgrade installs and upgrade installs between OSG versions.\n\n\nOnce you're satisfied with your list of parameters, submit the dag:\n\n\ncondor_submit_dag master-run.dag\n\n\n\n\n\n\nPromoting a Package to Testing\n\n\nOnce development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle.\n\n\nPreparing a Good Promotion Request\n\n\nDevelopers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected JIRA ticket and will be answered there as well. Below are some tips for writing a good promotion request:\n\n\n\n\nMake sure that relevant information about goals, history, and resolution is in the associated ticket(s)\n\n\nInclude globs for the NVRs to be promoted (or a detailed list, if it is that complicated, which it almost never is)\n\n\nIf you ran automated tests:\n\n\nLink to the results page(s)\n\n\nVerify that relevant tests ran successfully (as opposed to being skipped or failing) \u2013\u00a0briefly summarize your findings\n\n\nNote whether the automated tests are just regression tests or actually test the current change(s)\n\n\nIf there are \nany\n failures, explain why they are not important to the promotion request\n\n\n\n\n\n\nIf you ran manual tests:\n\n\nSummarize your tests and findings\n\n\nIf there were failures, explain why they are not important to the promotion request\n\n\n\n\n\n\nIf there are critical build dependencies that we typically check, include reports from the \nbuilt-against-pkgs\n tool\n\n\nNote: This step is really just for known, specific cases, like the {HTCondor, HTCondor-CE, BLAHP} set and the {BeStMan, GUMS, VOMS Admin, etc.} Java set\n\n\nOccasionally, the OSG Software manager will request the tool to be run for other cases\n\n\n\n\n\n\nIf other packages depend on the to-be-promoted package, explain whether the dependent packages must be rebuilt or, if not, why not\n\n\n\n\nFor example (hypothetical promotion request for HTCondor-CE):\n\n\n\n\nMay I promote \nhtcondor-ce-2.3.4-2.osg3*.el*\n? I ran a complete set of automated tests \nLINK THE PRECEDING TEXT OR SEPARATELY HERE>\n; the HTCondor-CE tests ran and passed in all cases. There were some spurious failures of RSV in the All condition for RHEL 6, but this is a known failure case that is independent of HTCondor-CE. I also did a few spot checks manually (one VM each for SL 6 and SL 7), and in each case setting \nuse_frobnosticator = true\n in the configuration resulted in the expected behavior as defined in the description field above. The \nbuilt-against-pkgs\n tool shows that I built against all the latest HTCondor and BLAHP builds, see below. \nJIRA-formatted table comes after>\n\n\n\n\nPromoting\n\n\nFollow these steps to request promotion, promote a package, and note the promotion in JIRA:\n\n\n\n\nMake sure the package update has at least one associated JIRA ticket; if there is no ticket, at least create one for releasing the package(s)\n\n\nObtain permission to promote the package(s) from the Software manager (see above)\n\n\nUse \nosg-promote\n to promote the package(s) from development to testing\n\n\nComment on the associated JIRA ticket(s) with osg-promote\u2019s JIRA-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing\n\n\nMark each associated JIRA ticket as \u201cReady For Testing\u201d and (done automatically for you:) set the Assignee to \u201cUnassigned\u201d", 
            "title": "Development Process"
        }, 
        {
            "location": "/software/development-process/#software-development-process", 
            "text": "This page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.)", 
            "title": "Software Development Process"
        }, 
        {
            "location": "/software/development-process/#overall-development-cycle", 
            "text": "For a typical update to an existing package, the overall development cycle is roughly as follows:   Download the new upstream source (tarball, source RPM, checkout) into  the UW AFS upstream area  In  a checkout of our packaging code , update  the reference to the upstream file  and, as needed,  the RPM spec file  Use  osg-build  to perform a scratch build of the updated package  Verify that the build succeeded; if not, redo previous steps until success  Optionally, lightly test the new RPM(s); if there are problems, redo previous steps until success  Use  osg-build  to perform an official build of the updated package (which will go into the development repos)  Perform standard developer testing of the new RPM(s) \u2014 see below for details  Obtain permission from the Software manager to promote the package  Promote the package to testing \u2014 see below for details", 
            "title": "Overall Development Cycle"
        }, 
        {
            "location": "/software/development-process/#versioning-guidelines", 
            "text": "OSG-owned software should contain three digits, X.Y.Z, where X represents the major version, Y the minor version, and Z the maintenance version. New releases of software should increment one of the major, minor, or maintenance according to the following guidelines:   Major:  Major new software, typically (but not limited to) full rewrites, new architectures, major new features; can certainly break backward compatibility (but should provide a smooth upgrade path). Worthy of introduction into Upcoming.  Minor:  Notable changes to the software, including significant feature changes, API changes, etc.; may break compatibility, but must provide an upgrade path from other versions within the same Major series.  Maintenance:  Bug fixes, minor feature tweaks, etc.; must not break compatibility with other versions within the same Major.Minor series.   If you are unsure about which version number to increment in a software update, consult the Software Manager.", 
            "title": "Versioning Guidelines"
        }, 
        {
            "location": "/software/development-process/#build-procedures", 
            "text": "", 
            "title": "Build Procedures"
        }, 
        {
            "location": "/software/development-process/#building-packages-for-multiple-osg-release-series", 
            "text": "The OSG Software team supports multiple release series, independent but in parallel to a large degree. In many cases, a single package is the same across release series, and therefore we want to build the package once and share it among the series. The procedure below suggests a way to accomplish this task.  Current definitions:   maintenance: OSG 3.3 (  branches/osg-3.3  )  current: OSG 3.4 (  trunk  )    Procedure:   Make changes to  trunk  Optionally, make and test a scratch build from  trunk  Commit the changes  Make an official build from  trunk  (e.g.:  osg-build koji  PACKAGE )  Perform the standard 4 tests for the  current  series (see below)  Merge the relevant commits from  trunk  into the  maintenance  branch (see below for tips)  Optionally, make and test a scratch build from the  maintenance  branch  Commit the merge  Make an official build from the  maintenance  branch (e.g.:  osg-build koji --repo=3.3  PACKAGE )  Perform the standard 4 tests for the  maintenance  series (see below)  As needed (or directed by the Software manager), perform the cross-series tests (see below)   Note:  Do not change the RPM Release number in the  maintenance  branch before rebuilding; the %dist tag will differ automatically, and hence the  maintenance  and  current  NVRs will not conflict.", 
            "title": "Building packages for multiple OSG release series"
        }, 
        {
            "location": "/software/development-process/#merging-changes-from-one-release-series-to-another", 
            "text": "These instructions assume that you are merging from  trunk  to  branches/osg-3.3 . They also assume that the current directory you are in is a checkout of  branches/osg-3.3 . I will use  $pkg  to refer to the name of your package.  First, you will need the commit numbers for your changes:  svn log \\^/native/redhat/trunk/$pkg | less  Write down the commits you want to merge.  If you only have one commit, merge that commit with -c as follows:  svn merge -c $commit_num \\^/native/redhat/trunk/$pkg $pkg  Where  $commit_num  is the SVN revision number of that commit (e.g. 17000). Merging an individual change like this is referred to as \"cherry-picking\".  If you have a range of commits and you wish to merge all commits within that range, then do the following:  svn merge -r $start_num:$end_num \\^/native/redhat/trunk/$pkg $pkg  Where  $start_num  is the SVN revision of the commit  BEFORE  your first commit, and  $end_num  is the SVN revision of your last commit in that range.  Note:  Be very careful when merging a range from trunk into the maintenance branch so that you do not introduce more changes to the maintenance branch than are necessary.  If you have multiple commits but they are not contiguous (i.e. there are commits made by you or someone else in that range that you do not want to merge), you will need to cherry-pick each individual commit.  svn merge -c $commit1 \\^/native/redhat/trunk/$pkg $pkg\nsvn merge -c $commit2 \\^/native/redhat/trunk/$pkg $pkg\n...  Where  $commit1 ,  $commit2  are the commit numbers of the individual changes.  Note that merge tracking in recent versions of SVN (1.5 or newer) should prevent commits from accidentally being merged multiple times. You should still look out for conflicts and examine the changes via  svn diff  before committing the merge.", 
            "title": "Merging changes from one release series to another"
        }, 
        {
            "location": "/software/development-process/#testing-procedures", 
            "text": "Before promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing.  If you are not doing your own development testing for a package , contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released.", 
            "title": "Testing Procedures"
        }, 
        {
            "location": "/software/development-process/#the-standard-4-tests-defined", 
            "text": "In most cases, the Software manager will ask a developer to perform the \u201cstandard 4\u201d tests on an updated package in a release series before promotion. This is a shorthand description for a standard set of 4 test runs:   Fresh install on el6  Fresh install on el7  Update install on el6  Update install on el7   An \u201cupdate install\u201d is a fresh install of the relevant package (or better yet, metapackage that includes it)  from the production repository , followed by an update to the new build  from the development repository .  For each test run, the amount of functional testing required will vary.   For very simple changes, it may be sufficient to verify that each installation succeeds and that the expected files are in place  For some changes, it may be sufficient to run osg-test on the resulting installation  For some changes, it will be necessary to perform careful functional tests of the affected component(s)   If you have questions, check with the Software Manager to determine the amount of testing that is required per test run.", 
            "title": "The \"Standard 4\" tests, defined"
        }, 
        {
            "location": "/software/development-process/#the-cross-series-test-defined", 
            "text": "The cross-series test may need to be run for packages that have been built for multiple release series of the OSG software stack (i.e. 3.3 and 3.4):   On el6, install from the 3.3 repositories, then update from the 3.4 repositories  On el7, install from the 3.3 repositories, then update from the 3.4 repositories   Viewed another way, this test is similar to the update installs, above, except from 3.3-release to 3.4-development.", 
            "title": "The \"Cross-Series\" test, defined"
        }, 
        {
            "location": "/software/development-process/#the-long-tail-tests-defined", 
            "text": "These tests may need to be run when updating a package that's also in the old, unsupported (3.2) branch. They will consist of:   Install from 3.2-release and update to 3.4-development (on el6 only)", 
            "title": "The \"Long Tail\" tests, defined"
        }, 
        {
            "location": "/software/development-process/#the-full-set-of-tests-defined", 
            "text": "All of the tests mentioned above.", 
            "title": "The \"full set of tests\", defined"
        }, 
        {
            "location": "/software/development-process/#running-the-tests-in-vm-universe", 
            "text": "In the case that the package you're testing is covered by osg-tested-internal, you can run the full set of tests in a manual VM universe test run. Make sure you meet the  pre-requisites  required to submit VM Universe jobs on  osghost.chtc.wisc.edu . After that's done, you can prepare the test suite by running:  osg-run-tests  Testing  change x   After you  cd  into the directory specified in the output of the previous command, you will need to edit the  *.yaml  files in  parameters.d  to reflect the tests that you will want to run i.e. clean installs, upgrade installs and upgrade installs between OSG versions.  Once you're satisfied with your list of parameters, submit the dag:  condor_submit_dag master-run.dag", 
            "title": "Running the tests in VM Universe"
        }, 
        {
            "location": "/software/development-process/#promoting-a-package-to-testing", 
            "text": "Once development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle.", 
            "title": "Promoting a Package to Testing"
        }, 
        {
            "location": "/software/development-process/#preparing-a-good-promotion-request", 
            "text": "Developers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected JIRA ticket and will be answered there as well. Below are some tips for writing a good promotion request:   Make sure that relevant information about goals, history, and resolution is in the associated ticket(s)  Include globs for the NVRs to be promoted (or a detailed list, if it is that complicated, which it almost never is)  If you ran automated tests:  Link to the results page(s)  Verify that relevant tests ran successfully (as opposed to being skipped or failing) \u2013\u00a0briefly summarize your findings  Note whether the automated tests are just regression tests or actually test the current change(s)  If there are  any  failures, explain why they are not important to the promotion request    If you ran manual tests:  Summarize your tests and findings  If there were failures, explain why they are not important to the promotion request    If there are critical build dependencies that we typically check, include reports from the  built-against-pkgs  tool  Note: This step is really just for known, specific cases, like the {HTCondor, HTCondor-CE, BLAHP} set and the {BeStMan, GUMS, VOMS Admin, etc.} Java set  Occasionally, the OSG Software manager will request the tool to be run for other cases    If other packages depend on the to-be-promoted package, explain whether the dependent packages must be rebuilt or, if not, why not   For example (hypothetical promotion request for HTCondor-CE):   May I promote  htcondor-ce-2.3.4-2.osg3*.el* ? I ran a complete set of automated tests  LINK THE PRECEDING TEXT OR SEPARATELY HERE> ; the HTCondor-CE tests ran and passed in all cases. There were some spurious failures of RSV in the All condition for RHEL 6, but this is a known failure case that is independent of HTCondor-CE. I also did a few spot checks manually (one VM each for SL 6 and SL 7), and in each case setting  use_frobnosticator = true  in the configuration resulted in the expected behavior as defined in the description field above. The  built-against-pkgs  tool shows that I built against all the latest HTCondor and BLAHP builds, see below.  JIRA-formatted table comes after>", 
            "title": "Preparing a Good Promotion Request"
        }, 
        {
            "location": "/software/development-process/#promoting", 
            "text": "Follow these steps to request promotion, promote a package, and note the promotion in JIRA:   Make sure the package update has at least one associated JIRA ticket; if there is no ticket, at least create one for releasing the package(s)  Obtain permission to promote the package(s) from the Software manager (see above)  Use  osg-promote  to promote the package(s) from development to testing  Comment on the associated JIRA ticket(s) with osg-promote\u2019s JIRA-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing  Mark each associated JIRA ticket as \u201cReady For Testing\u201d and (done automatically for you:) set the Assignee to \u201cUnassigned\u201d", 
            "title": "Promoting"
        }, 
        {
            "location": "/software/git-software-development/", 
            "text": "Git software development workflow\n\n\nThis document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software.\n\n\nGit and GitHub basics\n\n\nIf you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at \nhttps://help.github.com/categories/bootcamp/\n\n\nGetting shell access to GitHub\n\n\nThere are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience.\n\n\nThe instructions here are derived from \nGitHub's own instructions on using SSH keys\n.\n\n\nCreating a new SSH key (optional but recommended)\n\n\nIf you already have an SSH keypair in your \n~/.ssh\n directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub.\n\n\nThe instructions below will create an SSH public/private key pair with the private key stored in \n~/.ssh/id_github\n and public key stored in \n~/.ssh/id_github.pub\n.\n\n\nGenerating the key\n\n\nUse \nssh-keygen\n to generate the SSH keypair. For \nEMAIL_ADDRESS\n, use the email address associated with your GitHub account.\n\n\n[user@client ~ ] $\n ssh-keygen -t rsa -b \n4096\n -f ~/.ssh/id_github -C \nEMAIL_ADDRESS\n\n\n\n\n\n\nConfiguring SSH to use the key for GitHub\n\n\nMake sure SSH uses the new key by default to access GitHub. Create or edit \n~/.ssh/config\n and append the following lines:\n\n\nHost github.com\nIdentityFile \nYOUR_HOME_DIR\n/.ssh/id_github\n\n\n\n\n\nAdding the SSH public key to GitHub\n\n\nUsing the GitHub web interface:\n\n\n\n\nOn the upper right of the screen, click on your profile picture\n\n\nIn the menu that pops up, click \"Settings\"\n\n\nOn the left-hand sidebar, click \"SSH and GPG keys\"\n\n\nIn the top right of the \"SSH keys\" box, click \"New SSH key\"\n\n\nIn the \"Title\" field of the dialog that pops up, enter a descriptive name for the key\n\n\nOpen the public key file (e.g. \n~/.ssh/id_github.pub\n (don't forget the \n.pub\n)) in a text editor and copy its full contents to the clipboard\n\n\nIn the \"Key\" field, paste the public key\n\n\nBelow the \"Key\" field, click \"Add SSH key\"\n\n\n\n\nYou should see your new key in the \"SSH keys\" list.\n\n\nTesting that shell access works\n\n\nTo verify you can authenticate to GitHub using SSH, SSH to \ngit@github.com\n. You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.'\n\n\nContribution workflow\n\n\nWe use the standard GitHub \npull request\n workflow for making contributions to OSG software.\n\n\nIf you've never contributed to this project on GitHub before, do the following steps first:\n\n\n\n\nUsing the GitHub web interface, fork the repo you wish to contribute to.\n\n\n\n\nMake a clone of your forked repo on your local machine.\n\n\n[user@client ~ ] $\n git clone \ngit@github.com\n:\nUSERNAME/PROJECT\n\n\n\n\n\n\n\n\nNote\n\n\nIf you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above.\n\n\nIf you get some other error, \nthe GitHub page on SSH\n may contain useful information on troubleshooting.\n\n\n\n\n\n\n\n\nOnce you have your local repo, do the following:\n\n\n\n\n\n\nCreate a branch to hold changes that are related to the issue you are working on. Give the branch a name that will remind you of its purpose, such as \nsw2345-pathchange\n\n\n[user@client ~ ] $\n git checkout -b \nBRANCH\n\n\n\n\n\n\n\n\n\n\nMake your commits to this branch, then push the branch to your repo on GitHub.\n\n\n[user@client ~ ] $\n git push origin \nBRANCH\n\n\n\n\n\n\n\n\n\n\nSelect your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a JIRA ticket number in the message to automatically link the pull request to the JIRA ticket.\n\n\n\n\n\n\nRequest a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member.\n\n\n\n\nIf the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from.\n\n\nIf your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request.\n\n\n\n\n\n\n\n\nRelease workflow\n\n\nThis section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by \nopensciencegrid\n. (If you can approve pull requests, you have write access).\n\n\nA release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by \nopensciencegrid\n (the OSG repo):\n\n\n\n\n\n\nIf you haven't already, add the OSG repo as a \"remote\" to your repo:\n\n\n[user@client ~ ] $\n git remote add upstream git@github.com:opensciencegrid/\nPROJECT\n\n\n\n\n\n\n\n\n\n\nFetch changes from the OSG repo:\n\n\n[user@client ~ ] $\n git fetch upstream\n\n\n\n\n\n\n\n\n\nCompare your branch you are releasing from (probably \nmaster\n) to its copy in the OSG repo:\n\n\n[user@client ~ ] $\n git checkout master\n;\n git diff upstream/master\n\n\n\n\n\nThere should be no differences.\n\n\n\n\n\n\nOnce this is done, release the software as you usually do. This process varies from one project to another, but often it involves running \nmake upstream\n or similar. Check your project's \nREADME\n file for instructions.\n\n\n\n\nTest your software.\n\n\n\n\nTag the commit that you made the release from. Git release tags are conventionally called \nVERSION\n, where \nVERSION\n is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the tag \nv1.3.0\n.\n\n\n\n\nNote\n\n\nOnce a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from.\n\n\n\n\n\n\n\n\nCreate the tag in your local repo:\n\n\n[user@client ~ ] $\n git tag \nTAG\n\n\n\n\n\n\n\n\n\n\nPush the tag to your own GitHub repo:\n\n\n[user@client ~ ] $\n git push origin \nTAG\n\n\n\n\n\n\n\n\n\n\nPush the tag to the OSG repo:\n\n\n[user@client ~ ] $\n git push upstream \nTAG", 
            "title": "Git Software Development Process"
        }, 
        {
            "location": "/software/git-software-development/#git-software-development-workflow", 
            "text": "This document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software.", 
            "title": "Git software development workflow"
        }, 
        {
            "location": "/software/git-software-development/#git-and-github-basics", 
            "text": "If you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at  https://help.github.com/categories/bootcamp/", 
            "title": "Git and GitHub basics"
        }, 
        {
            "location": "/software/git-software-development/#getting-shell-access-to-github", 
            "text": "There are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience.  The instructions here are derived from  GitHub's own instructions on using SSH keys .", 
            "title": "Getting shell access to GitHub"
        }, 
        {
            "location": "/software/git-software-development/#creating-a-new-ssh-key-optional-but-recommended", 
            "text": "If you already have an SSH keypair in your  ~/.ssh  directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub.  The instructions below will create an SSH public/private key pair with the private key stored in  ~/.ssh/id_github  and public key stored in  ~/.ssh/id_github.pub .", 
            "title": "Creating a new SSH key (optional but recommended)"
        }, 
        {
            "location": "/software/git-software-development/#generating-the-key", 
            "text": "Use  ssh-keygen  to generate the SSH keypair. For  EMAIL_ADDRESS , use the email address associated with your GitHub account.  [user@client ~ ] $  ssh-keygen -t rsa -b  4096  -f ~/.ssh/id_github -C  EMAIL_ADDRESS", 
            "title": "Generating the key"
        }, 
        {
            "location": "/software/git-software-development/#configuring-ssh-to-use-the-key-for-github", 
            "text": "Make sure SSH uses the new key by default to access GitHub. Create or edit  ~/.ssh/config  and append the following lines:  Host github.com\nIdentityFile  YOUR_HOME_DIR /.ssh/id_github", 
            "title": "Configuring SSH to use the key for GitHub"
        }, 
        {
            "location": "/software/git-software-development/#adding-the-ssh-public-key-to-github", 
            "text": "Using the GitHub web interface:   On the upper right of the screen, click on your profile picture  In the menu that pops up, click \"Settings\"  On the left-hand sidebar, click \"SSH and GPG keys\"  In the top right of the \"SSH keys\" box, click \"New SSH key\"  In the \"Title\" field of the dialog that pops up, enter a descriptive name for the key  Open the public key file (e.g.  ~/.ssh/id_github.pub  (don't forget the  .pub )) in a text editor and copy its full contents to the clipboard  In the \"Key\" field, paste the public key  Below the \"Key\" field, click \"Add SSH key\"   You should see your new key in the \"SSH keys\" list.", 
            "title": "Adding the SSH public key to GitHub"
        }, 
        {
            "location": "/software/git-software-development/#testing-that-shell-access-works", 
            "text": "To verify you can authenticate to GitHub using SSH, SSH to  git@github.com . You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.'", 
            "title": "Testing that shell access works"
        }, 
        {
            "location": "/software/git-software-development/#contribution-workflow", 
            "text": "We use the standard GitHub  pull request  workflow for making contributions to OSG software.  If you've never contributed to this project on GitHub before, do the following steps first:   Using the GitHub web interface, fork the repo you wish to contribute to.   Make a clone of your forked repo on your local machine.  [user@client ~ ] $  git clone  git@github.com : USERNAME/PROJECT    Note  If you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above.  If you get some other error,  the GitHub page on SSH  may contain useful information on troubleshooting.     Once you have your local repo, do the following:    Create a branch to hold changes that are related to the issue you are working on. Give the branch a name that will remind you of its purpose, such as  sw2345-pathchange  [user@client ~ ] $  git checkout -b  BRANCH     Make your commits to this branch, then push the branch to your repo on GitHub.  [user@client ~ ] $  git push origin  BRANCH     Select your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a JIRA ticket number in the message to automatically link the pull request to the JIRA ticket.    Request a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member.   If the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from.  If your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request.", 
            "title": "Contribution workflow"
        }, 
        {
            "location": "/software/git-software-development/#release-workflow", 
            "text": "This section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by  opensciencegrid . (If you can approve pull requests, you have write access).  A release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by  opensciencegrid  (the OSG repo):    If you haven't already, add the OSG repo as a \"remote\" to your repo:  [user@client ~ ] $  git remote add upstream git@github.com:opensciencegrid/ PROJECT     Fetch changes from the OSG repo:  [user@client ~ ] $  git fetch upstream    Compare your branch you are releasing from (probably  master ) to its copy in the OSG repo:  [user@client ~ ] $  git checkout master ;  git diff upstream/master  There should be no differences.    Once this is done, release the software as you usually do. This process varies from one project to another, but often it involves running  make upstream  or similar. Check your project's  README  file for instructions.   Test your software.   Tag the commit that you made the release from. Git release tags are conventionally called  VERSION , where  VERSION  is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the tag  v1.3.0 .   Note  Once a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from.     Create the tag in your local repo:  [user@client ~ ] $  git tag  TAG     Push the tag to your own GitHub repo:  [user@client ~ ] $  git push origin  TAG     Push the tag to the OSG repo:  [user@client ~ ] $  git push upstream  TAG", 
            "title": "Release workflow"
        }, 
        {
            "location": "/software/ce-test-scaling/", 
            "text": "How to Run Scalability Tests on a CE\n\n\nIntroduction\n\n\nThis document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an \nHTCondor-CE\n, but they should be applicable for other CE software.\n\n\nThe focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued.\n\n\nSleeper Pool\n\n\nWith the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs.\n\n\nWhen large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work.\n\n\nSetting Up a Sleeper Pool\n\n\nA sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine:\n\n\nSTART = ( \\\n          (SlotID \n= 1) \n \\\n          (SlotID \n 33) \n \\\n          (RequiresWholeMachine =!= TRUE ) \n \\\n          (SleepSlot =!= TRUE) \n \\\n          (distro =?= \nRHEL6\n ) \n \\\n          (CPU_Only == TRUE ) \\\n          ) || \\\n          ( (SlotID \n= 33) \n (distro =?= \nRHEL6\n ) \n (SleepSlot == TRUE) )\n\n\n\n\n\nUsual Topology of the Tests\n\n\nA brief introduction to the topology involved in the tests.\n\n\nBatch System and Sleeper Pool\n\n\nThis is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator.\n\n\nCE\n\n\nThis is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications.\n\n\nSubmitter\n\n\nAn HTCondor submit host. It can be a virtual machine for most test submissions.\n\n\nMonitoring tools\n\n\nTo monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example:\n\n\ncondor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const \nJobStatus=?=2\n | wc -l\n\n\ngmetric --name RunningJobsCE \n\n\n\n\n\n\nGenerating Load\n\n\nLocation\n\n\nThe load_generators are found in the  \nOSgscal github repo\n. The binary of interest here is \nloadtest_condor\n\n\nUse\n\n\nJust untar it or check it out from mas on the HTCondor submit node (see above):\n\n\ngit checkout https://github.com/efajardo/osgscal\n\n\ncd load_generators/loadtest_condor/trunk/bin\n\n\n\n\n\n\nKeep in mind that you also need a valid proxy for grid submissions.\n\n\nFor example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command:\n\n\n./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50", 
            "title": "CE Scale Testing"
        }, 
        {
            "location": "/software/ce-test-scaling/#how-to-run-scalability-tests-on-a-ce", 
            "text": "", 
            "title": "How to Run Scalability Tests on a CE"
        }, 
        {
            "location": "/software/ce-test-scaling/#introduction", 
            "text": "This document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an  HTCondor-CE , but they should be applicable for other CE software.  The focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued.", 
            "title": "Introduction"
        }, 
        {
            "location": "/software/ce-test-scaling/#sleeper-pool", 
            "text": "With the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs.  When large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work.", 
            "title": "Sleeper Pool"
        }, 
        {
            "location": "/software/ce-test-scaling/#setting-up-a-sleeper-pool", 
            "text": "A sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine:  START = ( \\\n          (SlotID  = 1)   \\\n          (SlotID   33)   \\\n          (RequiresWholeMachine =!= TRUE )   \\\n          (SleepSlot =!= TRUE)   \\\n          (distro =?=  RHEL6  )   \\\n          (CPU_Only == TRUE ) \\\n          ) || \\\n          ( (SlotID  = 33)   (distro =?=  RHEL6  )   (SleepSlot == TRUE) )", 
            "title": "Setting Up a Sleeper Pool"
        }, 
        {
            "location": "/software/ce-test-scaling/#usual-topology-of-the-tests", 
            "text": "A brief introduction to the topology involved in the tests.", 
            "title": "Usual Topology of the Tests"
        }, 
        {
            "location": "/software/ce-test-scaling/#batch-system-and-sleeper-pool", 
            "text": "This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator.", 
            "title": "Batch System and Sleeper Pool"
        }, 
        {
            "location": "/software/ce-test-scaling/#ce", 
            "text": "This is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications.", 
            "title": "CE"
        }, 
        {
            "location": "/software/ce-test-scaling/#submitter", 
            "text": "An HTCondor submit host. It can be a virtual machine for most test submissions.", 
            "title": "Submitter"
        }, 
        {
            "location": "/software/ce-test-scaling/#monitoring-tools", 
            "text": "To monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example:  condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const  JobStatus=?=2  | wc -l  gmetric --name RunningJobsCE", 
            "title": "Monitoring tools"
        }, 
        {
            "location": "/software/ce-test-scaling/#generating-load", 
            "text": "", 
            "title": "Generating Load"
        }, 
        {
            "location": "/software/ce-test-scaling/#location", 
            "text": "The load_generators are found in the   OSgscal github repo . The binary of interest here is  loadtest_condor", 
            "title": "Location"
        }, 
        {
            "location": "/software/ce-test-scaling/#use", 
            "text": "Just untar it or check it out from mas on the HTCondor submit node (see above):  git checkout https://github.com/efajardo/osgscal  cd load_generators/loadtest_condor/trunk/bin   Keep in mind that you also need a valid proxy for grid submissions.  For example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command:  ./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50", 
            "title": "Use"
        }, 
        {
            "location": "/software/pki-overview/", 
            "text": "OSG PKI Overview\n\n\nAbout this Doc\n\n\nThis document serves as a high-level overview of how users can interact with OIM and in turn, how OIM manages and forwards those interactions to OSG's CILogon CA.\n\n\nOverview\n\n\nClients\n\n\nOIM provides a \nREST API\n that users can interact with through the \nweb interface\n, the \nITB web interface\n, or the \nOSG PKI tools\n (henceforth referred to as \nclients\n). Clients are responsible for sending properly formatted calls to the API and handling the responses.\n\n\nIf the client sends an improperly formatted call (i.e. a bug), OIM should return a failure response.\n\n\nOIM\n\n\nOIM acts as the filter for incoming client calls: it verifies client calls (e.g. VO must be specified for host cert requests for domains that support multiple VOs ) and ensures sufficient authorization (e.g. Does the user have GridAdmin privileges for the requested domain to approve the certificate?). Certificate issuances or revocations are forwarded onto CILogon while all other calls are used to manage certificates within OIM.\n\n\nosg-gridadmin-cert-request\n\n\nosg-gridadmin-cert-request\n serves as the Grid Admin's tool of choice for acquiring host certificates: it requests, approves, and retrieves certificates with a single command. Grid Admins are able to request certificates for single hosts or a series of hosts, up to 50 at a time.\n\n\nThe command-line tool performs its own argument verification, obtains the user's SSL credentials, and ensures that the user is below quota for the request they are making. After input is verified \nosg-gridadmin-cert-request\n generates CSRs for each host and submits them to OIM.\n\n\nIf the requesting user is authenticated as a Grid Admin for the requested domain and VO (which must be specified on the command-line if multiple VOs can issue certs for the requested domain), OIM approves the request and sends the \nRequest ID#\n back to \nosg-gridadmin-cert-request\n.\n\n\nUpon successful request, \nosg-gridadmin-cert-request\n sends an approval of the certificate to OIM, which is authenticatd and authorized as before, and OIM turns around and sends a request to CILogon. CILogon issues the certificate(s), sending it back to OIM, and OIM tells \nosg-gridadmin-cert-request\n that the certificate has been issued successfully.\n\n\nAt this point, \nosg-gridadmin-cert-request\n sends a retrieval request and OIM sends the certificate(s) back to the client.\n\n\n\n\nosg-cert-request\n\n\nosg-cert-request\n is a tool for non-Grid Admin users to request host certificates, one at a time. The user can provide their own CSR or a hostname, for which a CSR is generated and forwarded to OIM with the rest of the request. OIM responds with the \nRequest ID#\n, creates a ticket, and sends e-mails to the user and the Grid Admins of the requested domain + VO.\n\n\n\n\nosg-cert-retrieve\n\n\nosg-cert-retrieve\n is used to retrieve host certificates associated with a specific \nRequest ID#\n that have been issued (i.e. approved certificate request) by a Grid Admin. OIM responds to the retrieval request with the certificate(s) of the specified \nRequest ID#\n.\n\n\n\n\nosg-cert-revoke\n\n\nosg-cert-revoke\n is a tool for Grid Admins to revoke user or host certificates. With either a \nRequest ID#\n or certificate serial number, the tool obtains the user's SSL credentials and sends the revocation request to OIM. OIM authenticates the user's request and if authorized, sends a revocation request to CILogon. If CILogon has successfully received the request, a successful response is forwarded to OIM, which is then forwarded back to the client tool. Revocation requests themselves are handled manually by CILogon CA administrators within one business day.\n\n\n\n\nosg-user-cert-renew\n\n\nosg-user-cert-renew\n is used to expiring certificates. The user provides either the certificate serial number or \nRequest ID#\n and the password that's used to encrypt their new private key. The tool combines those inputs with the user's SSL credentials to generate the renewal request that is sent to OIM, which is forwarded onto CILogon as a user cert request. If successful, OIM responds with the \nRequest ID#\n and \nosg-user-cert-renew\n sends a retrieval request to OIM. If the retrieval request succeeds, OIM responds with requested certificate.\n\n\n\n\nosg-user-cert-revoke\n\n\nosg-user-cert-revoke\n is a simple wrapper that calls \nosg-cert-revoke --user\n.\n\n\n\n\nResources\n\n\n\n\nOIM REST API\n\n\nCILogon OSG CA Design Doc", 
            "title": "PKI Overview"
        }, 
        {
            "location": "/software/pki-overview/#osg-pki-overview", 
            "text": "", 
            "title": "OSG PKI Overview"
        }, 
        {
            "location": "/software/pki-overview/#about-this-doc", 
            "text": "This document serves as a high-level overview of how users can interact with OIM and in turn, how OIM manages and forwards those interactions to OSG's CILogon CA.", 
            "title": "About this Doc"
        }, 
        {
            "location": "/software/pki-overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/software/pki-overview/#clients", 
            "text": "OIM provides a  REST API  that users can interact with through the  web interface , the  ITB web interface , or the  OSG PKI tools  (henceforth referred to as  clients ). Clients are responsible for sending properly formatted calls to the API and handling the responses.  If the client sends an improperly formatted call (i.e. a bug), OIM should return a failure response.", 
            "title": "Clients"
        }, 
        {
            "location": "/software/pki-overview/#oim", 
            "text": "OIM acts as the filter for incoming client calls: it verifies client calls (e.g. VO must be specified for host cert requests for domains that support multiple VOs ) and ensures sufficient authorization (e.g. Does the user have GridAdmin privileges for the requested domain to approve the certificate?). Certificate issuances or revocations are forwarded onto CILogon while all other calls are used to manage certificates within OIM.", 
            "title": "OIM"
        }, 
        {
            "location": "/software/pki-overview/#osg-gridadmin-cert-request", 
            "text": "osg-gridadmin-cert-request  serves as the Grid Admin's tool of choice for acquiring host certificates: it requests, approves, and retrieves certificates with a single command. Grid Admins are able to request certificates for single hosts or a series of hosts, up to 50 at a time.  The command-line tool performs its own argument verification, obtains the user's SSL credentials, and ensures that the user is below quota for the request they are making. After input is verified  osg-gridadmin-cert-request  generates CSRs for each host and submits them to OIM.  If the requesting user is authenticated as a Grid Admin for the requested domain and VO (which must be specified on the command-line if multiple VOs can issue certs for the requested domain), OIM approves the request and sends the  Request ID#  back to  osg-gridadmin-cert-request .  Upon successful request,  osg-gridadmin-cert-request  sends an approval of the certificate to OIM, which is authenticatd and authorized as before, and OIM turns around and sends a request to CILogon. CILogon issues the certificate(s), sending it back to OIM, and OIM tells  osg-gridadmin-cert-request  that the certificate has been issued successfully.  At this point,  osg-gridadmin-cert-request  sends a retrieval request and OIM sends the certificate(s) back to the client.", 
            "title": "osg-gridadmin-cert-request"
        }, 
        {
            "location": "/software/pki-overview/#osg-cert-request", 
            "text": "osg-cert-request  is a tool for non-Grid Admin users to request host certificates, one at a time. The user can provide their own CSR or a hostname, for which a CSR is generated and forwarded to OIM with the rest of the request. OIM responds with the  Request ID# , creates a ticket, and sends e-mails to the user and the Grid Admins of the requested domain + VO.", 
            "title": "osg-cert-request"
        }, 
        {
            "location": "/software/pki-overview/#osg-cert-retrieve", 
            "text": "osg-cert-retrieve  is used to retrieve host certificates associated with a specific  Request ID#  that have been issued (i.e. approved certificate request) by a Grid Admin. OIM responds to the retrieval request with the certificate(s) of the specified  Request ID# .", 
            "title": "osg-cert-retrieve"
        }, 
        {
            "location": "/software/pki-overview/#osg-cert-revoke", 
            "text": "osg-cert-revoke  is a tool for Grid Admins to revoke user or host certificates. With either a  Request ID#  or certificate serial number, the tool obtains the user's SSL credentials and sends the revocation request to OIM. OIM authenticates the user's request and if authorized, sends a revocation request to CILogon. If CILogon has successfully received the request, a successful response is forwarded to OIM, which is then forwarded back to the client tool. Revocation requests themselves are handled manually by CILogon CA administrators within one business day.", 
            "title": "osg-cert-revoke"
        }, 
        {
            "location": "/software/pki-overview/#osg-user-cert-renew", 
            "text": "osg-user-cert-renew  is used to expiring certificates. The user provides either the certificate serial number or  Request ID#  and the password that's used to encrypt their new private key. The tool combines those inputs with the user's SSL credentials to generate the renewal request that is sent to OIM, which is forwarded onto CILogon as a user cert request. If successful, OIM responds with the  Request ID#  and  osg-user-cert-renew  sends a retrieval request to OIM. If the retrieval request succeeds, OIM responds with requested certificate.", 
            "title": "osg-user-cert-renew"
        }, 
        {
            "location": "/software/pki-overview/#osg-user-cert-revoke", 
            "text": "osg-user-cert-revoke  is a simple wrapper that calls  osg-cert-revoke --user .", 
            "title": "osg-user-cert-revoke"
        }, 
        {
            "location": "/software/pki-overview/#resources", 
            "text": "OIM REST API  CILogon OSG CA Design Doc", 
            "title": "Resources"
        }, 
        {
            "location": "/software/ipv6-testing/", 
            "text": "Testing Software with IPv6\n\n\nAbout this Document\n\n\nThis document provides instructions on setting up a host with an IPv6 address for testing the OSG software stack. The plan is to be able to spin up special Fermicloud VM\u2019s that have corresponding public IPv6 addresses meaning that there will be a limit of ~15 VM\u2019s at one time.\n\n\nFor more information on IPv6, consult \nWikipedia\n.\n\n\nRequirements\n\n\n\n\nBe familiar with your institute's network policy and firewall configuration 1 \nRoot\n access is required to configure \niptables\n\n\n\n\nEnabling IPV6\n\n\n\n\n\n\nDetermine the public IPv6 address of your host (highlighted in \nred\n): \n\n\nuser@host $\n nslookup -type\n=\naaaa \nhostname\n\n\nServer:     132.239.0.252      \n\n\nAddress:    132.239.0.252#53\n\n\n\nNon-authoritative answer:\n\n\nipv6vm001.fnal.gov  has AAAA address \n2001:400:2410:29::182\n\n\n\n\n\n\nReplacing \nhostname\n with your machine's hostname.\n\n\n\n\n\n\nAsk your network administrator for your IPv6 default gateway\n\n\n\n\n\n\nModify \n/etc/sysconfig/network-scripts/ifcfg-eth0\n and be sure these lines exist, and : \n\n\nIPV6INIT=yes\nIPV6_AUTOCONF=no\nIPV6ADDR=\nIPv6 address\n\nIPV6_DEFAULTGW=\nThe IPV6 Default Gateway\n\n\n\n\n\n\nReplacing \nIPv6 address\n with the address found in step 1.\n\n\n\n\n\n\nRestart the network devices:\n\n\nroot@host #\n service network restart\n\nShutting down interface eth0:                              [  OK  ]\n\n\nShutting down loopback interface:                          [  OK  ]\n\n\nBringing up loopback interface:                            [  OK  ]\n\n\nBringing up interface eth0:                                [  OK  ]\n\n\n\n\n\n\n\n\n\n\nTesting IPv6 connectivity\n\n\nTo verify that the VM is capable of IPv6 we will be using the \nping6\n command between the test VM and another IPv6 capable machine\n\n\n\n\n\n\nFrom another IPv6 capable machine, ping your VM:\n\n\nuser@host $\n ping6 ipv6vm001.fnal.gov\n\nPING ipv6vm001.fnal.gov(ipv6vm001.fnal.gov) 56 data bytes\n\n\n64 bytes from ipv6vm001.fnal.gov: icmp_seq=1 ttl=51 time=68.1 ms\n\n\n64 bytes from ipv6vm001.fnal.gov: icmp_seq=2 ttl=51 time=57.6 ms\n\n\n\n\n\n\n\n\n\n\nFrom your test VM, ping another IPv6 capable machine (a list of IPv6 machines can be found \nhere\n):\n\n\n[efajardo@ipv6vm001 ~]#\n ping6 uaf-4.t2.ucsd.edu\n\nPING uaf-4.t2.ucsd.edu(uaf-4.t2.ucsd.edu) 56 data bytes\n\n\n64 bytes from uaf-4.t2.ucsd.edu: icmp_seq=1 ttl=51 time=57.6 ms\n\n\n\n\n\n\n\n\n\n\nVerifying SSH over IPv6\n\n\nMake sure you can login to your VM over IPv6. Currently, Fermilab's kerberos does not support SSH over IPv6.\n\n\n\n\n\n\nAdd your ssh_key to your machine and make sure \n/etc/ssh/sshd_config\n has the following lines: \n\n\nRSAAuthentication yes\nPubkeyAuthentication yes\n\n\n\n\n\n\n\n\n\nTry connecting to you IPv6 enabled machine over SSH: \n\n\nefajardo@uaf-4 ~$\n ssh -6 root@ipv6vm001.fnal.gov\n\nLast login: Wed Jun 11 14:51:47 2014 from 2607:f720:1700:1b30:21f:c6ff:feeb:2631\n\n\n[root@ipv6vm001 ~]#\n\n\n\n\n\n\n\n\n\n\nDisabling IPv4\n\n\nIf you were able to log into your VM over IPv6, you can disable IPv4 and try to communicate exclusively over IPv6.\n\n\n\n\n\n\nComment the \nIPADDR\n line in \n/etc/sysconfig/network-scripts/ifcfg-eth0\n:\n\n\n#IPADDR=131.225.41.182\nIPV6ADDR=\n2607:f720:1700:1b30::9b\n\n\n\n\n\n\n\n\nNote\n\n\nEnsure that your \nIPV6ADDR\n is uncommented otherwise you will not be able to connect to the host again\n\n\n\n\n\n\n\n\nRestart the network services: \n\n\nroot@host #\n service network restart\n\n\n\n\n\n\n\n\n\nThe ping command should no longer work: \n\n\nroot@host #\n ping ipv6vm001.fnal.gov\n\nPING ipv6vm001.fnal.gov (131.225.41.182): 56 data bytes\n\n\nRequest timeout for icmp_seq 0\n\n\nRequest timeout for icmp_seq 1\n\n\n\n\n\n\n\n\n\n\nDisabling IPv6\n\n\nIn your testing, you may find the need to disable IPv6.\n\n\nroot@host #\n sysctl -w net.ipv6.conf.all.disable_ipv6\n=\n1\n\n\nroot@host #\n service network restart\n\n\n\n\n\nThe \nping6\n command should no longer work: \n\n\nroot@host #\n ping6 ipv6vm001.fnal.gov\n\nRequest timeout for icmp_seq 0\n\n\nRequest timeout for icmp_seq 1\n\n\n\n\n\n\nTesting in mixed mode\n\n\nTo test IPv6 in mixed mode, you can use the \nntop\n tool to monitor traffic over IPv6. \nntop\n is installed on all the test machines and you can access the web interface at \nhostname:3000\n. To see a table that displays network traffic between your VM and another host by going to \nAll Protocols\n -\n \nTraffic\n and looking at the IPv6 column.\n\n\n\n\nEnforcing communication over IPv6\n\n\nIf you want to enforce IPv6 over mixed mode you can try using the IPv6 address for whatever software that you are testing. For example with xrdcp:\n\n\nroot@host #\n xrdcp -d \n1\n /tmp/first_test root://\n[\n2607\n:f720:1700:1b30::a4\n]\n:1094//tmp/first_test_8\n\n[19B/19B][100%][==================================================][0B/s] \n\n\n\n\n\n\nNotice that the IPv6 address follows \nRFC2732\n.", 
            "title": "IPv6 Testing"
        }, 
        {
            "location": "/software/ipv6-testing/#testing-software-with-ipv6", 
            "text": "", 
            "title": "Testing Software with IPv6"
        }, 
        {
            "location": "/software/ipv6-testing/#about-this-document", 
            "text": "This document provides instructions on setting up a host with an IPv6 address for testing the OSG software stack. The plan is to be able to spin up special Fermicloud VM\u2019s that have corresponding public IPv6 addresses meaning that there will be a limit of ~15 VM\u2019s at one time.  For more information on IPv6, consult  Wikipedia .", 
            "title": "About this Document"
        }, 
        {
            "location": "/software/ipv6-testing/#requirements", 
            "text": "Be familiar with your institute's network policy and firewall configuration 1  Root  access is required to configure  iptables", 
            "title": "Requirements"
        }, 
        {
            "location": "/software/ipv6-testing/#enabling-ipv6", 
            "text": "Determine the public IPv6 address of your host (highlighted in  red ):   user@host $  nslookup -type = aaaa  hostname  Server:     132.239.0.252        Address:    132.239.0.252#53  Non-authoritative answer:  ipv6vm001.fnal.gov  has AAAA address  2001:400:2410:29::182   Replacing  hostname  with your machine's hostname.    Ask your network administrator for your IPv6 default gateway    Modify  /etc/sysconfig/network-scripts/ifcfg-eth0  and be sure these lines exist, and :   IPV6INIT=yes\nIPV6_AUTOCONF=no\nIPV6ADDR= IPv6 address \nIPV6_DEFAULTGW= The IPV6 Default Gateway   Replacing  IPv6 address  with the address found in step 1.    Restart the network devices:  root@host #  service network restart Shutting down interface eth0:                              [  OK  ]  Shutting down loopback interface:                          [  OK  ]  Bringing up loopback interface:                            [  OK  ]  Bringing up interface eth0:                                [  OK  ]", 
            "title": "Enabling IPV6"
        }, 
        {
            "location": "/software/ipv6-testing/#testing-ipv6-connectivity", 
            "text": "To verify that the VM is capable of IPv6 we will be using the  ping6  command between the test VM and another IPv6 capable machine    From another IPv6 capable machine, ping your VM:  user@host $  ping6 ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov(ipv6vm001.fnal.gov) 56 data bytes  64 bytes from ipv6vm001.fnal.gov: icmp_seq=1 ttl=51 time=68.1 ms  64 bytes from ipv6vm001.fnal.gov: icmp_seq=2 ttl=51 time=57.6 ms     From your test VM, ping another IPv6 capable machine (a list of IPv6 machines can be found  here ):  [efajardo@ipv6vm001 ~]#  ping6 uaf-4.t2.ucsd.edu PING uaf-4.t2.ucsd.edu(uaf-4.t2.ucsd.edu) 56 data bytes  64 bytes from uaf-4.t2.ucsd.edu: icmp_seq=1 ttl=51 time=57.6 ms", 
            "title": "Testing IPv6 connectivity"
        }, 
        {
            "location": "/software/ipv6-testing/#verifying-ssh-over-ipv6", 
            "text": "Make sure you can login to your VM over IPv6. Currently, Fermilab's kerberos does not support SSH over IPv6.    Add your ssh_key to your machine and make sure  /etc/ssh/sshd_config  has the following lines:   RSAAuthentication yes\nPubkeyAuthentication yes    Try connecting to you IPv6 enabled machine over SSH:   efajardo@uaf-4 ~$  ssh -6 root@ipv6vm001.fnal.gov Last login: Wed Jun 11 14:51:47 2014 from 2607:f720:1700:1b30:21f:c6ff:feeb:2631  [root@ipv6vm001 ~]#", 
            "title": "Verifying SSH over IPv6"
        }, 
        {
            "location": "/software/ipv6-testing/#disabling-ipv4", 
            "text": "If you were able to log into your VM over IPv6, you can disable IPv4 and try to communicate exclusively over IPv6.    Comment the  IPADDR  line in  /etc/sysconfig/network-scripts/ifcfg-eth0 :  #IPADDR=131.225.41.182\nIPV6ADDR= 2607:f720:1700:1b30::9b    Note  Ensure that your  IPV6ADDR  is uncommented otherwise you will not be able to connect to the host again     Restart the network services:   root@host #  service network restart    The ping command should no longer work:   root@host #  ping ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov (131.225.41.182): 56 data bytes  Request timeout for icmp_seq 0  Request timeout for icmp_seq 1", 
            "title": "Disabling IPv4"
        }, 
        {
            "location": "/software/ipv6-testing/#disabling-ipv6", 
            "text": "In your testing, you may find the need to disable IPv6.  root@host #  sysctl -w net.ipv6.conf.all.disable_ipv6 = 1  root@host #  service network restart  The  ping6  command should no longer work:   root@host #  ping6 ipv6vm001.fnal.gov Request timeout for icmp_seq 0  Request timeout for icmp_seq 1", 
            "title": "Disabling IPv6"
        }, 
        {
            "location": "/software/ipv6-testing/#testing-in-mixed-mode", 
            "text": "To test IPv6 in mixed mode, you can use the  ntop  tool to monitor traffic over IPv6.  ntop  is installed on all the test machines and you can access the web interface at  hostname:3000 . To see a table that displays network traffic between your VM and another host by going to  All Protocols  -   Traffic  and looking at the IPv6 column.", 
            "title": "Testing in mixed mode"
        }, 
        {
            "location": "/software/ipv6-testing/#enforcing-communication-over-ipv6", 
            "text": "If you want to enforce IPv6 over mixed mode you can try using the IPv6 address for whatever software that you are testing. For example with xrdcp:  root@host #  xrdcp -d  1  /tmp/first_test root:// [ 2607 :f720:1700:1b30::a4 ] :1094//tmp/first_test_8 [19B/19B][100%][==================================================][0B/s]    Notice that the IPv6 address follows  RFC2732 .", 
            "title": "Enforcing communication over IPv6"
        }, 
        {
            "location": "/policy/software-support/", 
            "text": "Software Team Support\n\n\nIncoming tickets (Operations staff)\n\n\nWhen a ticket arrives at the GOC and the Operations staff member decides that the ticket should be assigned to the Software Team, the operations staff member will do two things:\n\n\n\n\nThe ticket will be assigned to a pseudo-user called \"Software\".\n\n\nThe \"Next Action\" field will be set to \"SOFTWARE TRIAGE\".\n\n\nThe Software pseudo-user has an email list as its \"personal\" email address. This is: \n.\n\n\n\n\nTriage duty (Technology Area staff)\n\n\nAll OSG Technology Area Team members who are at least 50% on the will share triage duty (except Brian Bockelman). Each week (Monday through Friday), during normal work hours, there will be one person on triage duty. If you are on triage duty, this means:\n\n\n\n\nWatch the software incoming tickets. \nIf a ticket has not been assigned to a software team member, you assign it appropriately.\n You are responsible for assigning all incoming tickets that haven't been assigned. This includes tickets that have arrived over the weekend or were not handled by the previous person on triage duty.\n\n\nIf you can handle an incoming ticket, assign it to yourself and handle it. Leave the \"software\" user assigned to the ticket. Many tickets are common problems that most team members should be able to solve.\n\n\nIf you cannot handle an incoming ticket, collect initial details (versions, logs, etc...), and assign the ticket to the most appropriate software team member. Where appropriate, include people from other teams (i.e. security, operations, glidein...) Leave the \"software\" user assigned to the ticket.\n\n\nLook at assigned tickets. For tickets that are not being handled in a timely fashion, please remind the person that owns the ticket, or, if the ticket is waiting on the user, remind the user.\n\n\n\n\n\n\nNote\n\n\nBeing on triage duty does \nnot\n mean that you must personally solve all new tickets. It means that you handle the easy tickets and assign the other tickets appropriately.\n\n\n\n\nNote that the software pseudo-user has an email address that is a mailing list: \n. If you find it convenient, you can sign up for the mailing list to see all the incoming tickets. Alain recommends you do this during your triage duty, but you do not need to stay subscribed when you are not on triage duty.\n\n\nAll the currently opened tickets assigned to the software team can be seen here: \nGOC Open Tickets\n\n\nFlow of tickets\n\n\nNote that if you follow the above, we will end up with three assignees to each ticket. This is the overall flow:\n\n\n\n\nA ticket arrives at the GOC, either via the ticket creator, or created by Operations in response to an email.\n\n\nASSIGNMENT #1:\n The ticket is assigned to an Operations member. They're in charge of ushering the ticket through its whole lifetime, though for software tickets they won't do a whole lot on the technical work. Note that some software tickets may not be assigned to us, because they might assign them to the VO support center. This is good.\n\n\nASSIGNMENT #2:\n The Operations member looks at the ticket and decides it's a software ticket. (They might do some upfront work if they can.) They then assign it to \"Software Support (Triage)\". We now have two people assigned to the ticket.\n\n\nWhen assigned to \"Software Support (Triage)\", all changes to the ticket are sent to \n, so we leave this pseudo-person on the ticket. Watching the email to this mailing list is a nice (but optional) way for you to see what's happening when you're on triage duty.\n\n\n\n\nASSIGNMENT #3:\n The person on triage duty assigns it to the right person from the software team. We now have three assignees:\n\n\n\n\nGOC member\n\n\nSoftware Support (Triage)\n\n\nThe Technology team member who is responsible for guiding a ticket until it is resolved\n\n\n\n\nTo avoid any confusion around ticket ownership, assign only one Technology team member per ticket.\nIf the expertise of another Technology team member is needed on the ticket, add them to the CC list.\nInasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people.\n\n\n\n\n\n\nLCMAPS VOMS Transition\n\n\nThis section contains the process for helping sites transition from edg-mkgridmap or GUMS to the LCMAPS VOMS plugin as\npart of the \nVOMS Admin Server retirement\n.\n\n\n\n\n\n\nAsk if the site is using edg-mkgridmap or GUMS. If they're using GUMS, find out what GUMS clients they have at their \n   site. Possibilities include:\n\n\n\n\nHTCondor-CE\n\n\nGridFTP\n\n\nXRootD: if an ATLAS site, they use vomsxrd/xrootd-voms-plugin\n\n\ndCache: if an ATLAS site, encourage them to consult US-ATLAS mailing lists\n\n\nBeStMan: encourage the site to transition to GridFTP, as they cannot retire GUMS until BeStMan doesn't depend on it.\n  If CMS/ATLAS, encourage them to consult their US-ATLAS/US-CMS mailing lists for assistance.\n\n\n\n\n\n\n\n\nAsk them to follow the relevant instructions for their authorization solution \n\n\n\n\nedg-mkgridmap\n\n\nGUMS\n\n\n\n\n\n\n\n\nAfter they've completed the above instructions, use one of the following methods (in order of preference) to verify \n   LCMAPS VOMS mappings:\n\n\n\n\n\n\nIf the host is a CE, verify that they are still receiving pilots:\n\n\n\n\n\n\nQuery their CE directly:\n\n\n$\n condor_q -name \nCE HOSTNAME\n -pool \nCE HOSTNAME\n:9619\n\n\n\n\n\nThis may not work if the site has a strict firewall or do not run an HTCondor-CE.\n\n\n\n\n\n\nAdd factory ops or the relevant ATLAS (T2 vs T3) support center to the ticket under \nOSG Support Centers\n\n\n\n\nFor non-ATLAS sites, verify that the site's \npilot numbers\n\n       are non-zero.\n\n\n\n\n\n\n\n\nIf the host is a GridFTP server, verify file transfer with their VO support center.\n\n\n\n\n\n\n\n\n\n\nUpdating the triage calendar\n\n\nThe calendar is hosted on Tim Cartwright\u2019s Google Calendar account. If you need privileges to edit, ask Brian L. To update\n\n\n\n\nUpdate checkout (\nGitHub\n)\n\n\nGenerate next rotation:\n./triage.py --generateNextRotation \n rotation.txt\n\n\n\n\n\n\n\nCheck and update assignments according to team member outages\n\n\n\n\nLoad triage assignments into Google Calendar:\n\n\n./triage.py --load rotation.txt\n\n\n\n\n\n\n\n\n\n\nTo subscribe to this calendar in your calendar program, use the iCal URL: \nhttps://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics\n\n\n\n\n\nHandling tickets\n\n\n\n\nWe need to take good care of our users. We are in a small community. Please be friendly and patient even when the user is frustrated or lacking in knowledge.\n\n\nAlways sign your ticket with your full name, so people know who is responding.\n\n\nIf it's easy for you, include a signature at the bottom of your response.\n\n\nRemember that you can tell people to use the \nosg-system-profiler\n to collect information. It can shorten the number of times you ask for information because it collects quite a bit for you.\n\n\nIf you run across a problem that has a chance of being hit by other users, consider:\n\n\nIs there a bug we should fix in the software? Or something we could improve in the software?\n\n\nIs there a way to improve our documentation?\n\n\nCan you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users.\n\n\n\n\n\n\n\n\nDirect E-mail vs. Support\n\n\nIf someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are:\n\n\n\n\nIf it's easy to handle and you can definitely do it yourself, leave it in email.\n\n\nIf there's a chance that you can't do it in a timely fashion, turn it into a ticket.\n\n\nIf there's a chance that you might lose track of the email, turn it into a ticket.\n\n\nIf there's a chance that you might need help from others, turn it into a ticket.\n\n\nIf it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket.\n\n\n\n\nGOC vs JIRA\n\n\nJIRA is for tracking our work.\nIt's meant for internal usage, not for user support.\nIn general, users should not ask for support via JIRA.\nA single user support ticket might result in zero, one, or multiple JIRA tickets.\n\n\nGOC tickets are for user support.\nThis is where we help users debug, understand their problems, etc.\n\n\nIf actionable software team tasks arise from a GOC ticket, JIRA ticket(s) should be created to track that work. \nResultant JIRA tickets should:\n\n\n\n\nInclude a link to the original GOC ticket, a description of the problem, and a proposed solution to the problem.\n\n\nAdd the original reporter as a watcher if they have a JIRA account.\n\n\n\n\nWhen all the relevant JIRA tickets are created, ask the user if they would be ok with tracking the issue via JIRA. \nIf they say yes, close the GOC ticket.", 
            "title": "Software Support"
        }, 
        {
            "location": "/policy/software-support/#software-team-support", 
            "text": "", 
            "title": "Software Team Support"
        }, 
        {
            "location": "/policy/software-support/#incoming-tickets-operations-staff", 
            "text": "When a ticket arrives at the GOC and the Operations staff member decides that the ticket should be assigned to the Software Team, the operations staff member will do two things:   The ticket will be assigned to a pseudo-user called \"Software\".  The \"Next Action\" field will be set to \"SOFTWARE TRIAGE\".  The Software pseudo-user has an email list as its \"personal\" email address. This is:  .", 
            "title": "Incoming tickets (Operations staff)"
        }, 
        {
            "location": "/policy/software-support/#triage-duty-technology-area-staff", 
            "text": "All OSG Technology Area Team members who are at least 50% on the will share triage duty (except Brian Bockelman). Each week (Monday through Friday), during normal work hours, there will be one person on triage duty. If you are on triage duty, this means:   Watch the software incoming tickets.  If a ticket has not been assigned to a software team member, you assign it appropriately.  You are responsible for assigning all incoming tickets that haven't been assigned. This includes tickets that have arrived over the weekend or were not handled by the previous person on triage duty.  If you can handle an incoming ticket, assign it to yourself and handle it. Leave the \"software\" user assigned to the ticket. Many tickets are common problems that most team members should be able to solve.  If you cannot handle an incoming ticket, collect initial details (versions, logs, etc...), and assign the ticket to the most appropriate software team member. Where appropriate, include people from other teams (i.e. security, operations, glidein...) Leave the \"software\" user assigned to the ticket.  Look at assigned tickets. For tickets that are not being handled in a timely fashion, please remind the person that owns the ticket, or, if the ticket is waiting on the user, remind the user.    Note  Being on triage duty does  not  mean that you must personally solve all new tickets. It means that you handle the easy tickets and assign the other tickets appropriately.   Note that the software pseudo-user has an email address that is a mailing list:  . If you find it convenient, you can sign up for the mailing list to see all the incoming tickets. Alain recommends you do this during your triage duty, but you do not need to stay subscribed when you are not on triage duty.  All the currently opened tickets assigned to the software team can be seen here:  GOC Open Tickets", 
            "title": "Triage duty (Technology Area staff)"
        }, 
        {
            "location": "/policy/software-support/#flow-of-tickets", 
            "text": "Note that if you follow the above, we will end up with three assignees to each ticket. This is the overall flow:   A ticket arrives at the GOC, either via the ticket creator, or created by Operations in response to an email.  ASSIGNMENT #1:  The ticket is assigned to an Operations member. They're in charge of ushering the ticket through its whole lifetime, though for software tickets they won't do a whole lot on the technical work. Note that some software tickets may not be assigned to us, because they might assign them to the VO support center. This is good.  ASSIGNMENT #2:  The Operations member looks at the ticket and decides it's a software ticket. (They might do some upfront work if they can.) They then assign it to \"Software Support (Triage)\". We now have two people assigned to the ticket.  When assigned to \"Software Support (Triage)\", all changes to the ticket are sent to  , so we leave this pseudo-person on the ticket. Watching the email to this mailing list is a nice (but optional) way for you to see what's happening when you're on triage duty.   ASSIGNMENT #3:  The person on triage duty assigns it to the right person from the software team. We now have three assignees:   GOC member  Software Support (Triage)  The Technology team member who is responsible for guiding a ticket until it is resolved   To avoid any confusion around ticket ownership, assign only one Technology team member per ticket.\nIf the expertise of another Technology team member is needed on the ticket, add them to the CC list.\nInasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people.", 
            "title": "Flow of tickets"
        }, 
        {
            "location": "/policy/software-support/#lcmaps-voms-transition", 
            "text": "This section contains the process for helping sites transition from edg-mkgridmap or GUMS to the LCMAPS VOMS plugin as\npart of the  VOMS Admin Server retirement .    Ask if the site is using edg-mkgridmap or GUMS. If they're using GUMS, find out what GUMS clients they have at their \n   site. Possibilities include:   HTCondor-CE  GridFTP  XRootD: if an ATLAS site, they use vomsxrd/xrootd-voms-plugin  dCache: if an ATLAS site, encourage them to consult US-ATLAS mailing lists  BeStMan: encourage the site to transition to GridFTP, as they cannot retire GUMS until BeStMan doesn't depend on it.\n  If CMS/ATLAS, encourage them to consult their US-ATLAS/US-CMS mailing lists for assistance.     Ask them to follow the relevant instructions for their authorization solution    edg-mkgridmap  GUMS     After they've completed the above instructions, use one of the following methods (in order of preference) to verify \n   LCMAPS VOMS mappings:    If the host is a CE, verify that they are still receiving pilots:    Query their CE directly:  $  condor_q -name  CE HOSTNAME  -pool  CE HOSTNAME :9619  This may not work if the site has a strict firewall or do not run an HTCondor-CE.    Add factory ops or the relevant ATLAS (T2 vs T3) support center to the ticket under  OSG Support Centers   For non-ATLAS sites, verify that the site's  pilot numbers \n       are non-zero.     If the host is a GridFTP server, verify file transfer with their VO support center.", 
            "title": "LCMAPS VOMS Transition"
        }, 
        {
            "location": "/policy/software-support/#updating-the-triage-calendar", 
            "text": "The calendar is hosted on Tim Cartwright\u2019s Google Calendar account. If you need privileges to edit, ask Brian L. To update   Update checkout ( GitHub )  Generate next rotation: ./triage.py --generateNextRotation   rotation.txt   Check and update assignments according to team member outages   Load triage assignments into Google Calendar:  ./triage.py --load rotation.txt      To subscribe to this calendar in your calendar program, use the iCal URL:  https://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics", 
            "title": "Updating the triage calendar"
        }, 
        {
            "location": "/policy/software-support/#handling-tickets", 
            "text": "We need to take good care of our users. We are in a small community. Please be friendly and patient even when the user is frustrated or lacking in knowledge.  Always sign your ticket with your full name, so people know who is responding.  If it's easy for you, include a signature at the bottom of your response.  Remember that you can tell people to use the  osg-system-profiler  to collect information. It can shorten the number of times you ask for information because it collects quite a bit for you.  If you run across a problem that has a chance of being hit by other users, consider:  Is there a bug we should fix in the software? Or something we could improve in the software?  Is there a way to improve our documentation?  Can you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users.", 
            "title": "Handling tickets"
        }, 
        {
            "location": "/policy/software-support/#direct-e-mail-vs-support", 
            "text": "If someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are:   If it's easy to handle and you can definitely do it yourself, leave it in email.  If there's a chance that you can't do it in a timely fashion, turn it into a ticket.  If there's a chance that you might lose track of the email, turn it into a ticket.  If there's a chance that you might need help from others, turn it into a ticket.  If it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket.", 
            "title": "Direct E-mail vs. Support"
        }, 
        {
            "location": "/policy/software-support/#goc-vs-jira", 
            "text": "JIRA is for tracking our work.\nIt's meant for internal usage, not for user support.\nIn general, users should not ask for support via JIRA.\nA single user support ticket might result in zero, one, or multiple JIRA tickets.  GOC tickets are for user support.\nThis is where we help users debug, understand their problems, etc.  If actionable software team tasks arise from a GOC ticket, JIRA ticket(s) should be created to track that work. \nResultant JIRA tickets should:   Include a link to the original GOC ticket, a description of the problem, and a proposed solution to the problem.  Add the original reporter as a watcher if they have a JIRA account.   When all the relevant JIRA tickets are created, ask the user if they would be ok with tracking the issue via JIRA. \nIf they say yes, close the GOC ticket.", 
            "title": "GOC vs JIRA"
        }, 
        {
            "location": "/software/effort-tracking/", 
            "text": "Effort Tracking\n\n\nThis page describes a simple plan for tracking effort in the OSG Technology teams.\n\n\nBasic Ideas\n\n\nAt its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking.\n\n\nThere are just a few simple ideas to keep in mind:\n\n\n\n\n\n\nEach week, report your effort on OSG Technology activities\n\n\nUpdate your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example:\n\n\nEFFORT\nExternal development: 63% \nSupport: 12% \nLeave: 20% \nOutside: 5\n\n\n\n\n\n\n\n\n\nFollow standard federal regulations for calculating effort\n (e.g., OMB Circular A-21)\n\n\nThe main idea is that \nall\n of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%.\n\n\nReport 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.)\n\n\nIf you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see.\n\n\n\n\n\n\nEffort is reported as \ninteger\n percentages, no less accurate than 5% intervals\n \n     So please do not report percentages like 43.21% and please do not round to the nearest 10%.\n\n\n\n\n\n\nEffort Categories\n\n\nHere are the categories in which to track effort:\n\n\n\n\n\n\n\n\nInvestigations\n\n\nWork on the Investigations team\n\n\n\n\n\n\n\n\n\n\nExternal\n\n\nSoftware work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components\n\n\n\n\n\n\nInternal\n\n\nSoftware work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances\n\n\n\n\n\n\nDocumentation\n\n\nWork on our TWiki or Markdown documentation\n\n\n\n\n\n\nRelease\n\n\nRelease team activities, primarily acceptance testing and cutting releases\n\n\n\n\n\n\nSupport\n\n\nUser support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support.\n\n\n\n\n\n\nManagement\n\n\nThis is mainly for team leads; e.g., managing team activities and tickets (generally); hiring; \nleading\n (not just attending) meetings\n\n\n\n\n\n\nEducation\n\n\nNot for general learning or training activities\n The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities.\n\n\n\n\n\n\nAdmin\n\n\nGeneral administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014 \nuse sparingly!!\n\n\n\n\n\n\nOutside\n\n\nFor all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL)\n\n\n\n\n\n\nLeave\n\n\nThis is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours\n\n\n\n\n\n\n\n\nA few thoughts about tricky situations:\n\n\n\n\n\n\nMeetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time.\n\n\n\n\n\n\nAdministrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between \nAdmin\n and \nOutside\n according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities.\n\n\n\n\n\n\nOutside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not.\n\n\n\n\n\n\nLearning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into \nInternal\n. But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as \nAdmin\n, and maybe add a comment explaining the activity.\n\n\n\n\n\n\nUltimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.", 
            "title": "Effort Tracking"
        }, 
        {
            "location": "/software/effort-tracking/#effort-tracking", 
            "text": "This page describes a simple plan for tracking effort in the OSG Technology teams.", 
            "title": "Effort Tracking"
        }, 
        {
            "location": "/software/effort-tracking/#basic-ideas", 
            "text": "At its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking.  There are just a few simple ideas to keep in mind:    Each week, report your effort on OSG Technology activities  Update your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example:  EFFORT\nExternal development: 63% \nSupport: 12% \nLeave: 20% \nOutside: 5    Follow standard federal regulations for calculating effort  (e.g., OMB Circular A-21)  The main idea is that  all  of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%.  Report 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.)  If you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see.    Effort is reported as  integer  percentages, no less accurate than 5% intervals  \n     So please do not report percentages like 43.21% and please do not round to the nearest 10%.", 
            "title": "Basic Ideas"
        }, 
        {
            "location": "/software/effort-tracking/#effort-categories", 
            "text": "Here are the categories in which to track effort:     Investigations  Work on the Investigations team      External  Software work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components    Internal  Software work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances    Documentation  Work on our TWiki or Markdown documentation    Release  Release team activities, primarily acceptance testing and cutting releases    Support  User support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support.    Management  This is mainly for team leads; e.g., managing team activities and tickets (generally); hiring;  leading  (not just attending) meetings    Education  Not for general learning or training activities  The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities.    Admin  General administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014  use sparingly!!    Outside  For all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL)    Leave  This is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours     A few thoughts about tricky situations:    Meetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time.    Administrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between  Admin  and  Outside  according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities.    Outside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not.    Learning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into  Internal . But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as  Admin , and maybe add a comment explaining the activity.    Ultimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.", 
            "title": "Effort Categories"
        }, 
        {
            "location": "/software/release-planning/", 
            "text": "Plans for Future Releases\n\n\nThis informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see \nthis page\n.\n\n\nUnless explicitly noted, bullet points refer to software in the release repo.\n\n\nThis page is not meant to track minor bugfixes or updates -- rather, its focus should be new features.\n\n\nOSG 3.4 (May 2017)\n\n\n\n\n\n\n\n\nPackage(s)\n\n\nChange in osg-release\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\nDrop\n\n\nRetirement policy\n\n\n\n\n\n\nedg-mkgridmap\n\n\nDrop\n\n\nSOFTWARE-2600\n\n\n\n\n\n\nfrontier-squid\n\n\nModify\n\n\nVersion 3\n\n\n\n\n\n\nglexec\n\n\nDrop\n\n\nSOFTWARE-2620\n\n\n\n\n\n\nGRAM\n\n\nDrop\n\n\nSOFTWARE-2530\n\n\n\n\n\n\nGUMS\n\n\nDrop\n\n\nRetirement policy\n, \nSOFTWARE-2600\n\n\n\n\n\n\njglobus\n\n\nDrop\n\n\nSOFTWARE-2606\n\n\n\n\n\n\nnetlogger\n\n\nDrop\n\n\n\n\n\n\n\n\nosg-ce\n\n\nModify\n\n\nDrop \nGridFTP\n, \ngums-client\n\n\n\n\n\n\nosg-info-services\n\n\nDrop\n\n\n\n\n\n\n\n\nosg-version\n\n\nDrop\n\n\n\n\n\n\n\n\nsingularity\n\n\nAdd\n\n\n\n\n\n\n\n\nvoms-admin-server\n\n\nDrop\n\n\nRetirement policy\n\n\n\n\n\n\n\n\nTrack OSG 3.4 updates through its \nJIRA epic\n.\n\n\nSupport Policy for OSG 3.3\n\n\nAccording to our \nrelease support policy\n and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018.\n\n\nPrevious Releases\n\n\n12 November 2013\n\n\n\n\nOSG 3.1\n\n\nHTCondor-CE with PBS\n\n\nosg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change)\n\n\n\n\n\n\nOSG 3.2\n\n\nInitial release (\nhow to create\n)\n\n\nHDFS 2.0.0 (already done in Upcoming)\n\n\nHTCondor 8.0.4\n\n\nglideinWMS 3.2.0\n\n\nosg-info-services (Note: ReSS will likely be retired around year-end)\n\n\nOSG 3.1 updates\n\n\n\n\n\n\nUpcoming\n\n\nHTCondor 8.1 with unified RPM\n\n\nBOSCO\n\n\n\n\n\n\n\n\n10 December 2013\n\n\n\n\nOSG 3.2\n\n\nRSV-for-VOs\n\n\nSquid must be present on OSG-CE (??? what does this mean?)", 
            "title": "Release Planning"
        }, 
        {
            "location": "/software/release-planning/#plans-for-future-releases", 
            "text": "This informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see  this page .  Unless explicitly noted, bullet points refer to software in the release repo.  This page is not meant to track minor bugfixes or updates -- rather, its focus should be new features.", 
            "title": "Plans for Future Releases"
        }, 
        {
            "location": "/software/release-planning/#osg-34-may-2017", 
            "text": "Package(s)  Change in osg-release  Notes      BeStMan2  Drop  Retirement policy    edg-mkgridmap  Drop  SOFTWARE-2600    frontier-squid  Modify  Version 3    glexec  Drop  SOFTWARE-2620    GRAM  Drop  SOFTWARE-2530    GUMS  Drop  Retirement policy ,  SOFTWARE-2600    jglobus  Drop  SOFTWARE-2606    netlogger  Drop     osg-ce  Modify  Drop  GridFTP ,  gums-client    osg-info-services  Drop     osg-version  Drop     singularity  Add     voms-admin-server  Drop  Retirement policy     Track OSG 3.4 updates through its  JIRA epic .", 
            "title": "OSG 3.4 (May 2017)"
        }, 
        {
            "location": "/software/release-planning/#support-policy-for-osg-33", 
            "text": "According to our  release support policy  and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018.", 
            "title": "Support Policy for OSG 3.3"
        }, 
        {
            "location": "/software/release-planning/#previous-releases", 
            "text": "", 
            "title": "Previous Releases"
        }, 
        {
            "location": "/software/release-planning/#12-november-2013", 
            "text": "OSG 3.1  HTCondor-CE with PBS  osg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change)    OSG 3.2  Initial release ( how to create )  HDFS 2.0.0 (already done in Upcoming)  HTCondor 8.0.4  glideinWMS 3.2.0  osg-info-services (Note: ReSS will likely be retired around year-end)  OSG 3.1 updates    Upcoming  HTCondor 8.1 with unified RPM  BOSCO", 
            "title": "12 November 2013"
        }, 
        {
            "location": "/software/release-planning/#10-december-2013", 
            "text": "OSG 3.2  RSV-for-VOs  Squid must be present on OSG-CE (??? what does this mean?)", 
            "title": "10 December 2013"
        }, 
        {
            "location": "/software/new-team-member/", 
            "text": "Setup Instructions for New Team Members\n\n\n\n\nComputing account at FNAL\n\n\nTo get this, follow the instructions at \nhttps://fermi.service-now.com/kb_view.do?sysparm_article=KB0010797\n\n\n\n\n\n\nssh access to a UW CompSci account, including AFS access\n\n\nSend email to Tim C with top 3 requested usernames\n\n\n\n\n\n\nRead/write access to the UW Subversion repository;\n\n\nSend email to Mat or Tim C\n\n\n\n\n\n\nUser certificate\n\n\nObtain a user certificate here: \nhttps://oim.opensciencegrid.org/oim/certificate\n\n\nImport the certificate into your browser of choice\n\n\n\n\n\n\nAccess to FermiCloud\n\n\nhttp://fclweb.fnal.gov/\n\n\n\n\n\n\n\n\nRegister\n for a GGUS account with the following information:\n\n\n\n\nYour certificate's subject DN\n\n\nSelect \nnone\n from the \"Virtual Organization\" drop-down\n\n\nSelect \nyes\n for \"Do you want to have support access?\" and answer \"Why?\" with the following:\nYes, I need to comment on tickets as a member of the OSG Software \n Release Team\n(https://opensciencegrid.github.io/technology/#the-team)\n\n\n\n\n\n\n\n\n\n\n\n\n\nJira ticket system\n\n\n\n\nSend email to \n and request access to JIRA\n\n\n\n\n\n\nAccess to Koji\n\n\nFollow the instructions on the \nKoji user management page\n\n\n\n\n\n\nSign up for mailing lists\n\n\nosg-software@opensciencegrid.org\n\n\nosg-general@opensciencegrid.org\n\n\nosg-sites@opensciencegrid.org\n\n\nosg-commits@cs.wisc.edu\n\n\n\n\n\n\nGitHub team membership\n\n\nhttps://github.com/orgs/opensciencegrid/teams/software-and-release/members\n\n\n\n\n\n\nUNL repository access\n\n\nSend SSH public key to Tim T, Derek, or Brian L to gain access to the UNL repository (osgcollab@hcc-osg-software.unl.edu)", 
            "title": "New Team Member"
        }, 
        {
            "location": "/software/new-team-member/#setup-instructions-for-new-team-members", 
            "text": "Computing account at FNAL  To get this, follow the instructions at  https://fermi.service-now.com/kb_view.do?sysparm_article=KB0010797    ssh access to a UW CompSci account, including AFS access  Send email to Tim C with top 3 requested usernames    Read/write access to the UW Subversion repository;  Send email to Mat or Tim C    User certificate  Obtain a user certificate here:  https://oim.opensciencegrid.org/oim/certificate  Import the certificate into your browser of choice    Access to FermiCloud  http://fclweb.fnal.gov/     Register  for a GGUS account with the following information:   Your certificate's subject DN  Select  none  from the \"Virtual Organization\" drop-down  Select  yes  for \"Do you want to have support access?\" and answer \"Why?\" with the following: Yes, I need to comment on tickets as a member of the OSG Software   Release Team\n(https://opensciencegrid.github.io/technology/#the-team)      Jira ticket system   Send email to   and request access to JIRA    Access to Koji  Follow the instructions on the  Koji user management page    Sign up for mailing lists  osg-software@opensciencegrid.org  osg-general@opensciencegrid.org  osg-sites@opensciencegrid.org  osg-commits@cs.wisc.edu    GitHub team membership  https://github.com/orgs/opensciencegrid/teams/software-and-release/members    UNL repository access  Send SSH public key to Tim T, Derek, or Brian L to gain access to the UNL repository (osgcollab@hcc-osg-software.unl.edu)", 
            "title": "Setup Instructions for New Team Members"
        }, 
        {
            "location": "/release/cut-sw-release/", 
            "text": "Note\n\n\nIf you are performing a data release, please follow the instructions \nhere\n\n\n\n\nHow to Cut a Software Release\n\n\nThis document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found \nhere\n.\n\n\nDue to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.\n\n\nRequirements\n\n\n\n\nUser certificate registered with OSG's koji with build and release team privileges\n\n\nAn account on UW CS machines (e.g. \nlibrary\n, \ningwe\n) to access UW's AFS\n\n\nrelease-tools\n scripts in your \nPATH\n (\nGitHub\n)\n\n\nosg-build\n scripts in your \nPATH\n (installed via OSG yum repos or \nsource\n)\n\n\nAccess to the tarball repository at UNL (osgcollab@hcc-osg-software.unl.edu)\n\n\n\n\nPick the Version Number\n\n\nThe rest of this document makes references to \nVERSION(S)\n and \nNON-UPCOMING VERSIONS(S)\n, which refer to a space-delimited list of OSG version(s) and that same list minus \nupcoming\n (e.g. \n3.3.28 3.4.3 upcoming\n and \n3.3.28 3.4.3\n). If you are unsure about either the version or revision, please consult the release manager.\n\n\nDay 0: Generate Preliminary Release List\n\n\nThe release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release.\n\n\nStep 1: Update the osg-version RPM\n\n\nFor each release (excluding upcoming), update the version number in the osg-version RPM's spec file and build it in koji:\n\n\n# If building for the latest release out of trunk\n\nosg-build koji osg-version\n\n# If building for an older release out of a branch:\n\n\nMAJOR_VERSION\n=\nMAJOR VERSION\n\nosg-build koji --repo\n=\n$MAJOR_VERSION\n osg-version\n\n\n\n\n\nWhere \nMAJOR VERSION\n is of the format \nx.y\n (e.g. \n3.2\n).\n\n\nStep 2: Promote osg-version and generate the release list\n\n\nRun \n0-generate-pkg-list\n from a machine that has your koji-registered user certificate:\n\n\nVERSIONS\n=\nVERSION(S)\n\n\n\n\n\n\ngit clone https://github.com/opensciencegrid/release-tools.git\n\ncd\n release-tools\n\n0\n-generate-pkg-list \n$VERSIONS\n\n\n\n\n\n\nDay 1: Verify Pre-Release and Generate Tarballs\n\n\nThis section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs.\n\n\nStep 1: Verify Pre-Release\n\n\nCompare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated \nrelease-list\n in git). To do this, run the \n1-verify-prerelease\n script from git:\n\n\nVERSIONS\n=\nVERSION(S)\n\n\n\n\n\n\n1\n-verify-prerelease \n$VERSIONS\n\n\n\n\n\n\nIf there are any discrepancies consult the release manager. You may have to tag or untag packages with the \nosg-koji\n tool.\n\n\n\n\nNote\n\n\nPlease verify that the \nosg-version\n RPM is in your set of packages for the release! Also verify that if there is a new version of the \nosg-tested-internal\n RPM, then it is included in the release as well!\n\n\n\n\nStep 2: Test Pre-Release in VM Universe\n\n\nTo test pre-release, you will be kicking off a manual VM universe test run from \nosghost.chtc.wisc.edu\n.\n\n\n\n\nEnsure that you meet the \npre-requisites\n for submitting VM universe test runs\n\n\n\n\nPrepare the test suite by running:\n\n\nosg-run-tests \nTesting OSG pre-release\n\n\n\n\n\n\n\n\n\n\ncd\n into the directory specified in the output of the previous command\n\n\n\n\ncd\n into \nparameters.d\n and remove all files within it except for \nosg33.yaml\n and \nosg34.yaml\n\n\n\n\nEdit \nosg33.yaml\n so that it reads:\n\n\nplatforms\n:\n\n  \n-\n \ncentos_6_x86_64\n\n  \n-\n \nrhel_6_x86_64\n\n  \n-\n \nsl_6_x86_64\n\n  \n-\n \ncentos_7_x86_64\n\n  \n-\n \nrhel_7_x86_64\n\n  \n-\n \nsl_7_x86_64\n\n\n\nsources\n:\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n3\n;\n \nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n3\n;\n \nosg\n \n \nosg-prerelease\n\n\n\npackage_sets\n:\n\n  \n-\n \nlabel\n:\n \nAll\n \n(\njava\n)\n\n    \nselinux\n:\n \nTrue\n\n    \nosg_java\n:\n \nTrue\n\n    \nrng\n:\n \nTrue\n\n    \npackages\n:\n\n      \n-\n \nosg-tested-internal\n\n\n\n\n\n\n\n\n\n\nEdit \nosg34.yaml\n so that it reads:\n\n\nplatforms\n:\n\n  \n-\n \ncentos_6_x86_64\n\n  \n-\n \nrhel_6_x86_64\n\n  \n-\n \nsl_6_x86_64\n\n  \n-\n \ncentos_7_x86_64\n\n  \n-\n \nrhel_7_x86_64\n\n  \n-\n \nsl_7_x86_64\n\n\n\nsources\n:\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg\n \n \nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n3\n;\n \nosg\n \n \n3\n.\n4\n/\nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg-prerelease\n,\n \nosg-upcoming-prerelease\n,\n \nosg-upcoming\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg\n \n \nosg-prerelease\n,\n \nosg-upcoming-prerelease\n,\n \nosg-upcoming\n\n\n\npackage_sets\n:\n\n  \n-\n \nlabel\n:\n \nAll\n\n    \nselinux\n:\n \nTrue\n\n    \nosg_java\n:\n \nFalse\n\n    \npackages\n:\n\n      \n-\n \nosg-tested-internal\n\n\n\n\n\n\nIf you are not releasing packages into \nupcoming\n, delete the \nupcoming\n-related lines in the \nsources\n section.\n\n\n\n\n\n\ncd\n back into the root directory of the test run (e.g. \ncd ..\n)\n\n\n\n\nSubmit the DAG:\ncondor_submit_dag master-run.dag\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf there are failures, consult the release-manager before proceeding.\n\n\n\n\nStep 3: Test Pre-Release on the Madison ITB site\n\n\nTest the pre-release on the Madison ITB by following the \nITB pre-release testing instructions\n.\nIf you not local to Madison, consult the release manager for the designated person to do this testing.\n\n\nStep 4: Regenerate the build repositories\n\n\nTo avoid 404 errors when retrieving packages, it's necessary to regenerate the build repositories. Run the following script from a machine with your koji-registered user certificate:\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\n1\n-regen-repos \n$NON_UPCOMING_VERSIONS\n\n\n\n\n\n\nStep 5: Create the client tarballs\n\n\nCreate the client tarballs as root on an EL7 fermicloud machine using the relevant script from git:\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\ngit clone https://github.com/opensciencegrid/release-tools.git\n\ncd\n release-tools\n./1-client-tarballs \n$NON_UPCOMING_VERSIONS\n\n\n\n\n\n\nStep 6: Briefly test the client tarballs\n\n\nAs an \nunprivileged user\n, extract each tarball into a separate directory. Make sure osg-post-install works. Make sure \nosgrun osg-version\n works by running the following tests, replacing \nNON-UPCOMING VERSION(S)\n with the appropriate version numbers:\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\ndotest \n()\n \n{\n\n    \nfile\n=\n$dir\n/\n$client\n-\n$ver\n-1.\n$rhel\n.\n$arch\n.tar.gz\n    \nif\n \n[\n -e \n$file\n \n]\n;\n \nthen\n\n        \necho\n \nTesting \n$client\n-\n$ver\n-1.\n$rhel\n.\n$arch\n...\n\n        \nsize\n=\n$(\ndu -m \n$file\n \n|\n cut -f \n1\n)\n\n        \nif\n \n[\n \n$size\n -gt \n$max_size\n \n]\n;\n \nthen\n\n            \necho\n -e \n\\e[1;33mWARNING: \n$client\n-\n$ver\n-1.\n$rhel\n.\n$arch\n is too big. Check with release manager.\\e[0m\n\n        \nfi\n\n        mkdir -p \n$rhel\n-\n$arch\n\n        \npushd\n \n$rhel\n-\n$arch\n\n        tar xzf ../\n$file\n\n        \n$client\n/osg/osg-post-install\n        \n$client\n/osgrun osg-ca-manage setupCA --url osg\n        \n$client\n/osgrun osg-update-vos\n        \npopd\n\n        rm -rf \n$rhel\n-\n$arch\n\n    \nelse\n\n        \necho\n -e \n\\e[1;31mERROR: \n$client\n-\n$ver\n-1.\n$rhel\n.\n$arch\n tarball is missing.\\e[0m\n\n    \nfi\n\n\n}\n\n\n\npushd\n /tmp\n\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    \nmajor_version\n=\n${\nver\n%.*\n}\n\n    \nclients\n=\nosg-wn-client\n\n    \nif\n \n[\n \n$major_version\n \n=\n \n3.4\n \n]\n;\n \nthen\n\n        \nclients\n=\n$clients\n osg-afs-client\n\n    \nfi\n\n    \nfor\n client in \n$clients\n;\n \ndo\n\n        \nrhels\n=\nel6 el7\n\n        \nfor\n rhel in \n$rhels\n;\n \ndo\n\n            \nmax_size\n=\n28\n\n            \nif\n \n[\n \n$rhel\n \n=\n \nel7\n \n]\n;\n \nthen\n\n                \nmax_size\n=\n33\n\n            \nfi\n\n            \narchs\n=\nx86_64\n\n            \nif\n \n[\n \n$major_version\n \n=\n \n3.3\n -a \n$rhel\n \n=\n \nel6\n \n]\n;\n \nthen\n\n                \narchs\n=\ni386 \n$archs\n\n            \nfi\n\n            \nfor\n arch in \n$archs\n;\n \ndo\n\n                \ndir\n=\ntarballs/\n$major_version\n/\n$arch\n\n                dotest\n            \ndone\n\n        \ndone\n\n    \ndone\n\n\ndone\n\n\n\npopd\n\n\n\n\n\n\nIf you have time, try some of the binaries, such as grid-proxy-init.\n\n\n\n\nTodo\n\n\nWe need to automate this and have it run on the proper architectures and version of RHEL.\n\n\n\n\nStep 7: Update the UW AFS installation of the tarball client\n\n\nThe UW keeps an install of the tarball client in \n/p/vdt/workspace/tarball-client\n on the UW's AFS. To update it, run the following commands:\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    /p/vdt/workspace/tarball-client/afs-install-tarball-client \n$ver\n\n\ndone\n\n\n\n\n\n\nStep 8: Wait\n\n\nWait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day.\n\n\nDay 2: Pushing the Release\n\n\n\n\nWarning\n\n\nOperations would like to send out the release announcement prior to 3 PM Eastern time.\nDo not start this process after 2 PM Eastern time unless you check with Operations (specifically Kyle Gross) first.\n\n\n\n\nStep 1: Push from pre-release to release\n\n\nThis script moves the packages into release, clones releases into new version-specific release repos,\nlocks the repos and regenerates them.\n\n\nVERSIONS\n=\nVERSION(S)\n\n\n\n\n\n\n2\n-create-release \n$VERSIONS\n\n\n\n\n\n\n\n\n*.txt\n files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS.\n\n\nFor each release version, use the \n*release-note*\n files to update the relevant sections of the release note pages.\n\n\n\n\nStep 2: Upload the client tarballs\n\n\nAsk Tim Theisen, Brian Lin, or someone with privileges on the \nopensciencegrid.org\n repo servers to upload the tarballs with the following procedure:\n\n\nOn a CS machine\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    \nmajor_ver\n=\n${\nver\n%.*\n}\n\n    \ncd\n /p/vdt/public/html/tarball-client\n    ssh jump.grid.iu.edu mkdir /tmp/\n$ver\n/\n    scp -p \n$major_ver\n/*/osg-wn-client-\n$ver\n*gz jump.grid.iu.edu:/tmp/\n$ver\n/\n\ndone\n\n\n\n\n\n\nOn jump.grid.iu.edu\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    scp -pr /tmp/\n$ver\n repo1:/tmp/\n    scp -pr /tmp/\n$ver\n repo2:/tmp/\n    rm -rf /tmp/\n$ver\n\n\ndone\n\n\n\n\n\n\nOn repo1/repo2 (as root)\n\n\nYou can ssh to repo1 and repo2 from jump.grid.iu.edu; you will need to do this procedure on both systems.\n\n\nsudo su -\n\n\n\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    \nmajor_ver\n=\n${\nver\n%.*\n}\n\n    mv /tmp/\n$ver\n /usr/local/repo/tarball-install/\n$major_ver\n/\n    rm -f /usr/local/repo/tarball-install/\n$major_ver\n/*latest*\n\ndone\n\n/root/mk-sims.sh\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    \nmajor_ver\n=\n${\nver\n%.*\n}\n\n    ls -l /usr/local/repo/tarball-install/\n$major_ver\n/*latest* \n# verify the symlinks are correct\n\n\ndone\n\n\n\n\n\n\nUpload the tarballs with the following procedure:\n\n\nOn a CS machine\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\npushd\n /p/vdt/public/html/tarball-client\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    \nmajor_ver\n=\n${\nver\n%.*\n}\n\n    ssh osgcollab@hcc-osg-software.unl.edu mkdir -p /usr/local/repo/tarball-install/\n$major_ver\n/\n$ver\n\n    scp -p \n$major_ver\n/*/osg-wn-client-\n$ver\n*gz osgcollab@hcc-osg-software.unl.edu:/usr/local/repo/tarball-install/\n$major_ver\n/\n$ver\n\n\ndone\n\n\npopd\n\nssh osgcollab@hcc-osg-software.unl.edu bin/mk-sims.sh\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    \nmajor_ver\n=\n${\nver\n%.*\n}\n\n    ssh osgcollab@hcc-osg-software.unl.edu \ncd /usr/local/repo/tarball-install; ls -l \n$major_ver\n/*latest*\n\n\ndone\n\n\n# verify the \nlatest\n symlinks point to the version(s) just installed\n\n\n\n\n\n\nStep 3: Install the tarballs into OASIS\n\n\n\n\nNote\n\n\nYou must be an OASIS manager of the \nmis\n VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L. \n\n\n\n\nGet the uploader script from Git and run it with \nosgrun\n from the UW AFS install of the tarball client you made earlier. On a UW CSL machine:\n\n\nNON_UPCOMING_VERSIONS\n=\nNON-UPCOMING VERSION(S)\n\n\n\n\n\n\ncd\n /tmp\ngit clone --depth \n1\n file:///p/vdt/workspace/git/repo/tarball-client.git\n\nfor\n ver in \n$NON_UPCOMING_VERSIONS\n;\n \ndo\n\n    /p/vdt/workspace/tarball-client/current/sys/osgrun /tmp/tarball-client/upload-tarballs-to-oasis \n$ver\n\n\ndone\n\n\n\n\n\n\nThe script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process.\n\n\nStep 4: Remove old UW AFS installations of the tarball client\n\n\nTo keep space usage down, remove tarball client installations and symlinks under \n/p/vdt/workspace/tarball-client\n on UW's AFS that are more than 2 months old. The following command will remove them:\n\n\nfind /p/vdt/workspace/tarball-client -maxdepth \n1\n -mtime +60 -name \n3\n\\*\n -ls -exec rm -rf \n{}\n \n\\;\n\n\n\n\n\n\nStep 5: Update the Docker WN client\n\n\nUpdate the GitHub repo at \nopensciencegrid/docker-osg-wn\n using the \nupdate-all\n script found in \nopensciencegrid/docker-osg-wn-scripts\n. This requires push access to the \nopensciencegrid/docker-osg-wn\n repo.\n\n\nInstructions for using the script:\n\n\ngit clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\ndocker-osg-wn-scripts/update-all docker-osg-wn\n\ncd\n docker-osg-wn\n\n# Verify everything looks fine and run the \ngit push\n command\n\n\n# that \nupdate-all\n should have printed\n\n\n\n\n\n\nStep 6: Announce the release\n\n\nThe following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.\n\n\n\n\n\n\nThe release manager writes the a release announcement for each version and sends it out.\n    The announcement should mention a handful of the most important updates.\n    Due to downstream formatting issues, each major change should end at column 76 or earlier.\n    Here is a sample, replace \nBRACKETED TEXT\n with the appropriate values:\n\n\nSubject\n:\n \nAnnouncing\n \nOSG\n \nSoftware\n \nversion\n \nVERSION\n\n\n\nWe\n \nare\n \npleased\n \nto\n \nannounce\n \nOSG\n \nSoftware\n \nversion\n \nVERSION\n!\n\n\n\nChanges\n \nto\n \nOSG\n \nVERSION\n \ninclude\n:\n\n\n-\n \nMajor\n \nChange\n \n1\n\n\n-\n \nMajor\n \nChange\n \n2\n\n\n-\n \nMajor\n \nChange\n \n3\n\n\n\nRelease\n \nnotes\n \nand\n \npointers\n \nto\n \nmore\n \ndocumentation\n \ncan\n \nbe\n \nfound\n \nat\n:\n\n\n\nhttp\n://\nopensciencegrid\n.\ngithub\n.\nio\n/docs/release/\nSERIES.VERSION\n/release-\nRELEASE-VERSION\n/\n\n\n\nNeed\n \nhelp\n?\n \nLet\n \nus\n \nknow\n:\n\n\n\nhttp\n://\nopensciencegrid\n.\ngithub\n.\nio\n/docs/common/help/\n\n\n\nWe\n \nwelcome\n \nfeedback\n \non\n \nthis\n \nrelease\n!\n\n\n\n\n\n\n\n\n\n\nThe release manager emails the announcement to \nvdt-discuss@opensciencegrid.org\n\n\n\n\nThe release manager asks the GOC to distribute the announcement by \nopening a ticket\n\n\nThe release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function.\n    Also set the Fix Versions field to the appropriate value(s) and uncheck the box that reads \"Send mail for this update\"\n\n\n\n\nDay 3: Update the ITB\n\n\nNow that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following\nthe \nyum update section\n of the Madison ITB document.\nIf you are not local to Madison, consult the release manager for the designated person to do the update.\nRemember to stop the HTCondor and HTCondor-CE daemons according to the \nHTCondor pre-release testing instructions\n.\nThose daemons will need to be restarted after the upgraode.", 
            "title": "How to Cut a Release"
        }, 
        {
            "location": "/release/cut-sw-release/#how-to-cut-a-software-release", 
            "text": "This document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found  here .  Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.", 
            "title": "How to Cut a Software Release"
        }, 
        {
            "location": "/release/cut-sw-release/#requirements", 
            "text": "User certificate registered with OSG's koji with build and release team privileges  An account on UW CS machines (e.g.  library ,  ingwe ) to access UW's AFS  release-tools  scripts in your  PATH  ( GitHub )  osg-build  scripts in your  PATH  (installed via OSG yum repos or  source )  Access to the tarball repository at UNL (osgcollab@hcc-osg-software.unl.edu)", 
            "title": "Requirements"
        }, 
        {
            "location": "/release/cut-sw-release/#pick-the-version-number", 
            "text": "The rest of this document makes references to  VERSION(S)  and  NON-UPCOMING VERSIONS(S) , which refer to a space-delimited list of OSG version(s) and that same list minus  upcoming  (e.g.  3.3.28 3.4.3 upcoming  and  3.3.28 3.4.3 ). If you are unsure about either the version or revision, please consult the release manager.", 
            "title": "Pick the Version Number"
        }, 
        {
            "location": "/release/cut-sw-release/#day-0-generate-preliminary-release-list", 
            "text": "The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release.", 
            "title": "Day 0: Generate Preliminary Release List"
        }, 
        {
            "location": "/release/cut-sw-release/#step-1-update-the-osg-version-rpm", 
            "text": "For each release (excluding upcoming), update the version number in the osg-version RPM's spec file and build it in koji:  # If building for the latest release out of trunk \nosg-build koji osg-version # If building for an older release out of a branch:  MAJOR_VERSION = MAJOR VERSION \nosg-build koji --repo = $MAJOR_VERSION  osg-version  Where  MAJOR VERSION  is of the format  x.y  (e.g.  3.2 ).", 
            "title": "Step 1: Update the osg-version RPM"
        }, 
        {
            "location": "/release/cut-sw-release/#step-2-promote-osg-version-and-generate-the-release-list", 
            "text": "Run  0-generate-pkg-list  from a machine that has your koji-registered user certificate:  VERSIONS = VERSION(S)   git clone https://github.com/opensciencegrid/release-tools.git cd  release-tools 0 -generate-pkg-list  $VERSIONS", 
            "title": "Step 2: Promote osg-version and generate the release list"
        }, 
        {
            "location": "/release/cut-sw-release/#day-1-verify-pre-release-and-generate-tarballs", 
            "text": "This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs.", 
            "title": "Day 1: Verify Pre-Release and Generate Tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#step-1-verify-pre-release", 
            "text": "Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated  release-list  in git). To do this, run the  1-verify-prerelease  script from git:  VERSIONS = VERSION(S)   1 -verify-prerelease  $VERSIONS   If there are any discrepancies consult the release manager. You may have to tag or untag packages with the  osg-koji  tool.   Note  Please verify that the  osg-version  RPM is in your set of packages for the release! Also verify that if there is a new version of the  osg-tested-internal  RPM, then it is included in the release as well!", 
            "title": "Step 1: Verify Pre-Release"
        }, 
        {
            "location": "/release/cut-sw-release/#step-2-test-pre-release-in-vm-universe", 
            "text": "To test pre-release, you will be kicking off a manual VM universe test run from  osghost.chtc.wisc.edu .   Ensure that you meet the  pre-requisites  for submitting VM universe test runs   Prepare the test suite by running:  osg-run-tests  Testing OSG pre-release     cd  into the directory specified in the output of the previous command   cd  into  parameters.d  and remove all files within it except for  osg33.yaml  and  osg34.yaml   Edit  osg33.yaml  so that it reads:  platforms : \n   -   centos_6_x86_64 \n   -   rhel_6_x86_64 \n   -   sl_6_x86_64 \n   -   centos_7_x86_64 \n   -   rhel_7_x86_64 \n   -   sl_7_x86_64  sources : \n   -   opensciencegrid : master ;   3 . 3 ;   osg-prerelease \n   -   opensciencegrid : master ;   3 . 3 ;   osg     osg-prerelease  package_sets : \n   -   label :   All   ( java ) \n     selinux :   True \n     osg_java :   True \n     rng :   True \n     packages : \n       -   osg-tested-internal     Edit  osg34.yaml  so that it reads:  platforms : \n   -   centos_6_x86_64 \n   -   rhel_6_x86_64 \n   -   sl_6_x86_64 \n   -   centos_7_x86_64 \n   -   rhel_7_x86_64 \n   -   sl_7_x86_64  sources : \n   -   opensciencegrid : master ;   3 . 4 ;   osg-prerelease \n   -   opensciencegrid : master ;   3 . 4 ;   osg     osg-prerelease \n   -   opensciencegrid : master ;   3 . 3 ;   osg     3 . 4 / osg-prerelease \n   -   opensciencegrid : master ;   3 . 4 ;   osg-prerelease ,   osg-upcoming-prerelease ,   osg-upcoming \n   -   opensciencegrid : master ;   3 . 4 ;   osg     osg-prerelease ,   osg-upcoming-prerelease ,   osg-upcoming  package_sets : \n   -   label :   All \n     selinux :   True \n     osg_java :   False \n     packages : \n       -   osg-tested-internal   If you are not releasing packages into  upcoming , delete the  upcoming -related lines in the  sources  section.    cd  back into the root directory of the test run (e.g.  cd .. )   Submit the DAG: condor_submit_dag master-run.dag     Note  If there are failures, consult the release-manager before proceeding.", 
            "title": "Step 2: Test Pre-Release in VM Universe"
        }, 
        {
            "location": "/release/cut-sw-release/#step-3-test-pre-release-on-the-madison-itb-site", 
            "text": "Test the pre-release on the Madison ITB by following the  ITB pre-release testing instructions .\nIf you not local to Madison, consult the release manager for the designated person to do this testing.", 
            "title": "Step 3: Test Pre-Release on the Madison ITB site"
        }, 
        {
            "location": "/release/cut-sw-release/#step-4-regenerate-the-build-repositories", 
            "text": "To avoid 404 errors when retrieving packages, it's necessary to regenerate the build repositories. Run the following script from a machine with your koji-registered user certificate:  NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   1 -regen-repos  $NON_UPCOMING_VERSIONS", 
            "title": "Step 4: Regenerate the build repositories"
        }, 
        {
            "location": "/release/cut-sw-release/#step-5-create-the-client-tarballs", 
            "text": "Create the client tarballs as root on an EL7 fermicloud machine using the relevant script from git:  NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   git clone https://github.com/opensciencegrid/release-tools.git cd  release-tools\n./1-client-tarballs  $NON_UPCOMING_VERSIONS", 
            "title": "Step 5: Create the client tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#step-6-briefly-test-the-client-tarballs", 
            "text": "As an  unprivileged user , extract each tarball into a separate directory. Make sure osg-post-install works. Make sure  osgrun osg-version  works by running the following tests, replacing  NON-UPCOMING VERSION(S)  with the appropriate version numbers:  NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   dotest  ()   { \n     file = $dir / $client - $ver -1. $rhel . $arch .tar.gz\n     if   [  -e  $file   ] ;   then \n         echo   Testing  $client - $ver -1. $rhel . $arch ... \n         size = $( du -m  $file   |  cut -f  1 ) \n         if   [   $size  -gt  $max_size   ] ;   then \n             echo  -e  \\e[1;33mWARNING:  $client - $ver -1. $rhel . $arch  is too big. Check with release manager.\\e[0m \n         fi \n        mkdir -p  $rhel - $arch \n         pushd   $rhel - $arch \n        tar xzf ../ $file \n         $client /osg/osg-post-install\n         $client /osgrun osg-ca-manage setupCA --url osg\n         $client /osgrun osg-update-vos\n         popd \n        rm -rf  $rhel - $arch \n     else \n         echo  -e  \\e[1;31mERROR:  $client - $ver -1. $rhel . $arch  tarball is missing.\\e[0m \n     fi  }  pushd  /tmp for  ver in  $NON_UPCOMING_VERSIONS ;   do \n     major_version = ${ ver %.* } \n     clients = osg-wn-client \n     if   [   $major_version   =   3.4   ] ;   then \n         clients = $clients  osg-afs-client \n     fi \n     for  client in  $clients ;   do \n         rhels = el6 el7 \n         for  rhel in  $rhels ;   do \n             max_size = 28 \n             if   [   $rhel   =   el7   ] ;   then \n                 max_size = 33 \n             fi \n             archs = x86_64 \n             if   [   $major_version   =   3.3  -a  $rhel   =   el6   ] ;   then \n                 archs = i386  $archs \n             fi \n             for  arch in  $archs ;   do \n                 dir = tarballs/ $major_version / $arch \n                dotest\n             done \n         done \n     done  done  popd   If you have time, try some of the binaries, such as grid-proxy-init.   Todo  We need to automate this and have it run on the proper architectures and version of RHEL.", 
            "title": "Step 6: Briefly test the client tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#step-7-update-the-uw-afs-installation-of-the-tarball-client", 
            "text": "The UW keeps an install of the tarball client in  /p/vdt/workspace/tarball-client  on the UW's AFS. To update it, run the following commands:  NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   for  ver in  $NON_UPCOMING_VERSIONS ;   do \n    /p/vdt/workspace/tarball-client/afs-install-tarball-client  $ver  done", 
            "title": "Step 7: Update the UW AFS installation of the tarball client"
        }, 
        {
            "location": "/release/cut-sw-release/#step-8-wait", 
            "text": "Wait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day.", 
            "title": "Step 8: Wait"
        }, 
        {
            "location": "/release/cut-sw-release/#day-2-pushing-the-release", 
            "text": "Warning  Operations would like to send out the release announcement prior to 3 PM Eastern time.\nDo not start this process after 2 PM Eastern time unless you check with Operations (specifically Kyle Gross) first.", 
            "title": "Day 2: Pushing the Release"
        }, 
        {
            "location": "/release/cut-sw-release/#step-1-push-from-pre-release-to-release", 
            "text": "This script moves the packages into release, clones releases into new version-specific release repos,\nlocks the repos and regenerates them.  VERSIONS = VERSION(S)   2 -create-release  $VERSIONS    *.txt  files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS.  For each release version, use the  *release-note*  files to update the relevant sections of the release note pages.", 
            "title": "Step 1: Push from pre-release to release"
        }, 
        {
            "location": "/release/cut-sw-release/#step-2-upload-the-client-tarballs", 
            "text": "Ask Tim Theisen, Brian Lin, or someone with privileges on the  opensciencegrid.org  repo servers to upload the tarballs with the following procedure:", 
            "title": "Step 2: Upload the client tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#on-a-cs-machine", 
            "text": "NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   for  ver in  $NON_UPCOMING_VERSIONS ;   do \n     major_ver = ${ ver %.* } \n     cd  /p/vdt/public/html/tarball-client\n    ssh jump.grid.iu.edu mkdir /tmp/ $ver /\n    scp -p  $major_ver /*/osg-wn-client- $ver *gz jump.grid.iu.edu:/tmp/ $ver / done", 
            "title": "On a CS machine"
        }, 
        {
            "location": "/release/cut-sw-release/#on-jumpgridiuedu", 
            "text": "NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   for  ver in  $NON_UPCOMING_VERSIONS ;   do \n    scp -pr /tmp/ $ver  repo1:/tmp/\n    scp -pr /tmp/ $ver  repo2:/tmp/\n    rm -rf /tmp/ $ver  done", 
            "title": "On jump.grid.iu.edu"
        }, 
        {
            "location": "/release/cut-sw-release/#on-repo1repo2-as-root", 
            "text": "You can ssh to repo1 and repo2 from jump.grid.iu.edu; you will need to do this procedure on both systems.  sudo su -  NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   for  ver in  $NON_UPCOMING_VERSIONS ;   do \n     major_ver = ${ ver %.* } \n    mv /tmp/ $ver  /usr/local/repo/tarball-install/ $major_ver /\n    rm -f /usr/local/repo/tarball-install/ $major_ver /*latest* done \n/root/mk-sims.sh for  ver in  $NON_UPCOMING_VERSIONS ;   do \n     major_ver = ${ ver %.* } \n    ls -l /usr/local/repo/tarball-install/ $major_ver /*latest*  # verify the symlinks are correct  done   Upload the tarballs with the following procedure:", 
            "title": "On repo1/repo2 (as root)"
        }, 
        {
            "location": "/release/cut-sw-release/#on-a-cs-machine_1", 
            "text": "NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   pushd  /p/vdt/public/html/tarball-client for  ver in  $NON_UPCOMING_VERSIONS ;   do \n     major_ver = ${ ver %.* } \n    ssh osgcollab@hcc-osg-software.unl.edu mkdir -p /usr/local/repo/tarball-install/ $major_ver / $ver \n    scp -p  $major_ver /*/osg-wn-client- $ver *gz osgcollab@hcc-osg-software.unl.edu:/usr/local/repo/tarball-install/ $major_ver / $ver  done  popd \nssh osgcollab@hcc-osg-software.unl.edu bin/mk-sims.sh for  ver in  $NON_UPCOMING_VERSIONS ;   do \n     major_ver = ${ ver %.* } \n    ssh osgcollab@hcc-osg-software.unl.edu  cd /usr/local/repo/tarball-install; ls -l  $major_ver /*latest*  done  # verify the  latest  symlinks point to the version(s) just installed", 
            "title": "On a CS machine"
        }, 
        {
            "location": "/release/cut-sw-release/#step-3-install-the-tarballs-into-oasis", 
            "text": "Note  You must be an OASIS manager of the  mis  VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L.    Get the uploader script from Git and run it with  osgrun  from the UW AFS install of the tarball client you made earlier. On a UW CSL machine:  NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S)   cd  /tmp\ngit clone --depth  1  file:///p/vdt/workspace/git/repo/tarball-client.git for  ver in  $NON_UPCOMING_VERSIONS ;   do \n    /p/vdt/workspace/tarball-client/current/sys/osgrun /tmp/tarball-client/upload-tarballs-to-oasis  $ver  done   The script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process.", 
            "title": "Step 3: Install the tarballs into OASIS"
        }, 
        {
            "location": "/release/cut-sw-release/#step-4-remove-old-uw-afs-installations-of-the-tarball-client", 
            "text": "To keep space usage down, remove tarball client installations and symlinks under  /p/vdt/workspace/tarball-client  on UW's AFS that are more than 2 months old. The following command will remove them:  find /p/vdt/workspace/tarball-client -maxdepth  1  -mtime +60 -name  3 \\*  -ls -exec rm -rf  {}   \\;", 
            "title": "Step 4: Remove old UW AFS installations of the tarball client"
        }, 
        {
            "location": "/release/cut-sw-release/#step-5-update-the-docker-wn-client", 
            "text": "Update the GitHub repo at  opensciencegrid/docker-osg-wn  using the  update-all  script found in  opensciencegrid/docker-osg-wn-scripts . This requires push access to the  opensciencegrid/docker-osg-wn  repo.  Instructions for using the script:  git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\ndocker-osg-wn-scripts/update-all docker-osg-wn cd  docker-osg-wn # Verify everything looks fine and run the  git push  command  # that  update-all  should have printed", 
            "title": "Step 5: Update the Docker WN client"
        }, 
        {
            "location": "/release/cut-sw-release/#step-6-announce-the-release", 
            "text": "The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.    The release manager writes the a release announcement for each version and sends it out.\n    The announcement should mention a handful of the most important updates.\n    Due to downstream formatting issues, each major change should end at column 76 or earlier.\n    Here is a sample, replace  BRACKETED TEXT  with the appropriate values:  Subject :   Announcing   OSG   Software   version   VERSION  We   are   pleased   to   announce   OSG   Software   version   VERSION !  Changes   to   OSG   VERSION   include :  -   Major   Change   1  -   Major   Change   2  -   Major   Change   3  Release   notes   and   pointers   to   more   documentation   can   be   found   at :  http :// opensciencegrid . github . io /docs/release/ SERIES.VERSION /release- RELEASE-VERSION /  Need   help ?   Let   us   know :  http :// opensciencegrid . github . io /docs/common/help/  We   welcome   feedback   on   this   release !     The release manager emails the announcement to  vdt-discuss@opensciencegrid.org   The release manager asks the GOC to distribute the announcement by  opening a ticket  The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function.\n    Also set the Fix Versions field to the appropriate value(s) and uncheck the box that reads \"Send mail for this update\"", 
            "title": "Step 6: Announce the release"
        }, 
        {
            "location": "/release/cut-sw-release/#day-3-update-the-itb", 
            "text": "Now that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following\nthe  yum update section  of the Madison ITB document.\nIf you are not local to Madison, consult the release manager for the designated person to do the update.\nRemember to stop the HTCondor and HTCondor-CE daemons according to the  HTCondor pre-release testing instructions .\nThose daemons will need to be restarted after the upgraode.", 
            "title": "Day 3: Update the ITB"
        }, 
        {
            "location": "/release/cut-data-release/", 
            "text": "Note\n\n\nIf you are performing a software release, please follow the instructions \nhere\n\n\n\n\nHow to Cut a Data Release\n\n\nThis document details the process for releasing new OSG Data Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found \nhere\n.\n\n\nDue to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.\n\n\nRequirements\n\n\n\n\nUser certificate registered with OSG's koji with build and release team privileges\n\n\nAn account on UW CS machines (e.g. \nlibrary\n, \ningwe\n) to access UW's AFS\n\n\nrelease-tools\n scripts in your \nPATH\n (\nGitHub\n)\n\n\nosg-build\n scripts in your \nPATH\n (installed via OSG yum repos or \nsource\n)\n\n\n\n\nPick the Version and Revision Numbers\n\n\nThe rest of this document makes references to \nREVISION\n and \nVERSION(S)\n , which refer to the space-delimited list of OSG version(s) and data revision, respectively (e.g. \n3.3.28 3.4.3\n and \n2\n, respectively). If you are unsure about either the version or revision, please consult the release manager.\n\n\nDay 0: Generate Preliminary Release List\n\n\nThe release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run \n0-generate-pkg-list\n from a machine that has your koji-registered user certificate:\n\n\nVERSIONS\n=\nVERSION(S)\n\n\nREVISION\n=\nREVISION\n\n\n\n\n\n\ngit clone https://github.com/opensciencegrid/release-tools.git\n\ncd\n release-tools\n\n0\n-generate-pkg-list -d \n$REVISION\n \n$VERSIONS\n\n\n\n\n\n\nDay 1: Verify Pre-Release\n\n\nThis section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release.\n\n\nStep 1: Generate the release list\n\n\nCompare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated \nrelease-list\n in git). To do this, run the \n1-verify-prerelease\n script from git:\n\n\nVERSIONS\n=\nVERSION(S)\n\n\n\n\n\n\n1\n-verify-prerelease \n$VERSIONS\n\n\n\n\n\n\nIf there are any discrepancies consult the release manager. You may have to tag packages with the \nosg-koji\n tool.\n\n\nStep 2: Test the Pre-Release on the Madison ITB site\n\n\nTest the pre-release on the Madison ITB by following the \nITB pre-release testing instructions\n.\n\n\nDay 2: Pushing the Release\n\n\n\n\nWarning\n\n\nOperations would like to send out the release announcement prior to 3 PM Eastern time.\nDo not start this process after 2 PM Eastern time unless you check with Operations (specifically Kyle Gross) first.\n\n\n\n\nStep 1: Push from pre-release to release\n\n\nThis script moves the packages into release, clones releases into new version-specific release repos,\nlocks the repos and regenerates them.\n\n\nVERSIONS\n=\nVERSION(S)\n\n\nREVISION\n=\nREVISION\n\n\n\n\n\n\n2\n-create-release -d \n$REVISION\n \n$VERSIONS\n\n\n\n\n\n\n\n\n*.txt\n files are also created and it should be verified that they've been moved to \n/p/vdt/public/html/release-info/\n on UW's AFS.\n\n\nFor each release version, use the \n*release-note*\n files to update the relevant sections of the release note pages.\n\n\n\n\nStep 2: Update the Docker WN client\n\n\nUpdate the GitHub repo at \nopensciencegrid/docker-osg-wn\n using the \nupdate-all\n script found in \nopensciencegrid/docker-osg-wn-scripts\n. This requires push access to the \nopensciencegrid/docker-osg-wn\n repo.\n\n\nInstructions for using the script:\n\n\ngit clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\ndocker-osg-wn-scripts/update-all docker-osg-wn\n\ncd\n docker-osg-wn\n\n# Verify everything looks fine and run the \ngit push\n command\n\n\n# that \nupdate-all\n should have printed\n\n\n\n\n\n\nStep 3: Verify the VO Package and/or CA certificates\n\n\nWait for the \nCA certificates\n to be updated.\nIt may take a while for the updates to reach the mirror used to update the web site.\nThe repository is checked hourly for updated CA certificates.\nOnce the web page is updated, run the following command to update the VO Package and/or CA certificates in the tarball installations and\nverify that the version of the VO Package and/or CA certificates match the version that was promoted to release.\n\n\n/p/vdt/workspace/tarball-client/current/amd64_rhel6/osgrun osg-update-data\n/p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data\n\n\n\n\n\nStep 4: Announce the release\n\n\nThe following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.\n\n\n\n\n\n\nThe release manager writes the a release announcement for each version and sends it out.\n    The announcement should mention a handful of the most important updates.\n    Due to downstream formatting issues, each major change should end at column 76 or earlier.\n    Here is a sample, replace \nBRACKETED TEXT\n with the appropriate values:\n    If you are only updating certificates or only updated the VO package, delete the corresponding text:\n\n\nSubject\n:\n \nAnnouncing\n \nOSG\n \nCA\n \nCertificate\n \nand\n \nVO\n \nPackage\n \nUpdates\n\n\nSubject\n:\n \nAnnouncing\n \nOSG\n \nCA\n \nCertificate\n \nUpdate\n\n\nSubject\n:\n \nAnnouncing\n \nVO\n \nPackage\n \nUpdate\n\n\n\nWe\n \nare\n \npleased\n \nto\n \nannounce\n \na\n \ndata\n \nrelease\n \nfor\n \nthe\n \nOSG\n \nSoftware\n \nStack\n.\n\n\nData\n \nreleases\n \ndo\n \nnot\n \ncontain\n \nany\n \nsoftware\n \nchanges\n.\n\n\n\nThis\n \nrelease\n \ncontains\n \nupdated\n \nCA\n \nCertificates\n \nbased\n \non\n \nIGTF\n \nVERSION\n:\n\n\n-\n \nChange\n \n1\n \nfrom\n \nIGTF\n \nchangelog\n\n\n-\n \nChange\n \n2\n \nfrom\n \nIGTF\n \nchangelog\n\n\n\nThis\n \nrelease\n \ncontains\n \nVO\n \nPackage\n \nv\nVERSION\n:\n\n\nThis\n \nrelease\n \nalso\n \ncontains\n \nVO\n \nPackage\n \nv\nVERSION\n:\n\n\n-\n \nChange\n \n1\n \nfrom\n \nVO\n \nchangelog\n\n\n-\n \nChange\n \n2\n \nfrom\n \nVO\n \nchangelog\n\n\n\nRelease\n \nnotes\n \nand\n \npointers\n \nto\n \nmore\n \ndocumentation\n \ncan\n \nbe\n \nfound\n \nat\n:\n\n\n\nhttp\n://\nopensciencegrid\n.\ngithub\n.\nio\n/docs/release/\nSERIES.VERSION\n/release-\nRELEASE-VERSION\n/\n\n\n\nNeed\n \nhelp\n?\n \nLet\n \nus\n \nknow\n:\n\n\n\nhttp\n://\nopensciencegrid\n.\ngithub\n.\nio\n/docs/common/help/\n\n\n\nWe\n \nwelcome\n \nfeedback\n \non\n \nthis\n \nrelease\n!\n\n\n\n\n\n\n\n\n\n\nThe release manager emails the announcement to \nvdt-discuss@opensciencegrid.org\n\n\n\n\nThe release manager asks the GOC to distribute the announcement by \nopening a ticket\n\n\nThe release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"\n\n\n\n\nDay 3: Update the ITB\n\n\nNow that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following\nthe \nyum update section\n of the Madison ITB document.", 
            "title": "How to Cut a Data Release"
        }, 
        {
            "location": "/release/cut-data-release/#how-to-cut-a-data-release", 
            "text": "This document details the process for releasing new OSG Data Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found  here .  Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.", 
            "title": "How to Cut a Data Release"
        }, 
        {
            "location": "/release/cut-data-release/#requirements", 
            "text": "User certificate registered with OSG's koji with build and release team privileges  An account on UW CS machines (e.g.  library ,  ingwe ) to access UW's AFS  release-tools  scripts in your  PATH  ( GitHub )  osg-build  scripts in your  PATH  (installed via OSG yum repos or  source )", 
            "title": "Requirements"
        }, 
        {
            "location": "/release/cut-data-release/#pick-the-version-and-revision-numbers", 
            "text": "The rest of this document makes references to  REVISION  and  VERSION(S)  , which refer to the space-delimited list of OSG version(s) and data revision, respectively (e.g.  3.3.28 3.4.3  and  2 , respectively). If you are unsure about either the version or revision, please consult the release manager.", 
            "title": "Pick the Version and Revision Numbers"
        }, 
        {
            "location": "/release/cut-data-release/#day-0-generate-preliminary-release-list", 
            "text": "The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run  0-generate-pkg-list  from a machine that has your koji-registered user certificate:  VERSIONS = VERSION(S)  REVISION = REVISION   git clone https://github.com/opensciencegrid/release-tools.git cd  release-tools 0 -generate-pkg-list -d  $REVISION   $VERSIONS", 
            "title": "Day 0: Generate Preliminary Release List"
        }, 
        {
            "location": "/release/cut-data-release/#day-1-verify-pre-release", 
            "text": "This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release.", 
            "title": "Day 1: Verify Pre-Release"
        }, 
        {
            "location": "/release/cut-data-release/#step-1-generate-the-release-list", 
            "text": "Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated  release-list  in git). To do this, run the  1-verify-prerelease  script from git:  VERSIONS = VERSION(S)   1 -verify-prerelease  $VERSIONS   If there are any discrepancies consult the release manager. You may have to tag packages with the  osg-koji  tool.", 
            "title": "Step 1: Generate the release list"
        }, 
        {
            "location": "/release/cut-data-release/#step-2-test-the-pre-release-on-the-madison-itb-site", 
            "text": "Test the pre-release on the Madison ITB by following the  ITB pre-release testing instructions .", 
            "title": "Step 2: Test the Pre-Release on the Madison ITB site"
        }, 
        {
            "location": "/release/cut-data-release/#day-2-pushing-the-release", 
            "text": "Warning  Operations would like to send out the release announcement prior to 3 PM Eastern time.\nDo not start this process after 2 PM Eastern time unless you check with Operations (specifically Kyle Gross) first.", 
            "title": "Day 2: Pushing the Release"
        }, 
        {
            "location": "/release/cut-data-release/#step-1-push-from-pre-release-to-release", 
            "text": "This script moves the packages into release, clones releases into new version-specific release repos,\nlocks the repos and regenerates them.  VERSIONS = VERSION(S)  REVISION = REVISION   2 -create-release -d  $REVISION   $VERSIONS    *.txt  files are also created and it should be verified that they've been moved to  /p/vdt/public/html/release-info/  on UW's AFS.  For each release version, use the  *release-note*  files to update the relevant sections of the release note pages.", 
            "title": "Step 1: Push from pre-release to release"
        }, 
        {
            "location": "/release/cut-data-release/#step-2-update-the-docker-wn-client", 
            "text": "Update the GitHub repo at  opensciencegrid/docker-osg-wn  using the  update-all  script found in  opensciencegrid/docker-osg-wn-scripts . This requires push access to the  opensciencegrid/docker-osg-wn  repo.  Instructions for using the script:  git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\ndocker-osg-wn-scripts/update-all docker-osg-wn cd  docker-osg-wn # Verify everything looks fine and run the  git push  command  # that  update-all  should have printed", 
            "title": "Step 2: Update the Docker WN client"
        }, 
        {
            "location": "/release/cut-data-release/#step-3-verify-the-vo-package-andor-ca-certificates", 
            "text": "Wait for the  CA certificates  to be updated.\nIt may take a while for the updates to reach the mirror used to update the web site.\nThe repository is checked hourly for updated CA certificates.\nOnce the web page is updated, run the following command to update the VO Package and/or CA certificates in the tarball installations and\nverify that the version of the VO Package and/or CA certificates match the version that was promoted to release.  /p/vdt/workspace/tarball-client/current/amd64_rhel6/osgrun osg-update-data\n/p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data", 
            "title": "Step 3: Verify the VO Package and/or CA certificates"
        }, 
        {
            "location": "/release/cut-data-release/#step-4-announce-the-release", 
            "text": "The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.    The release manager writes the a release announcement for each version and sends it out.\n    The announcement should mention a handful of the most important updates.\n    Due to downstream formatting issues, each major change should end at column 76 or earlier.\n    Here is a sample, replace  BRACKETED TEXT  with the appropriate values:\n    If you are only updating certificates or only updated the VO package, delete the corresponding text:  Subject :   Announcing   OSG   CA   Certificate   and   VO   Package   Updates  Subject :   Announcing   OSG   CA   Certificate   Update  Subject :   Announcing   VO   Package   Update  We   are   pleased   to   announce   a   data   release   for   the   OSG   Software   Stack .  Data   releases   do   not   contain   any   software   changes .  This   release   contains   updated   CA   Certificates   based   on   IGTF   VERSION :  -   Change   1   from   IGTF   changelog  -   Change   2   from   IGTF   changelog  This   release   contains   VO   Package   v VERSION :  This   release   also   contains   VO   Package   v VERSION :  -   Change   1   from   VO   changelog  -   Change   2   from   VO   changelog  Release   notes   and   pointers   to   more   documentation   can   be   found   at :  http :// opensciencegrid . github . io /docs/release/ SERIES.VERSION /release- RELEASE-VERSION /  Need   help ?   Let   us   know :  http :// opensciencegrid . github . io /docs/common/help/  We   welcome   feedback   on   this   release !     The release manager emails the announcement to  vdt-discuss@opensciencegrid.org   The release manager asks the GOC to distribute the announcement by  opening a ticket  The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"", 
            "title": "Step 4: Announce the release"
        }, 
        {
            "location": "/release/cut-data-release/#day-3-update-the-itb", 
            "text": "Now that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following\nthe  yum update section  of the Madison ITB document.", 
            "title": "Day 3: Update the ITB"
        }, 
        {
            "location": "/release/release-policy/", 
            "text": "Software Release Policy\n\n\nThis document doesn't talk about technical details of how to do a release. \nThe release process is discussed elsewhere\n.\n\n\nSoftware Repositories\n\n\nThe Software Team maintains five primary software repositories\n\n\n\n\nosg-development\n: This is the \"wild west\", the place where software goes while it is being worked on by the software team.\n\n\nosg-testing\n: This is where software goes when it is ready for wide-spread testing.\n\n\nosg-prerelease\n: This is where software goes just before being released, for final verification.\n\n\nosg-release\n: This is the official, production release of the software stack. This is the main repository for end-users.\n\n\nosg-contrib\n: This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful.\n\n\n\n\nWe also create a repository per release, called \nosg-release-VERSION\n (such as osg-release-3.0.4). This is intended mostly for testing purposes, though users may occasionally find it useful.\n\n\nOccasionally there may be other repositories for specific short-term purposes.\n\n\nVersion Numbers\n\n\nThere is a single version number that is used to summarize the contents of the \nosg-release\n repository. Having a single version number is very useful for a variety of reasons, including:\n\n\n\n\nEvery time changes are made to the \nosg-release\n repository, we update the version number and write release notes.\n\n\nWe have a shorthand for referring to the state of the repository; we can talk about specific releases.\n\n\n\n\nHowever, there are important caveats about the version number:\n\n\n\n\nEven if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered.\n\n\nThe version number is only meaningful in the \nosg-release\n repository, though for technical reasons it's present (as an RPM) in other repositories.\n\n\n\n\nThe version number is communicated in two ways:\n\n\n\n\nEvery time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number.\n\n\nThere is an \nosg-version\n RPM that reports the version of the release. Major metapackages (osg-ce, osg-client, etc...) depend on this RPM. The RPM itself has the version number in it. It also provide a program that reports the version, and a text file that contains the version number.\n\n\n\n\nThe version number will be of the form X.Y.Z. As of this writing, version numbers are 3.0.Z, where Z indicates a minor revision.\n\n\nProgression of repositories\n\n\nThis figure shows the progression of repositories that packages will go through:\n\n\n osg-development -\n osg-testing -\n osg-prerelease -\n osg-release\n                  \\\n                   -\n osg-contrib\n\n\n\n\n\nRelease policies\n\n\nAdding packages to osg-development\n\n\nNew packages will only be added to \nosg-development\n with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager.\n\n\nMoving packages to osg-testing\n\n\nA package may be moved from \nosg-development\n to \nosg-testing\n when the individual maintainer of that package decides that it is ready for widespread testing and when the package is eventually intended for a production release, and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team.\n\n\nMoving packages to osg-prerelease; Readying the release\n\n\nWhen we are ready to make a production release, we first move the correct subset of packages from \nosg-testing\n into \nosg-prerelease\n. This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of \nosg-prerelease\n is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the \nosg-testing\n repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to \nosg-prerelease\n, the team member doing the release will:\n\n\n\n\nUpdate the osg-version RPM to reflect the new version. Push this RPM through \nosg-development\n, \nosg-testing\n, and into \nosg-prerelease\n.\n\n\nFind the correct set of packages to push from \nosg-testing\n into \nosg-prerelease\n.\n\n\nAt a minimum, run the automated test suite on the contents of \nosg-prerelease\n. In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.)\n\n\n\n\nWe expect that in most cases, this process of updating and testing the \nosg-prelease\n repository will be less than one day. If there are urgent security updates to release, this process may be shortened.\n\n\nNote that, except in exceptional circumstances, we release software on Tuesdays. Therefore the osg-prerelease cache is probably updated and readied on a Monday (or perhaps late the previous week).\n\n\nMoving packages to osg-release\n\n\nNote that, except in exceptional circumstances, we release software on Tuesdays, so this process will only happen on Tuesdays.\n\n\nWhen the \nosg-prerelease\n repository has been updated and verified, all of the changed software can be moved into the \nosg-release\n repository. As part of this move, two important tasks must be done:\n\n\n\n\nRecord the complete set of packages in the new release repository.\n\n\nUpdate the \nRelease Notes\n. Note that each release has a separate page to describe the release, and it's linked from the main page. The individual page lists the changes at a high level (i.e. Updated package X to version Y) and the complete set of RPMs that changed.\n\n\nCreate a ticket on ticket.opensciencegrid.org with a release announcement. Operations will distribute it to the right places.\n\n\n\n\nIn addition, we will make another Koji tag/yum repository called \nosg-release-VERSION\n. All of the latest packages in osg-release will be tagged to be in this repository, and the tag will be locked. This will give us a reproducible way to install any given OSG Software release.\n\n\nMoving packages to osg-release-VERSION\n\n\nWhen we make a specific release, we copy the osg-release repository to a versioned osg-release-VERSION repository. This allows us to do testing with specific versions and in rare cases allows users to use a specific release.\n\n\nMoving packages to osg-contrib\n\n\nThe \nosg-contrib\n repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in \nosg-contrib\n. Contrib packages should be tested in \nosg-development\n first.\n\n\nTiming of releases\n\n\nNormally, releases happen on Tuesdays.\n\n\nCode freezes happen two business days in advance of the release (normally Friday). Specifically: RPM updates intended to be included in the next release (that is, pushed to the osg-release yum repo) must be in the osg-testing yum repo by noon Central Time two business days in advance of the release. This will allow time for final testing, discussions, reverts, etc.\n\n\nWe will make exceptions for urgent situations; consult with the release manager when needed.\n\n\nCA Certificates and VO Client packages\n\n\nPackages that contain only data are not part of the usual release cycle.\nCurrently, these are the CA certificate packages and the VO Client packages.\nUpdates to these packages come from the Security Team and Operations Team, respectively.\nThey still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level.\nHowever, the actual releases of these packages do not increment the version number of the software stack.\n\n\nThe release process for data packages is discussed here.", 
            "title": "Release Policy"
        }, 
        {
            "location": "/release/release-policy/#software-release-policy", 
            "text": "This document doesn't talk about technical details of how to do a release.  The release process is discussed elsewhere .", 
            "title": "Software Release Policy"
        }, 
        {
            "location": "/release/release-policy/#software-repositories", 
            "text": "The Software Team maintains five primary software repositories   osg-development : This is the \"wild west\", the place where software goes while it is being worked on by the software team.  osg-testing : This is where software goes when it is ready for wide-spread testing.  osg-prerelease : This is where software goes just before being released, for final verification.  osg-release : This is the official, production release of the software stack. This is the main repository for end-users.  osg-contrib : This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful.   We also create a repository per release, called  osg-release-VERSION  (such as osg-release-3.0.4). This is intended mostly for testing purposes, though users may occasionally find it useful.  Occasionally there may be other repositories for specific short-term purposes.", 
            "title": "Software Repositories"
        }, 
        {
            "location": "/release/release-policy/#version-numbers", 
            "text": "There is a single version number that is used to summarize the contents of the  osg-release  repository. Having a single version number is very useful for a variety of reasons, including:   Every time changes are made to the  osg-release  repository, we update the version number and write release notes.  We have a shorthand for referring to the state of the repository; we can talk about specific releases.   However, there are important caveats about the version number:   Even if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered.  The version number is only meaningful in the  osg-release  repository, though for technical reasons it's present (as an RPM) in other repositories.   The version number is communicated in two ways:   Every time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number.  There is an  osg-version  RPM that reports the version of the release. Major metapackages (osg-ce, osg-client, etc...) depend on this RPM. The RPM itself has the version number in it. It also provide a program that reports the version, and a text file that contains the version number.   The version number will be of the form X.Y.Z. As of this writing, version numbers are 3.0.Z, where Z indicates a minor revision.", 
            "title": "Version Numbers"
        }, 
        {
            "location": "/release/release-policy/#progression-of-repositories", 
            "text": "This figure shows the progression of repositories that packages will go through:   osg-development -  osg-testing -  osg-prerelease -  osg-release\n                  \\\n                   -  osg-contrib", 
            "title": "Progression of repositories"
        }, 
        {
            "location": "/release/release-policy/#release-policies", 
            "text": "", 
            "title": "Release policies"
        }, 
        {
            "location": "/release/release-policy/#adding-packages-to-osg-development", 
            "text": "New packages will only be added to  osg-development  with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager.", 
            "title": "Adding packages to osg-development"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-testing", 
            "text": "A package may be moved from  osg-development  to  osg-testing  when the individual maintainer of that package decides that it is ready for widespread testing and when the package is eventually intended for a production release, and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team.", 
            "title": "Moving packages to osg-testing"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-prerelease-readying-the-release", 
            "text": "When we are ready to make a production release, we first move the correct subset of packages from  osg-testing  into  osg-prerelease . This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of  osg-prerelease  is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the  osg-testing  repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to  osg-prerelease , the team member doing the release will:   Update the osg-version RPM to reflect the new version. Push this RPM through  osg-development ,  osg-testing , and into  osg-prerelease .  Find the correct set of packages to push from  osg-testing  into  osg-prerelease .  At a minimum, run the automated test suite on the contents of  osg-prerelease . In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.)   We expect that in most cases, this process of updating and testing the  osg-prelease  repository will be less than one day. If there are urgent security updates to release, this process may be shortened.  Note that, except in exceptional circumstances, we release software on Tuesdays. Therefore the osg-prerelease cache is probably updated and readied on a Monday (or perhaps late the previous week).", 
            "title": "Moving packages to osg-prerelease; Readying the release"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-release", 
            "text": "Note that, except in exceptional circumstances, we release software on Tuesdays, so this process will only happen on Tuesdays.  When the  osg-prerelease  repository has been updated and verified, all of the changed software can be moved into the  osg-release  repository. As part of this move, two important tasks must be done:   Record the complete set of packages in the new release repository.  Update the  Release Notes . Note that each release has a separate page to describe the release, and it's linked from the main page. The individual page lists the changes at a high level (i.e. Updated package X to version Y) and the complete set of RPMs that changed.  Create a ticket on ticket.opensciencegrid.org with a release announcement. Operations will distribute it to the right places.   In addition, we will make another Koji tag/yum repository called  osg-release-VERSION . All of the latest packages in osg-release will be tagged to be in this repository, and the tag will be locked. This will give us a reproducible way to install any given OSG Software release.", 
            "title": "Moving packages to osg-release"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-release-version", 
            "text": "When we make a specific release, we copy the osg-release repository to a versioned osg-release-VERSION repository. This allows us to do testing with specific versions and in rare cases allows users to use a specific release.", 
            "title": "Moving packages to osg-release-VERSION"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-contrib", 
            "text": "The  osg-contrib  repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in  osg-contrib . Contrib packages should be tested in  osg-development  first.", 
            "title": "Moving packages to osg-contrib"
        }, 
        {
            "location": "/release/release-policy/#timing-of-releases", 
            "text": "Normally, releases happen on Tuesdays.  Code freezes happen two business days in advance of the release (normally Friday). Specifically: RPM updates intended to be included in the next release (that is, pushed to the osg-release yum repo) must be in the osg-testing yum repo by noon Central Time two business days in advance of the release. This will allow time for final testing, discussions, reverts, etc.  We will make exceptions for urgent situations; consult with the release manager when needed.", 
            "title": "Timing of releases"
        }, 
        {
            "location": "/release/release-policy/#ca-certificates-and-vo-client-packages", 
            "text": "Packages that contain only data are not part of the usual release cycle.\nCurrently, these are the CA certificate packages and the VO Client packages.\nUpdates to these packages come from the Security Team and Operations Team, respectively.\nThey still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level.\nHowever, the actual releases of these packages do not increment the version number of the software stack.  The release process for data packages is discussed here.", 
            "title": "CA Certificates and VO Client packages"
        }, 
        {
            "location": "/release/release-schedule/", 
            "text": "OSG Software Release Schedule\n\n\nTo help with predictability and align with the GOC update schedule, we have adopted a fixed release schedule. OSG Software will be released on the second Tuesday of the month. Urgent releases could happen on the fourth Tuesday of the month.\n\n\nRelease Cycle Phases\n\n\nVarious \"Freeze\" dates will happen in the weeks leading up to the release. They are as follows:\n\n\n\n\nDevelopment Freeze (end of the day on the Monday two weeks before release):\n\n\nThe remaining testing is planned for this release.\n\n\nPackages are considered for inclusion in the release on this date.\n\n\nSome additional work on the packages may be done later in the day.\n\n\nAll packages to be tested, need to be promoted to the testing repository by the end of the day.\n\n\nWe will schedule testing for all packages at this date.\n\n\nIf a package is promoted after this date, it may not get tested in time for the release.\n\n\n\n\n\n\nPackage Freeze (end of the day on the Monday one week before release):\n\n\nThe intended release set is determined.\n\n\nPackages that have been tested will be promoted to the prerelease repository as they are ready.\n\n\nNew packages that have short predictable testing cycle will be accepted for release.\n\n\n\n\n\n\nPrerelease Freeze (2 PM Central on the Friday before release):\n\n\nLast minute decision of which packages are included.\n\n\nPackages will be run through the automated tests just prior to release.\n\n\nTarball clients will be created just prior to release.\n\n\n\n\n\n\nRelease (second or fourth Tuesday of the month):\n\n\nThe software packages are placed in the release repositories.\n\n\nThe release is announced.\n\n\n\n\n\n\n\n\nIf a freeze date falls on a holiday, it will be moved to the next business day. Exceptions for freezes may be made at the discretion of the Release Manager.\n\n\nExample Schedule\n\n\nExample schedule for the September 2017 release:\n\n\n\n\n\n\n\n\nPhase\n\n\nDate\n\n\n\n\n\n\n\n\n\n\nDevelopment Freeze\n\n\nMonday, 8/28 at 5 PM\n\n\n\n\n\n\nPackage Freeze\n\n\nMonday, 9/5 at 5 PM\n\n\n\n\n\n\nPrerelease Freeze\n\n\nFriday, 9/9 at 2 PM\n\n\n\n\n\n\nRelease\n\n\nTuesday, 9/12", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/release/release-schedule/#osg-software-release-schedule", 
            "text": "To help with predictability and align with the GOC update schedule, we have adopted a fixed release schedule. OSG Software will be released on the second Tuesday of the month. Urgent releases could happen on the fourth Tuesday of the month.", 
            "title": "OSG Software Release Schedule"
        }, 
        {
            "location": "/release/release-schedule/#release-cycle-phases", 
            "text": "Various \"Freeze\" dates will happen in the weeks leading up to the release. They are as follows:   Development Freeze (end of the day on the Monday two weeks before release):  The remaining testing is planned for this release.  Packages are considered for inclusion in the release on this date.  Some additional work on the packages may be done later in the day.  All packages to be tested, need to be promoted to the testing repository by the end of the day.  We will schedule testing for all packages at this date.  If a package is promoted after this date, it may not get tested in time for the release.    Package Freeze (end of the day on the Monday one week before release):  The intended release set is determined.  Packages that have been tested will be promoted to the prerelease repository as they are ready.  New packages that have short predictable testing cycle will be accepted for release.    Prerelease Freeze (2 PM Central on the Friday before release):  Last minute decision of which packages are included.  Packages will be run through the automated tests just prior to release.  Tarball clients will be created just prior to release.    Release (second or fourth Tuesday of the month):  The software packages are placed in the release repositories.  The release is announced.     If a freeze date falls on a holiday, it will be moved to the next business day. Exceptions for freezes may be made at the discretion of the Release Manager.", 
            "title": "Release Cycle Phases"
        }, 
        {
            "location": "/release/release-schedule/#example-schedule", 
            "text": "Example schedule for the September 2017 release:     Phase  Date      Development Freeze  Monday, 8/28 at 5 PM    Package Freeze  Monday, 9/5 at 5 PM    Prerelease Freeze  Friday, 9/9 at 2 PM    Release  Tuesday, 9/12", 
            "title": "Example Schedule"
        }, 
        {
            "location": "/release/new-release-series/", 
            "text": "How to Prepare a New Release Series\n\n\n\n\nWarning\n\n\nThis document was specifically written for OSG 3.4 and will need to be\nadapted to work for 3.5, 3.6, etc.\n\n\nThe information was taken from\n\nSOFTWARE-2622\n\nand\n\nMatyas's notes for the OSG 3.4 release\n.\n\n\n\n\nDo first, anytime before the month of the release\n\n\n\n\n\n\nAdd 3.4 koji tags and targets\n\n\n\n\nModify this script as appropriate and run: \nhttps://github.com/opensciencegrid/osg-next-tools/blob/osg34/koji/create-new-koji-osg34-tags-etc\n\n\nAt first, upcoming-build should continue to inherit from 3.3-devel\n\n\nCreate osg-3.4-elX-bootstrap and have the build tag inherit from it\n\n\n\n\n\n\n\n\nAdd Koji package signing\n\n\n\n\nEdit /etc/koji-hub/plugins/sign.conf; copy and modify one of the other osg entries\n\n\nEnsure the permissions are 0600 apache:apache\n\n\nSave the result with \netckeeper commit\n\n\n\n\n\n\n\n\nUpdate \nosg-build\n to use the new koji tags and targets (not by default of course)\n\n\n\n\nSee the \nGit commits\n on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this\n\n\nYou will be using this version of \nosg-build\n for some tasks, even if it hasn't been released\n\n\n\n\n\n\n\n\nDo afterward, anytime before the month of the release\n\n\n\n\n\n\nCreate a blank \nosg-3.4\n SVN branch and add \nbuildsys-macros\n package\n\n\n\n\n\n\nsvn copy \nbuildsys-macros\n from trunk and hand-edit it to hardcode all the new values\n\n\nFor EL 6:\n\n\n\n\nEdit the spec file and set \ndver\n to 6\n\n\nRun the following commands (adjust the NVR as necessary):\n$\n osg-build rpmbuild --el6\n\n$\n osg-koji import _build_results/buildsys-macros-*.el6.src.rpm\n\n$\n osg-koji import _build_results/buildsys-macros-*.el6.noarch.rpm\n\n$\n osg-koji tag-pkg osg-3.4-el6-development buildsys-macros-7-5.osg34.el6\n\n\n\n\n\n\n\n\n\nThen do the same for EL 7:\n\n\n\n\nEdit the spec file and set \ndver\n to 7\n\n\nRun the following commands (adjust the NVR as necessary):\n$\n osg-build rpmbuild --el7\n\n$\n osg-koji import _build_results/buildsys-macros-*.el7.src.rpm\n\n$\n osg-koji import _build_results/buildsys-macros-*.el7.noarch.rpm\n\n$\n osg-koji tag-pkg osg-3.4-el7-development buildsys-macros-7-5.osg34.el7\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo a 'real' build of \nbuildsys-macros\n\n\n\n\n\n\nBump the revision in the \nbuildsys-macros\n spec file and edit the \n%changelog\n.\n    \nAgain, you will need a version of osg-build with 3.4 support.\n\n\n\n\n\n\nSet \ndver\n to 6. Commit\n\n\n$\n osg-build koji --repo\n=\n3\n.4 --el6\n\n\n\n\n\n\n\n\n\nSet \ndver\n to 7. Commit\n\n\n$\n osg-build koji --repo\n=\n3\n.4 --el7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate \ntarball-client\n scripts\n\n\n\n\nbundles.ini\n\n\npatches/\n\n\nupload-tarballs-to-oasis\n (for 3.4, \nforeach_dver_arch\n will need to be updated because we're dropping i386 support)\n\n\n\n\n\n\n\n\nPopulate the \nbootstrap\n tags\n\n\nNeed to have them inherit from the 3.3 development tags, but only packages, not builds (hence the \n--noconfig\n; yes, the name is weird)\n\n\n$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n    \nfor\n repo in \n3\n.3 upcoming\n;\n \ndo\n \n\\\n\n        osg-koji add-tag-inheritance --noconfig --priority\n=\n2\n \n\\\n\n            osg-3.4-\n$el\n-bootstrap osg-\n$repo\n-\n$el\n-development\n;\n \n\\\n\n    \ndone\n;\n \n\\\n\n\ndone\n\n\n\n\n\n\n\n\n\n\nGet the actual NVRs to tag\n\n\n\n\nI put Brian's spreadsheet into Excel and used its filtering feature to separate out:\n\n\nthe packages going into 3.4.0\n\n\nthe el6 versus el7 packages\n\n\n\n\n\n\nand copied the column containing the NVRs into Vim, and did a search\nreplace of the dver to the appropriate version\n\n\nsaved two text files (pkgtotag-el6.txt and pkgtotag-el7.txt)\n\n\n\n\nTagging:\n\n\n$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n    xargs -a pkgtotag-\n$el\n osg-koji tag-pkg osg-3.4-\n$el\n-bootstrap\n;\n \n\\\n\n\ndone\n\n\n\n\n\n\n(btw, xargs -a doesn't work on a Mac)\n\n\n\n\n\n\n\n\n\n\nDo on the month of the 3.4.0 release\n\n\n\n\nPopulate SVN branch and tags (as in fill it with the packages we're going to release for 3.4)\n\n\nMass rebuild\n\n\nDon't forget to update the \nempty\n and \ncontrib\n tags with the appropriate packages;\n    \nremove the \nempty*\n packages from the development tags after they've been tagged into the \nempty\n tags\n\n\n\n\n\n\nUpdate mash (coordinate with steige)\n\n\nOn repo-itb\n\n\nOn repo1/repo2\n\n\n\n\n\n\nUpdate documentation at\n    \nhttps://opensciencegrid.github.io/technology/software/development-process/\n\n\nUpdate osg-test / vmu-test-runs\n\n\nThey're only going to test from minefield (and eventurally testing) until the release\n\n\n\n\n\n\n\n\nDo immediately after the 3.4.0 release\n\n\n\n\nUpdate tag inheritance on the \nupcoming-build\n tags to inherit from \n3.4-devel\n instead of \n3.3-devel\n\n\nDrop the \nosg-3.4-elX-bootstrap\n koji tags\n\n\n\n\nDo sometime after the 3.4.0 release\n\n\n\n\nDo these three at the same time:\n\n\nMove the SVN \ntrunk\n to \nbranches/osg-3.3\n and move \nbranches/osg-3.4\n to \ntrunk\n\n\nUpdate the koji \nosg-elX\n build targets to build from and to 3.4 instead of 3.3\n\n\nNotify the software list of this change\n\n\n\n\n\n\nUpdate osg-test / vmu-test-runs again to add release and release -\n testing tests\n\n\n\n\nUpdate the docker-osg-wn-client scripts to build from 3.4 (need direct push access)\n\n\n\n\nUpdate the constants in the \ngenbranches\n script in the \ndocker-osg-wn-scripts\n repo\n\n\n\n\nUpdate the branches in \ndocker-osg-wn-client\n; a script like this ought to work:\n\n\ngit clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\n\ncd\n docker-osg-wn-scripts\n./genbranches\n\ncd\n ../docker-osg-wn\n\nfor\n bpath in ../docker-osg-wn-scripts/branches/*\n;\n \ndo\n\n    \nb\n=\n${\nbpath\n##*/\n}\n\n    git checkout -b \n$b\n master \n \n\\\n\n        mv \n$bpath\n Dockerfile.in \n \n\\\n\n        git add Dockerfile.in \n \n\\\n\n        git commit -m \nAdd branch \n$b\n\n\ndone\n\n\n\n\n\n\nand then run a similar script to update the existing branches\nCheck the results before pushing, and then run \ngit push --all\n\n\n\n\n\n\nUpdate the arrays in \nupdate-all\n and \nosg-wn-nightly-build\n in \ndocker-osg-wn-scripts\n\n\n\n\n\n\n\n\n\n\nUpdate the default promotion route aliases in \nosg-promote\n\n\n\n\n\n\nUpdate documentation again to reflect that 3.4 is now the \nmain\n branch and 3.3 is the \nmaintenance\n branch:\n    \nhttps://opensciencegrid.github.io/technology/software/development-process/", 
            "title": "New Release Series"
        }, 
        {
            "location": "/release/new-release-series/#how-to-prepare-a-new-release-series", 
            "text": "Warning  This document was specifically written for OSG 3.4 and will need to be\nadapted to work for 3.5, 3.6, etc.  The information was taken from SOFTWARE-2622 \nand Matyas's notes for the OSG 3.4 release .", 
            "title": "How to Prepare a New Release Series"
        }, 
        {
            "location": "/release/new-release-series/#do-first-anytime-before-the-month-of-the-release", 
            "text": "Add 3.4 koji tags and targets   Modify this script as appropriate and run:  https://github.com/opensciencegrid/osg-next-tools/blob/osg34/koji/create-new-koji-osg34-tags-etc  At first, upcoming-build should continue to inherit from 3.3-devel  Create osg-3.4-elX-bootstrap and have the build tag inherit from it     Add Koji package signing   Edit /etc/koji-hub/plugins/sign.conf; copy and modify one of the other osg entries  Ensure the permissions are 0600 apache:apache  Save the result with  etckeeper commit     Update  osg-build  to use the new koji tags and targets (not by default of course)   See the  Git commits  on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this  You will be using this version of  osg-build  for some tasks, even if it hasn't been released", 
            "title": "Do first, anytime before the month of the release"
        }, 
        {
            "location": "/release/new-release-series/#do-afterward-anytime-before-the-month-of-the-release", 
            "text": "Create a blank  osg-3.4  SVN branch and add  buildsys-macros  package    svn copy  buildsys-macros  from trunk and hand-edit it to hardcode all the new values  For EL 6:   Edit the spec file and set  dver  to 6  Run the following commands (adjust the NVR as necessary): $  osg-build rpmbuild --el6 $  osg-koji import _build_results/buildsys-macros-*.el6.src.rpm $  osg-koji import _build_results/buildsys-macros-*.el6.noarch.rpm $  osg-koji tag-pkg osg-3.4-el6-development buildsys-macros-7-5.osg34.el6    Then do the same for EL 7:   Edit the spec file and set  dver  to 7  Run the following commands (adjust the NVR as necessary): $  osg-build rpmbuild --el7 $  osg-koji import _build_results/buildsys-macros-*.el7.src.rpm $  osg-koji import _build_results/buildsys-macros-*.el7.noarch.rpm $  osg-koji tag-pkg osg-3.4-el7-development buildsys-macros-7-5.osg34.el7      Do a 'real' build of  buildsys-macros    Bump the revision in the  buildsys-macros  spec file and edit the  %changelog .\n     Again, you will need a version of osg-build with 3.4 support.    Set  dver  to 6. Commit  $  osg-build koji --repo = 3 .4 --el6    Set  dver  to 7. Commit  $  osg-build koji --repo = 3 .4 --el7        Update  tarball-client  scripts   bundles.ini  patches/  upload-tarballs-to-oasis  (for 3.4,  foreach_dver_arch  will need to be updated because we're dropping i386 support)     Populate the  bootstrap  tags  Need to have them inherit from the 3.3 development tags, but only packages, not builds (hence the  --noconfig ; yes, the name is weird)  $   for  el in el6 el7 ;   do   \\ \n     for  repo in  3 .3 upcoming ;   do   \\ \n        osg-koji add-tag-inheritance --noconfig --priority = 2   \\ \n            osg-3.4- $el -bootstrap osg- $repo - $el -development ;   \\ \n     done ;   \\  done     Get the actual NVRs to tag   I put Brian's spreadsheet into Excel and used its filtering feature to separate out:  the packages going into 3.4.0  the el6 versus el7 packages    and copied the column containing the NVRs into Vim, and did a search replace of the dver to the appropriate version  saved two text files (pkgtotag-el6.txt and pkgtotag-el7.txt)   Tagging:  $   for  el in el6 el7 ;   do   \\ \n    xargs -a pkgtotag- $el  osg-koji tag-pkg osg-3.4- $el -bootstrap ;   \\  done   (btw, xargs -a doesn't work on a Mac)", 
            "title": "Do afterward, anytime before the month of the release"
        }, 
        {
            "location": "/release/new-release-series/#do-on-the-month-of-the-340-release", 
            "text": "Populate SVN branch and tags (as in fill it with the packages we're going to release for 3.4)  Mass rebuild  Don't forget to update the  empty  and  contrib  tags with the appropriate packages;\n     remove the  empty*  packages from the development tags after they've been tagged into the  empty  tags    Update mash (coordinate with steige)  On repo-itb  On repo1/repo2    Update documentation at\n     https://opensciencegrid.github.io/technology/software/development-process/  Update osg-test / vmu-test-runs  They're only going to test from minefield (and eventurally testing) until the release", 
            "title": "Do on the month of the 3.4.0 release"
        }, 
        {
            "location": "/release/new-release-series/#do-immediately-after-the-340-release", 
            "text": "Update tag inheritance on the  upcoming-build  tags to inherit from  3.4-devel  instead of  3.3-devel  Drop the  osg-3.4-elX-bootstrap  koji tags", 
            "title": "Do immediately after the 3.4.0 release"
        }, 
        {
            "location": "/release/new-release-series/#do-sometime-after-the-340-release", 
            "text": "Do these three at the same time:  Move the SVN  trunk  to  branches/osg-3.3  and move  branches/osg-3.4  to  trunk  Update the koji  osg-elX  build targets to build from and to 3.4 instead of 3.3  Notify the software list of this change    Update osg-test / vmu-test-runs again to add release and release -  testing tests   Update the docker-osg-wn-client scripts to build from 3.4 (need direct push access)   Update the constants in the  genbranches  script in the  docker-osg-wn-scripts  repo   Update the branches in  docker-osg-wn-client ; a script like this ought to work:  git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git cd  docker-osg-wn-scripts\n./genbranches cd  ../docker-osg-wn for  bpath in ../docker-osg-wn-scripts/branches/* ;   do \n     b = ${ bpath ##*/ } \n    git checkout -b  $b  master    \\ \n        mv  $bpath  Dockerfile.in    \\ \n        git add Dockerfile.in    \\ \n        git commit -m  Add branch  $b  done   and then run a similar script to update the existing branches\nCheck the results before pushing, and then run  git push --all    Update the arrays in  update-all  and  osg-wn-nightly-build  in  docker-osg-wn-scripts      Update the default promotion route aliases in  osg-promote    Update documentation again to reflect that 3.4 is now the  main  branch and 3.3 is the  maintenance  branch:\n     https://opensciencegrid.github.io/technology/software/development-process/", 
            "title": "Do sometime after the 3.4.0 release"
        }, 
        {
            "location": "/release/old-release-removal/", 
            "text": "Old Release Series Removal Plan\n\n\nIn order to reduce clutter and disk usage on our repositories and build system,\nwe will remove older OSG Software release series.  This will result in packages\nfrom those series becoming unavailable, so we will remove a release series when\nits packages are no longer needed.\n\n\nWe will remove a release series when the \nfollowing\n series is completely out\nof support.  For example, OSG 3.1 will be removed when OSG 3.2 is out of\nsupport, and OSG 3.2 will be removed when OSG 3.3 is out of support.\n\n\nTasks\n\n\nRemoving a release series requires work from both Operations and Software \n\nRelease.  The first step is to create a JIRA ticket in the SOFTWARE project to\ntrack the work.  Second, Software \n Release will enumerate the directories for\nOperations to remove.\n\n\nOperations tasks should be completed before Software \n Release tasks.\n\n\nOperations\n\n\nThese tasks should be completed in order.\n\n\n\n\n\n\nTwo weeks in advance, notify sites (including mirror sites) that the\n    release series is going away.  See the \ntemplate email\n\n    below.\n\n\n\n\n\n\nRemove the series from the mash configs on the repo.opensciencegrid.org machines:\n\n\n\n\nAdd the koji tags for the old series to the \n/usr/local/osg-tags.excluded\n file:\n```\n\n\n\n\ncd /usr/local\n\n\nfgrep osg-3.1 osg-tags\n\n\nosg-3.1-el5-contrib\nosg-3.1-el5-development\nosg-3.1-el5-release\nosg-3.1-el5-testing\nosg-3.1-el6-contrib\nosg-3.1-el6-development\nosg-3.1-el6-release\nosg-3.1-el6-testing\n\n\nfgrep osg-3.1 osg-tags \n osg-tags.exclude\n\n\n- Re-run `update_mashfiles.sh` to update the mash config files:\n\n\n./update_mashfiles.sh\n\n\n```\n\n\n\n\n\n\nRemove the appropriate repo directories from \n/usr/local/repo/osg\n.\n    \n# rm -rf repo*/osg/3.1/\n\n\n\n\n\n\nReclaim space from any cached rpms in the mash cache which are no longer linked elsewhere:\n    \n# find mash/cache/ -name \\*.rpm -type f -links 1 -delete\n\n\n\n\n\n\nWait for mash to run and verify that the repos are no longer getting\n    updated:\n\n\n\n\nLook at the mash logs in \n/var/log/repo\n.\n\n\nVerify that mash did not recreate the repo directory under\n   \n/usr/local/repo/osg\n corresponding to the old release series.\n\n\n\n\n\n\n\n\nSoftware \n Release\n\n\nThese tasks can be completed in any order.\n\n\n\n\n\n\nTag and remove the SVN branch corresponding to the release series.\n\n\n\n\n\n\nEdit \nvm-test-runs\n and remove any \"long tail\" tests that reference the\n  series.\n\n\n\n\n\n\nEdit \ntarball-client\n:\n\n\n\n\nRemove bundles from \nbundles.ini\n.\n\n\nRemove patch and other files that were used only by those bundles.\n\n\nTest that the current bundles didn't get broken by your changes.\n\n\n\n\n\n\n\n\nEdit \nosg-build\n:\n\n\n\n\nRemove the promotion routes from \npromoter.ini\n.\n\n\nRemove references in \nconstants.py\n.\n\n\nTest your changes; also run the unit tests.\n\n\n\n\n\n\n\n\nRemove things from Koji:\n\n\n\n\nAll targets referencing the series.\n\n\nAll tags referencing the series.\n\n\n\n\n\n\n\n\nRemove references to the series from \nopensciencegrid/docker-osg-wn-scripts\n\n  on GitHub, including the \ngenbranches\n and \nupdate-all\n scripts.\n\n\n\n\n\n\nRemove branches from \nopensciencegrid/docker-osg-wn\n on GitHub.\n\n\n\n\n\n\nMove files in \n/p/vdt/public/html/release-info\n to its \nattic\n subdirectory.\n\n\n\n\n\n\nUndoing\n\n\nIf we really need RPMs from a removed release series, we can look at the text\nfiles in \n/p/vdt/public/html/release-info/attic\n to determine the exact NVRs we\nneed, and download them from Koji.\n\n\nTemplate Email\n\n\n\n\nOn \nDAYNAME, MONTH DAY\n, the OSG will be removing the OSG \n3.X\n release\n  series from our repositories.  This includes both RPMs and tarballs hosted\n  on repo.opensciencegrid.org.\n\n\nAll support for OSG \n3.X\n had been discontinued at the end of \nMONTH YEAR\n.\n\n\nAny sites running OSG \n3.X\n should upgrade to the current release series,\n  OSG \n3.Y\n.  If you need assistance upgrading, please contact us at\n  goc@opensciencegrid.org.\n\n\n\n\nIf we're dropping support for a distro (e.g. EL 5 when we drop OSG 3.2), add\nthe following after the first paragraph:\n\n\n\n\nNote that OSG \n3.X\n was the last release to support Enterprise Linux \nZ\n\n  distributions.", 
            "title": "Old Release Series Removal"
        }, 
        {
            "location": "/release/old-release-removal/#old-release-series-removal-plan", 
            "text": "In order to reduce clutter and disk usage on our repositories and build system,\nwe will remove older OSG Software release series.  This will result in packages\nfrom those series becoming unavailable, so we will remove a release series when\nits packages are no longer needed.  We will remove a release series when the  following  series is completely out\nof support.  For example, OSG 3.1 will be removed when OSG 3.2 is out of\nsupport, and OSG 3.2 will be removed when OSG 3.3 is out of support.", 
            "title": "Old Release Series Removal Plan"
        }, 
        {
            "location": "/release/old-release-removal/#tasks", 
            "text": "Removing a release series requires work from both Operations and Software  \nRelease.  The first step is to create a JIRA ticket in the SOFTWARE project to\ntrack the work.  Second, Software   Release will enumerate the directories for\nOperations to remove.  Operations tasks should be completed before Software   Release tasks.", 
            "title": "Tasks"
        }, 
        {
            "location": "/release/old-release-removal/#operations", 
            "text": "These tasks should be completed in order.    Two weeks in advance, notify sites (including mirror sites) that the\n    release series is going away.  See the  template email \n    below.    Remove the series from the mash configs on the repo.opensciencegrid.org machines:   Add the koji tags for the old series to the  /usr/local/osg-tags.excluded  file:\n```", 
            "title": "Operations"
        }, 
        {
            "location": "/release/old-release-removal/#cd-usrlocal", 
            "text": "", 
            "title": "cd /usr/local"
        }, 
        {
            "location": "/release/old-release-removal/#fgrep-osg-31-osg-tags", 
            "text": "osg-3.1-el5-contrib\nosg-3.1-el5-development\nosg-3.1-el5-release\nosg-3.1-el5-testing\nosg-3.1-el6-contrib\nosg-3.1-el6-development\nosg-3.1-el6-release\nosg-3.1-el6-testing", 
            "title": "fgrep osg-3.1 osg-tags"
        }, 
        {
            "location": "/release/old-release-removal/#fgrep-osg-31-osg-tags-osg-tagsexclude", 
            "text": "- Re-run `update_mashfiles.sh` to update the mash config files:", 
            "title": "fgrep osg-3.1 osg-tags &gt;&gt; osg-tags.exclude"
        }, 
        {
            "location": "/release/old-release-removal/#update_mashfilessh", 
            "text": "```    Remove the appropriate repo directories from  /usr/local/repo/osg .\n     # rm -rf repo*/osg/3.1/    Reclaim space from any cached rpms in the mash cache which are no longer linked elsewhere:\n     # find mash/cache/ -name \\*.rpm -type f -links 1 -delete    Wait for mash to run and verify that the repos are no longer getting\n    updated:   Look at the mash logs in  /var/log/repo .  Verify that mash did not recreate the repo directory under\n    /usr/local/repo/osg  corresponding to the old release series.", 
            "title": "./update_mashfiles.sh"
        }, 
        {
            "location": "/release/old-release-removal/#software-release", 
            "text": "These tasks can be completed in any order.    Tag and remove the SVN branch corresponding to the release series.    Edit  vm-test-runs  and remove any \"long tail\" tests that reference the\n  series.    Edit  tarball-client :   Remove bundles from  bundles.ini .  Remove patch and other files that were used only by those bundles.  Test that the current bundles didn't get broken by your changes.     Edit  osg-build :   Remove the promotion routes from  promoter.ini .  Remove references in  constants.py .  Test your changes; also run the unit tests.     Remove things from Koji:   All targets referencing the series.  All tags referencing the series.     Remove references to the series from  opensciencegrid/docker-osg-wn-scripts \n  on GitHub, including the  genbranches  and  update-all  scripts.    Remove branches from  opensciencegrid/docker-osg-wn  on GitHub.    Move files in  /p/vdt/public/html/release-info  to its  attic  subdirectory.", 
            "title": "Software &amp; Release"
        }, 
        {
            "location": "/release/old-release-removal/#undoing", 
            "text": "If we really need RPMs from a removed release series, we can look at the text\nfiles in  /p/vdt/public/html/release-info/attic  to determine the exact NVRs we\nneed, and download them from Koji.", 
            "title": "Undoing"
        }, 
        {
            "location": "/release/old-release-removal/#template-email", 
            "text": "On  DAYNAME, MONTH DAY , the OSG will be removing the OSG  3.X  release\n  series from our repositories.  This includes both RPMs and tarballs hosted\n  on repo.opensciencegrid.org.  All support for OSG  3.X  had been discontinued at the end of  MONTH YEAR .  Any sites running OSG  3.X  should upgrade to the current release series,\n  OSG  3.Y .  If you need assistance upgrading, please contact us at\n  goc@opensciencegrid.org.   If we're dropping support for a distro (e.g. EL 5 when we drop OSG 3.2), add\nthe following after the first paragraph:   Note that OSG  3.X  was the last release to support Enterprise Linux  Z \n  distributions.", 
            "title": "Template Email"
        }, 
        {
            "location": "/release/itb-testing/", 
            "text": "Testing OSG Software Prereleases on the Madison ITB Site\n\n\nThis document contains basic recipes for testing a OSG software prereleases on the Madison ITB site, which includes\nHTCondor prerelease builds and full OSG software stack prereleases from Yum.\n\n\nPrerequisites\n\n\nThe following items are known prerequisites to using this recipe.  If you are not running the Ansible commands from\nosghost, there are almost certainly other prerequisites that are not listed below.  And even using osghost for Ansible\nand itb-submit for the submissions, there may be other prerequisites missing.  Please improve this document by adding\nother prerequisites as they are identified!\n\n\n\n\nA checkout of the osgitb directory from our local git instance (not GitHub)\n\n\nYour X.509 DN in the \nosgitb/unmanaged/htcondor-ce/grid-mapfile\n file and (via Ansible) on \nitb-ce1\n and \nitb-ce2\n\n\n\n\nGathering Information\n\n\nTechnically skippable, this section is about checking on the state of the ITB machines before making changes.  The plan\nis to keep the ITB machines generally up-to-date independently, so those steps are not listed here.  And honestly, the\nsteps below are just some ideas; do whatever makes sense for the given update.\n\n\nThe commands can be run as-is from within the \nosgitb\n directory from git.\n\n\n\n\n\n\nCheck OS versions for all current ITB hosts:\n\n\nansible current -i inventory -f 20 -o -m command -a \ncat /etc/redhat-release\n\n\n\n\n\n\n\n\n\n\nCheck the date and time on all hosts (in case NTP stops working):\n\n\nansible current -i inventory -f 20 -o -m command -a \ndate\n\n\n\n\n\n\n\n\n\n\nCheck software versions for certain hosts (e.g., for the \ncondor\n package on hosts in the \nworkers\n group):\n\n\nansible workers -i inventory -f 20 -o -m command -a \nrpm -q condor\n\n\n\n\n\n\n\n\n\n\nInstalling HTCondor Prerelease\n\n\nUse this section to install a new version of HTCondor, specifically a prerelease build from the development or\nupcoming-development repository, on the test hosts.\n\n\n\n\n\n\nObtain the NVR of the HTCondor prerelease build from OSG to test.  Do this by talking to Tim\nT. and checking\n   Koji.\n\n\n\n\n\n\nShut down HTCondor and HTCondor-CE on prerelease machines:\n\n\nansible \ntesting:\nces\n -i inventory -bK -f 20 -m service -a \nname=condor-ce state=stopped\n\n\nansible \ntesting:\ncondor\n -i inventory -bK -f 20 -m service -a \nname=condor state=stopped\n\n\n\n\n\n\n\n\n\n\nInstall new version of HTCondor on prerelease machines:\n\n\nansible \ntesting:\ncondor\n -i inventory -bK -f 10 -m command -a \nyum --enablerepo=osg-development --assumeyes update condor\n\n\n\n\n\n\nor, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed:\n\n\nansible \ntesting:\ncondor\n -i inventory -bK -f 10 -m command -a \nyum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp\n\n\n\n\n\n\n\n\n\n\nVerify correct RPM versions across the site:\n\n\nansible condor -i inventory -f 20 -o -m command -a \nrpm -q condor\n\n\n\n\n\n\n\n\n\n\nRestart HTCondor and HTCondor-CE on prerelease machines:\n\n\nansible \ntesting:\ncondor\n -i inventory -bK -f 20 -m service -a \nname=condor state=started\n\n\nansible \ntesting:\nces\n -i inventory -bK -f 20 -m service -a \nname=condor-ce state=started\n\n\n\n\n\n\n\n\n\n\nInstalling a Prerelease of the OSG Software Stack\n\n\nUse this section to install new versions of all OSG software from a prerelease repository in Yum.\n\n\n\n\n\n\nCheck with the Release Manager to make sure that the prerelease repository has been populated with the desired\n   package versions.\n\n\n\n\n\n\nMake sure that software is generally up-to-date on the hosts\n\u2014 see\n   \nthe MadisonITB page\n for more details\n\n\nIt may be desirable to update only non-OSG software at this stage, in which case one could simply disable the OSG\nrepositories by adding command-line options to the \nyum update\n commands.\n\n\n\n\n\n\nInstall new software on prerelease hosts:\n\n\nansible testing -i inventory -bK -f 20 -m command -a \nyum --enablerepo=osg-prerelease --assumeyes update\n\n\n\n\n\n\n\n\n\n\nRead the Yum output carefully, and follow up on any warnings, etc.\n\n\n\n\n\n\nIf the \nosg-configure\n package was updated on any host(s), run the \nosg-configure\n command on the host(s):\n\n\nansible testing -i inventory -bK -f 20 -m command -a \nosg-configure -v\n -l [HOST(S)]\n\n\nansible testing -i inventory -bK -f 20 -m command -a \nosg-configure -c\n -l [HOST(S)]\n\n\n\n\n\n\n\n\n\n\nVerify OSG software updates by inspecting the Yum output carefully or examining specific package versions:\n\n\nansible current -i inventory -f 20 -o -m command -a \nrpm -q osg-wn-client\n\n\n\n\n\n\nUse an inventory group and package names that best fit the situation.\n\n\n\n\n\n\nRunning Tests\n\n\nFor the first two test workflows, use your personal space on \nitb-submit\n.  Copy or checkout the \nosgitb/htcondor-tests\n\ndirectory to get the test directories.\n\n\nPart \u2160: Submitting jobs directly\n\n\n\n\n\n\nChange into the \n1-direct-jobs\n subdirectory\n\n\n\n\n\n\nIf there are old result files in the directory, remove them:\n\n\nmake distclean\n\n\n\n\n\n\n\n\n\n\nSubmit the test workflow\n\n\ncondor_submit_dag test.dag\n\n\n\n\n\n\n\n\n\n\nMonitor the jobs until they are complete or stuck\n\n\nIn the initial test runs, the entire workflow ran in a few minutes.  If the DAG or jobs exit immediately, go on\nhold, or otherwise fail, then you have some troubleshooting to do!  Keep trying steps 2 and 3 until you get a clean\nrun (or one or more HTCondor bug tickets).\n\n\n\n\n\n\nCheck the final output file:\n\n\ncat count-by-hostnames.txt\n\n\n\n\n\n\nYou should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per\nmachine and the fact that HTCondor can and will reuse claims to process many jobs on a single host.  Especially\nwatch out for a case in which no jobs run on the newly updated hosts (at the time of writing: \nitb-data[456]\n).\n\n\n\n\n\n\n(Optional) Clean up, using the \nmake clean\n or \nmake distclean\n commands.  Use the \nclean\n target to remove\n   intermediate result and log files generated by a workflow run but preserve the final output file; use the \ndistclean\n\n   target to remove all workflow-generated files (plus Emacs backup files).\n\n\n\n\n\n\nPart \u2161: Submitting jobs using HTCondor-C\n\n\nIf direct submissions fail, there is probably no point to doing this step.\n\n\n\n\n\n\nChange into the \n2-htcondor-c-jobs\n subdirectory\n\n\n\n\n\n\nIf there are old result files in the directory, remove them:\n\n\nmake distclean\n\n\n\n\n\n\n\n\n\n\nGet a proxy for your X.509 credentials\n\n\nvoms-proxy-init\n\n\n\n\n\n\n\n\n\n\nSubmit the test workflow\n\n\ncondor_submit_dag test.dag\n\n\n\n\n\n\n\n\n\n\nMonitor the jobs until they are complete or stuck\n\n\nIn the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the\ndirect submission test, because of the layers of indirection.  Also, status updates from the CEs back to the submit\nhost are infrequent.  For direct information about the CEs, log in to \nitb-ce1\n and \nitb-ce2\n to check status; don\u2019t\nforget to check both \ncondor_ce_q\n and \ncondor_q\n on the CEs, probably in that order.\n\n\nIf the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do!  Keep\ntrying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets).\n\n\n\n\n\n\nCheck the final output file:\n\n\ncat count-by-hostnames.txt\n\n\n\n\n\n\nAgain, look for a reasonable distribution of jobs by hostname.\n\n\n\n\n\n\n(Optional) Clean up, using the \nmake clean\n or \nmake distclean\n commands.\n\n\n\n\n\n\nPart \u2162: Submitting jobs from a GlideinWMS VO Frontend\n\n\nFor this workflow, use your personal space on \nglidein3.chtc.wisc.edu\n.  Copy or checkout the \nosgitb/htcondor-tests\n\ndirectory to get the test directories.  Again, if previous steps fail, do not bother with this step.\n\n\n\n\n\n\nChange into the \n3-frontend-jobs\n subdirectory\n\n\n\n\n\n\nIf there are old result files in the directory, remove them:\n\n\nmake distclean\n\n\n\n\n\n\n\n\n\n\nSubmit the test workflow\n\n\ncondor_submit_dag test.dag\n\n\n\n\n\n\n\n\n\n\nMonitor the jobs until they are complete or stuck\n\n\nThis workflow could take much longer than the first two, maybe an hour or so.  Also, unless there are active\nglideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs.  Thus it is\nhelpful to monitor \ncondor_q -totals\n until all of the jobs are submitted (there should be 2001), then switch to\nmonitoring \ncondor_status\n until glideins start appearing.  After the first jobs start running and finishing, it is\nprobably safe to ignore the rest of the run.  If the jobs do not appear in the local queue, if glideins do not\nappear, or if jobs do not start running on the glideins, it is time to start troubleshooting.\n\n\n\n\n\n\nCheck the final output file:\n\n\ncat count-by-hostnames.txt\n\n\n\n\n\n\nThe distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in\nwhich pilots ramp up over time and how HTCondor allocates jobs to slots.\n\n\n\n\n\n\n(Optional) Clean up, using the \nmake clean\n or \nmake distclean\n commands.", 
            "title": "ITB Prerelease Testing"
        }, 
        {
            "location": "/release/itb-testing/#testing-osg-software-prereleases-on-the-madison-itb-site", 
            "text": "This document contains basic recipes for testing a OSG software prereleases on the Madison ITB site, which includes\nHTCondor prerelease builds and full OSG software stack prereleases from Yum.", 
            "title": "Testing OSG Software Prereleases on the Madison ITB Site"
        }, 
        {
            "location": "/release/itb-testing/#prerequisites", 
            "text": "The following items are known prerequisites to using this recipe.  If you are not running the Ansible commands from\nosghost, there are almost certainly other prerequisites that are not listed below.  And even using osghost for Ansible\nand itb-submit for the submissions, there may be other prerequisites missing.  Please improve this document by adding\nother prerequisites as they are identified!   A checkout of the osgitb directory from our local git instance (not GitHub)  Your X.509 DN in the  osgitb/unmanaged/htcondor-ce/grid-mapfile  file and (via Ansible) on  itb-ce1  and  itb-ce2", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/release/itb-testing/#gathering-information", 
            "text": "Technically skippable, this section is about checking on the state of the ITB machines before making changes.  The plan\nis to keep the ITB machines generally up-to-date independently, so those steps are not listed here.  And honestly, the\nsteps below are just some ideas; do whatever makes sense for the given update.  The commands can be run as-is from within the  osgitb  directory from git.    Check OS versions for all current ITB hosts:  ansible current -i inventory -f 20 -o -m command -a  cat /etc/redhat-release     Check the date and time on all hosts (in case NTP stops working):  ansible current -i inventory -f 20 -o -m command -a  date     Check software versions for certain hosts (e.g., for the  condor  package on hosts in the  workers  group):  ansible workers -i inventory -f 20 -o -m command -a  rpm -q condor", 
            "title": "Gathering Information"
        }, 
        {
            "location": "/release/itb-testing/#installing-htcondor-prerelease", 
            "text": "Use this section to install a new version of HTCondor, specifically a prerelease build from the development or\nupcoming-development repository, on the test hosts.    Obtain the NVR of the HTCondor prerelease build from OSG to test.  Do this by talking to Tim T. and checking\n   Koji.    Shut down HTCondor and HTCondor-CE on prerelease machines:  ansible  testing: ces  -i inventory -bK -f 20 -m service -a  name=condor-ce state=stopped  ansible  testing: condor  -i inventory -bK -f 20 -m service -a  name=condor state=stopped     Install new version of HTCondor on prerelease machines:  ansible  testing: condor  -i inventory -bK -f 10 -m command -a  yum --enablerepo=osg-development --assumeyes update condor   or, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed:  ansible  testing: condor  -i inventory -bK -f 10 -m command -a  yum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp     Verify correct RPM versions across the site:  ansible condor -i inventory -f 20 -o -m command -a  rpm -q condor     Restart HTCondor and HTCondor-CE on prerelease machines:  ansible  testing: condor  -i inventory -bK -f 20 -m service -a  name=condor state=started  ansible  testing: ces  -i inventory -bK -f 20 -m service -a  name=condor-ce state=started", 
            "title": "Installing HTCondor Prerelease"
        }, 
        {
            "location": "/release/itb-testing/#installing-a-prerelease-of-the-osg-software-stack", 
            "text": "Use this section to install new versions of all OSG software from a prerelease repository in Yum.    Check with the Release Manager to make sure that the prerelease repository has been populated with the desired\n   package versions.    Make sure that software is generally up-to-date on the hosts \u2014 see\n    the MadisonITB page  for more details  It may be desirable to update only non-OSG software at this stage, in which case one could simply disable the OSG\nrepositories by adding command-line options to the  yum update  commands.    Install new software on prerelease hosts:  ansible testing -i inventory -bK -f 20 -m command -a  yum --enablerepo=osg-prerelease --assumeyes update     Read the Yum output carefully, and follow up on any warnings, etc.    If the  osg-configure  package was updated on any host(s), run the  osg-configure  command on the host(s):  ansible testing -i inventory -bK -f 20 -m command -a  osg-configure -v  -l [HOST(S)]  ansible testing -i inventory -bK -f 20 -m command -a  osg-configure -c  -l [HOST(S)]     Verify OSG software updates by inspecting the Yum output carefully or examining specific package versions:  ansible current -i inventory -f 20 -o -m command -a  rpm -q osg-wn-client   Use an inventory group and package names that best fit the situation.", 
            "title": "Installing a Prerelease of the OSG Software Stack"
        }, 
        {
            "location": "/release/itb-testing/#running-tests", 
            "text": "For the first two test workflows, use your personal space on  itb-submit .  Copy or checkout the  osgitb/htcondor-tests \ndirectory to get the test directories.", 
            "title": "Running Tests"
        }, 
        {
            "location": "/release/itb-testing/#part-i-submitting-jobs-directly", 
            "text": "Change into the  1-direct-jobs  subdirectory    If there are old result files in the directory, remove them:  make distclean     Submit the test workflow  condor_submit_dag test.dag     Monitor the jobs until they are complete or stuck  In the initial test runs, the entire workflow ran in a few minutes.  If the DAG or jobs exit immediately, go on\nhold, or otherwise fail, then you have some troubleshooting to do!  Keep trying steps 2 and 3 until you get a clean\nrun (or one or more HTCondor bug tickets).    Check the final output file:  cat count-by-hostnames.txt   You should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per\nmachine and the fact that HTCondor can and will reuse claims to process many jobs on a single host.  Especially\nwatch out for a case in which no jobs run on the newly updated hosts (at the time of writing:  itb-data[456] ).    (Optional) Clean up, using the  make clean  or  make distclean  commands.  Use the  clean  target to remove\n   intermediate result and log files generated by a workflow run but preserve the final output file; use the  distclean \n   target to remove all workflow-generated files (plus Emacs backup files).", 
            "title": "Part \u2160: Submitting jobs directly"
        }, 
        {
            "location": "/release/itb-testing/#part-ii-submitting-jobs-using-htcondor-c", 
            "text": "If direct submissions fail, there is probably no point to doing this step.    Change into the  2-htcondor-c-jobs  subdirectory    If there are old result files in the directory, remove them:  make distclean     Get a proxy for your X.509 credentials  voms-proxy-init     Submit the test workflow  condor_submit_dag test.dag     Monitor the jobs until they are complete or stuck  In the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the\ndirect submission test, because of the layers of indirection.  Also, status updates from the CEs back to the submit\nhost are infrequent.  For direct information about the CEs, log in to  itb-ce1  and  itb-ce2  to check status; don\u2019t\nforget to check both  condor_ce_q  and  condor_q  on the CEs, probably in that order.  If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do!  Keep\ntrying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets).    Check the final output file:  cat count-by-hostnames.txt   Again, look for a reasonable distribution of jobs by hostname.    (Optional) Clean up, using the  make clean  or  make distclean  commands.", 
            "title": "Part \u2161: Submitting jobs using HTCondor-C"
        }, 
        {
            "location": "/release/itb-testing/#part-iii-submitting-jobs-from-a-glideinwms-vo-frontend", 
            "text": "For this workflow, use your personal space on  glidein3.chtc.wisc.edu .  Copy or checkout the  osgitb/htcondor-tests \ndirectory to get the test directories.  Again, if previous steps fail, do not bother with this step.    Change into the  3-frontend-jobs  subdirectory    If there are old result files in the directory, remove them:  make distclean     Submit the test workflow  condor_submit_dag test.dag     Monitor the jobs until they are complete or stuck  This workflow could take much longer than the first two, maybe an hour or so.  Also, unless there are active\nglideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs.  Thus it is\nhelpful to monitor  condor_q -totals  until all of the jobs are submitted (there should be 2001), then switch to\nmonitoring  condor_status  until glideins start appearing.  After the first jobs start running and finishing, it is\nprobably safe to ignore the rest of the run.  If the jobs do not appear in the local queue, if glideins do not\nappear, or if jobs do not start running on the glideins, it is time to start troubleshooting.    Check the final output file:  cat count-by-hostnames.txt   The distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in\nwhich pilots ramp up over time and how HTCondor allocates jobs to slots.    (Optional) Clean up, using the  make clean  or  make distclean  commands.", 
            "title": "Part \u2162: Submitting jobs from a GlideinWMS VO Frontend"
        }, 
        {
            "location": "/release/empty-pkgs/", 
            "text": "Procedure for updating empty-* packages\n\n\nBackground\n\n\nThe \nempty-*\n packages were introduced a workaround for sites that install certain software (for example HTCondor or CA certs) from tarballs or other means that do not involve Yum/RPM.\n\n\nThe packages contain no files, and exist merely to satisfy RPM dependencies so that other packages can be installed. It is the admin's responsibility to make sure that whatever component they installed the empty package for is functional.\n\n\nThe empty packages are kept in a separate repository to prevent them from being accidentally installed instead of the component they claim to provide. Because of this, they do not go through the normal release process of development to testing to prerelease to release, but are \nmoved\n straight from \nosg-development\n into \nosg-empty\n after developer testing. \n\n\n\n\nWarning\n\n\nIt is important to untag the packages from \nosg-development\n immediately after promotion to \nosg-empty\n\n\n\n\nProcedure\n\n\n\n\nPrepare the package update, but do not build yet.\n\n\nCoordinate with the Software and Release Managers to set aside a good time to update the package. An empty package should not remain in the development repos for longer than a few hours.\n\n\nBuild into development.\n\n\nTest out of development. \nBe thorough\n, as there is no separate acceptance testing for empty packages.\n\n\nIn the JIRA ticket, document your testing procedure and request permission from \nboth\n the Software and the Release Managers. (Since there is no acceptance testing, both of them have to sign off on the new build).\n\n\nAfter receiving permission, tag the builds into the \nosg-empty\n tags, and untag them from the \nosg-development\n tags. Then regenerate the \nosg-empty\n repos. \n\n\n\n\nosg-koji move-pkg osg-3.3-el6-development osg-3.3-el6-empty \nEL6_BUILD_NVR\n\nosg-koji move-pkg osg-3.3-el7-development osg-3.3-el7-empty \nEL7_BUILD_NVR\n\nosg-koji regen-repo --nowait osg-3.3-el6-empty\nosg-koji regen-repo --nowait osg-3.3-el7-empty", 
            "title": "Empty Packages"
        }, 
        {
            "location": "/release/empty-pkgs/#procedure-for-updating-empty-42-packages", 
            "text": "", 
            "title": "Procedure for updating empty-* packages"
        }, 
        {
            "location": "/release/empty-pkgs/#background", 
            "text": "The  empty-*  packages were introduced a workaround for sites that install certain software (for example HTCondor or CA certs) from tarballs or other means that do not involve Yum/RPM.  The packages contain no files, and exist merely to satisfy RPM dependencies so that other packages can be installed. It is the admin's responsibility to make sure that whatever component they installed the empty package for is functional.  The empty packages are kept in a separate repository to prevent them from being accidentally installed instead of the component they claim to provide. Because of this, they do not go through the normal release process of development to testing to prerelease to release, but are  moved  straight from  osg-development  into  osg-empty  after developer testing.    Warning  It is important to untag the packages from  osg-development  immediately after promotion to  osg-empty", 
            "title": "Background"
        }, 
        {
            "location": "/release/empty-pkgs/#procedure", 
            "text": "Prepare the package update, but do not build yet.  Coordinate with the Software and Release Managers to set aside a good time to update the package. An empty package should not remain in the development repos for longer than a few hours.  Build into development.  Test out of development.  Be thorough , as there is no separate acceptance testing for empty packages.  In the JIRA ticket, document your testing procedure and request permission from  both  the Software and the Release Managers. (Since there is no acceptance testing, both of them have to sign off on the new build).  After receiving permission, tag the builds into the  osg-empty  tags, and untag them from the  osg-development  tags. Then regenerate the  osg-empty  repos.    osg-koji move-pkg osg-3.3-el6-development osg-3.3-el6-empty  EL6_BUILD_NVR \nosg-koji move-pkg osg-3.3-el7-development osg-3.3-el7-empty  EL7_BUILD_NVR \nosg-koji regen-repo --nowait osg-3.3-el6-empty\nosg-koji regen-repo --nowait osg-3.3-el7-empty", 
            "title": "Procedure"
        }, 
        {
            "location": "/release/acceptance-testing/", 
            "text": "Acceptance Testing\n\n\nThe OSG Release Team collects and maintains testing procedures for major components in the OSG Sofware Stack. These test should verify that basic functionality of the component works in typically deployed configurations.\n\n\nCVMFS\n\n\n\n\nNote\n\n\nThis acceptance testing recipe was created when access to a machine with sufficient disk space to make a complete replica of OASIS was not available.\n\n\n\n\nCreating a CVMFS Repository Server (Stratum 0)\n\n\n\n\n\n\nDisable SELinux by setting the following in \n/etc/selinux/config\n.\n\n\nSELINUX=disabled\n\n\n\n\n\n\n\n\n\nCheck kernel version.\n\n\nuname -a\n\n\n\n\n\n\n\n\n\n\nCVMFS for EL7 requires OverlayFS (as of kernel version 4.2.x). If default kernel is \n= 4.2.x, update kernel.\n\n\nroot@host #\n rpm --import \nhttps://www.elrepo.org/RPM-GPG-KEY-elrepo.org\n\n\nroot@host #\n rpm -Uvh \nhttp://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm\n\n\nroot@host #\n yum install yum-plugin-fastestmirror\n\nroot@host #\n yum --enablerepo\n=\nelrepo-kernel install kernel-ml\n\n\n\n\n\n\n\n\n\nSelect updated kernel by editing \n/etc/default/grub\n.\n\n\nGRUB_DEFAULT=0\n\n\n\n\n\nand run:\n\n\nroot@host #\n grub2-mkconfig -o /boot/grub2/grub.cfg\n\n\n\n\n\n\n\n\n\nReboot system. \n\n\n\n\n\n\nCheck kernel version again and make sure SELinux is disabled.\n\n\nroot@host #\n uname -a\n\nroot@host #\n sestatus\n\n\n\n\n\n\n\n\n\nIf kernel \n= 4.2 and SELinux is disabled, then update system and install CVMFS server and client packages.\n\n\nroot@host #\n yum update\n\nroot@host #\n yum install epel-release\n\nroot@host #\n yum install yum-plugin-priorities\n\nroot@host #\n rpm -Uvh \nhttps://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\nroot@host #\n yum install cvmfs cvmfs-server\n\n\n\n\n\n\n\n\n\nConfigure web server and start it up. Edit \n/etc/httpd/conf.d/cvmfs.conf\n:\n\n\nListen 8000\nKeepAlive On\n\n\n\n\n\nand run:\n\n\nroot@host #\n chkconfig httpd on\n\nroot@host #\n service httpd start\n\n\n\n\n\n\n\n\n\nMake new repository.\n\n\nroot@host #\n cvmfs_server mkfs test.cvmfs-stratum-0.novalocal\n\n\n\n\n\n\n\n\n\nRun transaction on new repository to enable write access.\n\n\nroot@host #\n cvmfs_server transaction test.cvmfs-stratum-0.novalocal\n\n\n\n\n\n\n\n\n\nPlace some sample code in new repository directory and then publish it.\n\n\nroot@host #\n \ncd\n /cvmfs/test.cvmfs-stratum-0.novalocal\n\nroot@host #\n vi \n[\nbash\n\\_\npi.sh\n](\n%ATTACHURL%/bash_pi.sh\n)\n\n\nroot@host #\n chmod +x bash\n\\_\npi.sh\n\nroot@host #\n cvmfs\n\\_\nserver publish test.cvmfs-stratum-0.novalocal\n\n\n\n\n\n\n\n\n\nCheck repository status after publication.\n\n\nroot@host #\n cvmfs\n\\_\nserver check\n\nroot@host #\n cvmfs\n\\_\nserver tag\n\nroot@host #\n wget -qO- \nhttp://localhost:8000/cvmfs/test.cvmfs-stratum-0.novalocal/.cvmfswhitelist%7Ccat\n -v\n\n\n\n\n\n\n\n\n\nDownload a copy of the CVMFS repository's public key e.g., /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub\n\n\n\n\n\n\nCreating a CVMFS Replica Server (Stratum 1)\n\n\n\n\n\n\nRepeat steps 1 though 8 in the previous section on \"Creating a CVMFS Repository Server \". However, now also install \nmod_wsgi\n.\n\n\nroot@host #\n yum install cvmfs cvmfs-server mod\n\\_\nwsgi\n\n\n\n\n\n\n\n\n\nUpload a copy of the CVMFS repository's public key and place in \n/etc/cvmfs/keys\n directory. \n\n\n\n\n\n\nAdd replica of the repository.\n\n\nroot@host #\n cvmfs_server add-replica -o root \nhttp://10.128.3.96:8000/cvmfs/test.cvmfs-stratum-0.novalocal\n /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub\n\n\n\n\n\n\n\n\n\nMake a snapshot of the repository.\n\n\nroot@host #\n cvmfs\n\\_\nserver snapshot test.cvmfs-stratum-0.novalocal\n\n\n\n\n\n\n\n\n\nCreating a CVMFS client\n\n\n\n\n\n\nUpdate system and install CVMFS client package.\n\n\nroot@host #\n yum update\n\nroot@host #\n yum install epel-release\n\nroot@host #\n yum install yum-plugin-priorities\n\nroot@host #\n rpm -Uvh \nhttps://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\nroot@host #\n yum install cvmfs\n\n\n\n\n\n\n\n\n\nUpload a copy of the CVMFS repository's public key and place in \n/etc/cvmfs/keys\n directory.\n\n\n\n\n\n\nEdit fuse configuration \n/etc/fuse.conf\n.\n\n\nuser_allow_other\n\n\n\n\n\n\n\n\n\nEdit autofs configuration and restart service \n/etc/auto.master\n.\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\n\nand run:\n\n\nroot@host #\n service autofs restart\n\n\n\n\n\n\n\n\n\nEdit cvmfs configuration (\n/etc/cvmfs/default.local\n) to point to replica server.\n\n\nCVMFS_SERVER_URL\n=\nhttp://10.128.3.97:8000/cvmfs/@fqrn@\n\n\nCVMFS_REPOSITORIES\n=\ntest.cvmfs-stratum-0.novalocal\n\n\nCVMFS_HTTP_PROXY\n=\nDIRECT\n\n\n\n\n\n\n\n\n\n\nRemove OSG CVMFS configuration file.\n\n\nrm /etc/cvmfs/default.d/60-osg.conf\n\n\n\n\n\n\n\n\n\n\nRun CVMFS config probe.\n\n\ncvmfs_config probe test.cvmfs-stratum-0.novalocal\n\n\n\n\n\n\n\n\n\n\nCheck CVMFS config status.\n\n\ncvmfs_config stat -v test.cvmfs-stratum-0.novalocal\n\n\n\n\n\n\n\n\n\n\nExecute sample code published to repository from client.\n\n\n/cvmfs/test.cvmfs-stratum-0.novalocal/bash_pi.sh -b 8 -r 5 -s 10000\n\n\n\n\n\n\n\n\n\n\nCreating an OASIS client\n\n\n\n\nUpdate system and install CVMFS client package.\n\n\nyum update\n\n\nyum install epel-release\n\n\nyum install yum-plugin-priorities\n\n\nrpm -Uvh \nhttps://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\nyum install osg-oasis \n\n\n\n\n\n\nVerify latest versions of cvmfs, cvmfs-config-osg, and cvmfs-x509-helper have been installed. \n\n\nEdit fuse configuration.\n\n\nvi /etc/fuse.conf\n\n\nuser_allow_other \n\n\n\n\n\n\n\n\n\n\nEdit cvmfs configuration to point to replica server.\n\n\nvi /etc/cvmfs/default.local\n\n\nCVMFS_REPOSITORIES=\"`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\"\n\n\nCVMFS_QUOTA_LIMIT=20000\n\n\nCVMFS_HTTP_PROXY=DIRECT\n\n\n\n\n\n\n\n\n\n\nEdit autofs configuration and restart service.\n\n\nvi /etc/auto.master\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\n\n\nservice autofs restart\n\n\n\n\n\n\nRun CVMFS config probe.\n\n\ncvmfs_config probe \n\n\n\n\n\n\nCheck CVMFS config status.\n\n\ncvmfs_config stat -v oasis.opensciencegrid.org\n\n\n\n\n\n\n\n\nAdditional Documentation\n\n\n\n\nCERN's CVMFS Documentation\n\n\nOSG's CVMFS Replica Server\n\n\nOSG's CVMFS Client Documentation\n\n\n\n\nOSG's OASIS Documentation\n\n\n\n\n\n\nbash_pi.sh\n: A bash script that uses a simple Monte Carlo method to estimate the value of Pi\n\n\n\n\n\n\nGratia Probe\n\n\nThis section documents the testing procedure to test the gratia probes sufficiently tested to be promoted to the osg-testing repository. The test procedure is the same on both SL6 and SL7.\n\n\n\n\ninstall or update the \ngratia-probe-condor\n rpm as appropriate\n\n\nOn each VM download the gratia-probe-setup.sh script and run it\n\n\nIn \n/etc/gratia/condor/ProbeConfig\n, verify the following have been changed:\n\n\nchange \nSiteName\n to something aside from \nGeneric Site\n\n\nchange \nEnableProbe\n to 1\n\n\nchange \nCollectorHost\n, \nSSLHost\n, and \nSSLRegistrationHost\n to the an invalid host (E.g. test.com) or the localhost\n\n\n\n\n\n\nCreate \n/var/lib/osg/\n and download the attached \nuser-vo-map\n file and place it in that directory\n\n\nEdit the \nuser-vo-map\n file and change the account from \nsthapa\n to the account you'll be using to submit the condor jobs in the following step\n\n\nDownload and submit the attached condor_submit file (note, on the default fermicloud VM, this takes about 3 hours, so you may want to set \nNUM_CPUS\n to 50 so that 50 jobs will run at a time)\n\n\nRun \n/usr/share/gratia/condor/condor_meter\n\n\nCheck \n/var/lib/gratia/tmp/gratiafiles/\n for a \nsubdir.condor_...\n directory and verify that there are 200 xml jobs and the cpus/wall times are appropriate (either PT0S or PT1M).\n\n\n\n\nGSI OpenSSH\n\n\nTo test a fresh installation:\n\n\n\n\nSpin up two VM's and set up the EPEL/OSG repos on both of them.\n\n\nChoose one of the VM's, it will be the server VM. Consult these \ninstructions\n to set up the server.\n\n\n\n\nFrom the other VM (client):\n\n\n\n\n\n\nInstall the necessary packages:\n\n\nroot@host #\n yum install globus-proxy-utils gsi-openssh-clients\n\n\n\n\n\n\n\n\n\nInitialize your proxy. After this, none of the gsi commands should prompt you for your password.\n\n\n\n\n\n\nConnect to the server:\n\n\nuser@host $\n gsissh -p \n2222\n \nserver hostname\n\n\n\n\n\n\n\n\n\n\nCopy a test file to the server:\n\n\nuser@host $\n gsiscp -p \n2222\n testfile \nserver hostname\n:/tmp\n\n\n\n\n\n\n\n\n\nConnect to the server via SFTP and grab files:\n\n\nuser@host $\n gsisftp -P \n2222\n \nserver hostname\n\n\nuser@host $\n \ncd\n /tmp\n\nuser@host $\n get testfile\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE Collector (WIP)\n\n\nThe CE Collector is a stripped-down version of HTCondor-CE that contains mostly just the collector daemon and configs. It was introduced in htcondor-ce-1.6. The production CE Collectors run at the GOC, but you may want to set up your own for testing.\n\n\n\n\nMake 2 VMs with the EPEL/OSG repos installed: one for the collector, and one for the CE\n\n\n\n\nSetting Up the Collector\n\n\n\n\nInstall \nhtcondor-ce-collector\n\n\n\n\nCreate a file called \n/etc/condor-ce/config.d/99-local.conf\n that contains this line:\n\n\nCOLLECTOR.ALLOW_ADVERTISE_SCHEDD = $(COLLECTOR.ALLOW_ADVERTISE_SCHEDD), your_htcondor_ce_host.example.net\n/pre\n\n\n\n\n\n\n(with \nyour_htcondor_ce_host\n replaced by the hostname the HTCondor-CE VM)\n\n\n\n\n\n\nRun \nservice condor-ce-collector start\n\n\n\n\n\n\nSetting Up the CE\n\n\n\n\nInstall \nosg-htcondor-ce-condor\n (replace condor with the batch system of your choice)\n\n\nEnsure osg-configure \n= 1.0.60-2 is installed\n\n\n\n\nConfigure your CE using osg-configure\n\n\n\n\nYou should use the \nHTCondor-CE Install Docs\n as a reference, although you can skip several of the steps\n\n\nYou can skip setting up Squid: set \nenabled\n to \nTrue\n and \nlocation\n to \nUNAVAILABLE\n in \n01-squid.ini\n\n\nSet \nhtcondor_gateway_enabled\n to \nTrue\n in \n10-gateway.ini\n\n\nYou probably don't need GUMS, but if you want it, use the Fermi GUMS server (set \ngums_host\n to \ngums.fnal.gov\n and \nauthorization_method\n to \nxacml\n in 10-misc.ini)\n\n\n\n\nTo keep osg-configure from complaining about storage, edit \n10-storage.ini\n:\n\n\n\n\nSet \nse_available\n to \nFalse\n\n\nSet \napp_dir\n to \n/osg/app_dir\n\n\nSet \ndata_dir\n to \n/osg/data_dir\n\n\nDo \nmkdir -p /osg/app_dir/etc; mkdir -p /osg/data_dir; chmod 1777 /osg/app_dir{,/etc}\n\n\n\n\n\n\n\n\nEnable your batch system by setting \nenabled\n to \nTrue\n in \n20-\nbatch system\n.ini\n\n\n\n\nSet up the site info in \n40-siteinfo.ini\n ; in particular, you'll need to set the \nresource\n and \nresource_group\n settings\n    \\ (you just need to pick a name; I concatenate my login name with the short host name and use that, e.g. matyasfermicloud001).\n    \\ You can also use the following settings:\n\n\ngroup=OSG-ITB\n\n\nsponsor=local\n\n\ncity=Batavia, IL\n\n\ncountry=US\n\n\nlongitude=-88\n\n\nlatitude=41\n\n\n\n\n\n\n\n\n\n\n\n\nEdit the file \n/etc/osg/config.d/30-infoservices.ini\n and set \nce_collectors\n to the collector host\n\n\n\n\nRun \nosg-configure -dc\n\n\nStart up your batch system\n\n\nRun \nservice condor-ce start\n\n\n\n\nThe CE will report to the collector host every five minutes. If you want to force it to send now, run \ncondor_ce_reconfig\n. You should see your CE if you run \ncondor_ce_status -schedd\n on the collector host.\n\n\nRSV\n\n\nTesting a fresh installation:\n\n\n\n\nmake sure the yum repositories required by OSG is installed on your host\n\n\nrpm -Uvh \nhttp://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm\n OR rpm -Uvh \nhttp://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n\nalso make sure epel repo is set up. \n\n\n\n\n\n\ninstall the rpm\n\n\nyum --enablerepo=osg-testing install rsv \n\n\n\n\n\n\nedit /etc/osg/config.d/30-rsv.ini file\n\n\nin my case, I don't have a service cert for testing, so I use my own personal cert to create the proxy, but later on the owner of the proxy should be changed to \"rsv\" user that is created during the rpm install.\n\n\nin the config file, for the ce_hosts and gridftp_hosts, put in a test server, as the result from this test will be uploaded to OSG GOC, which may mess up your production service monitoring if you chose a production server for the test.\n\n\n\n\n\n\nosg-configure -v\n\n\nosg-configure -c\n\n\n/etc/init.d/condor-cron start\n\n\n/etc/init.d/rsv start\n\n\nrsv-control --list\n\n\nrsv-control --version\n\n\nrsv-control --run --all-enabled 11. make sure the results from the above commands look fine.\n\n\n\n\nTesting an upgrade installation:\n\n\n\n\nmake sure to enable the osg-testing repo, and set its priority higher than the other repos\n\n\nyum --enablerepo=osg-testing upgrade rsv*\n\n\nyou can use the old 30-rsv.ini file for configuration\n\n\nrepeat steps 4)~11) as mentioned in the last section.\n\n\n\n\nSlurm\n\n\nThis section goes through the steps needed to set up a slurm install on a VM. This is a necessary prerequisite for testing Slurm related components (CE integration, gratia, etc.). Note that the slurm setup used for this uses weak passwords for mysql. It should be sufficient for a quick setup, testing, and then tear down but should not be used without changes if it will be running for any appreciable length of time.\n\n\n\n\nNote\n\n\nneed to have a VM with 2+ GB of memory\n\n\n\n\nInstallation and setup\n\n\n\n\n\n\nDownload scripts and config files: \n\n\ncd /tmp/\ngit clone \nhttps://github.com/sthapa/utilities.git\n\ncd utilities/slurm\n\n\n\n\n\n\n\n\n\nsetup and install slurm components\n\n\nexport username=\nUSERNAME\n \\# user that jobs will run as\nexport version=\n14.11.7\n \\# slurm version to install (e.g. 16.05.2 or 14.11.7)\n./slurm-setup.sh\n\n\n\n\n\n\n\n\n\nAfter successful completion, slurm and slurm gratia probes should be setup and enabled.\n\n\n\n\n\n\nRunning a job using slurm\n\n\n\n\n\n\nGenerate test.sh with the following:\n\n\n#/bin/bash\necho \nIn the directory: `pwd`\n echo \nAs the user: `whoami`\n echo \u201cHostname:\n /bin/hostname sleep 60 \n/pre\n\n\n\n\n\n\n\n\n\n\nrun \nsbatch test.sh\n\n\n\n\nthe output from the jobs should appear in the current working directory as \ntest.sh.[eo].nnnnn\n where nnnnn is a job id\n\n\n\n\nVO Client\n\n\nThis document contains a basic recipe for testing a VO Package release\n\n\nPrerequisites\n\n\nTesting the VO package requires a few components:\n   * X.509 certificate with membership to at least one VO\n   * System with working GUMS installation\n   * System with OSG installation (voms-proxy-init and edg-mkgridmap)\n\n\nTesting \nvoms-proxy-init\n\n\nLogin in the system that has voms-proxy-init installed.  Make sure that you have the correct \nvo-client rpms installed and that your X.509 certificate is in your home directory.  For each\nVO that you have membership in, run the following \nvoms-proxy-init -voms [VO]\n where \n[VO]\n is \nthe appropriate VO (e.g. osg, cms, etc.).  You should be able to generate a voms-proxy for that\nVO without errors.\n\n\nTesting \nedg-mkgridmap\n\n\nLog on to a system with \nedg-mkgridmap\n installed.  Make sure you have the correct vo-client rpms \ninstalled (vo-client-edgmkgridmap).  Run \nedg-mkgridmap\n and check the log output for errors.\n\nThere will be some errors so compare your errors with the errors on previous vo-package tickets to\nmake sure no new errors have appeared.\n\n\nTesting GUMS\n\n\nLog on to a system with a working GUMS install.  Make sure that you have the correct vo-client \nrpms (osg-gums-config) installed.  \n\n\n\n\nMake a backup of \n/etc/gums/gums.config\n \n\n\nCopy the mysql database information from \n/etc/gums/gums.config\n to \n/etc/gums/gums.config.template\n\n\nCopy \n/etc/gums/gums.config.template\n to \n/etc/gums/gums.config\n\n\nStart the \ntomcat6\n service \n\n\nGo to the GUMS interface (e.g. https://my.host:8443/gums)\n\n\nGo to the Update VO members page and click on the \nupdate VO members\n button\n\n\nOnce completed, there will probably be some errors.\n\n\nCompare errors to errors on prior vo package update tickets and make sure no new errors have occurred. \n\n\n\n\nVOMS Admin Server\n\n\nInstall and configure voms-admin-server\n\n\nInstall and configure voms-admin-server with this \nvoms-install.sh\n script, entering \nosg-testing\n when prompted for the \nREPO\n and your own e-mail address when prompted for \nEMAIL_FROM\n.\n\n\nSet up TEST_VO and add yourself as an admin\n\n\nTo add a test VO (named TEST_VO) and add yourself as an admin, use the following script, replacing the \nUSER_EMAIL\n, \nUSER_CERT_SUBJECT\n, and \nUSER_COMMON_NAME\n variables with your own:\n\n\n#!/bin/bash\n\n\n\nVO_NAME\n=\nTEST_VO\n\n\nTOMCAT_PORT\n=\n8443\n\n\nUSER_EMAIL\n=\nYOUR_EMAIL\n\n\nUSER_CERT_SUBJECT\n=\nYOUR CERT DN\n\n\nUSER_CERT_ISSUER\n=\n/DC=com/DC=DigiCert-Grid/O=DigiCert Grid/CN=DigiCert Grid CA-1\n\n\nUSER_COMMON_NAME\n=\nYOUR CERT CN\n\n\nvoms-admin --vo \n$VO_NAME\n --host \n$HOSTNAME\n --nousercert create-user \n\\\n\n    \n$USER_CERT_SUBJECT\n \n$USER_CERT_ISSUER\n \n$USER_COMMON_NAME\n \n$USER_EMAIL\n\n\nvoms-admin --vo \n$VO_NAME\n --host \n$HOSTNAME\n --nousercert assign-role \n\\\n\n    /\n$VO_NAME\n VO-Admin \n$USER_CERT_SUBJECT\n \n$USER_CERT_ISSUER\n\n\n\necho\n web ui: https://\n$HOSTNAME\n:\n$TOMCAT_PORT\n/voms/\n$VO_NAME\n\n\n\n\n\n\nTesting the web UI\n\n\n\n\nAdd/suspend/restore/delete a user (using your e-mail address for contact)\n\n\nVerify e-mail receipt of suspension message\n\n\n\n\nXRootD VOMS Testing\n\n\nThis section is intended for OSG Software/Release teams to gather information on testing vomsxrd/xrootd-voms-plugin package. Original plugin named \nvomsxrd\n, similar to lcmaps that extracts information for authorization within xrootd of a proxy's voms extension.\n\n\nYou need an \nxrootd server installation\n\n\nIn the xrootd server yum install the following packages:\n\n\n\n\nxrootd\n\n\nxrootd-voms-plugin\n\n\nvo-client\n\n\n\n\nIn the xrootd client yum install the following packages:\n\n\n\n\nxrootd-client\n\n\nvoms-clients\n\n\nvo-client\n\n\n\n\nIn the xrootd server add this lines to file \n/etc/xrootd/xrootd-clustered.cfg\n\n\nxrootd.seclib /usr/lib64/libXrdSec.so\nsec.protparm gsi -vomsfun:/usr/lib64/libXrdSecgsiVOMS.so -vomsfunparms:certfmt=raw|vos=cms|dbg -vomsat:2\nsec.protocol /usr/lib64 gsi -ca:1 -crl:3\n\n\n\n\n\nThis configuration will only authorize members of VO \ncms\n. You can change it to another VO.\n\n\nMake sure \nfetch-crl\n has been run otherwise the xrootd service may fail to start.\n\n\nIn the xrootd client get a proxy without voms extension or with another VO extension different that the one used in the configuration:\n\n\nuser@host $\n voms-proxy-init -voms mis\n\nEnter GRID pass phrase:\n\n\nYour identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020\n\n\nCreating temporary proxy ........................... Done\n\n\nContacting  voms.opensciencegrid.org:15001 [/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=http/voms.opensciencegrid.org] \nmis\n Done\n\n\nCreating proxy ............................................... Done\n\n\nuser@host $\n xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/\n\n[0B/0B][100%][==================================================][0B/s]  \n\n\nRun: [FATAL] Auth failed\n\n\n\n\n\n\nNow get a proxy with cms extension and run it again:\n\n\nuser@host $\n voms-proxy-init -voms cms\n\nEnter GRID pass phrase:\n\n\nYour identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020\n\n\nCreating temporary proxy ...................................... Done\n\n\nContacting  voms2.cern.ch:15002 [/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch] \ncms\n Done\n\n\nCreating proxy .......................................... Done\n\n\nYour proxy is valid until Thu Dec  4 22:53:29 2014\n\n\nuser@host $\n xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/\n\n[0B/0B][100%][==================================================][0B/s] \n\n\n\n\n\n\nXRootD Plugins\n\n\nHadoop name node installation\n\n\nUse the following script with option 1:\n\n\n#!/bin/bash\n\n\nset\n -e\n\n\nselect\n NODETYPE in namenode datanode gridftp\n;\n \ndo\n\n  \n[[\n \n$NODETYPE\n \n]]\n \n \nbreak\n\n\ndone\n\n\ncase\n \n$NODETYPE\n in\n  namenode \n)\n \nNAMENODE\n=\n$HOSTNAME\n \n;;\n\n         * \n)\n \nread\n -p \nhostname for NAMENODE? \n NAMENODE \n;;\n\n\nesac\n\n\necho\n \nNODETYPE\n=\n$NODETYPE\n\n\necho\n \nNAMENODE\n=\n$NAMENODE\n\n\nread\n -p \nok? [y/N] \n ok\n\ncase\n \n$ok\n in\n  y*\n|\nY*\n)\n \n;;\n  \n# ok\n\n      *\n)\n \nexit\n \n;;\n\n\nesac\n\n\n#yum install --enablerepo=osg-minefield osg-se-hadoop-$NODETYPE\n\nyum install osg-se-hadoop-\n$NODETYPE\n\n\ncase\n \n$NODETYPE\n in\n  namenode\n|\ndatanode \n)\n\n    mkdir -p /data/\n{\nhadoop,scratch,checkpoint\n}\n\n    chown -R hdfs:hdfs /data\n    sed -i s/NAMENODE/\n$NAMENODE\n/ /etc/hadoop/conf.osg/\n{\ncore,hdfs\n}\n-site.xml\n    cp /etc/hadoop/conf.osg/\n{\ncore,hdfs\n}\n-site.xml /etc/hadoop/conf/\n    touch /etc/hosts_exclude\n    \n;;\n\n  gridftp \n)\n\n    ln -snf conf.osg /etc/hadoop/conf\n    sed -i s/NAMENODE/\n$NAMENODE\n/ /etc/hadoop/conf.osg/\n{\ncore,hdfs\n}\n-site.xml\n    \necho\n \nhadoop-fuse-dfs# /mnt/hadoop fuse server=\n$NAMENODE\n,port=9000,rdbuffer=131072,allow_other 0 0\n \n /etc/fstab\n    mkdir /mnt/hadoop\n    mount /mnt/hadoop\n    cp -v /etc/redhat-release /mnt/hadoop/test-file\n    sed -i \n/globus_mapping/s/^# *//\n /etc/grid-security/gsi-authz.conf\n    sed -i s/yourgums.yourdomain/gums.fnal.gov/ /etc/lcmaps.db\n    mkdir /mnt/hadoop/fnalgrid\n    useradd fnalgrid -g fnalgrid\n    chown fnalgrid:fnalgrid /mnt/hadoop/fnalgrid\n    service globus-gridftp-server start\n    \nif\n \ntype\n -t globus-url-copy \n/dev/null\n;\n \nthen\n\n      globus-url-copy file:////bin/bash  gsiftp://\n$HOSTNAME\n/mnt/hadoop/fnalgrid/first_test\n    \nelse\n\n      \necho\n globus-url-copy not installed\n    \nfi\n\n    \n;;\n\n\nesac\n\n\ncase\n \n$NODETYPE\n in\n  namenode \n)\n su - hdfs -c \nhadoop namenode -format\n \n;;\n\n\nesac\n\nservice hadoop-hdfs-\n$NODETYPE\n start\n\n\n\n\n\nHadoop data node installation\n\n\n\n\nRun same script as before but with option number 2.\n\n\n\n\nInstall xrootd-server:\n\n\nyum install xrootd-server\n\n\n\n\n\n\n\n\n\nInstall xrootd-plugins\n\n\nyum install xrootd-cmstfc xrootd-hdfs\n\n\n\n\n\n\n\n\n\nGridFTP installation\n\n\nRun same as script but with option number.\n\n\nOn the name node\n\n\n[root@fermicloud092 ~]#\n hdfs dfs -ls /test-file\n\nFound 1 items\n\n\n-rw-r--r--   2 root root          0 2014-07-21 15:57 /test-file\n\n\n\n\n\n\nThis means your hadoop is working.\n\n\nModify the file \n/etc/xrootd/xrootd-clustered.cfg\n to look like this:\n\n\nxrd.port 1094\n\n# The roles this server will play.                                                                                            \nall.role server\nall.role manager if xrootd.unl.edu\n# The known managers                                                                                                          \nall.manager srm.unl.edu:1213\n#all.manager xrootd.ultralight.org:1213                                                                                       \n\n# Allow any path to be exported; this is further refined in the authfile.                                                     \nall.export / nostage\n\n# Hosts allowed to use this xrootd cluster                                                                                    \ncms.allow host *\n\n### Standard directives                                                                                                       \n# Simple sites probably don\nt need to touch these.                                                                            \n# Logging verbosity                                                                                                           \nxrootd.trace all -debug\nofs.trace all -debug\nxrd.trace all -debug\ncms.trace all -debug\n\n# Integrate with CMS TFC, placed in /etc/storage.xml                                                                          \noss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop\n\nxrootd.seclib /usr/lib64/libXrdSec.so\nxrootd.fslib /usr/lib64/libXrdOfs.so\nofs.osslib /usr/lib64/libXrdHdfs.so\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\ncms.delay startup 10\ncms.fxhold 60s\ncms.perf int 30s pgm /usr/bin/XrdOlbMonPerf 30\n\noss.space default_stage /opt/xrootd_cache\n\n\n\n\n\nCreate file \n/etc/xrootd/storage.xml\n and place this:\n\n\nstorage-mapping\n\n\nlfn-to-pfn\n \nprotocol=\nhadoop\n \ndestination-match=\n.*\n \npath-match=\n.*/+tmp2/test-file\n \nresult=\n/test-file\n/\n\n\n/storage-mapping\n\n\n\n\n\n\nFor el7 the instrucctions are a little bit different. See:\n\n\nhttps://jira.opensciencegrid.org/browse/SOFTWARE-2198?focusedCommentId=334667\npage=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-334667\n\n\nNow from a node do:\n\n\nxrdcp --debug 3 root://yourdatanode.yourdomain:1094//tmp2/test-file .\n\n\n\n\n\n\nIf it is sucessful it would have tested both cmstfc and hdfs plugins", 
            "title": "Acceptance Testing"
        }, 
        {
            "location": "/release/acceptance-testing/#acceptance-testing", 
            "text": "The OSG Release Team collects and maintains testing procedures for major components in the OSG Sofware Stack. These test should verify that basic functionality of the component works in typically deployed configurations.", 
            "title": "Acceptance Testing"
        }, 
        {
            "location": "/release/acceptance-testing/#cvmfs", 
            "text": "Note  This acceptance testing recipe was created when access to a machine with sufficient disk space to make a complete replica of OASIS was not available.", 
            "title": "CVMFS"
        }, 
        {
            "location": "/release/acceptance-testing/#creating-a-cvmfs-repository-server-stratum-0", 
            "text": "Disable SELinux by setting the following in  /etc/selinux/config .  SELINUX=disabled    Check kernel version.  uname -a     CVMFS for EL7 requires OverlayFS (as of kernel version 4.2.x). If default kernel is  = 4.2.x, update kernel.  root@host #  rpm --import  https://www.elrepo.org/RPM-GPG-KEY-elrepo.org  root@host #  rpm -Uvh  http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm  root@host #  yum install yum-plugin-fastestmirror root@host #  yum --enablerepo = elrepo-kernel install kernel-ml    Select updated kernel by editing  /etc/default/grub .  GRUB_DEFAULT=0  and run:  root@host #  grub2-mkconfig -o /boot/grub2/grub.cfg    Reboot system.     Check kernel version again and make sure SELinux is disabled.  root@host #  uname -a root@host #  sestatus    If kernel  = 4.2 and SELinux is disabled, then update system and install CVMFS server and client packages.  root@host #  yum update root@host #  yum install epel-release root@host #  yum install yum-plugin-priorities root@host #  rpm -Uvh  https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm  root@host #  yum install cvmfs cvmfs-server    Configure web server and start it up. Edit  /etc/httpd/conf.d/cvmfs.conf :  Listen 8000\nKeepAlive On  and run:  root@host #  chkconfig httpd on root@host #  service httpd start    Make new repository.  root@host #  cvmfs_server mkfs test.cvmfs-stratum-0.novalocal    Run transaction on new repository to enable write access.  root@host #  cvmfs_server transaction test.cvmfs-stratum-0.novalocal    Place some sample code in new repository directory and then publish it.  root@host #   cd  /cvmfs/test.cvmfs-stratum-0.novalocal root@host #  vi  [ bash \\_ pi.sh ]( %ATTACHURL%/bash_pi.sh )  root@host #  chmod +x bash \\_ pi.sh root@host #  cvmfs \\_ server publish test.cvmfs-stratum-0.novalocal    Check repository status after publication.  root@host #  cvmfs \\_ server check root@host #  cvmfs \\_ server tag root@host #  wget -qO-  http://localhost:8000/cvmfs/test.cvmfs-stratum-0.novalocal/.cvmfswhitelist%7Ccat  -v    Download a copy of the CVMFS repository's public key e.g., /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub", 
            "title": "Creating a CVMFS Repository Server (Stratum 0)"
        }, 
        {
            "location": "/release/acceptance-testing/#creating-a-cvmfs-replica-server-stratum-1", 
            "text": "Repeat steps 1 though 8 in the previous section on \"Creating a CVMFS Repository Server \". However, now also install  mod_wsgi .  root@host #  yum install cvmfs cvmfs-server mod \\_ wsgi    Upload a copy of the CVMFS repository's public key and place in  /etc/cvmfs/keys  directory.     Add replica of the repository.  root@host #  cvmfs_server add-replica -o root  http://10.128.3.96:8000/cvmfs/test.cvmfs-stratum-0.novalocal  /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub    Make a snapshot of the repository.  root@host #  cvmfs \\_ server snapshot test.cvmfs-stratum-0.novalocal", 
            "title": "Creating a CVMFS Replica Server (Stratum 1)"
        }, 
        {
            "location": "/release/acceptance-testing/#creating-a-cvmfs-client", 
            "text": "Update system and install CVMFS client package.  root@host #  yum update root@host #  yum install epel-release root@host #  yum install yum-plugin-priorities root@host #  rpm -Uvh  https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm  root@host #  yum install cvmfs    Upload a copy of the CVMFS repository's public key and place in  /etc/cvmfs/keys  directory.    Edit fuse configuration  /etc/fuse.conf .  user_allow_other    Edit autofs configuration and restart service  /etc/auto.master .  /cvmfs /etc/auto.cvmfs  and run:  root@host #  service autofs restart    Edit cvmfs configuration ( /etc/cvmfs/default.local ) to point to replica server.  CVMFS_SERVER_URL = http://10.128.3.97:8000/cvmfs/@fqrn@  CVMFS_REPOSITORIES = test.cvmfs-stratum-0.novalocal  CVMFS_HTTP_PROXY = DIRECT     Remove OSG CVMFS configuration file.  rm /etc/cvmfs/default.d/60-osg.conf     Run CVMFS config probe.  cvmfs_config probe test.cvmfs-stratum-0.novalocal     Check CVMFS config status.  cvmfs_config stat -v test.cvmfs-stratum-0.novalocal     Execute sample code published to repository from client.  /cvmfs/test.cvmfs-stratum-0.novalocal/bash_pi.sh -b 8 -r 5 -s 10000", 
            "title": "Creating a CVMFS client"
        }, 
        {
            "location": "/release/acceptance-testing/#creating-an-oasis-client", 
            "text": "Update system and install CVMFS client package.  yum update  yum install epel-release  yum install yum-plugin-priorities  rpm -Uvh  https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm  yum install osg-oasis     Verify latest versions of cvmfs, cvmfs-config-osg, and cvmfs-x509-helper have been installed.   Edit fuse configuration.  vi /etc/fuse.conf  user_allow_other       Edit cvmfs configuration to point to replica server.  vi /etc/cvmfs/default.local  CVMFS_REPOSITORIES=\"`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\"  CVMFS_QUOTA_LIMIT=20000  CVMFS_HTTP_PROXY=DIRECT      Edit autofs configuration and restart service.  vi /etc/auto.master  /cvmfs /etc/auto.cvmfs    service autofs restart    Run CVMFS config probe.  cvmfs_config probe     Check CVMFS config status.  cvmfs_config stat -v oasis.opensciencegrid.org", 
            "title": "Creating an OASIS client"
        }, 
        {
            "location": "/release/acceptance-testing/#additional-documentation", 
            "text": "CERN's CVMFS Documentation  OSG's CVMFS Replica Server  OSG's CVMFS Client Documentation   OSG's OASIS Documentation    bash_pi.sh : A bash script that uses a simple Monte Carlo method to estimate the value of Pi", 
            "title": "Additional Documentation"
        }, 
        {
            "location": "/release/acceptance-testing/#gratia-probe", 
            "text": "This section documents the testing procedure to test the gratia probes sufficiently tested to be promoted to the osg-testing repository. The test procedure is the same on both SL6 and SL7.   install or update the  gratia-probe-condor  rpm as appropriate  On each VM download the gratia-probe-setup.sh script and run it  In  /etc/gratia/condor/ProbeConfig , verify the following have been changed:  change  SiteName  to something aside from  Generic Site  change  EnableProbe  to 1  change  CollectorHost ,  SSLHost , and  SSLRegistrationHost  to the an invalid host (E.g. test.com) or the localhost    Create  /var/lib/osg/  and download the attached  user-vo-map  file and place it in that directory  Edit the  user-vo-map  file and change the account from  sthapa  to the account you'll be using to submit the condor jobs in the following step  Download and submit the attached condor_submit file (note, on the default fermicloud VM, this takes about 3 hours, so you may want to set  NUM_CPUS  to 50 so that 50 jobs will run at a time)  Run  /usr/share/gratia/condor/condor_meter  Check  /var/lib/gratia/tmp/gratiafiles/  for a  subdir.condor_...  directory and verify that there are 200 xml jobs and the cpus/wall times are appropriate (either PT0S or PT1M).", 
            "title": "Gratia Probe"
        }, 
        {
            "location": "/release/acceptance-testing/#gsi-openssh", 
            "text": "To test a fresh installation:   Spin up two VM's and set up the EPEL/OSG repos on both of them.  Choose one of the VM's, it will be the server VM. Consult these  instructions  to set up the server.   From the other VM (client):    Install the necessary packages:  root@host #  yum install globus-proxy-utils gsi-openssh-clients    Initialize your proxy. After this, none of the gsi commands should prompt you for your password.    Connect to the server:  user@host $  gsissh -p  2222   server hostname     Copy a test file to the server:  user@host $  gsiscp -p  2222  testfile  server hostname :/tmp    Connect to the server via SFTP and grab files:  user@host $  gsisftp -P  2222   server hostname  user@host $   cd  /tmp user@host $  get testfile", 
            "title": "GSI OpenSSH"
        }, 
        {
            "location": "/release/acceptance-testing/#htcondor-ce-collector-wip", 
            "text": "The CE Collector is a stripped-down version of HTCondor-CE that contains mostly just the collector daemon and configs. It was introduced in htcondor-ce-1.6. The production CE Collectors run at the GOC, but you may want to set up your own for testing.   Make 2 VMs with the EPEL/OSG repos installed: one for the collector, and one for the CE", 
            "title": "HTCondor-CE Collector (WIP)"
        }, 
        {
            "location": "/release/acceptance-testing/#setting-up-the-collector", 
            "text": "Install  htcondor-ce-collector   Create a file called  /etc/condor-ce/config.d/99-local.conf  that contains this line:  COLLECTOR.ALLOW_ADVERTISE_SCHEDD = $(COLLECTOR.ALLOW_ADVERTISE_SCHEDD), your_htcondor_ce_host.example.net /pre   (with  your_htcondor_ce_host  replaced by the hostname the HTCondor-CE VM)    Run  service condor-ce-collector start", 
            "title": "Setting Up the Collector"
        }, 
        {
            "location": "/release/acceptance-testing/#setting-up-the-ce", 
            "text": "Install  osg-htcondor-ce-condor  (replace condor with the batch system of your choice)  Ensure osg-configure  = 1.0.60-2 is installed   Configure your CE using osg-configure   You should use the  HTCondor-CE Install Docs  as a reference, although you can skip several of the steps  You can skip setting up Squid: set  enabled  to  True  and  location  to  UNAVAILABLE  in  01-squid.ini  Set  htcondor_gateway_enabled  to  True  in  10-gateway.ini  You probably don't need GUMS, but if you want it, use the Fermi GUMS server (set  gums_host  to  gums.fnal.gov  and  authorization_method  to  xacml  in 10-misc.ini)   To keep osg-configure from complaining about storage, edit  10-storage.ini :   Set  se_available  to  False  Set  app_dir  to  /osg/app_dir  Set  data_dir  to  /osg/data_dir  Do  mkdir -p /osg/app_dir/etc; mkdir -p /osg/data_dir; chmod 1777 /osg/app_dir{,/etc}     Enable your batch system by setting  enabled  to  True  in  20- batch system .ini   Set up the site info in  40-siteinfo.ini  ; in particular, you'll need to set the  resource  and  resource_group  settings\n    \\ (you just need to pick a name; I concatenate my login name with the short host name and use that, e.g. matyasfermicloud001).\n    \\ You can also use the following settings:  group=OSG-ITB  sponsor=local  city=Batavia, IL  country=US  longitude=-88  latitude=41       Edit the file  /etc/osg/config.d/30-infoservices.ini  and set  ce_collectors  to the collector host   Run  osg-configure -dc  Start up your batch system  Run  service condor-ce start   The CE will report to the collector host every five minutes. If you want to force it to send now, run  condor_ce_reconfig . You should see your CE if you run  condor_ce_status -schedd  on the collector host.", 
            "title": "Setting Up the CE"
        }, 
        {
            "location": "/release/acceptance-testing/#rsv", 
            "text": "Testing a fresh installation:   make sure the yum repositories required by OSG is installed on your host  rpm -Uvh  http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm  OR rpm -Uvh  http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm  also make sure epel repo is set up.     install the rpm  yum --enablerepo=osg-testing install rsv     edit /etc/osg/config.d/30-rsv.ini file  in my case, I don't have a service cert for testing, so I use my own personal cert to create the proxy, but later on the owner of the proxy should be changed to \"rsv\" user that is created during the rpm install.  in the config file, for the ce_hosts and gridftp_hosts, put in a test server, as the result from this test will be uploaded to OSG GOC, which may mess up your production service monitoring if you chose a production server for the test.    osg-configure -v  osg-configure -c  /etc/init.d/condor-cron start  /etc/init.d/rsv start  rsv-control --list  rsv-control --version  rsv-control --run --all-enabled 11. make sure the results from the above commands look fine.   Testing an upgrade installation:   make sure to enable the osg-testing repo, and set its priority higher than the other repos  yum --enablerepo=osg-testing upgrade rsv*  you can use the old 30-rsv.ini file for configuration  repeat steps 4)~11) as mentioned in the last section.", 
            "title": "RSV"
        }, 
        {
            "location": "/release/acceptance-testing/#slurm", 
            "text": "This section goes through the steps needed to set up a slurm install on a VM. This is a necessary prerequisite for testing Slurm related components (CE integration, gratia, etc.). Note that the slurm setup used for this uses weak passwords for mysql. It should be sufficient for a quick setup, testing, and then tear down but should not be used without changes if it will be running for any appreciable length of time.   Note  need to have a VM with 2+ GB of memory", 
            "title": "Slurm"
        }, 
        {
            "location": "/release/acceptance-testing/#installation-and-setup", 
            "text": "Download scripts and config files:   cd /tmp/\ngit clone  https://github.com/sthapa/utilities.git \ncd utilities/slurm    setup and install slurm components  export username= USERNAME  \\# user that jobs will run as\nexport version= 14.11.7  \\# slurm version to install (e.g. 16.05.2 or 14.11.7)\n./slurm-setup.sh    After successful completion, slurm and slurm gratia probes should be setup and enabled.", 
            "title": "Installation and setup"
        }, 
        {
            "location": "/release/acceptance-testing/#running-a-job-using-slurm", 
            "text": "Generate test.sh with the following:  #/bin/bash\necho  In the directory: `pwd`  echo  As the user: `whoami`  echo \u201cHostname:  /bin/hostname sleep 60  /pre     run  sbatch test.sh   the output from the jobs should appear in the current working directory as  test.sh.[eo].nnnnn  where nnnnn is a job id", 
            "title": "Running a job using slurm"
        }, 
        {
            "location": "/release/acceptance-testing/#vo-client", 
            "text": "This document contains a basic recipe for testing a VO Package release", 
            "title": "VO Client"
        }, 
        {
            "location": "/release/acceptance-testing/#prerequisites", 
            "text": "Testing the VO package requires a few components:\n   * X.509 certificate with membership to at least one VO\n   * System with working GUMS installation\n   * System with OSG installation (voms-proxy-init and edg-mkgridmap)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/release/acceptance-testing/#testing-voms-proxy-init", 
            "text": "Login in the system that has voms-proxy-init installed.  Make sure that you have the correct \nvo-client rpms installed and that your X.509 certificate is in your home directory.  For each\nVO that you have membership in, run the following  voms-proxy-init -voms [VO]  where  [VO]  is \nthe appropriate VO (e.g. osg, cms, etc.).  You should be able to generate a voms-proxy for that\nVO without errors.", 
            "title": "Testing voms-proxy-init"
        }, 
        {
            "location": "/release/acceptance-testing/#testing-edg-mkgridmap", 
            "text": "Log on to a system with  edg-mkgridmap  installed.  Make sure you have the correct vo-client rpms \ninstalled (vo-client-edgmkgridmap).  Run  edg-mkgridmap  and check the log output for errors. \nThere will be some errors so compare your errors with the errors on previous vo-package tickets to\nmake sure no new errors have appeared.", 
            "title": "Testing edg-mkgridmap"
        }, 
        {
            "location": "/release/acceptance-testing/#testing-gums", 
            "text": "Log on to a system with a working GUMS install.  Make sure that you have the correct vo-client \nrpms (osg-gums-config) installed.     Make a backup of  /etc/gums/gums.config    Copy the mysql database information from  /etc/gums/gums.config  to  /etc/gums/gums.config.template  Copy  /etc/gums/gums.config.template  to  /etc/gums/gums.config  Start the  tomcat6  service   Go to the GUMS interface (e.g. https://my.host:8443/gums)  Go to the Update VO members page and click on the  update VO members  button  Once completed, there will probably be some errors.  Compare errors to errors on prior vo package update tickets and make sure no new errors have occurred.", 
            "title": "Testing GUMS"
        }, 
        {
            "location": "/release/acceptance-testing/#voms-admin-server", 
            "text": "", 
            "title": "VOMS Admin Server"
        }, 
        {
            "location": "/release/acceptance-testing/#install-and-configure-voms-admin-server", 
            "text": "Install and configure voms-admin-server with this  voms-install.sh  script, entering  osg-testing  when prompted for the  REPO  and your own e-mail address when prompted for  EMAIL_FROM .", 
            "title": "Install and configure voms-admin-server"
        }, 
        {
            "location": "/release/acceptance-testing/#set-up-test_vo-and-add-yourself-as-an-admin", 
            "text": "To add a test VO (named TEST_VO) and add yourself as an admin, use the following script, replacing the  USER_EMAIL ,  USER_CERT_SUBJECT , and  USER_COMMON_NAME  variables with your own:  #!/bin/bash  VO_NAME = TEST_VO  TOMCAT_PORT = 8443  USER_EMAIL = YOUR_EMAIL  USER_CERT_SUBJECT = YOUR CERT DN  USER_CERT_ISSUER = /DC=com/DC=DigiCert-Grid/O=DigiCert Grid/CN=DigiCert Grid CA-1  USER_COMMON_NAME = YOUR CERT CN \n\nvoms-admin --vo  $VO_NAME  --host  $HOSTNAME  --nousercert create-user  \\ \n     $USER_CERT_SUBJECT   $USER_CERT_ISSUER   $USER_COMMON_NAME   $USER_EMAIL \n\nvoms-admin --vo  $VO_NAME  --host  $HOSTNAME  --nousercert assign-role  \\ \n    / $VO_NAME  VO-Admin  $USER_CERT_SUBJECT   $USER_CERT_ISSUER  echo  web ui: https:// $HOSTNAME : $TOMCAT_PORT /voms/ $VO_NAME", 
            "title": "Set up TEST_VO and add yourself as an admin"
        }, 
        {
            "location": "/release/acceptance-testing/#testing-the-web-ui", 
            "text": "Add/suspend/restore/delete a user (using your e-mail address for contact)  Verify e-mail receipt of suspension message", 
            "title": "Testing the web UI"
        }, 
        {
            "location": "/release/acceptance-testing/#xrootd-voms-testing", 
            "text": "This section is intended for OSG Software/Release teams to gather information on testing vomsxrd/xrootd-voms-plugin package. Original plugin named  vomsxrd , similar to lcmaps that extracts information for authorization within xrootd of a proxy's voms extension.  You need an  xrootd server installation  In the xrootd server yum install the following packages:   xrootd  xrootd-voms-plugin  vo-client   In the xrootd client yum install the following packages:   xrootd-client  voms-clients  vo-client   In the xrootd server add this lines to file  /etc/xrootd/xrootd-clustered.cfg  xrootd.seclib /usr/lib64/libXrdSec.so\nsec.protparm gsi -vomsfun:/usr/lib64/libXrdSecgsiVOMS.so -vomsfunparms:certfmt=raw|vos=cms|dbg -vomsat:2\nsec.protocol /usr/lib64 gsi -ca:1 -crl:3  This configuration will only authorize members of VO  cms . You can change it to another VO.  Make sure  fetch-crl  has been run otherwise the xrootd service may fail to start.  In the xrootd client get a proxy without voms extension or with another VO extension different that the one used in the configuration:  user@host $  voms-proxy-init -voms mis Enter GRID pass phrase:  Your identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020  Creating temporary proxy ........................... Done  Contacting  voms.opensciencegrid.org:15001 [/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=http/voms.opensciencegrid.org]  mis  Done  Creating proxy ............................................... Done  user@host $  xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/ [0B/0B][100%][==================================================][0B/s]    Run: [FATAL] Auth failed   Now get a proxy with cms extension and run it again:  user@host $  voms-proxy-init -voms cms Enter GRID pass phrase:  Your identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020  Creating temporary proxy ...................................... Done  Contacting  voms2.cern.ch:15002 [/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch]  cms  Done  Creating proxy .......................................... Done  Your proxy is valid until Thu Dec  4 22:53:29 2014  user@host $  xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/ [0B/0B][100%][==================================================][0B/s]", 
            "title": "XRootD VOMS Testing"
        }, 
        {
            "location": "/release/acceptance-testing/#xrootd-plugins", 
            "text": "", 
            "title": "XRootD Plugins"
        }, 
        {
            "location": "/release/acceptance-testing/#hadoop-name-node-installation", 
            "text": "Use the following script with option 1:  #!/bin/bash  set  -e select  NODETYPE in namenode datanode gridftp ;   do \n   [[   $NODETYPE   ]]     break  done  case   $NODETYPE  in\n  namenode  )   NAMENODE = $HOSTNAME   ;; \n         *  )   read  -p  hostname for NAMENODE?   NAMENODE  ;;  esac  echo   NODETYPE = $NODETYPE  echo   NAMENODE = $NAMENODE  read  -p  ok? [y/N]   ok case   $ok  in\n  y* | Y* )   ;;    # ok \n      * )   exit   ;;  esac  #yum install --enablerepo=osg-minefield osg-se-hadoop-$NODETYPE \nyum install osg-se-hadoop- $NODETYPE  case   $NODETYPE  in\n  namenode | datanode  ) \n    mkdir -p /data/ { hadoop,scratch,checkpoint } \n    chown -R hdfs:hdfs /data\n    sed -i s/NAMENODE/ $NAMENODE / /etc/hadoop/conf.osg/ { core,hdfs } -site.xml\n    cp /etc/hadoop/conf.osg/ { core,hdfs } -site.xml /etc/hadoop/conf/\n    touch /etc/hosts_exclude\n     ;; \n  gridftp  ) \n    ln -snf conf.osg /etc/hadoop/conf\n    sed -i s/NAMENODE/ $NAMENODE / /etc/hadoop/conf.osg/ { core,hdfs } -site.xml\n     echo   hadoop-fuse-dfs# /mnt/hadoop fuse server= $NAMENODE ,port=9000,rdbuffer=131072,allow_other 0 0    /etc/fstab\n    mkdir /mnt/hadoop\n    mount /mnt/hadoop\n    cp -v /etc/redhat-release /mnt/hadoop/test-file\n    sed -i  /globus_mapping/s/^# *//  /etc/grid-security/gsi-authz.conf\n    sed -i s/yourgums.yourdomain/gums.fnal.gov/ /etc/lcmaps.db\n    mkdir /mnt/hadoop/fnalgrid\n    useradd fnalgrid -g fnalgrid\n    chown fnalgrid:fnalgrid /mnt/hadoop/fnalgrid\n    service globus-gridftp-server start\n     if   type  -t globus-url-copy  /dev/null ;   then \n      globus-url-copy file:////bin/bash  gsiftp:// $HOSTNAME /mnt/hadoop/fnalgrid/first_test\n     else \n       echo  globus-url-copy not installed\n     fi \n     ;;  esac  case   $NODETYPE  in\n  namenode  )  su - hdfs -c  hadoop namenode -format   ;;  esac \nservice hadoop-hdfs- $NODETYPE  start", 
            "title": "Hadoop name node installation"
        }, 
        {
            "location": "/release/acceptance-testing/#hadoop-data-node-installation", 
            "text": "Run same script as before but with option number 2.   Install xrootd-server:  yum install xrootd-server    Install xrootd-plugins  yum install xrootd-cmstfc xrootd-hdfs", 
            "title": "Hadoop data node installation"
        }, 
        {
            "location": "/release/acceptance-testing/#gridftp-installation", 
            "text": "Run same as script but with option number.", 
            "title": "GridFTP installation"
        }, 
        {
            "location": "/release/acceptance-testing/#on-the-name-node", 
            "text": "[root@fermicloud092 ~]#  hdfs dfs -ls /test-file Found 1 items  -rw-r--r--   2 root root          0 2014-07-21 15:57 /test-file   This means your hadoop is working.  Modify the file  /etc/xrootd/xrootd-clustered.cfg  to look like this:  xrd.port 1094\n\n# The roles this server will play.                                                                                            \nall.role server\nall.role manager if xrootd.unl.edu\n# The known managers                                                                                                          \nall.manager srm.unl.edu:1213\n#all.manager xrootd.ultralight.org:1213                                                                                       \n\n# Allow any path to be exported; this is further refined in the authfile.                                                     \nall.export / nostage\n\n# Hosts allowed to use this xrootd cluster                                                                                    \ncms.allow host *\n\n### Standard directives                                                                                                       \n# Simple sites probably don t need to touch these.                                                                            \n# Logging verbosity                                                                                                           \nxrootd.trace all -debug\nofs.trace all -debug\nxrd.trace all -debug\ncms.trace all -debug\n\n# Integrate with CMS TFC, placed in /etc/storage.xml                                                                          \noss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop\n\nxrootd.seclib /usr/lib64/libXrdSec.so\nxrootd.fslib /usr/lib64/libXrdOfs.so\nofs.osslib /usr/lib64/libXrdHdfs.so\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\ncms.delay startup 10\ncms.fxhold 60s\ncms.perf int 30s pgm /usr/bin/XrdOlbMonPerf 30\n\noss.space default_stage /opt/xrootd_cache  Create file  /etc/xrootd/storage.xml  and place this:  storage-mapping  lfn-to-pfn   protocol= hadoop   destination-match= .*   path-match= .*/+tmp2/test-file   result= /test-file /  /storage-mapping   For el7 the instrucctions are a little bit different. See:  https://jira.opensciencegrid.org/browse/SOFTWARE-2198?focusedCommentId=334667 page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-334667  Now from a node do:  xrdcp --debug 3 root://yourdatanode.yourdomain:1094//tmp2/test-file .   If it is sucessful it would have tested both cmstfc and hdfs plugins", 
            "title": "On the name node"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/", 
            "text": "Service Migrations - Spring 2018\n\n\nThe Open Science Grid (OSG) is transitioning effort from Indiana, requiring a redistribution of support and services.\nSome services will be retired, most services will be migrated to other locations (with minimal expected sites impact),\nand some services will be migrated that will result in significant impact on sites.\n\n\nThis document is intended to guide OSG site administrators through these changes, highlighting where the site administrator\naction is required.\n\n\n\n\nNote\n\n\nSpecific service migrations will be announced prior to retirement or migration.\n\n\n\n\nWe understand sites will have many questions regarding this transition.\nThe OSG Technology team will be holding office hours for any questions or comments about these service \nchanges; see the \nOffice Hours section\n for details.\n\n\nSite Checklist Summary\n\n\nThe remainder of the document provides a service-by-service overview of site impacts.  In this section, we\nprovide a short outline of the steps sites are recommended to take:\n\n\n\n\nReview all site configuration (yum repositories, CVMFS configurations) to ensure they are the latest from the\n   OSG.  In particular, replace all references to \ngrid.iu.edu\n with equivalent \nopensciencegrid.org\n entries.\n\n\nRenew all local host certificates within the OSG CA prior to May 1.  A list of affected hosts at your sites\n   can be provided by OSG support upon request.\n\n\nLHC sites should verify local admins have a GGUS accounts and are familiar with its web interface.\n\n\nSites at institutions with an \nInCommon CA subscription\n\n   should determine their local procedure for obtaining certificates.\n\n\nSites should transition to the\n   \nLCMAPS VOMS authentication method\n\n   if they have not done so already.\n\n\n\n\nOffice Hours\n\n\nIf you have questions or concerns that are not addressed in this document, please join us for our office hours!\n\n\nWhere:\n\n\n\n\nhttps://unl.zoom.us/j/277958559\n\n\nOr Telephone: US: +1 408 638 0968  or +1 646 876 9923  or +1 669 900 6833\n  Meeting ID: 277 958 559\n\n\n\n\nWhen:\n\n\n\n\nMondays, 4-5 PM CDT\n\n\nTuesdays, 1-3 PM CDT\n\n\nThursdays, 10-11 AM CDT\n\n\n\n\nWe can also be contacted at the usual locations:\n\n\n\n\nhelp@opensciencegrid.org\n\n\nosg-software@opensciencegrid.org\n - General discussion amongst team members\n\n\nSlack channel\n - if you can't create an account, \n   send an e-mail to \nosg-software@opensciencegrid.org\n\n\n\n\nService-specific details\n\n\nOSG CA\n\n\nThe OSG CA service offers certificate request, renewal, and revocation through the \nOIM\n web interface, \nthe OIM REST API, and the \nosg-pki-tools\n command-line tool.\nThis service will be retired by May 31 but the OSG CA certificate will remain in the IGTF distribution, so\nyour certificates remain valid until they expire.\nTherefore, to extend the window for transitioning to any new CA service, we have the following recommendations:\n\n\n\n\nAction item(s)\n\n\n\n\n\n\nRequest fresh certificates for each publicly facing host and service at your site, such as:\n\n\n\n\nHTCondor-CE\n\n\nGlideinWMS VO Frontend: frontend, proxy, and VOMS certificates\n\n\nGlideinWMS Factory\n\n\nGridFTP\n\n\nXRootD\n\n\nRSV\n\n\nGUMS\n\n\nBeStMan\n\n\nVOMS Admin Server\n\n\n\n\n\n\n\n\nRenew your \nuser certificate\n.  LHC sites should take the\n   opportunity to switch users to the CERN CA.\n\n\n\n\n\n\n\n\nIn the future, we will use the following CA certificate services:\n\n\n\n\n\n\n\n\nFor...\n\n\nWe plan to use the following Certificate Authorities...\n\n\n\n\n\n\n\n\n\n\nHost and Service Certificates\n\n\nInCommon\n and \nLet\u2019s Encrypt\n\n\n\n\n\n\nUser Certificates\n\n\nCILogon Basic\n for non-LHC users\n\n\n\n\n\n\n\n\nLHC users should continue to request their user certificates from CERN.\n\n\n\n\n\n\nWeb-Based services\n\n\nLet's Encrypt\n\n\n\n\n\n\n\n\nNew processes for requesting host, service, and user certificates against the aforementioned CAs are forthcoming.\n\n\nThe exact date for the retirement of the OSG CA service will be announced, but will be around the end of May.\nIf you experience any problems with the OSG CA service, please contact us at\n\nhelp@opensciencegrid.org\n.\n\n\nSoftware Repository\n\n\nThe OSG Software repository includes the YUM repositories, client tarballs, and CA tarballs.\nThe physical hosting location of this service will be changing; no other changes are planned.\nTo ensure a smooth transition at your site, verify that the OSG repository files are up to date on all of your OSG hosts:\n\n\n\n\nAction item\n\n\n\n\n\n\nIf your OSG repository files have been installed via RPM, verify that the version of \nosg-release\n is at least \n  \n3.3-6\n or \n3.4-4\n, for OSG 3.3 and 3.4, respectively:\n\n\nuser@host $\n rpm -q osg-release\n\n\n\n\n\nIf the version is older, update your \nosg-release\n RPM:\n\n\nroot@host #\n yum update osg-release\n\n\n\n\n\n\n\n\n\nIf your OSG repository files have not been installed via RPM, ensure that there are no references to \ngrid.iu.edu\n:\n\n\nuser@host $\n grep grid.iu.edu /etc/yum.repos.d/osg*.repo\n\n\n\n\n\nReplace all instances of \ngrid.iu.edu\n with \nopensciencegrid.org\n.\n\n\n\n\n\n\n\n\nThe exact date for moving the physical hosting location will be announced.\nIf you experience any problems with the OSG Software repository, please contact us at\n\nhelp@opensciencegrid.org\n.\n\n\nMyOSG and OIM\n\n\nThe \nMyOSG\n service provides web and REST interfaces to access information about\nOSG site topology, projects, and VOs.\nThe MyOSG web interface will be retired but we will continue to offer the same REST interface.\nIf you run a service that queries MyOSG:\n\n\n\n\nAction items\n\n\n\n\nEnsure that the services use \nhttps://my.opensciencegrid.org\n instead of \nhttps://myosg.grid.iu.edu\n\n\nLet us know what queries you\u2019re making and why you\u2019re making them\n\n\n\n\n\n\nOIM\n serves as the database for the information used by MyOSG with a web\ninterface for data updates.\nThe OIM web interface will be retired but we will migrate its data to a series of YAML files held in a GitHub repository.\nAfter OIM is retired, updates to the aforementioned data can be requested via email or pull request.\nDocumentation forthcoming.\n\n\n\n\nNote\n\n\nPlease see the \nOSG CA\n section for information regarding the OIM certificate service.\n\n\n\n\nThe exact dates for retiring the MyOSG and OIM web interfaces will be announced.\nIf you experience any problems with the OSG Software repository, please contact us at\n\nhelp@opensciencegrid.org\n.\n\n\nGRACC Accounting and WLCG Accounting\n\n\nNo changes are planned for the \nGRACC accounting\n\nservice at this time.  The integration of OSG with WLCG accounting should see no interruption.\nIf you experience any problems with GRACC accounting, please contact us at\n\nhelp@opensciencegrid.org\n.\n\n\nOASIS and CVMFS\n\n\nThe OASIS (OSG Application and Software Installation Service) is a service used to distribute common applications and\nsoftware to OSG sites via CVMFS.\nThe OSG hosts a CVMFS Stratum-0 for keysigning, a repository server, and a CVMFS Stratum-1.\nThe physical hosting location of these services will be moved to Nebraska but we do not plan any other changes and do not expect\nthis to affect sites.\nThe exact date for moving the hosting location will be announced.\nIf you experience any problems with OASIS or CVMFS, please contact us at\n\nhelp@opensciencegrid.org\n.\n\n\nVOMS Admin Server\n\n\nThe \nOSG VOMS\n service is used to sign VOMS attributes\nfor members of the OSG VO and can respond to queries for a list of VO members.\nThe \ndeprecation of VOMS Admin Server\n software (and therefore VOMS servers), started 10 months ago;\nthe OSG VOMS servers will be retired as previously planned.\n\n\n\n\nAction item\n\n\nIf your site accepts OSG jobs, transition your hosts to \n\nLCMAPS VOMS authentication\n.\n\n\n\n\nRSV\n\n\nThe \ncentral RSV service\n\nis a monitoring tool that displays every service status information about OSG sites that elect to provide it.\nIt will be retired since there is no longer a need to monitor OSG site status as a whole.\nIf you would like to monitor your OSG services, you can access the status page of your local\n\nRSV\n instance.\n\n\n\n\nAction item\n\n\nBefore the retirement, you will need to disable the \ngratia-consumer\n on your local RSV host,\nwhich uploads status data to the central RSV service:\n\n\nroot@server #\n rsv-control --disable --host \nYOUR RSV HOST\n gratia-consumer\n\nroot@server #\n rsv-control --off --host \nYOUR RSV HOST\n gratia-consumer\n\n\n\n\n\n\n\nThe exact date for retirement of the central RSV service will be announced.\nIf you experience any problems with the central RSV service, please contact us at\n\nhelp@opensciencegrid.org\n.\n\n\nCollector\n\n\nThe \ncentral Collector\n is a central database service that provides details about\npilot jobs currently running in the OSG.\nThe physical hosting location of the central Collector will be moved but we do not for plan any other changes and do not\nexpect this to affect sites. \nThe exact date for moving the hosting location will be announced. If you experience any problems with the central \nCollector, please contact us at \nhelp@opensciencegrid.org\n.", 
            "title": "Service Migrations - Spring 2018"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#service-migrations-spring-2018", 
            "text": "The Open Science Grid (OSG) is transitioning effort from Indiana, requiring a redistribution of support and services.\nSome services will be retired, most services will be migrated to other locations (with minimal expected sites impact),\nand some services will be migrated that will result in significant impact on sites.  This document is intended to guide OSG site administrators through these changes, highlighting where the site administrator\naction is required.   Note  Specific service migrations will be announced prior to retirement or migration.   We understand sites will have many questions regarding this transition.\nThe OSG Technology team will be holding office hours for any questions or comments about these service \nchanges; see the  Office Hours section  for details.", 
            "title": "Service Migrations - Spring 2018"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#site-checklist-summary", 
            "text": "The remainder of the document provides a service-by-service overview of site impacts.  In this section, we\nprovide a short outline of the steps sites are recommended to take:   Review all site configuration (yum repositories, CVMFS configurations) to ensure they are the latest from the\n   OSG.  In particular, replace all references to  grid.iu.edu  with equivalent  opensciencegrid.org  entries.  Renew all local host certificates within the OSG CA prior to May 1.  A list of affected hosts at your sites\n   can be provided by OSG support upon request.  LHC sites should verify local admins have a GGUS accounts and are familiar with its web interface.  Sites at institutions with an  InCommon CA subscription \n   should determine their local procedure for obtaining certificates.  Sites should transition to the\n    LCMAPS VOMS authentication method \n   if they have not done so already.", 
            "title": "Site Checklist Summary"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#office-hours", 
            "text": "If you have questions or concerns that are not addressed in this document, please join us for our office hours!  Where:   https://unl.zoom.us/j/277958559  Or Telephone: US: +1 408 638 0968  or +1 646 876 9923  or +1 669 900 6833\n  Meeting ID: 277 958 559   When:   Mondays, 4-5 PM CDT  Tuesdays, 1-3 PM CDT  Thursdays, 10-11 AM CDT   We can also be contacted at the usual locations:   help@opensciencegrid.org  osg-software@opensciencegrid.org  - General discussion amongst team members  Slack channel  - if you can't create an account, \n   send an e-mail to  osg-software@opensciencegrid.org", 
            "title": "Office Hours"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#service-specific-details", 
            "text": "", 
            "title": "Service-specific details"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#osg-ca", 
            "text": "The OSG CA service offers certificate request, renewal, and revocation through the  OIM  web interface, \nthe OIM REST API, and the  osg-pki-tools  command-line tool.\nThis service will be retired by May 31 but the OSG CA certificate will remain in the IGTF distribution, so\nyour certificates remain valid until they expire.\nTherefore, to extend the window for transitioning to any new CA service, we have the following recommendations:   Action item(s)    Request fresh certificates for each publicly facing host and service at your site, such as:   HTCondor-CE  GlideinWMS VO Frontend: frontend, proxy, and VOMS certificates  GlideinWMS Factory  GridFTP  XRootD  RSV  GUMS  BeStMan  VOMS Admin Server     Renew your  user certificate .  LHC sites should take the\n   opportunity to switch users to the CERN CA.     In the future, we will use the following CA certificate services:     For...  We plan to use the following Certificate Authorities...      Host and Service Certificates  InCommon  and  Let\u2019s Encrypt    User Certificates  CILogon Basic  for non-LHC users     LHC users should continue to request their user certificates from CERN.    Web-Based services  Let's Encrypt     New processes for requesting host, service, and user certificates against the aforementioned CAs are forthcoming.  The exact date for the retirement of the OSG CA service will be announced, but will be around the end of May.\nIf you experience any problems with the OSG CA service, please contact us at help@opensciencegrid.org .", 
            "title": "OSG CA"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#software-repository", 
            "text": "The OSG Software repository includes the YUM repositories, client tarballs, and CA tarballs.\nThe physical hosting location of this service will be changing; no other changes are planned.\nTo ensure a smooth transition at your site, verify that the OSG repository files are up to date on all of your OSG hosts:   Action item    If your OSG repository files have been installed via RPM, verify that the version of  osg-release  is at least \n   3.3-6  or  3.4-4 , for OSG 3.3 and 3.4, respectively:  user@host $  rpm -q osg-release  If the version is older, update your  osg-release  RPM:  root@host #  yum update osg-release    If your OSG repository files have not been installed via RPM, ensure that there are no references to  grid.iu.edu :  user@host $  grep grid.iu.edu /etc/yum.repos.d/osg*.repo  Replace all instances of  grid.iu.edu  with  opensciencegrid.org .     The exact date for moving the physical hosting location will be announced.\nIf you experience any problems with the OSG Software repository, please contact us at help@opensciencegrid.org .", 
            "title": "Software Repository"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#myosg-and-oim", 
            "text": "The  MyOSG  service provides web and REST interfaces to access information about\nOSG site topology, projects, and VOs.\nThe MyOSG web interface will be retired but we will continue to offer the same REST interface.\nIf you run a service that queries MyOSG:   Action items   Ensure that the services use  https://my.opensciencegrid.org  instead of  https://myosg.grid.iu.edu  Let us know what queries you\u2019re making and why you\u2019re making them    OIM  serves as the database for the information used by MyOSG with a web\ninterface for data updates.\nThe OIM web interface will be retired but we will migrate its data to a series of YAML files held in a GitHub repository.\nAfter OIM is retired, updates to the aforementioned data can be requested via email or pull request.\nDocumentation forthcoming.   Note  Please see the  OSG CA  section for information regarding the OIM certificate service.   The exact dates for retiring the MyOSG and OIM web interfaces will be announced.\nIf you experience any problems with the OSG Software repository, please contact us at help@opensciencegrid.org .", 
            "title": "MyOSG and OIM"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#gracc-accounting-and-wlcg-accounting", 
            "text": "No changes are planned for the  GRACC accounting \nservice at this time.  The integration of OSG with WLCG accounting should see no interruption.\nIf you experience any problems with GRACC accounting, please contact us at help@opensciencegrid.org .", 
            "title": "GRACC Accounting and WLCG Accounting"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#oasis-and-cvmfs", 
            "text": "The OASIS (OSG Application and Software Installation Service) is a service used to distribute common applications and\nsoftware to OSG sites via CVMFS.\nThe OSG hosts a CVMFS Stratum-0 for keysigning, a repository server, and a CVMFS Stratum-1.\nThe physical hosting location of these services will be moved to Nebraska but we do not plan any other changes and do not expect\nthis to affect sites.\nThe exact date for moving the hosting location will be announced.\nIf you experience any problems with OASIS or CVMFS, please contact us at help@opensciencegrid.org .", 
            "title": "OASIS and CVMFS"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#voms-admin-server", 
            "text": "The  OSG VOMS  service is used to sign VOMS attributes\nfor members of the OSG VO and can respond to queries for a list of VO members.\nThe  deprecation of VOMS Admin Server  software (and therefore VOMS servers), started 10 months ago;\nthe OSG VOMS servers will be retired as previously planned.   Action item  If your site accepts OSG jobs, transition your hosts to  LCMAPS VOMS authentication .", 
            "title": "VOMS Admin Server"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#rsv", 
            "text": "The  central RSV service \nis a monitoring tool that displays every service status information about OSG sites that elect to provide it.\nIt will be retired since there is no longer a need to monitor OSG site status as a whole.\nIf you would like to monitor your OSG services, you can access the status page of your local RSV  instance.   Action item  Before the retirement, you will need to disable the  gratia-consumer  on your local RSV host,\nwhich uploads status data to the central RSV service:  root@server #  rsv-control --disable --host  YOUR RSV HOST  gratia-consumer root@server #  rsv-control --off --host  YOUR RSV HOST  gratia-consumer   The exact date for retirement of the central RSV service will be announced.\nIf you experience any problems with the central RSV service, please contact us at help@opensciencegrid.org .", 
            "title": "RSV"
        }, 
        {
            "location": "/policy/service-migrations-spring-2018/#collector", 
            "text": "The  central Collector  is a central database service that provides details about\npilot jobs currently running in the OSG.\nThe physical hosting location of the central Collector will be moved but we do not for plan any other changes and do not\nexpect this to affect sites. \nThe exact date for moving the hosting location will be announced. If you experience any problems with the central \nCollector, please contact us at  help@opensciencegrid.org .", 
            "title": "Collector"
        }, 
        {
            "location": "/policy/gums-retire/", 
            "text": "GUMS Retirement\n\n\nThis document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack.\n\n\nIntroduction\n\n\nGUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to\nlocal UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from\nmultiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG\ncommunity, we have found that sites use the following GUMS features:\n\n\n\n\nMapping based on VOMS attributes\n\n\nHost-based mappings\n\n\nBanning users/VOs\n\n\nSupporting pool accounts\n\n\n\n\nGUMS is a large Java web application that is more complex than necessary for the subset of features used in the\nOSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG\nSoftware team.\n\n\nOSG's plans to retire GUMS has two major components:\n\n\n\n\nFind a suitable replacement for GUMS\n\n\nProvide documentation, tooling, and support to aid in the transition from GUMS to the intended solution\n\n\n\n\nSite Transition Plans\n\n\nWe have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS\nextensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as\nuser and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and\nthe distributed verification of VOMS extensions makes this feature unnecessary.\n\n\nPool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific\ndocument. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work\nwith any site for which this solution does not work.\n\n\nLCMAPS VOMS plugin installation and configuration documentation can be\nfound \nhere\n.\n\n\nTimeline\n\n\n\n\nApril 2017 (completed): \nlcmaps-plugins-voms\n shipped and supported by OSG.\n\n\nMay 2017 (completed): \nosg-configure\n and documentation necessary for using \nlcmaps-plugins-voms\n is shipped.\n\n\nJune 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, \nedg-mkgridmap\n, or GUMS.\n\n\nJuly 2017 (completed): OSG 3.4 CEs can be configured with 3.3 GUMS hosts\n\n\nMarch 2018: Complete transition for sites not using pool accounts\n\n\nMay 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.", 
            "title": "GUMS Retirement"
        }, 
        {
            "location": "/policy/gums-retire/#gums-retirement", 
            "text": "This document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack.", 
            "title": "GUMS Retirement"
        }, 
        {
            "location": "/policy/gums-retire/#introduction", 
            "text": "GUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to\nlocal UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from\nmultiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG\ncommunity, we have found that sites use the following GUMS features:   Mapping based on VOMS attributes  Host-based mappings  Banning users/VOs  Supporting pool accounts   GUMS is a large Java web application that is more complex than necessary for the subset of features used in the\nOSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG\nSoftware team.  OSG's plans to retire GUMS has two major components:   Find a suitable replacement for GUMS  Provide documentation, tooling, and support to aid in the transition from GUMS to the intended solution", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/gums-retire/#site-transition-plans", 
            "text": "We have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS\nextensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as\nuser and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and\nthe distributed verification of VOMS extensions makes this feature unnecessary.  Pool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific\ndocument. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work\nwith any site for which this solution does not work.  LCMAPS VOMS plugin installation and configuration documentation can be\nfound  here .", 
            "title": "Site Transition Plans"
        }, 
        {
            "location": "/policy/gums-retire/#timeline", 
            "text": "April 2017 (completed):  lcmaps-plugins-voms  shipped and supported by OSG.  May 2017 (completed):  osg-configure  and documentation necessary for using  lcmaps-plugins-voms  is shipped.  June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin,  edg-mkgridmap , or GUMS.  July 2017 (completed): OSG 3.4 CEs can be configured with 3.3 GUMS hosts  March 2018: Complete transition for sites not using pool accounts  May 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.", 
            "title": "Timeline"
        }, 
        {
            "location": "/policy/bestman2-retire/", 
            "text": "BeStMan2 Retirement\n\n\nThis document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack.\n\n\nIntroduction\n\n\nBeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol.  SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\"  Additionally, SRM can be used to mediate transfer protocol selection.\n\n\nOSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers.  This functionality is redundant to what can be accomplished with GridFTP alone.\n\n\nBeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize.  OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents.  We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan.\n\n\nSite Transition Plans\n\n\nWe have released \ndocumentation\n\nfor a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple\nGridFTP endpoints.\n\n\nSites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2.\n\n\nTimeline\n\n\n\n\nMarch 2017 (completed):\n  Release \nload balanced GridFTP\n\n  documentation\n\n\nJune 2017 (completed): OSG 3.4.0 is released without BeStMan\n\n\nDecember 2018 (completed): Security-only support for OSG 3.3 series and BeStMan is provided\n\n\nMay 2018: Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.", 
            "title": "BeStMan2 Retirement"
        }, 
        {
            "location": "/policy/bestman2-retire/#bestman2-retirement", 
            "text": "This document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack.", 
            "title": "BeStMan2 Retirement"
        }, 
        {
            "location": "/policy/bestman2-retire/#introduction", 
            "text": "BeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol.  SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\"  Additionally, SRM can be used to mediate transfer protocol selection.  OSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers.  This functionality is redundant to what can be accomplished with GridFTP alone.  BeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize.  OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents.  We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan.", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/bestman2-retire/#site-transition-plans", 
            "text": "We have released  documentation \nfor a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple\nGridFTP endpoints.  Sites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2.", 
            "title": "Site Transition Plans"
        }, 
        {
            "location": "/policy/bestman2-retire/#timeline", 
            "text": "March 2017 (completed):\n  Release  load balanced GridFTP \n  documentation  June 2017 (completed): OSG 3.4.0 is released without BeStMan  December 2018 (completed): Security-only support for OSG 3.3 series and BeStMan is provided  May 2018: Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.", 
            "title": "Timeline"
        }, 
        {
            "location": "/policy/voms-admin-retire/", 
            "text": "VOMS-Admin Retirement\n\n\nIntroduction\n\n\nThis document provides an overview of the planned retirement of support for VOMS-Admin\nin the OSG Software Stack.\n\n\nSupport for the VOMS infrastructure has three major components:\n\n\n\n\nVOMS-Admin\n: A web interface for maintaining the list of authorized users in\n    a VO and their various authorizations (group membership, roles, attributes, etc).\n\n\nVOMS-Server\n: A TCP service which signs a cryptographic extension on an X509\n    proxy certificate asserting the authorizations available to the authenticated user.\n\n\nVOMS Client\n: Software for extracting and validating the signed VOMS extension from\n    an X509 proxy.  The validation is meant to be distributed: the VOMS client does not\n    need to contact the VOMS-Admin server.  However, OSG has historically used software\n    such as GUMS or \nedg-mkgridmap\n to cache a list of authorizations from the VOMS-Admin\n    interface, creating a dependency between VOMS client and VOMS-Admin.\n\n\n\n\nVOMS-Admin is a large, complex Java web application.  Over the last\nfew years, upstream support has tailed off - particularly as OSG has been unable\nto update to VOMS-Admin version 3.  As a result, the maintenance burden has largely\nfallen on the OSG Software team.\n\n\nGiven that VOMS-Admin is deeply tied to X509 security infrastructure - and is\nmaintenance-only from OSG Software - there is no path forward to eliminate the use\nof X509 certificates in the web browser, a high-priority goal\n\n\nIn discussions with the OSG community, we have found very few VOs utilize VOMS-Admin\nto manage their VO users.  Instead, the majority use VOMS-Admin to whitelist a pilot\ncertificate: this can be done without a VOMS-Admin endpoint.\n\n\nOSG's plans to retire VOMS-Admin has three major components:\n\n\n\n\n(Sites) Enable distributed validation of VOMS extensions in the VOMS client.\n\n\n(VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing\n  of VOMS proxies.\n\n\n(VOs) Migrate remaining VOs to a central \ncomanage\n instance for managing user\n  authorizations; maintain a plugin to enable direct callouts from VOMS-Server\n  to \ncomanage\n for authorization lookups.\n\n\n\n\nSite Transition Plans\n\n\nWe will release a configuration of the LCMAPS authorization framework that performs\ndistributed verification of VOMS extensions; this verification eludes the need to\ncontact the VOMS-Admin interface for a list of authorizations.\n\n\nIn 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when\nLCMAPS asserts the validation was performed.  Hence, when GUMS sites update clients to the\nlatest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer\nneeded. \nNote\n: In parallel to the VOMS-Admin transition, OSG Software plans to \nretire GUMS\n.\nThere is no need to complete one transition before the other.\n\n\nSites using \nedg-mkgridmap\n will need to use its replacement, \nlcmaps-plugins-voms\n (this\nprocess is documented \nhere\n).\n\n\nVO Transition Plans\n\n\nBased on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain\na list of authorized pilots.  For these VOs, we will help convert invocations of \nvoms-proxy-init\n:\n\n\nvoms-proxy-init -voms hcc:/hcc/Role=pilot\n\n\n\n\n\nto an equivalent call to \nvoms-proxy-fake\n:\n\n\nvoms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\\n                -hostkey /etc/grid-security/voms/vomskey.pem \\\n                -fqan /hcc/Role=pilot/Capability=NULL \\\n                -voms hcc -uri hcc-voms.unl.edu:15000\n\n\n\n\n\nThe latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate\ncurrently on the VOMS-Admin server to be kept on the frontend host.  The frontend's account may also need access\nto the certificate.\n\n\nWe plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to \ncomanage\n.  It is\nnot clear there are any such VOs that need support from OSG.  If there are, a hosted version of \ncomanage\n is expected\nto be available in summer 2017 from the CILogon 2.0 project.  If you feel your VO is affected, please contact the\nOSG and we will build a custom timeline.  If there are no such VOs, we will not need to adopt \ncomanage\n for this\nuse case (other uses of \ncomanage\n are expected to proceed regardless).\n\n\nTimeline\n\n\n\n\nApril 2017 (completed): \nlcmaps-plugins-voms\n shipped and supported by OSG.\n\n\nMay 2017 (completed): \nosg-configure\n and documentation necessary for using \nlcmaps-plugins-voms\n is shipped.\n\n\nJune 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, \nedg-mkgridmap\n, or GUMS.  Sites begin transition\n  to validating VOMS extensions.\n\n\nSummer 2017 (completed): As necessary, VOs are given access to a hosted \ncomanage\n instance.\n\n\nMarch 2017: First VOs begin to retire VOMS-Admin.\n\n\nMay 2018: Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.", 
            "title": "VOMS Admin Retirement"
        }, 
        {
            "location": "/policy/voms-admin-retire/#voms-admin-retirement", 
            "text": "", 
            "title": "VOMS-Admin Retirement"
        }, 
        {
            "location": "/policy/voms-admin-retire/#introduction", 
            "text": "This document provides an overview of the planned retirement of support for VOMS-Admin\nin the OSG Software Stack.  Support for the VOMS infrastructure has three major components:   VOMS-Admin : A web interface for maintaining the list of authorized users in\n    a VO and their various authorizations (group membership, roles, attributes, etc).  VOMS-Server : A TCP service which signs a cryptographic extension on an X509\n    proxy certificate asserting the authorizations available to the authenticated user.  VOMS Client : Software for extracting and validating the signed VOMS extension from\n    an X509 proxy.  The validation is meant to be distributed: the VOMS client does not\n    need to contact the VOMS-Admin server.  However, OSG has historically used software\n    such as GUMS or  edg-mkgridmap  to cache a list of authorizations from the VOMS-Admin\n    interface, creating a dependency between VOMS client and VOMS-Admin.   VOMS-Admin is a large, complex Java web application.  Over the last\nfew years, upstream support has tailed off - particularly as OSG has been unable\nto update to VOMS-Admin version 3.  As a result, the maintenance burden has largely\nfallen on the OSG Software team.  Given that VOMS-Admin is deeply tied to X509 security infrastructure - and is\nmaintenance-only from OSG Software - there is no path forward to eliminate the use\nof X509 certificates in the web browser, a high-priority goal  In discussions with the OSG community, we have found very few VOs utilize VOMS-Admin\nto manage their VO users.  Instead, the majority use VOMS-Admin to whitelist a pilot\ncertificate: this can be done without a VOMS-Admin endpoint.  OSG's plans to retire VOMS-Admin has three major components:   (Sites) Enable distributed validation of VOMS extensions in the VOMS client.  (VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing\n  of VOMS proxies.  (VOs) Migrate remaining VOs to a central  comanage  instance for managing user\n  authorizations; maintain a plugin to enable direct callouts from VOMS-Server\n  to  comanage  for authorization lookups.", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/voms-admin-retire/#site-transition-plans", 
            "text": "We will release a configuration of the LCMAPS authorization framework that performs\ndistributed verification of VOMS extensions; this verification eludes the need to\ncontact the VOMS-Admin interface for a list of authorizations.  In 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when\nLCMAPS asserts the validation was performed.  Hence, when GUMS sites update clients to the\nlatest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer\nneeded.  Note : In parallel to the VOMS-Admin transition, OSG Software plans to  retire GUMS .\nThere is no need to complete one transition before the other.  Sites using  edg-mkgridmap  will need to use its replacement,  lcmaps-plugins-voms  (this\nprocess is documented  here ).", 
            "title": "Site Transition Plans"
        }, 
        {
            "location": "/policy/voms-admin-retire/#vo-transition-plans", 
            "text": "Based on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain\na list of authorized pilots.  For these VOs, we will help convert invocations of  voms-proxy-init :  voms-proxy-init -voms hcc:/hcc/Role=pilot  to an equivalent call to  voms-proxy-fake :  voms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\\n                -hostkey /etc/grid-security/voms/vomskey.pem \\\n                -fqan /hcc/Role=pilot/Capability=NULL \\\n                -voms hcc -uri hcc-voms.unl.edu:15000  The latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate\ncurrently on the VOMS-Admin server to be kept on the frontend host.  The frontend's account may also need access\nto the certificate.  We plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to  comanage .  It is\nnot clear there are any such VOs that need support from OSG.  If there are, a hosted version of  comanage  is expected\nto be available in summer 2017 from the CILogon 2.0 project.  If you feel your VO is affected, please contact the\nOSG and we will build a custom timeline.  If there are no such VOs, we will not need to adopt  comanage  for this\nuse case (other uses of  comanage  are expected to proceed regardless).", 
            "title": "VO Transition Plans"
        }, 
        {
            "location": "/policy/voms-admin-retire/#timeline", 
            "text": "April 2017 (completed):  lcmaps-plugins-voms  shipped and supported by OSG.  May 2017 (completed):  osg-configure  and documentation necessary for using  lcmaps-plugins-voms  is shipped.  June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin,  edg-mkgridmap , or GUMS.  Sites begin transition\n  to validating VOMS extensions.  Summer 2017 (completed): As necessary, VOs are given access to a hosted  comanage  instance.  March 2017: First VOs begin to retire VOMS-Admin.  May 2018: Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.", 
            "title": "Timeline"
        }, 
        {
            "location": "/policy/release-series/", 
            "text": "OSG Software Release Series Support Policy\n\n\nThis document describes the OSG policy for managing its software releases. Use this policy to help plan when to perform major OSG software updates at your site.\n\n\nOSG software releases are organized into \nrelease series\n, with the intent that software updates within a series do not take long to perform, cause significant downtime, or break dependent software. New series can be more disruptive, allowing OSG to add, substantially change, and remove software components. Releases are assigned versions, such as \"OSG 3.2.1\", where the first two numbers designate the release series and the third number increments within the series. Changes to the first number are infrequent and indicate sweeping changes to the way in which OSG software is distributed.\n\n\nOSG supports at most two concurrent release series, \ncurrent\n and \nprevious\n, where the goal is to begin a new release series about every 18 months. Once a new series starts, OSG will support the previous series for at least 12 months and will announce its end-of-life date at least 6 months in advance. During the first 6 months of a series, OSG will endeavor to apply backward-compatible changes to the previous series as well; afterward, OSG will apply only critical bug and security fixes. When support ends for a release series, it means that OSG no longer updates the software, fixes issues, or troubleshoots installations for releases within the series. The plan is to maintain interoperability between supported series, but there is no guarantee that unsupported series will continue to function in OSG.\n\n\nOSG Operations will handle deviations from this policy, in consultation with OSG Technology and stakeholders.\n\n\nLife-cycle Dates\n\n\n\n\n\n\n\n\nRelease Series\n\n\nInitial Release\n\n\nEnd of Regular Support\n\n\nEnd of Critical Bug/Security Support\n\n\n\n\n\n\n\n\n\n\n3.4\n\n\nJune 2017\n\n\nNot set\n\n\nNot set\n\n\n\n\n\n\n3.3\n\n\nAugust 2015\n\n\nDecember 2017\n\n\nMay 2018\n\n\n\n\n\n\n3.2\n\n\nNovember 2013\n\n\nFebruary 2016\n\n\nAugust 2016\n\n\n\n\n\n\n3.1\n\n\nApril 2012\n\n\nOctober 2014\n\n\nApril 2015", 
            "title": "Release Series Support"
        }, 
        {
            "location": "/policy/release-series/#osg-software-release-series-support-policy", 
            "text": "This document describes the OSG policy for managing its software releases. Use this policy to help plan when to perform major OSG software updates at your site.  OSG software releases are organized into  release series , with the intent that software updates within a series do not take long to perform, cause significant downtime, or break dependent software. New series can be more disruptive, allowing OSG to add, substantially change, and remove software components. Releases are assigned versions, such as \"OSG 3.2.1\", where the first two numbers designate the release series and the third number increments within the series. Changes to the first number are infrequent and indicate sweeping changes to the way in which OSG software is distributed.  OSG supports at most two concurrent release series,  current  and  previous , where the goal is to begin a new release series about every 18 months. Once a new series starts, OSG will support the previous series for at least 12 months and will announce its end-of-life date at least 6 months in advance. During the first 6 months of a series, OSG will endeavor to apply backward-compatible changes to the previous series as well; afterward, OSG will apply only critical bug and security fixes. When support ends for a release series, it means that OSG no longer updates the software, fixes issues, or troubleshoots installations for releases within the series. The plan is to maintain interoperability between supported series, but there is no guarantee that unsupported series will continue to function in OSG.  OSG Operations will handle deviations from this policy, in consultation with OSG Technology and stakeholders.", 
            "title": "OSG Software Release Series Support Policy"
        }, 
        {
            "location": "/policy/release-series/#life-cycle-dates", 
            "text": "Release Series  Initial Release  End of Regular Support  End of Critical Bug/Security Support      3.4  June 2017  Not set  Not set    3.3  August 2015  December 2017  May 2018    3.2  November 2013  February 2016  August 2016    3.1  April 2012  October 2014  April 2015", 
            "title": "Life-cycle Dates"
        }, 
        {
            "location": "/policy/globus-toolkit/", 
            "text": "OSG Support of the Globus Toolkit\n\n\n6 June 2017\n\n\nMany in the OSG community have heard the news about the \nend of support for the open-source Globus Toolkit\n.\n\n\nWhat does this imply for the OSG Software stack?  Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period.\n\n\nNote the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology.  During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software.  The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software. \n\n\nThis model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception.  Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus. \n\n\nOver the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI.  \n\n\nPlease reach out to \ngoc@opensciencegrid.org\n with your questions, comments, and concerns.\n\n\nA copy of this statement can be found at \nhttps://opensciencegrid.github.io/technology/policy/globus-toolkit\n.", 
            "title": "Globus Toolkit Support"
        }, 
        {
            "location": "/policy/globus-toolkit/#osg-support-of-the-globus-toolkit", 
            "text": "6 June 2017  Many in the OSG community have heard the news about the  end of support for the open-source Globus Toolkit .  What does this imply for the OSG Software stack?  Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period.  Note the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology.  During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software.  The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software.   This model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception.  Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus.   Over the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI.    Please reach out to  goc@opensciencegrid.org  with your questions, comments, and concerns.  A copy of this statement can be found at  https://opensciencegrid.github.io/technology/policy/globus-toolkit .", 
            "title": "OSG Support of the Globus Toolkit"
        }, 
        {
            "location": "/policy/external-oasis-repos/", 
            "text": "Policy for OSG Mirroring of External CVMFS repositories\n\n\n12 October 2017\n\n\nThis document provides an overview of the policies and security understanding with regards to OSG mirroring of CVMFS\nrepositories of external organizations.  It aims to help external repositories and OSG VOs understand what OSG is\nattempting to achieve with the mirroring service.\nThis is not a service-level agreement but rather a statement of responsibilities.\n\n\n\n\nNote\n\n\nTo actually understand the technical procedure for mirroring a repository, \nsee the following page\n.  This document solely covers the policy aspects.\n\n\n\n\nIntroduction\n\n\nThe OSG provides a network of CVMFS Stratum-1 servers for mirroring content of externally-managed repositories.  These repositories are often\nhosted by large HEP or physics VOs for the purpose of distributing software for high-throughput computing jobs.  Additionally, OSG provides a\nrepository (\n) for smaller VOs; this is not covered here.\n\n\nOSG will include additional repositories into the content distribution network (CDN) at the request of an OSG-affiliated VO.  These repositories\nare meant to help the OSG-affiliated VO accomplish their domain science.\n\n\nThe goal of this mirroring provides an improved quality-of-service for the VO end-users running at OSG sites.  OSG does not provide support\nfor use of the software in external repositories, but will help end-users contact the VO for help as necessary.\n\n\nOSG Responsibilities\n\n\n\n\nOSG will provide the Stratum-1 server network according to \nthe OASIS SLA\n\n\nOSG will provide a best-effort mirror of the full contents of the external repo.  We will attempt to provide best-effort integrity of the\n  object contents, but assume users of the Stratum-1 will do further integrity checking.  No SLA is provided covering potential data corruptions.\n\n\nOSG will provide best-effort notification to the mirrored repository in case OSG detects a service outage of the external repo.\n\n\nIn the event of a security incident, the operations group will replace the compromised repository with an empty directory, signed by\nthe key managed by them. This will be done in consultation with the security team or, in the unlikely event they cannot be reached, at the discretion of the Operations Coordinator.\n\n\nOnce the external repository is approved, OSG will distribute the corresponding repository signing keys in a valid whitelist.  The whitelist\n  will be signed by the OSG Stratum-0.  This whitelist attests to the authenticity of the key, but not a statement about repository contents.\n\n\n\n\nVO Responsibilities\n\n\n\n\nThe individual responsible on behalf of the VO will be registered with the OASIS Manager role in OIM.\n\n\nThe requesting VO should only include targeted repositories they need to support their science.\n\n\nThe VO should understand that in the event of a reported security incident, the contents of this repository may be replaced with an\n  empty directory and signed by the OSG repository key.  Depending on the OSG Security team's evaluation of the severity and urgency\n  of the incident, the blanking may be done immediately without VO notification or after some notification period.\n\n\nIn the case of a security incident, the VO and OSG Security team will need to mutually agree that the incident is resolved before the\n  repository is unblanked.\n\n\nThe VO is ultimately responsible for the contents of the repository.  OSG provides a mirror.\n\n\nIf the external repository is \nnot\n operated by the VO, OSG may work directly with the external repository maintainers.  This is done for\n  ease of operations and may be limited to day-to-day, non-security-related support.\n\n\n\n\nOperational Policies\n\n\nTo help us provide the best operational setup possible, we have a few additional replication policies:\n\n\n\n\nOSG Operations only hosts the shared \noasis.opensciencegrid.org\n repository; VO-dedicated software respositories\n    (such as \nnova.opensciencegrid.org\n for the NoVA VO) should be operated by the VO.\n\n\nVOs are asked to either run their own repository or utilize the shared repository, but not both.\n\n\nThere is a finite amount of high-performance storage on the CDN.   A minimum of 100 GB per repository is guaranteed.\n    Larger limits may be requested.\n\n\nVOs may ask the OSG to replicate their repositories to the European Grid Infrastructure (EGI); however, this can\n    only be done if the repository name ends in \n.opensciencegrid.org\n.", 
            "title": "OASIS Repository Mirroring"
        }, 
        {
            "location": "/policy/external-oasis-repos/#policy-for-osg-mirroring-of-external-cvmfs-repositories", 
            "text": "12 October 2017  This document provides an overview of the policies and security understanding with regards to OSG mirroring of CVMFS\nrepositories of external organizations.  It aims to help external repositories and OSG VOs understand what OSG is\nattempting to achieve with the mirroring service.\nThis is not a service-level agreement but rather a statement of responsibilities.   Note  To actually understand the technical procedure for mirroring a repository,  see the following page .  This document solely covers the policy aspects.", 
            "title": "Policy for OSG Mirroring of External CVMFS repositories"
        }, 
        {
            "location": "/policy/external-oasis-repos/#introduction", 
            "text": "The OSG provides a network of CVMFS Stratum-1 servers for mirroring content of externally-managed repositories.  These repositories are often\nhosted by large HEP or physics VOs for the purpose of distributing software for high-throughput computing jobs.  Additionally, OSG provides a\nrepository ( ) for smaller VOs; this is not covered here.  OSG will include additional repositories into the content distribution network (CDN) at the request of an OSG-affiliated VO.  These repositories\nare meant to help the OSG-affiliated VO accomplish their domain science.  The goal of this mirroring provides an improved quality-of-service for the VO end-users running at OSG sites.  OSG does not provide support\nfor use of the software in external repositories, but will help end-users contact the VO for help as necessary.", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/external-oasis-repos/#osg-responsibilities", 
            "text": "OSG will provide the Stratum-1 server network according to  the OASIS SLA  OSG will provide a best-effort mirror of the full contents of the external repo.  We will attempt to provide best-effort integrity of the\n  object contents, but assume users of the Stratum-1 will do further integrity checking.  No SLA is provided covering potential data corruptions.  OSG will provide best-effort notification to the mirrored repository in case OSG detects a service outage of the external repo.  In the event of a security incident, the operations group will replace the compromised repository with an empty directory, signed by\nthe key managed by them. This will be done in consultation with the security team or, in the unlikely event they cannot be reached, at the discretion of the Operations Coordinator.  Once the external repository is approved, OSG will distribute the corresponding repository signing keys in a valid whitelist.  The whitelist\n  will be signed by the OSG Stratum-0.  This whitelist attests to the authenticity of the key, but not a statement about repository contents.", 
            "title": "OSG Responsibilities"
        }, 
        {
            "location": "/policy/external-oasis-repos/#vo-responsibilities", 
            "text": "The individual responsible on behalf of the VO will be registered with the OASIS Manager role in OIM.  The requesting VO should only include targeted repositories they need to support their science.  The VO should understand that in the event of a reported security incident, the contents of this repository may be replaced with an\n  empty directory and signed by the OSG repository key.  Depending on the OSG Security team's evaluation of the severity and urgency\n  of the incident, the blanking may be done immediately without VO notification or after some notification period.  In the case of a security incident, the VO and OSG Security team will need to mutually agree that the incident is resolved before the\n  repository is unblanked.  The VO is ultimately responsible for the contents of the repository.  OSG provides a mirror.  If the external repository is  not  operated by the VO, OSG may work directly with the external repository maintainers.  This is done for\n  ease of operations and may be limited to day-to-day, non-security-related support.", 
            "title": "VO Responsibilities"
        }, 
        {
            "location": "/policy/external-oasis-repos/#operational-policies", 
            "text": "To help us provide the best operational setup possible, we have a few additional replication policies:   OSG Operations only hosts the shared  oasis.opensciencegrid.org  repository; VO-dedicated software respositories\n    (such as  nova.opensciencegrid.org  for the NoVA VO) should be operated by the VO.  VOs are asked to either run their own repository or utilize the shared repository, but not both.  There is a finite amount of high-performance storage on the CDN.   A minimum of 100 GB per repository is guaranteed.\n    Larger limits may be requested.  VOs may ask the OSG to replicate their repositories to the European Grid Infrastructure (EGI); however, this can\n    only be done if the repository name ends in  .opensciencegrid.org .", 
            "title": "Operational Policies"
        }, 
        {
            "location": "/policy/flexible-release-model/", 
            "text": "OSG Software Flexible Release Model\n\n\nIntroduction\n\n\nBefore November 2017, the OSG software stack was released on the second Tuesday of each month, except in the case of urgent releases, which were infrequent. This schedule had been in place since early 2013. Since then, conditions within and outside of the Software team have changed, and we have adjusted the release schedule and associated processes.\n\n\nThe previous release model had the recurring problem of a \"release crunch,\" where it was difficult to find the effort required to test large changes before their deadline had passed. Sometimes the lack of timely effort led to software being pushed to the next release (a month later), because there was insufficient testing time.\n\n\nBased on software support tickets, we noticed that many sites follow a local update schedule that is independent of the OSG Release team schedule; some sites upgrade every few months, skipping interim releases, other sites upgrade individual packages as needed. In addition, upstream software developers do not follow our release schedule either, releasing software on their own development timelines. As a result, some site administrators would prefer to have OSG software updates more often, closer to when they become available, rather than tied to a monthly cycle.\n\n\nFor these reasons, we created a new release model.\n\n\nRelease Model\n\n\nThe OSG Release team releases batches of integrated, tested software on an ad hoc basis, with the process outlined below (changes from the old process are highlighted):\n\n\n\n\n\n\nSoftware and Release Team members develop packages and mark them for testing\n\n\n\n\n\n\nSoftware and Release Team members test the packages, possibly with help from the community\n\n\n\n\n\n\nOnce adequate testing is complete and successful, the Release Manager approves packages for release\n\n\n\n\n\n\nWeekly, the Release Manager evaluates packages that are ready for release; when a sufficient number of important packages are ready\n[1]\n, the Release Manager schedules a release date and announces it. For urgent changes, the Release Manager evaluates the packages as soon as they are tested\n\n\n\n\n\n\nThe Software and Release Team performs pre-release testing, releases the packages, and announces the release\n\n\n\n\n\n\nNote: The release dates of parallel release series (e.g., 3.3 and 3.4) do not have to coincide, as they have in the past.\n\n\n[1] The threshold for \u201csufficient number of important packages\u201d is determined by the Release Manager, with input from the other Technology Area leaders.", 
            "title": "Flexible Release Model"
        }, 
        {
            "location": "/policy/flexible-release-model/#osg-software-flexible-release-model", 
            "text": "", 
            "title": "OSG Software Flexible Release Model"
        }, 
        {
            "location": "/policy/flexible-release-model/#introduction", 
            "text": "Before November 2017, the OSG software stack was released on the second Tuesday of each month, except in the case of urgent releases, which were infrequent. This schedule had been in place since early 2013. Since then, conditions within and outside of the Software team have changed, and we have adjusted the release schedule and associated processes.  The previous release model had the recurring problem of a \"release crunch,\" where it was difficult to find the effort required to test large changes before their deadline had passed. Sometimes the lack of timely effort led to software being pushed to the next release (a month later), because there was insufficient testing time.  Based on software support tickets, we noticed that many sites follow a local update schedule that is independent of the OSG Release team schedule; some sites upgrade every few months, skipping interim releases, other sites upgrade individual packages as needed. In addition, upstream software developers do not follow our release schedule either, releasing software on their own development timelines. As a result, some site administrators would prefer to have OSG software updates more often, closer to when they become available, rather than tied to a monthly cycle.  For these reasons, we created a new release model.", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/flexible-release-model/#release-model", 
            "text": "The OSG Release team releases batches of integrated, tested software on an ad hoc basis, with the process outlined below (changes from the old process are highlighted):    Software and Release Team members develop packages and mark them for testing    Software and Release Team members test the packages, possibly with help from the community    Once adequate testing is complete and successful, the Release Manager approves packages for release    Weekly, the Release Manager evaluates packages that are ready for release; when a sufficient number of important packages are ready [1] , the Release Manager schedules a release date and announces it. For urgent changes, the Release Manager evaluates the packages as soon as they are tested    The Software and Release Team performs pre-release testing, releases the packages, and announces the release    Note: The release dates of parallel release series (e.g., 3.3 and 3.4) do not have to coincide, as they have in the past.  [1] The threshold for \u201csufficient number of important packages\u201d is determined by the Release Manager, with input from the other Technology Area leaders.", 
            "title": "Release Model"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/", 
            "text": "How to Restore Koji\n\n\nThis document contains recipes on how to restore the Koji services and the database they require. It is divided into two sections: one for the database (to be done if something happens to \ndb-01\n), and one for the server hosting the koji services (to be done if something happens to \nkoji.chtc\n).\n\n\nIn case both the database and the hub need to be restored, the database should be restored first.\n\n\nBackground information\n\n\nBackups of \nkoji.chtc.wisc.edu\n and \ndb-01.batlab.org\n are on \nhost-3.chtc.wisc.edu\n in \n/export/backup/\nDATE\n. That machine is in WID. (Same room as \nkoji.chtc\n itself, which is why we have offsite backups). It's a homebrew rsync-based backup system. (Not our home -- Nate told me it was written for Midwest Tier 2.) They go back up to a week, with a monthly snapshot for a year.\n\n\nSetting up your environment\n\n\nFor all of these steps, you will need a root shell on \nhost-3.chtc.wisc.edu\n and have the following environment variables defined:\n\n\nNEWDB=\nFQDN OF NEW DATABASE SERVER\n\n\nNEWKOJI=\nFQDN OF NEW KOJI HOST\n\n\nDATE=\nYYYY-MM-DD DATE OF MOST RECENT GOOD BACKUP\n\n\nDBBACKUP=/export/backup/$DATE/db-01.batlab.org\n\n\nKOJIBACKUP=/export/backup/$DATE/koji.chtc.wisc.edu\n\n\nRSYNC=\nrsync --archive --hard-links --verbose\n\n\n\n\n\n\nRestoring the database\n\n\nThe entire filesystem of \ndb-01\n is backed up -- this includes all of \n/var/lib/pgsql\n, including the database as-is. In theory, this means that we could just rsync all the files to a blank hard drive, boot up, and we'd have a \ndb-01\n again. However, the Postgres manual warns against restoring the database from a filesystem backup that was made while the database was live, and we do not shut down the database before backups.\n\n\nWe might be able to restore every other part of the filesystem besides the database, which would speed up the overall restoration process, but only the fresh install was tested.\n\n\nThe new database server is called \nnewdb\n in these instructions.\n\n\nRestoring Services\n\n\nPrerequisites for \nnewdb\n: an EL 6+ host with an SSH server set up and accessible (as root) from \nhost-3.chtc.wisc.edu\n\n\n#\n# On newdb:\n\n\n#\n# Install postgres, get a blank DB up and create the user that koji\n\n\n#\n# will be using.\n\n\n[root@newdb]#\n yum install -y postgresql-server\n\n[root@newdb]#\n service postgresql initdb\n\n[root@newdb]#\n useradd -r -m koji\n\n#\n# Make a directory we\nll put the restored files into.\n\n\n[root@newdb]#\n mkdir -p /root/dbrestore\n\n\n#\n# On host-3:\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$DBBACKUP\n/homefs/  \n$NEWDB\n:/root/dbrestore/home\n\n[you@host-3]$\n \nfor\n dir in root etc var\n;\n \ndo\n \n\\\n\n   sudo \n$RSYNC\n \n$DBBACKUP\n/rootfs/\n$dir\n/ \n\\\n\n      \n$NEWDB\n:/root/dbrestore/\n$dir\n \n\\\n\n   \ndone\n\n\n\n\n\n\nContinue on to the next section\n\n\nRestoring Database Contents\n\n\nAssumes you have restored the /var directory from backup into \n/root/dbrestore/var\n.\n\n\n\n\n\n\nRestore the postgres config files so the koji-hub daemon can log in:\n\n\n#\n# On newdb:\n\n\n[root@newdb]#\n service postgresql stop\n\n[root@newdb]#\n cp -a /root/dbrestore/var/lib/pgsql/data/\n{\n*.conf,postmaster.opts\n}\n \n\\\n\n    /var/lib/pgsql/data/\n\n\n\n\n\n\n\n\n\nEdit \n/var/lib/pgsql/data/pg_hba.conf\n. There are lines like:\n\n\n# Koji-hub IPv4:\nhost koji koji 128.104.100.41/32 md5\n\n\n\n\n\nChange the IP address to the public IP address of the host that will serve as the new hub.\n\n\n\n\n\n\nRestore the actual database:\n\n\n#\n# On newdb:\n\n\n[root@newdb]#\n chown -R postgres:postgres /var/lib/pgsql/*\n\n[root@newdb]#\n service postgresql start\n\n[root@newdb]#\n gunzip -c /root/dbrestore/var/lib/pgsql-backup/postgres-db-01.sql.gz \n|\n\n\n    psql -U postgres postgres\n\n\n\n\n\n\n\n\n\n\nValidation\n\n\nDo the following tests to make sure the database is ready to use:\n\n\n\n\n\n\nTest that the contents got properly restored:\n\n\n[root@newdb]#\n psql -U koji koji koji\n\n\n\n\n\n\n\n\nkoji=\n \nselect\n \n*\n \nfrom\n \nusers\n;\n\n\nkoji=\n \nselect\n \n*\n \nfrom\n \nbuild\n \norder\n \nby\n \nid\n \ndesc\n \nlimit\n \n10\n;\n\n\n\n\n\n\n\n\n\n\nTest logging in as the koji user:\n\n\n[root@newdb]#\n psql -U koji -h newdb koji\n\n\n\n\n\n(you must use the FQDN of \nnewdb\n, not \nlocalhost\n).\nBe sure you get prompted for a password, and the password from \n/etc/koji-hub/hub.conf\n works.\n\n\n\n\n\n\nContinue to \"Restoring Koji\" if needed, otherwise skip to \"Starting Services and Validation\"\n\n\n\n\n\n\nRestoring Koji\n\n\nBoth the root filesystem of \nkoji.chtc\n and \n/mnt/koji\n are backed up. The root filesystem backups are in the \nrootfs\n subdirectory of \n/export/backup/$DATE/koji.chtc.wisc.edu\n and the backups of \n/mnt/koji\n are in the \nkojifs\n subdirectory.\n\n\nThe following instructions show how to restore the critical components of Koji onto a new machine.\n\n\nIn the instructions, the new host will be named \nnewkoji\n.\n\n\nInstalling the OS\n\n\nPrerequisites for \nnewkoji\n: an EL 6 host with an SSH server set up and accessible (as root) from \nhost-3.chtc.wisc.edu\n \n\n(This recipe was tested for EL 6, on the same machine as \nnewdb\n).\n\n\n\n\n\n\nInstall EPEL and OSG repos:\n\n\n[root@newkoji]#\n rpm -Uvh \n\\\n\n    https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm \n\\\n\n    https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n[root@newkoji]#\n yum install -y yum-plugin-priorities\n\n\n\n\n\n\n\n\n\nEdit \n/etc/yum.repos.d/osg-*development.repo\n:\n\n\n\n\nEnable the development repo\n\n\nAdd \nincludepkg=koji*\n to the definition for the development repo\n\n\n\n\n\n\n\n\nGo through the other repo files and make sure that EPEL and OS priorities are worse than 98.\n    This means absent or numerically greater.\n    Especially look at \ncobbler-config.repo\n if it exists.\n\n\n\n\n\n\nInstall the koji packages and dependencies, making sure the koji packages themselves come from osg:\n\n\n[root@newkoji]#\n yum install koji koji-builder koji-hub koji-plugin-sign \n\\\n\n    koji-theme-fedora koji-utils koji-web mod_ssl postgresql\n\n\n\n\n\n\n\n\n\nMount \n/mnt/koji\n if necessary\n\n\n\n\n\n\nRestore the contents of the koji filesystem. On \nhost-3\n:\n\n\n#\n# At a minimum, you must restore the /mnt/koji/packages directory\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/packages/ \n$NEWKOJI\n:/mnt/koji/packages\n\n#\n# The other directories are optional, though it saves a lot of time to restore /mnt/koji/repos\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/repos/ \n$NEWKOJI\n:/mnt/koji/repos\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/work/ \n$NEWKOJI\n:/mnt/koji/work\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/scratch/ \n$NEWKOJI\n:/mnt/koji/scratch\n\n#\n# Any dirs you did not restore should be created.\n\n\n\n\n\n\n\n\n\n\nFix permissions if needed. On \nnewkoji\n:\n\n\n[root@newkoji]#\n chown -R apache:apache /mnt/koji/\n{\npackages,repos,work,scratch\n}\n\n\n[root@newkoji]#\n chmod \n0755\n /mnt/koji/\n{\npackages,repos,work,scratch\n}\n\n\n\n\n\n\n\n\n\n\nContinue on to the next section\n\n\n\n\n\n\nRestoring Configuration\n\n\nOn \nnewkoji\n, define the shell function \ndirclone\n, listed below:\n\n\ndirclone \n()\n \n{\n\n   \nsrcdir\n=\n$(\ndirname \n$1\n)\n/\n$(\nbasename \n$1\n)\n\n   \ndestdir\n=\n$(\ndirname \n$2\n)\n/\n$(\nbasename \n$2\n)\n\n   mkdir -p \n$(\ndirname \n$2\n)\n\n   rsync --archive --delete-after --acls --xattrs \n\\\n\n         --partial --partial-dir\n=\n.rsync-partial \n\\\n\n         \n$srcdir\n/\n \n$destdir\n\n\n}\n\n\n\n\n\n\n\n\n\n\nOn \nnewkoji\n:\n\n\n[root@newkoji]#\n mkdir -p /root/hubrestore\n\n\n\n\n\n\n\n\n\nOn \nhost-3\n:\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/rootfs/\n{\nroot,home,etc\n}\n \n$NEWKOJI\n:/root/hubrestore/\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/varfs/ \n$NEWKOJI\n:/root/hubrestore/var/\n\n\n\n\n\n\n\n\n\nOn \nnewkoji\n, install some utils we will need later:\n\n\n[root@newkoji]#\n yum install -y dos2unix vim-enhanced\n\n\n\n\n\n(vim-enhanced is used for vimdiff)\n\n\n\n\n\n\nOn \nnewkoji\n:\n\n\n#\n# Restore some of the directories in /etc:\n\n\n[root@newkoji]#\n \nwhile\n \nread\n subtree\n;\n \ndo\n\n\n    dirclone /root/hubrestore/etc/$subtree /etc/$subtree\n\n\ndone \n___END___\n\n\nhttpd\n\n\nkojid\n\n\nkoji-hub\n\n\nkojira\n\n\nkoji-sign-plugin\n\n\nkojiweb\n\n\nmock\n\n\npki/koji\n\n\npki/tls/certs\n\n\npki/tls/private\n\n\n___END___\n\n\n#\n# Restore some of the files:\n\n\n[root@newkoji]#\n cp -a /root/hubrestore/etc/koji.conf /etc/\n\n[root@newkoji]#\n cp -a /root/hubrestore/etc/sysconfig/\n{\nhttpd,kojid,kojira\n}\n /etc/sysconfig/\n\n\n\n\n\n\n\n\n\nRestore users and home directories\n\n\n\n\n\n\nIf \nnewkoji\n is on a separate host from \nnewdb\n, then just simply copy over the files:\n\n\n[root@newkoji]#\n dirclone /root/hubrestore/home /home\n\n[root@newkoji]#\n cp -a /root/hubrestore/etc/\n{\npasswd,shadow,group,gshadow\n}\n /etc\n\n\n\n\n\n\n\n\n\nIf \nnewkoji\n is on the same host as \nnewdb\n, then you will have to be more careful:\n\n\n#\n# Skip home directories for the special users\n\n\n[root@newkoji]#\n \nfor\n dir in /root/hubrestore/home/*\n;\n \ndo\n\n\n    bndir=$(basename \n$dir\n)\n\n\n    if [[ $bndir = koji \n $bndir = postgres ]]; then\n\n\n        dirclone \n$dir\n /home/\n$bndir\n\n\n    fi\n\n\ndone\n\n\n#\n# Now merge the passwd, group, shadow, and gshadow files in /etc.\n\n\n#\n# Make sure that your editor does not create backup files\n\n\n#\n# (\nset nobackup\n in vim), and that shadow and gshadow are owned by\n\n\n#\n# root and have 0400 permissions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnsure a 'koji' user exists\n\n\n\n\n\n\nFix dirs in \n/var\n:\n\n\n[root@newkoji]#\n rm -rf /var/lib/mock/*\n\n[root@newkoji]#\n chown root:mock /var/lib/mock\n\n[root@newkoji]#\n chmod \n2775\n /var/lib/mock\n\n\n\n\n\n\n\n\n\nRestore \n/var/www/html\n and \n/var/spool/cron\n \n\n    (TODO) \n/var\n should have been backed up, but in case it isn't, the following files need to exist in \n/var/www/html\n:\n\n\n\n\nA symlink \nmnt -\n /mnt\n\n\nA robots.txt with contents\nUser-agent: *\nDisallow: /\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixing Names\n\n\nThis section should be done if \nnewdb\n or \nnewkoji\n do not have the same as the previous db server and hub (i.e. \ndb-01.batlab.org\n and \nkoji.chtc.wisc.edu\n). This section should be completed on \nnewkoji\n.\n\n\nFixing config files if \nnewdb\n was renamed\n\n\nThe only change that's needed if \nnewdb\n was renamed is to \n/etc/koji-hub/hub.conf\n. Edit that file and change the DBHost line to point to the new hostname. After editing, make sure \nhub.conf\n is owned by \nroot:apache\n and chmodded 0640.\n\n\nInstalling new cert/key pairs for \nnewkoji\n\n\nYou will need a cert/key pair for the new hostname. Run \ndos2unix\n on all cert and key files before using them. Define the shell function \nmakepem\n, listed below. \nmakepem\n combines a public and private keypair to make a .pem file that the Koji services use.\n\n\nUsage: \nmakepem \nCERTFILE\n \nKEYFILE\n \nOUTPUT_FILE\n\n\nmakepem \n()\n \n{\n\n    \ncertfile\n=\n$1\n\n    \nkeyfile\n=\n$2\n\n    \noutputfile\n=\n$3\n\n    \n(\nset\n -e\n    \nkeymodulus\n=\n$(\nopenssl rsa -noout -modulus -in \n$keyfile\n)\n\n    \ncertmodulus\n=\n$(\nopenssl x509 -noout -modulus -in \n$certfile\n)\n\n    \nif\n \n[[\n \n$keymodulus\n \n=\n \n$certmodulus\n \n]]\n \n;\n \nthen\n\n        \necho\n \nkeyfile and certfile do not match\n;\n \nreturn\n \n1\n\n    \nfi\n\n    \nif\n \n[[\n -f \n$outputfile\n \n]]\n \n;\n \nthen\n\n        mv -f \n$outputfile\n{\n,.bak\n}\n\n    \nfi\n\n    \n(\ndos2unix \n \n$certfile\n;\n echo\n;\n dos2unix \n \n$keyfile\n)\n \n \n$outputfile\n\n    chmod \n0600\n \n$outputfile\n\n    \n)\n\n\n}\n\n\n\n\n\n\nPlace cert and key files into the following paths:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhost cert\n\n\n/root/hostcert.pem\n\n\n\n\n\n\nhost key\n\n\n/root/hostkey.pem\n\n\n\n\n\n\n\n\nThen use \nmakepem\n to combine the certs and put them in the proper locations.\n\n\n[root@newkoji]#\n makepem /root/hostcert.pem /root/hostkey.pem \n\\\n\n   /etc/pki/tls/private/kojiweb.pem\n\n[root@newkoji]#\n chown apache:apache /etc/pki/tls/private/kojiweb.pem\n\n\n\n\n\nIn addition, copy the host cert and key into the locations HTTPD expects it them.\n\n\n[root@newkoji]#\n cp -a /root/hostcert.pem /etc/pki/tls/certs/hostcert.pem\n\n[root@newkoji]#\n cp -a /root/hostkey.pem /etc/pki/tls/private/hostkey.pem\n\n\n\n\n\nFixing hostname in config files\n\n\nUse sed to replace the hostname in the following config files in /etc:\n\n\n\n\n/etc/kojira/kojira.conf\n\n\n/etc/koji.conf\n\n\n/etc/koji-hub/hub.conf\n\n\n/etc/httpd/conf.d/kojiweb.conf\n\n\n/etc/httpd/conf/httpd.conf\n\n\n/etc/kojid/kojid.conf\n\n\n\n\nYou will need to fix \n/etc/kojid/kojid.conf\n on all builder machines as well (e.g. \nkojibuilder2.chtc.wisc.edu\n).\n\n\nFixing hostname in database\n\n\nYou will need to find and fix entries that contain the hostname in the following tables:\n\n\n\n\nhost\n (should be 1 entry)\n\n\nusers\n (should be 2 entries, one for the host, and one for the kojira user)\n\n\n\n\nFixing hostname elsewhere\n\n\nThese steps are only necessary if you cannot get a DNS Canonical Name (CN) record such that \nkoji.chtc.wisc.edu\n resolves to \nnewkoji\n.\n\n\n\n\nUpdate the repo definitions in the \nosg-release\n package\n\n\nUpdate the mash script(s) at the GOC\n\n\nMail the software team and users that anyone using the \nminefield\n repos will need to update \nosg-release\n\n\nFix all the build machines to point to the new name\n\n\nFix the following files in \nosg-build\n and make a new release\n\n\ndata/osg-koji-home.conf\n\n\ndata/osg-koji-site.conf\n\n\nosgbuild/constants.py\n\n\nosgbuild/kojiinter.py\n\n\n\n\n\n\nMail people that they will need to update \nosg-build\n and rerun \nosg-koji setup\n\n\n\n\nStarting Services and Validation\n\n\nNow you will start up Koji services and verify that they function.\nPrerequisite: previous restore steps have been completed and \npostgresql\n is running on the database host.\n\n\nAll steps will be run on \nnewkoji\n.\n\n\n\n\n\n\nStart the main koji daemon:\n\n\n[root@newkoji]#\n service httpd start\n\n\n\n\n\n\n\n\n\nUse \nps\n to verify that it came up\n\n\n\n\nConnect to the web interface in your browser.\n    Make sure you can use https and you can log in.\n\n\nAs yourself, run the \nkoji\n command-line tool and make a few queries (e.g. list-tags)\n\n\n\n\nStart the koji build daemon:\n\n\n[root@newkoji]#\n service kojid start\n\n\n\n\n\n\n\n\n\nUse \nps\n to verify that it came up\n\n\n\n\n\n\nIf you did not restore the \n/mnt/koji/repos\n directory, you will now need to regenerate the build repos.\nUse \nkoji list-tags\n to get a list of tags and run \nkoji regen-repo\n on all of the ones with \n-build\n in the name.\nThis \nwill\n take several hours.\nYou will also need to regen the \n-development\n repos so that \nminefield\n works again. Keep an eye on the tasks in the web interface to make sure they are getting farmed out to the right hosts.\n\n\n\n\nTry a scratch build\n\n\n\n\nStart \nkojira\n:\n\n\n[root@newkoji]#\n service kojira start\n\n\n\n\n\n\n\n\n\nUse \nps\n to verify that it came up\n\n\n\n\nWait half a minute and use \nps\n to verify that \nkojid\n is still up; the two processes can kick each other off if they are both using the same certificate\n\n\nBump a package if needed and try a real, non-scratch build\n\n\nMake sure that kojira is regenerating the repos\n\n\n\n\nIf you have updated \nosg-release\n and/or \nosg-build\n, rebuild those packages now.", 
            "title": "Koji Restore Recipe"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#how-to-restore-koji", 
            "text": "This document contains recipes on how to restore the Koji services and the database they require. It is divided into two sections: one for the database (to be done if something happens to  db-01 ), and one for the server hosting the koji services (to be done if something happens to  koji.chtc ).  In case both the database and the hub need to be restored, the database should be restored first.", 
            "title": "How to Restore Koji"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#background-information", 
            "text": "Backups of  koji.chtc.wisc.edu  and  db-01.batlab.org  are on  host-3.chtc.wisc.edu  in  /export/backup/ DATE . That machine is in WID. (Same room as  koji.chtc  itself, which is why we have offsite backups). It's a homebrew rsync-based backup system. (Not our home -- Nate told me it was written for Midwest Tier 2.) They go back up to a week, with a monthly snapshot for a year.", 
            "title": "Background information"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#setting-up-your-environment", 
            "text": "For all of these steps, you will need a root shell on  host-3.chtc.wisc.edu  and have the following environment variables defined:  NEWDB= FQDN OF NEW DATABASE SERVER  NEWKOJI= FQDN OF NEW KOJI HOST  DATE= YYYY-MM-DD DATE OF MOST RECENT GOOD BACKUP  DBBACKUP=/export/backup/$DATE/db-01.batlab.org  KOJIBACKUP=/export/backup/$DATE/koji.chtc.wisc.edu  RSYNC= rsync --archive --hard-links --verbose", 
            "title": "Setting up your environment"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-the-database", 
            "text": "The entire filesystem of  db-01  is backed up -- this includes all of  /var/lib/pgsql , including the database as-is. In theory, this means that we could just rsync all the files to a blank hard drive, boot up, and we'd have a  db-01  again. However, the Postgres manual warns against restoring the database from a filesystem backup that was made while the database was live, and we do not shut down the database before backups.  We might be able to restore every other part of the filesystem besides the database, which would speed up the overall restoration process, but only the fresh install was tested.  The new database server is called  newdb  in these instructions.", 
            "title": "Restoring the database"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-services", 
            "text": "Prerequisites for  newdb : an EL 6+ host with an SSH server set up and accessible (as root) from  host-3.chtc.wisc.edu  # # On newdb:  # # Install postgres, get a blank DB up and create the user that koji  # # will be using.  [root@newdb]#  yum install -y postgresql-server [root@newdb]#  service postgresql initdb [root@newdb]#  useradd -r -m koji # # Make a directory we ll put the restored files into.  [root@newdb]#  mkdir -p /root/dbrestore # # On host-3:  [you@host-3]$  sudo  $RSYNC   $DBBACKUP /homefs/   $NEWDB :/root/dbrestore/home [you@host-3]$   for  dir in root etc var ;   do   \\ \n   sudo  $RSYNC   $DBBACKUP /rootfs/ $dir /  \\ \n       $NEWDB :/root/dbrestore/ $dir   \\ \n    done   Continue on to the next section", 
            "title": "Restoring Services"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-database-contents", 
            "text": "Assumes you have restored the /var directory from backup into  /root/dbrestore/var .    Restore the postgres config files so the koji-hub daemon can log in:  # # On newdb:  [root@newdb]#  service postgresql stop [root@newdb]#  cp -a /root/dbrestore/var/lib/pgsql/data/ { *.conf,postmaster.opts }   \\ \n    /var/lib/pgsql/data/    Edit  /var/lib/pgsql/data/pg_hba.conf . There are lines like:  # Koji-hub IPv4:\nhost koji koji 128.104.100.41/32 md5  Change the IP address to the public IP address of the host that will serve as the new hub.    Restore the actual database:  # # On newdb:  [root@newdb]#  chown -R postgres:postgres /var/lib/pgsql/* [root@newdb]#  service postgresql start [root@newdb]#  gunzip -c /root/dbrestore/var/lib/pgsql-backup/postgres-db-01.sql.gz  |      psql -U postgres postgres", 
            "title": "Restoring Database Contents"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#validation", 
            "text": "Do the following tests to make sure the database is ready to use:    Test that the contents got properly restored:  [root@newdb]#  psql -U koji koji koji   koji=   select   *   from   users ;  koji=   select   *   from   build   order   by   id   desc   limit   10 ;     Test logging in as the koji user:  [root@newdb]#  psql -U koji -h newdb koji  (you must use the FQDN of  newdb , not  localhost ).\nBe sure you get prompted for a password, and the password from  /etc/koji-hub/hub.conf  works.    Continue to \"Restoring Koji\" if needed, otherwise skip to \"Starting Services and Validation\"", 
            "title": "Validation"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-koji", 
            "text": "Both the root filesystem of  koji.chtc  and  /mnt/koji  are backed up. The root filesystem backups are in the  rootfs  subdirectory of  /export/backup/$DATE/koji.chtc.wisc.edu  and the backups of  /mnt/koji  are in the  kojifs  subdirectory.  The following instructions show how to restore the critical components of Koji onto a new machine.  In the instructions, the new host will be named  newkoji .", 
            "title": "Restoring Koji"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#installing-the-os", 
            "text": "Prerequisites for  newkoji : an EL 6 host with an SSH server set up and accessible (as root) from  host-3.chtc.wisc.edu   \n(This recipe was tested for EL 6, on the same machine as  newdb ).    Install EPEL and OSG repos:  [root@newkoji]#  rpm -Uvh  \\ \n    https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm  \\ \n    https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm [root@newkoji]#  yum install -y yum-plugin-priorities    Edit  /etc/yum.repos.d/osg-*development.repo :   Enable the development repo  Add  includepkg=koji*  to the definition for the development repo     Go through the other repo files and make sure that EPEL and OS priorities are worse than 98.\n    This means absent or numerically greater.\n    Especially look at  cobbler-config.repo  if it exists.    Install the koji packages and dependencies, making sure the koji packages themselves come from osg:  [root@newkoji]#  yum install koji koji-builder koji-hub koji-plugin-sign  \\ \n    koji-theme-fedora koji-utils koji-web mod_ssl postgresql    Mount  /mnt/koji  if necessary    Restore the contents of the koji filesystem. On  host-3 :  # # At a minimum, you must restore the /mnt/koji/packages directory  [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/packages/  $NEWKOJI :/mnt/koji/packages # # The other directories are optional, though it saves a lot of time to restore /mnt/koji/repos  [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/repos/  $NEWKOJI :/mnt/koji/repos [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/work/  $NEWKOJI :/mnt/koji/work [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/scratch/  $NEWKOJI :/mnt/koji/scratch # # Any dirs you did not restore should be created.     Fix permissions if needed. On  newkoji :  [root@newkoji]#  chown -R apache:apache /mnt/koji/ { packages,repos,work,scratch }  [root@newkoji]#  chmod  0755  /mnt/koji/ { packages,repos,work,scratch }     Continue on to the next section", 
            "title": "Installing the OS"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-configuration", 
            "text": "On  newkoji , define the shell function  dirclone , listed below:  dirclone  ()   { \n    srcdir = $( dirname  $1 ) / $( basename  $1 ) \n    destdir = $( dirname  $2 ) / $( basename  $2 ) \n   mkdir -p  $( dirname  $2 ) \n   rsync --archive --delete-after --acls --xattrs  \\ \n         --partial --partial-dir = .rsync-partial  \\ \n          $srcdir /   $destdir  }     On  newkoji :  [root@newkoji]#  mkdir -p /root/hubrestore    On  host-3 :  [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /rootfs/ { root,home,etc }   $NEWKOJI :/root/hubrestore/ [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /varfs/  $NEWKOJI :/root/hubrestore/var/    On  newkoji , install some utils we will need later:  [root@newkoji]#  yum install -y dos2unix vim-enhanced  (vim-enhanced is used for vimdiff)    On  newkoji :  # # Restore some of the directories in /etc:  [root@newkoji]#   while   read  subtree ;   do      dirclone /root/hubrestore/etc/$subtree /etc/$subtree  done  ___END___  httpd  kojid  koji-hub  kojira  koji-sign-plugin  kojiweb  mock  pki/koji  pki/tls/certs  pki/tls/private  ___END___  # # Restore some of the files:  [root@newkoji]#  cp -a /root/hubrestore/etc/koji.conf /etc/ [root@newkoji]#  cp -a /root/hubrestore/etc/sysconfig/ { httpd,kojid,kojira }  /etc/sysconfig/    Restore users and home directories    If  newkoji  is on a separate host from  newdb , then just simply copy over the files:  [root@newkoji]#  dirclone /root/hubrestore/home /home [root@newkoji]#  cp -a /root/hubrestore/etc/ { passwd,shadow,group,gshadow }  /etc    If  newkoji  is on the same host as  newdb , then you will have to be more careful:  # # Skip home directories for the special users  [root@newkoji]#   for  dir in /root/hubrestore/home/* ;   do      bndir=$(basename  $dir )      if [[ $bndir = koji   $bndir = postgres ]]; then          dirclone  $dir  /home/ $bndir      fi  done  # # Now merge the passwd, group, shadow, and gshadow files in /etc.  # # Make sure that your editor does not create backup files  # # ( set nobackup  in vim), and that shadow and gshadow are owned by  # # root and have 0400 permissions.       Ensure a 'koji' user exists    Fix dirs in  /var :  [root@newkoji]#  rm -rf /var/lib/mock/* [root@newkoji]#  chown root:mock /var/lib/mock [root@newkoji]#  chmod  2775  /var/lib/mock    Restore  /var/www/html  and  /var/spool/cron   \n    (TODO)  /var  should have been backed up, but in case it isn't, the following files need to exist in  /var/www/html :   A symlink  mnt -  /mnt  A robots.txt with contents User-agent: *\nDisallow: /", 
            "title": "Restoring Configuration"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-names", 
            "text": "This section should be done if  newdb  or  newkoji  do not have the same as the previous db server and hub (i.e.  db-01.batlab.org  and  koji.chtc.wisc.edu ). This section should be completed on  newkoji .", 
            "title": "Fixing Names"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-config-files-if-newdb-was-renamed", 
            "text": "The only change that's needed if  newdb  was renamed is to  /etc/koji-hub/hub.conf . Edit that file and change the DBHost line to point to the new hostname. After editing, make sure  hub.conf  is owned by  root:apache  and chmodded 0640.", 
            "title": "Fixing config files if newdb was renamed"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#installing-new-certkey-pairs-for-newkoji", 
            "text": "You will need a cert/key pair for the new hostname. Run  dos2unix  on all cert and key files before using them. Define the shell function  makepem , listed below.  makepem  combines a public and private keypair to make a .pem file that the Koji services use.  Usage:  makepem  CERTFILE   KEYFILE   OUTPUT_FILE  makepem  ()   { \n     certfile = $1 \n     keyfile = $2 \n     outputfile = $3 \n     ( set  -e\n     keymodulus = $( openssl rsa -noout -modulus -in  $keyfile ) \n     certmodulus = $( openssl x509 -noout -modulus -in  $certfile ) \n     if   [[   $keymodulus   =   $certmodulus   ]]   ;   then \n         echo   keyfile and certfile do not match ;   return   1 \n     fi \n     if   [[  -f  $outputfile   ]]   ;   then \n        mv -f  $outputfile { ,.bak } \n     fi \n     ( dos2unix    $certfile ;  echo ;  dos2unix    $keyfile )     $outputfile \n    chmod  0600   $outputfile \n     )  }   Place cert and key files into the following paths:           host cert  /root/hostcert.pem    host key  /root/hostkey.pem     Then use  makepem  to combine the certs and put them in the proper locations.  [root@newkoji]#  makepem /root/hostcert.pem /root/hostkey.pem  \\ \n   /etc/pki/tls/private/kojiweb.pem [root@newkoji]#  chown apache:apache /etc/pki/tls/private/kojiweb.pem  In addition, copy the host cert and key into the locations HTTPD expects it them.  [root@newkoji]#  cp -a /root/hostcert.pem /etc/pki/tls/certs/hostcert.pem [root@newkoji]#  cp -a /root/hostkey.pem /etc/pki/tls/private/hostkey.pem", 
            "title": "Installing new cert/key pairs for newkoji"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-hostname-in-config-files", 
            "text": "Use sed to replace the hostname in the following config files in /etc:   /etc/kojira/kojira.conf  /etc/koji.conf  /etc/koji-hub/hub.conf  /etc/httpd/conf.d/kojiweb.conf  /etc/httpd/conf/httpd.conf  /etc/kojid/kojid.conf   You will need to fix  /etc/kojid/kojid.conf  on all builder machines as well (e.g.  kojibuilder2.chtc.wisc.edu ).", 
            "title": "Fixing hostname in config files"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-hostname-in-database", 
            "text": "You will need to find and fix entries that contain the hostname in the following tables:   host  (should be 1 entry)  users  (should be 2 entries, one for the host, and one for the kojira user)", 
            "title": "Fixing hostname in database"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-hostname-elsewhere", 
            "text": "These steps are only necessary if you cannot get a DNS Canonical Name (CN) record such that  koji.chtc.wisc.edu  resolves to  newkoji .   Update the repo definitions in the  osg-release  package  Update the mash script(s) at the GOC  Mail the software team and users that anyone using the  minefield  repos will need to update  osg-release  Fix all the build machines to point to the new name  Fix the following files in  osg-build  and make a new release  data/osg-koji-home.conf  data/osg-koji-site.conf  osgbuild/constants.py  osgbuild/kojiinter.py    Mail people that they will need to update  osg-build  and rerun  osg-koji setup", 
            "title": "Fixing hostname elsewhere"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#starting-services-and-validation", 
            "text": "Now you will start up Koji services and verify that they function.\nPrerequisite: previous restore steps have been completed and  postgresql  is running on the database host.  All steps will be run on  newkoji .    Start the main koji daemon:  [root@newkoji]#  service httpd start    Use  ps  to verify that it came up   Connect to the web interface in your browser.\n    Make sure you can use https and you can log in.  As yourself, run the  koji  command-line tool and make a few queries (e.g. list-tags)   Start the koji build daemon:  [root@newkoji]#  service kojid start    Use  ps  to verify that it came up    If you did not restore the  /mnt/koji/repos  directory, you will now need to regenerate the build repos.\nUse  koji list-tags  to get a list of tags and run  koji regen-repo  on all of the ones with  -build  in the name.\nThis  will  take several hours.\nYou will also need to regen the  -development  repos so that  minefield  works again. Keep an eye on the tasks in the web interface to make sure they are getting farmed out to the right hosts.   Try a scratch build   Start  kojira :  [root@newkoji]#  service kojira start    Use  ps  to verify that it came up   Wait half a minute and use  ps  to verify that  kojid  is still up; the two processes can kick each other off if they are both using the same certificate  Bump a package if needed and try a real, non-scratch build  Make sure that kojira is regenerating the repos   If you have updated  osg-release  and/or  osg-build , rebuild those packages now.", 
            "title": "Starting Services and Validation"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/", 
            "text": "Notes on Koji-Hub Setup\n\n\nCurrent Koji documentation\n may be of use.\n\n\nTags\n\n\nBase tags\n\n\ndist-el[567]-build\n\n\nTag list\n\n\nThese tags contains 3 external repos each, hosted locally under \nhttp://mirror.batlab.org/pub/linux\n:\n\n\n\n\ndist-el[567]-epel\n: A mirror of the EPEL 5/6/7 repositories\n\n\ndist-el[567]-centos*-os\n: A mirror of the base CentOS repositories\n\n\ndist-el[567]-centos*-updates*\n: A mirror of the CentOS Updates repositories\n\n\n\n\nWe don't put any packages in them (except for ones required for building, like \nbuildsys-macros\n and \nfetch-sources\n), and generally don't build from them directly, but use tag inheritance.\n\n\nosg-el[567]\n\n\nTag list\n\n\nThese tags contains all the \npackage names\n that we put into OSG; \nkoji add-pkg\n adds to them. \nosg-build\n does this automatically. The tags do not actually contain any builds (i.e. packages with version-release). All the other \nosg-*\n tags inherit from these (either directly or indirectly). The purpose of this is to make promoting builds easier, since it keeps you from having to run \nadd-pkg\n when you promote.\n\n\nkojira-fake\n\n\nTag\n\n\nThis tag (and targets building to it) were created because \nkojira\n does not automatically regenerate a repo unless it's the source of another repo. Without this, the osg-development tags (for example) wouldn't get regenerated automatically after a build.\n\n\nMain OSG tags\n\n\nosg-3.[123]-el[567]-build\n\n\nTag list\n\n\nThese are used to initialize the buildroot of most packages we make. They inherit from their respective dist-build and osg-development tags. The EL5 and EL6 tags also contain the \njpackage[56]-bin\n external repos under \nhttp://mirror.batlab.org/pub/jpackage/\n since we use those for some builds.\n\n\nNote\n that the JPackage external repos must have a better priority than the OS and EPEL external repos to avoid build problems for Java packages.\n\n\nosg-upcoming-el[567]-build\n\n\nTag list\n\n\nThese tags are special in that they also need to inherit from the latest mainline osg-build repo (that is, if trunk is 3.3, then \nosg-upcoming-el6-build\n should inherit from \nosg-3.3-el6-build\n).\n\n\nosg-*-el[567]-development\n\n\nTag list\n\n\nThese contain the builds in the \nosg-minefield\n repos. The \nosg-development\n repos hosted by the GOC take packages from this, so \nosg-development\n is pretty much \nosg-minefield\n after a 1-hour delay. They inherit from osg-testing (and occasionally from the more specialized branches like el5-gt52-experimental, though that is now discouraged). Builds that are made using the \nosg-el[567]\n targets (default if you're using \nosg-build\n) get their buildroots from the newest osg-build tags and put their results in the newest osg-development tags.\n\n\nosg-*-el[567]-testing\n\n\nTag list\n\n\nThese contain the builds in the \nosg-testing\n repos. They inherit from the respective \nosg-release\n tags.\n\n\nosg-*-el[567]-prerelease\n\n\nTag list\n\n\nThese are a staging are for packages that we are \ncertain\n will be released in the next release. They are otherwise empty. These are used for testing and for building the tarball clients.\n\n\nosg-*-el[567]-release\n\n\nTag list\n\n\nThese contain the builds in the \nosg-release\n repos. They should be locked except for when moving packages from the \nosg-prerelease\n repos to the \nosg-release\n repos. They inherit from \nosg-el[567]\n.\n\n\nosg-3.[123]-el[567]-release-build\n\n\nTag list\n\n\nThese inherit the \ndist-*-build\n tags and the \nosg-*-release\n tags, putting a base OS along with OSG packages in a single repo, without the need for yum priorities. It is used, along with \nosg-*-prerelease\n, for building the tarball client. Note that there are no \nrelease-build\n repos for upcoming.\n\n\nosg-3.[123]-el[567]-contrib\n\n\nTag list\n\n\nThese contain the builds in the \nosg-contrib\n repos. Note that there are no \nosg-upcoming-contrib\n repos.\n\n\nSpecialized tags\n\n\nThese tags are generally made for long projects which may be in an unstable state and should not interfere with the main development of OSG packages. An example is a full-scale Globus update, where many packages have to be built, using each other as dependencies, and the whole system is not considered usable until all the updates are done. They should generally be removed after the work is done.\n\n\nel[67]-globus and el[67]-globus-build\n\n\nThese tags were made by Matyas Selmeci for mass Globus updates.\n\n\nCollaborator Tags\n\n\nhcc-*\n\n\nFor use by the Holland Computing Center at UNL.\n\n\nBuild Targets\n\n\nA koji \ntarget\n pairs a build tag (which contains packages needed to build software) and a destination tag (which the software will be tagged into once it is built).\n\n\nosg-el[567]\n\n\nThese build from the osg-*-el[567]-build tags for the current release series into the osg-*-el[567]-development tags and are the primary targets used for building OSG software.\n\n\nosg-3.[123]-el[567]\n\n\nThese build from the osg-*-el[567]-build tags into the osg-*-el[567]-development tags and are used for building to releases other than the current one.\n\n\nosg-upcoming-el[567]\n\n\nThese build from the osg-upcoming-el[567]-build tags into the osg-upcoming-el[567]-development tags and are used for building to upcoming.\n\n\ndist-el[567]-build\n\n\nThese build from the \ndist-el*-build\n tag directly into the \ndist-el*-build\n tag. It is used for making builds that should be in every buildroot.\n\n\nkojira-fake-*\n\n\nThese fool kojira into regenerating their build tags as repos we can yum install from. Without this, the osg-development tags (for example) wouldn't get regenerated and osg-minefield wouldn't work.\n\n\nhcc-el[567], panda-el6\n\n\nThese were made for builds made for our collaborators to build into their tags.\n\n\nSigning plugin\n\n\nThe signing plugin is used to sign packages right after they are built. We give it a GPG signing key and corresponding passphrase. It is configured per build tag. The current default is to use the OSG key to sign if a configuration is not specified. This is because it's very difficult to sign packages after the fact, so it's better to erroneously sign some of them with the wrong key than to not sign them.\n\n\nIt is therefore important that whenever a new build tag is created, a corresponding config section for the signing plugin is added, too.\n\n\nThis comes from the package \nkoji-plugin-sign\n and has configs in \n/etc/koji-sign-plugin\n and \n/etc/koji-hub/plugins\n. There is a script called \nfix-permissions\n in both directories that will make sure the plugin can read the config.\n\n\nTweaks\n\n\nThese are local config changes we needed to make to get certain features to work.\n\n\nUsing proxy certs\n\n\n/etc/sysconfig/httpd\n needed to be changed to include the following lines:\n\n\nOPENSSL_ALLOW_PROXY=1\nOPENSSL_ALLOW_PROXY_CERTS=1\n\nexport OPENSSL_ALLOW_PROXY\nexport OPENSSL_ALLOW_PROXY_CERTS\n\n\n\n\n\nThe user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.\n\n\nProcedures\n\n\nUser cert switch\n\n\nThis procedure is now documented on the\n\nuser management page\n.\n\n\nAdding CAs for user authentication\n\n\nSince our Koji instance uses certs for auth, we specify which CAs we trust for signing user certs. The CA certs for user auth are concatenated together in the file \n/etc/pki/tls/certs/allowed-cas-for-users.crt\n. A comment line before the \nBEGIN CERTIFICATE\n line is used to name the file the cert comes from. We take the certs from the \nosg-ca-certs\n repository.\n\n\nFor example, when I added the CERN CAs to the bundle, I installed osg-ca-certs onto a Fermicloud VM, copied \n/etc/grid-security/certificates/CERN-TCA.pem\n (which signed user certs) and \nCERN-Root.pem\n (which signed \nCERN-TCA.pem\n) to \nkoji.chtc.wisc.edu\n, catted them to the end of the \nallowed-cas-for-users.crt\n file, edited the file to add comments before the certs, and restarted \nhttpd\n.\n\n\nA few tidbits of knowledge for administrators of our Koji server:\n\n\n\n\nThree services need to be running for koji-hub to be functioning: kojira, kojid, and the Apache web server. To restart these:\n\n\nservice kojid restart\n\n\nservice kojira restart\n\n\nservice httpd restart\n\n\n\n\n\n\nLogfiles can be found here:\n\n\n/var/log/kojid.log\n\n\n/var/log/kojira.log\n\n\n/var/log/messages\n\n\n\n\n\n\nkojid is configured to stop starting new tasks if it has less than 8GB free.\n\n\nFailed build roots are kept for 4 hours, and each build root is about 1GB currently. Hence, if too many tasks fail, the kojid might stop accepting new tasks for 4 hours.\n\n\nManually clean these out.\n\n\n\n\n\n\n\n\nKoji Permissions\n\n\nTo add a new user to Koji for someone with a given DN, first extract the CN. For example, Alain has the DN \n/DC=org/DC=doegrids/OU=People/CN=Alain Roy 424511\n, and the CN is just \nAlain Roy 424511\n. The commands below use just the CN.\n\n\n[you@client ~]$\n osg-koji add-user \nCN\n\n\n[you@client ~]$\n osg-koji grant-permission build \nCN\n\n\n[you@client ~]$\n osg-koji grant-permission repo \nCN\n\n\n\n\n\n\nIf you want to see the set of possible permissions:\n\n\n[you@client ~]$\n koji list-permissions \n\nEnter PEM pass phrase: \n\n\nadmin\n\n\nbuild\n\n\nrepo\n\n\nlivecd\n\n\nmaven-import\n\n\nwin-import\n\n\nwin-admin\n\n\nappliance\n\n\n\n\n\n\nIf you want to see someone's permissions:\n\n\n[you@client ~]$\n koji list-permissions --user \nAlain Roy 424511\n\n\nEnter PEM pass phrase: \n\n\nadmin\n\n\n\n\n\n\nIf you want to see your own permissions:\n\n\n[you@client ~]$\n koji list-permissions --mine\n\nEnter PEM pass phrase: \n\n\nadmin\n\n\n\n\n\n\nTo see the list of users:\n\n\n[you@client ~]$\n koji search user \n*\n\n\n\n\n\n\nRenewing host and service certs\n\n\nCerts and keys are stored in \n/p/condor/home/certificates/...\n\n\nTo obtain renewed certificates, use \nthe OSG PKI commandline clients\n or the web interface at \nhttps://oim.opensciencegrid.org/oim/certificaterequesthost\n.\n\n\nThe following cert files are necessary:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhostcert.pem\n\n\nkoji.chtc host cert (\nCN=koji.chtc.wisc.edu\n)\n\n\n\n\n\n\nhostkey.pem\n\n\nkoji.chtc host key\n\n\n\n\n\n\nkojiracert.pem\n\n\nkoji.chtc/kojira service cert (\nCN=koji.chtc.wisc.edu/kojira\n)\n\n\n\n\n\n\nkojirakey.pem\n\n\nkoji.chtc/kojira service key\n\n\n\n\n\n\nkojiweb.pem\n\n\nConcatenation of \nhostcert.pem\n and \nhostkey.pem\n\n\n\n\n\n\nkojira.pem\n\n\nConcatenation of \nkojiracert.pem\n and \nkojirakey.pem\n\n\n\n\n\n\n\n\nTo create \nkojiweb.pem\n and \nkojira.pem\n from their respective cert/key files, do:\n\n\n[root@koji ~]#\n \n(\ndos2unix \n hostcert.pem\n;\n echo\n;\n dos2unix \n hostkey.pem\n)\n \n kojiweb.pem\n\n[root@koji ~]#\n chmod \n0600\n kojiweb.pem\n\n[root@koji ~]#\n \n(\ndos2unix \n kojiracert.pem\n;\n echo\n;\n dos2unix \n kojirakey.pem\n)\n \n kojira.pem\n\n[root@koji ~]#\n chmod \n0600\n kojira.pem\n\n\n\n\n\nAs \nroot\n in \n/etc\n on \nkoji.chtc.wisc.edu\n:\n\n\n\n\nRun \ngit status\n.\n\n\nRun \netckeeper commit\n to commit any uncommited changes (including unversioned files).\n\n\n\n\nPut the files onto \nkoji.chtc.wisc.edu\n as follows:\n\n\n\n\n\n\n\n\nFile\n\n\nLocation\n\n\nchown\n\n\nchmod\n\n\n\n\n\n\n\n\n\n\nhostcert.pem\n\n\n/etc/pki/tls/certs/hostcert.pem\n\n\nroot:root\n\n\n0644\n\n\n\n\n\n\nhostkey.pem\n\n\n/etc/pki/tls/private/hostkey.pem\n\n\nroot:root\n\n\n0600\n\n\n\n\n\n\nkojiweb.pem\n\n\n/etc/pki/tls/private/kojiweb.pem\n\n\napache:apache\n\n\n0600\n\n\n\n\n\n\nkojira.pem\n\n\n/etc/pki/tls/private/kojira.pem\n\n\nroot:root\n\n\n0600\n\n\n\n\n\n\n\n\n\n\nShut down \nkojira\n and \nkojid\n.\n\n\nRestart \nhttpd\n.\n\n\nLog in via your own cert to the web interface to verify that it is working.\n\n\nRun \nosg-koji list-permissions --mine\n to verify command-line access is working.\n\n\nRun \netckeeper commit\n to commit your changes in \n/etc\n.\n\n\nStart up \nkojira\n and \nkojid\n.", 
            "title": "Koji-Hub Setup"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#notes-on-koji-hub-setup", 
            "text": "Current Koji documentation  may be of use.", 
            "title": "Notes on Koji-Hub Setup"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#tags", 
            "text": "", 
            "title": "Tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#base-tags", 
            "text": "", 
            "title": "Base tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#dist-el9156793-build", 
            "text": "Tag list  These tags contains 3 external repos each, hosted locally under  http://mirror.batlab.org/pub/linux :   dist-el[567]-epel : A mirror of the EPEL 5/6/7 repositories  dist-el[567]-centos*-os : A mirror of the base CentOS repositories  dist-el[567]-centos*-updates* : A mirror of the CentOS Updates repositories   We don't put any packages in them (except for ones required for building, like  buildsys-macros  and  fetch-sources ), and generally don't build from them directly, but use tag inheritance.", 
            "title": "dist-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-el9156793", 
            "text": "Tag list  These tags contains all the  package names  that we put into OSG;  koji add-pkg  adds to them.  osg-build  does this automatically. The tags do not actually contain any builds (i.e. packages with version-release). All the other  osg-*  tags inherit from these (either directly or indirectly). The purpose of this is to make promoting builds easier, since it keeps you from having to run  add-pkg  when you promote.", 
            "title": "osg-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#kojira-fake", 
            "text": "Tag  This tag (and targets building to it) were created because  kojira  does not automatically regenerate a repo unless it's the source of another repo. Without this, the osg-development tags (for example) wouldn't get regenerated automatically after a build.", 
            "title": "kojira-fake"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#main-osg-tags", 
            "text": "", 
            "title": "Main OSG tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793-build", 
            "text": "Tag list  These are used to initialize the buildroot of most packages we make. They inherit from their respective dist-build and osg-development tags. The EL5 and EL6 tags also contain the  jpackage[56]-bin  external repos under  http://mirror.batlab.org/pub/jpackage/  since we use those for some builds.  Note  that the JPackage external repos must have a better priority than the OS and EPEL external repos to avoid build problems for Java packages.", 
            "title": "osg-3.[123]-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-upcoming-el9156793-build", 
            "text": "Tag list  These tags are special in that they also need to inherit from the latest mainline osg-build repo (that is, if trunk is 3.3, then  osg-upcoming-el6-build  should inherit from  osg-3.3-el6-build ).", 
            "title": "osg-upcoming-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-development", 
            "text": "Tag list  These contain the builds in the  osg-minefield  repos. The  osg-development  repos hosted by the GOC take packages from this, so  osg-development  is pretty much  osg-minefield  after a 1-hour delay. They inherit from osg-testing (and occasionally from the more specialized branches like el5-gt52-experimental, though that is now discouraged). Builds that are made using the  osg-el[567]  targets (default if you're using  osg-build ) get their buildroots from the newest osg-build tags and put their results in the newest osg-development tags.", 
            "title": "osg-*-el[567]-development"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-testing", 
            "text": "Tag list  These contain the builds in the  osg-testing  repos. They inherit from the respective  osg-release  tags.", 
            "title": "osg-*-el[567]-testing"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-prerelease", 
            "text": "Tag list  These are a staging are for packages that we are  certain  will be released in the next release. They are otherwise empty. These are used for testing and for building the tarball clients.", 
            "title": "osg-*-el[567]-prerelease"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-release", 
            "text": "Tag list  These contain the builds in the  osg-release  repos. They should be locked except for when moving packages from the  osg-prerelease  repos to the  osg-release  repos. They inherit from  osg-el[567] .", 
            "title": "osg-*-el[567]-release"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793-release-build", 
            "text": "Tag list  These inherit the  dist-*-build  tags and the  osg-*-release  tags, putting a base OS along with OSG packages in a single repo, without the need for yum priorities. It is used, along with  osg-*-prerelease , for building the tarball client. Note that there are no  release-build  repos for upcoming.", 
            "title": "osg-3.[123]-el[567]-release-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793-contrib", 
            "text": "Tag list  These contain the builds in the  osg-contrib  repos. Note that there are no  osg-upcoming-contrib  repos.", 
            "title": "osg-3.[123]-el[567]-contrib"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#specialized-tags", 
            "text": "These tags are generally made for long projects which may be in an unstable state and should not interfere with the main development of OSG packages. An example is a full-scale Globus update, where many packages have to be built, using each other as dependencies, and the whole system is not considered usable until all the updates are done. They should generally be removed after the work is done.", 
            "title": "Specialized tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#el916793-globus-and-el916793-globus-build", 
            "text": "These tags were made by Matyas Selmeci for mass Globus updates.", 
            "title": "el[67]-globus and el[67]-globus-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#collaborator-tags", 
            "text": "", 
            "title": "Collaborator Tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#hcc-42", 
            "text": "For use by the Holland Computing Center at UNL.", 
            "title": "hcc-*"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#build-targets", 
            "text": "A koji  target  pairs a build tag (which contains packages needed to build software) and a destination tag (which the software will be tagged into once it is built).", 
            "title": "Build Targets"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-el9156793_1", 
            "text": "These build from the osg-*-el[567]-build tags for the current release series into the osg-*-el[567]-development tags and are the primary targets used for building OSG software.", 
            "title": "osg-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793", 
            "text": "These build from the osg-*-el[567]-build tags into the osg-*-el[567]-development tags and are used for building to releases other than the current one.", 
            "title": "osg-3.[123]-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-upcoming-el9156793", 
            "text": "These build from the osg-upcoming-el[567]-build tags into the osg-upcoming-el[567]-development tags and are used for building to upcoming.", 
            "title": "osg-upcoming-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#dist-el9156793-build_1", 
            "text": "These build from the  dist-el*-build  tag directly into the  dist-el*-build  tag. It is used for making builds that should be in every buildroot.", 
            "title": "dist-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#kojira-fake-42", 
            "text": "These fool kojira into regenerating their build tags as repos we can yum install from. Without this, the osg-development tags (for example) wouldn't get regenerated and osg-minefield wouldn't work.", 
            "title": "kojira-fake-*"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#hcc-el9156793-panda-el6", 
            "text": "These were made for builds made for our collaborators to build into their tags.", 
            "title": "hcc-el[567], panda-el6"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#signing-plugin", 
            "text": "The signing plugin is used to sign packages right after they are built. We give it a GPG signing key and corresponding passphrase. It is configured per build tag. The current default is to use the OSG key to sign if a configuration is not specified. This is because it's very difficult to sign packages after the fact, so it's better to erroneously sign some of them with the wrong key than to not sign them.  It is therefore important that whenever a new build tag is created, a corresponding config section for the signing plugin is added, too.  This comes from the package  koji-plugin-sign  and has configs in  /etc/koji-sign-plugin  and  /etc/koji-hub/plugins . There is a script called  fix-permissions  in both directories that will make sure the plugin can read the config.", 
            "title": "Signing plugin"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#tweaks", 
            "text": "These are local config changes we needed to make to get certain features to work.", 
            "title": "Tweaks"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#using-proxy-certs", 
            "text": "/etc/sysconfig/httpd  needed to be changed to include the following lines:  OPENSSL_ALLOW_PROXY=1\nOPENSSL_ALLOW_PROXY_CERTS=1\n\nexport OPENSSL_ALLOW_PROXY\nexport OPENSSL_ALLOW_PROXY_CERTS  The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.", 
            "title": "Using proxy certs"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#procedures", 
            "text": "", 
            "title": "Procedures"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#user-cert-switch", 
            "text": "This procedure is now documented on the user management page .", 
            "title": "User cert switch"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#adding-cas-for-user-authentication", 
            "text": "Since our Koji instance uses certs for auth, we specify which CAs we trust for signing user certs. The CA certs for user auth are concatenated together in the file  /etc/pki/tls/certs/allowed-cas-for-users.crt . A comment line before the  BEGIN CERTIFICATE  line is used to name the file the cert comes from. We take the certs from the  osg-ca-certs  repository.  For example, when I added the CERN CAs to the bundle, I installed osg-ca-certs onto a Fermicloud VM, copied  /etc/grid-security/certificates/CERN-TCA.pem  (which signed user certs) and  CERN-Root.pem  (which signed  CERN-TCA.pem ) to  koji.chtc.wisc.edu , catted them to the end of the  allowed-cas-for-users.crt  file, edited the file to add comments before the certs, and restarted  httpd .  A few tidbits of knowledge for administrators of our Koji server:   Three services need to be running for koji-hub to be functioning: kojira, kojid, and the Apache web server. To restart these:  service kojid restart  service kojira restart  service httpd restart    Logfiles can be found here:  /var/log/kojid.log  /var/log/kojira.log  /var/log/messages    kojid is configured to stop starting new tasks if it has less than 8GB free.  Failed build roots are kept for 4 hours, and each build root is about 1GB currently. Hence, if too many tasks fail, the kojid might stop accepting new tasks for 4 hours.  Manually clean these out.", 
            "title": "Adding CAs for user authentication"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#koji-permissions", 
            "text": "To add a new user to Koji for someone with a given DN, first extract the CN. For example, Alain has the DN  /DC=org/DC=doegrids/OU=People/CN=Alain Roy 424511 , and the CN is just  Alain Roy 424511 . The commands below use just the CN.  [you@client ~]$  osg-koji add-user  CN  [you@client ~]$  osg-koji grant-permission build  CN  [you@client ~]$  osg-koji grant-permission repo  CN   If you want to see the set of possible permissions:  [you@client ~]$  koji list-permissions  Enter PEM pass phrase:   admin  build  repo  livecd  maven-import  win-import  win-admin  appliance   If you want to see someone's permissions:  [you@client ~]$  koji list-permissions --user  Alain Roy 424511  Enter PEM pass phrase:   admin   If you want to see your own permissions:  [you@client ~]$  koji list-permissions --mine Enter PEM pass phrase:   admin   To see the list of users:  [you@client ~]$  koji search user  *", 
            "title": "Koji Permissions"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#renewing-host-and-service-certs", 
            "text": "Certs and keys are stored in  /p/condor/home/certificates/...  To obtain renewed certificates, use  the OSG PKI commandline clients  or the web interface at  https://oim.opensciencegrid.org/oim/certificaterequesthost .  The following cert files are necessary:           hostcert.pem  koji.chtc host cert ( CN=koji.chtc.wisc.edu )    hostkey.pem  koji.chtc host key    kojiracert.pem  koji.chtc/kojira service cert ( CN=koji.chtc.wisc.edu/kojira )    kojirakey.pem  koji.chtc/kojira service key    kojiweb.pem  Concatenation of  hostcert.pem  and  hostkey.pem    kojira.pem  Concatenation of  kojiracert.pem  and  kojirakey.pem     To create  kojiweb.pem  and  kojira.pem  from their respective cert/key files, do:  [root@koji ~]#   ( dos2unix   hostcert.pem ;  echo ;  dos2unix   hostkey.pem )    kojiweb.pem [root@koji ~]#  chmod  0600  kojiweb.pem [root@koji ~]#   ( dos2unix   kojiracert.pem ;  echo ;  dos2unix   kojirakey.pem )    kojira.pem [root@koji ~]#  chmod  0600  kojira.pem  As  root  in  /etc  on  koji.chtc.wisc.edu :   Run  git status .  Run  etckeeper commit  to commit any uncommited changes (including unversioned files).   Put the files onto  koji.chtc.wisc.edu  as follows:     File  Location  chown  chmod      hostcert.pem  /etc/pki/tls/certs/hostcert.pem  root:root  0644    hostkey.pem  /etc/pki/tls/private/hostkey.pem  root:root  0600    kojiweb.pem  /etc/pki/tls/private/kojiweb.pem  apache:apache  0600    kojira.pem  /etc/pki/tls/private/kojira.pem  root:root  0600      Shut down  kojira  and  kojid .  Restart  httpd .  Log in via your own cert to the web interface to verify that it is working.  Run  osg-koji list-permissions --mine  to verify command-line access is working.  Run  etckeeper commit  to commit your changes in  /etc .  Start up  kojira  and  kojid .", 
            "title": "Renewing host and service certs"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/", 
            "text": "Koji Infrastructure Overview\n\n\nIn Madison\n\n\nIn WID\n\n\n\n\nkoji.chtc.wisc.edu\n is the main Koji server. It runs:\n\n\nkoji-hub\n (httpd/mod_python service) -- controls everything else\n\n\nkoji-web\n (httpd/mod_python service) -- provides the web interface to koji-hub\n\n\nkojid\n (standalone daemon) -- builds packages\n\n\nkojira\n (standalone daemon) -- creates tasks to regen repos automatically\n\n\nrsyncd\n (via xinetd)\n\n\npuppetd\n (cron job)\n\n\n(also stores RPMs in its \n/mnt/koji\n directory)\n\n\n\n\n\n\ndb-01.batlab.org\n is the database server. It runs:\n\n\npostgres\n (standalone daemon)\n\n\nrsyncd\n (via xinetd)\n\n\npuppetd\n (cron job)\n\n\n(others)\n\n\n\n\n\n\nhost-3.chtc.wisc.edu\n is a backup server. It runs:\n\n\nrsync\n (cron job)\n\n\npuppetd\n (cron job)\n\n\n(others)\n\n\n\n\n\n\nwid-service-1.chtc.wisc.edu\n is a Puppet Master. It runs:\n\n\npuppetmaster\n (standalone daemon)\n\n\n(others)\n\n\n\n\n\n\n\n\nIn CS (3370A)\n\n\n\n\nosghost.chtc.wisc.edu\n is a VM host. It runs:\n\n\nkojibuilder2.chtc.wisc.edu\n and \nkojibuilder3.chtc.wisc.edu\n are builder VMs. Each runs:\n\n\nkojid\n (standalone daemon) -- builds packages\n\n\npuppetd\n (cron job)\n\n\n\n\n\n\n\n\n\n\n\n\nIn Indiana\n\n\n\n\nrepo1.grid.iu.edu\n, \nrepo2.grid.iu.edu\n and \nrepo-itb.grid.iu.edu\n are repo hosts. Each runs:\n\n\nmash\n (cron job) -- pulls RPMs from a \nkoji-hub\n\n\n(others)\n\n\n\n\n\n\nrepo.grid.iu.edu\n is a DNS alias pointing to either \nrepo1\n or \nrepo2\n\n\n\n\nLines of Communication\n\n\n\n\nkoji-web\n provides a web interface to \nkoji-hub\n\n\nkoji-hub\n sends jobs to all \nkojid\n daemons and gets back the results\n\n\nkojira\n sends requests to \nkoji-hub\n to create tasks\n\n\nkoji-hub\n talks directly to \npostgres\n for all metadata\n\n\nkoji-hub\n writes to and reads from \n/mnt/koji\n\n\nmash\n pulls RPMs from \n/mnt/koji\n and communicates with \nkoji-hub\n to get tag information\n\n\npuppetd\n on all Madison machines pulls Puppet configuration from \npuppetmaster\n on \nwid-service-1\n\n\nrsync\n on \nhost-3.chtc.wisc.edu\n pulls files from \nrsyncd\n on \nkoji.chtc.wisc.edu\n and \ndb-01.batlab.org\n\n\n\n\nManagement\n\n\nManaged by the GOC:\n\n\n\n\nrepo1.grid.iu.edu\n\n\nrepo2.grid.iu.edu\n\n\nrepo.grid.iu.edu\n\n\nrepo-itb.grid.iu.edu\n\n\n\n\nFully managed by CHTC Infrastructure:\n\n\n\n\ndb-01.batlab.org\n\n\nhost-3.chtc.wisc.edu\n\n\nwid-service-1.chtc.wisc.edu\n\n\n\n\nManagement split between CHTC infrastructure and OSG-Software:\n\n\n\n\nkoji.chtc.wisc.edu\n\n\nkojibuilder2.chtc.wisc.edu\n\n\nkojibuilder3.chtc.wisc.edu\n\n\nosghost.chtc.wisc.edu\n\n\n\n\n(In general, CHTC-inf takes care of accounts, firewalls, other basic config, and OSG takes care of the Koji services)", 
            "title": "Koji Infrastructure Overview"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#koji-infrastructure-overview", 
            "text": "", 
            "title": "Koji Infrastructure Overview"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-madison", 
            "text": "", 
            "title": "In Madison"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-wid", 
            "text": "koji.chtc.wisc.edu  is the main Koji server. It runs:  koji-hub  (httpd/mod_python service) -- controls everything else  koji-web  (httpd/mod_python service) -- provides the web interface to koji-hub  kojid  (standalone daemon) -- builds packages  kojira  (standalone daemon) -- creates tasks to regen repos automatically  rsyncd  (via xinetd)  puppetd  (cron job)  (also stores RPMs in its  /mnt/koji  directory)    db-01.batlab.org  is the database server. It runs:  postgres  (standalone daemon)  rsyncd  (via xinetd)  puppetd  (cron job)  (others)    host-3.chtc.wisc.edu  is a backup server. It runs:  rsync  (cron job)  puppetd  (cron job)  (others)    wid-service-1.chtc.wisc.edu  is a Puppet Master. It runs:  puppetmaster  (standalone daemon)  (others)", 
            "title": "In WID"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-cs-3370a", 
            "text": "osghost.chtc.wisc.edu  is a VM host. It runs:  kojibuilder2.chtc.wisc.edu  and  kojibuilder3.chtc.wisc.edu  are builder VMs. Each runs:  kojid  (standalone daemon) -- builds packages  puppetd  (cron job)", 
            "title": "In CS (3370A)"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-indiana", 
            "text": "repo1.grid.iu.edu ,  repo2.grid.iu.edu  and  repo-itb.grid.iu.edu  are repo hosts. Each runs:  mash  (cron job) -- pulls RPMs from a  koji-hub  (others)    repo.grid.iu.edu  is a DNS alias pointing to either  repo1  or  repo2", 
            "title": "In Indiana"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#lines-of-communication", 
            "text": "koji-web  provides a web interface to  koji-hub  koji-hub  sends jobs to all  kojid  daemons and gets back the results  kojira  sends requests to  koji-hub  to create tasks  koji-hub  talks directly to  postgres  for all metadata  koji-hub  writes to and reads from  /mnt/koji  mash  pulls RPMs from  /mnt/koji  and communicates with  koji-hub  to get tag information  puppetd  on all Madison machines pulls Puppet configuration from  puppetmaster  on  wid-service-1  rsync  on  host-3.chtc.wisc.edu  pulls files from  rsyncd  on  koji.chtc.wisc.edu  and  db-01.batlab.org", 
            "title": "Lines of Communication"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#management", 
            "text": "Managed by the GOC:   repo1.grid.iu.edu  repo2.grid.iu.edu  repo.grid.iu.edu  repo-itb.grid.iu.edu   Fully managed by CHTC Infrastructure:   db-01.batlab.org  host-3.chtc.wisc.edu  wid-service-1.chtc.wisc.edu   Management split between CHTC infrastructure and OSG-Software:   koji.chtc.wisc.edu  kojibuilder2.chtc.wisc.edu  kojibuilder3.chtc.wisc.edu  osghost.chtc.wisc.edu   (In general, CHTC-inf takes care of accounts, firewalls, other basic config, and OSG takes care of the Koji services)", 
            "title": "Management"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/", 
            "text": "Koji permissions and policy\n\n\nThese are some notes I wrote on Koji ACLs/policy after doing some source diving in the Koji code.\nThe version of Koji was 1.6.0.\nI later found some documentation at \nhttps://docs.pagure.org/koji/defining_hub_policies/\n. Go read it, that page has better examples.\n\n\nDefault policies (defined in hub/kojixmlrpc.py)\n\n\nbuild_from_srpm =\n       has_perm admin :: allow\n       all :: deny\nbuild_from_repo_id =\n       has_perm admin :: allow\n       all :: deny\nchannel =\n       has req_channel :: req\n       is_child_task :: parent\n       all :: use default\npackage_list =\n       has_perm admin :: allow\n       all :: deny\nvm =\n       has_perm admin win-admin :: allow\n       all :: deny\n\n\n\n\n\nIf \nMissingPolicyOk\n is true (default), then policies that do not exist default to \"allow\".\n\n\nPolicy syntax\n\n\nPolicies are set in the \n[policy]\n section of \n/etc/koji-hub/hub.conf\n\n\nEach policy definition starts with\n\n\npolicy_name =\n\n\n\n\n\nThe rest of the definition is indented. The lines in the policy definition have the following format:\n\n\nSimple tests:\n\n\ntest [params] [\n test [params] ...] :: action-if-true\ntest [params] [\n test [params] ...] !! action-if-false\n\n\n\n\n\nComplex tests:\n\n\ntest [params [\n ...]] :: {\n   test [params [\n ...]] :: action\n   test [params [\n ...]] :: {\n      ...\n      }\n}\n\n\n\n\n\nThe following generic tests are defined in \nkoji/policy.py\n:\n\n\n\n\n\n\ntrue\n / \nall\n \n\n    always true\n\n\n\n\n\n\nfalse\n / \nnone\n \n\n    always false\n\n\n\n\n\n\nhas FIELD\n \n\n    true if policy data contains a field called FIELD\n\n\n\n\n\n\nbool FIELD\n \n\n    true if FIELD is true\n\n\n\n\n\n\nmatch FIELD PATTERN1 [PATTERN2 ...]\n \n\n    true if FIELD matches any of the patterns (globs)\n\n\n\n\n\n\ncompare FIELD OP NUMBER\n \n\n    compare FIELD against a number. OP can be \n, \n, \n=, \n=, =, !=\n\n\n\n\n\n\nthe following koji-specific tests are defined in \nhub/kojihub.py\n:\n\n\n\n\n\n\nbuildtag PATTERN1 [PATTERN2 ...]\n \n\n    true if the build tag of a build matches a pattern\n\n\n\n\n\n\nfromtag PATTERN1 [PATTERN2 ...]\n \n\n    true if the tag we're moving a package from matches a pattern\n\n\n\n\n\n\nhas_perm PATTERN1 [PATTERN2 ...]\n \n\n    true if user has any matching permission\n\n\n\n\n\n\nhastag TAG\n \n\n    true if the build has the tag TAG\n\n\n\n\n\n\nimported\n \n\n    true if the build was imported\n\n\n\n\n\n\nis_build_owner\n \n\n    true if the user doing this task owns the build\n\n\n\n\n\n\nis_child_task\n \n\n    true if the task is a child of some other task\n\n\n\n\n\n\nis_new_package\n \n\n    true if the package being looked at is new (i.e. doesn't have an 'id' yet)\n\n\n\n\n\n\nmethod PATTERN1 [PATTERN2 ...]\n \n\n    true if the method matches a pattern\n\n\n\n\n\n\noperation PATTERN1 [PATTERN2 ...]\n \n\n    true if current operation matches any of the patterns\n\n\n\n\n\n\npackage PATTERN1 [PATTERN2 ...]\n \n\n    true if the package name matches any of the patterns\n\n\n\n\n\n\npolicy POLICY\n \n\n    true if the named policy is true\n\n\n\n\n\n\nskip_tag\n \n\n    true if the skip_tag option is true\n\n\n\n\n\n\nsource PATTERN1 [PATTERN2 ...]\n \n\n    true if source matches patterns\n\n\n\n\n\n\ntag PATTERN1 [PATTERN2 ...]\n \n\n    true if the tag name matches any of the patterns\n\n\n\n\n\n\nuser PATTERN1 [PATTERN2 ...]\n \n\n    true if username matches a pattern\n\n\n\n\n\n\nuser_in_group PATTERN1 [PATTERN2 ...]\n \n\n    true if the user is in any matching group\n\n\n\n\n\n\nvm_name PATTERN1 [PATTERN2 ...]\n \n\n    true if vm name matches a pattern\n\n\n\n\n\n\nThe actions are:\n\n\n\n\n\n\nallow\n \n\n    allow the action\n\n\n\n\n\n\ndeny\n \n\n    deny the action\n\n\n\n\n\n\nreq\n \n\n    ?\n\n\n\n\n\n\nparent\n \n\n    ?\n\n\n\n\n\n\nuse default\n \n\n    ?\n\n\n\n\n\n\nDefault permissions\n\n\nThese are the permissions that people can be given in koji:\n\n\n\n\nadmin\n\n\nbuild\n\n\nrepo\n\n\nlivecd\n\n\nmaven-import\n\n\nwin-import\n\n\nwin-admin\n\n\nappliance\n\n\n\n\nAdditional permissions can be added with \ngrant-permission --new\n, see below.\n\n\nThe following permissions are checked by name in the koji command-line utility (i.e. policies are not used):\n\n\n\n\n\n\nadmin\n: \n\n\nadd-group\n, \nadd-tag\n, \nadd-target\n, \nclone-tag\n, \nedit-target\n, \nremove-tag\n, \nremove-target\n, \nwrapper-rpm\n\n\n\n\n\n\nmaven-import\n: \n\n\nimport-archive\n with the \n--type=maven\n option\n\n\n\n\n\n\nwin-import\n: \n\n\nimport-archive\n with the \n--type=win\n option\n\n\n\n\n\n\nrepo\n: \n\n\nregen-repo\n\n\n\n\n\n\nI haven't found out where some of the other permissions are used.\n\n\nAdding permissions\n\n\nYou can give someone a permission that doesn't exist by doing:\n\n\nosg-koji grant-permission --new \nPERMISSION\n \nUSER\n\n\n\n\n\n\nDoing so creates the permission as a side effect.\nI'm not aware of a way to create the permission without granting it, other than with database hackery.\n\n\nWhere policies are used and what policy data is passed on:\n\n\nbuild_from_srpm\n\n\nsource:\n\n\n\n\nbuilder/kojid:BuildTask.handler\n \n\n    used when source url points to an SRPM (as opposed to an scm) and the build is not a scratch build.\n\n\n\n\npolicy data:\n\n\n\n\n\n\nuser_id\n \n\n    the owner of the task\n\n\n\n\n\n\nsource\n \n\n    the url of the source file\n\n\n\n\n\n\ntask_id\n \n\n    the id of the task\n\n\n\n\n\n\nbuild_tag\n \n\n    the id of the build tag\n\n\n\n\n\n\nskip_tag\n \n\n    true if we're not tagging this build (\n--scratch\n or \n--skip-tag\n passed on the command line)\n\n\n\n\n\n\ntarget\n \n\n    the build target (only if we have one?)\n\n\n\n\n\n\ntag\n \n\n    the destination tag (only if \nskip_tag\n is false)\n\n\n\n\n\n\nbuild_from_repo_id\n\n\nsource:\n\n\n\n\nbuilder/kojid:BuildTask.handler\n \n\n    used when the \n--repo-id\n option is passed to \nkoji build\n\n\n\n\npolicy data:\n same as \nbuild_from_srpm\n\n\npackage_list\n\n\nsource:\n\n\n\n\nhub/kojihub.py:pkglist_add\n \n\n\nadd-pkg\n, \nblock-pkg\n, \nset-pkg-arches\n, \nset-pkg-owner\n commands\n\n\n\n\npolicy data:\n\n\n\n\n\n\naction\n \n\n    'add', 'update', 'block' depending on what is being done\n\n\n\n\n\n\nforce\n \n\n    true if \n--force\n is passed on the command line\n\n\n\n\n\n\npackage\n \n\n    package info (the id I think?)\n\n\n\n\n\n\ntag\n \n\n    the id of the tag we're trying to add the package to/package is in\n\n\n\n\n\n\nsource:\n\n\n\n\nhub/kojihub.py:pkglist_remove\n \n\n    used internally by the \nkoji clone-tag\n command?\n\n\n\n\npolicy data:\n same as above, except \naction\n is 'remove'\n\n\nsource:\n\n\n\n\nhub/kojihub.py:pkglist_unblock\n \n\n\nunblock-pkg\n command\n\n\n\n\npolicy data:\n same as above, except \naction\n is 'unblock'\n\n\ntag\n\n\n\n\nNote\n\n\nRootExports is the class containing functions exported via XMLRPC. In general, each function corresponds to a koji task.\n\n\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.tagBuild\n \n\n    tagging builds\n\n\n\n\npolicy data:\n\n\n\n\n\n\nbuild\n \n\n    the id of the build\n\n\n\n\n\n\nfromtag\n \n\n    the id of the tag we're moving the build from, if there is one\n\n\n\n\n\n\noperation\n \n\n    'tag' or 'move'\n\n\n\n\n\n\ntag\n \n\n    the id of the tag\n\n\n\n\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.untagBuild\n \n\n    untagging builds\n\n\n\n\npolicy data:\n same as above, except \noperation\n is 'untag', and \ntag\n is None\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.moveAllBuilds\n \n\n    moving all builds of a package from tag1 to tag2\n\n\n\n\npolicy data:\n same as for \ntagBuild\n, except \noperation\n is 'move'. The policy is checked once for each build being moved.\n\n\nsource:\n\n\n\n\nhub/kojihub.py:HostExports.tagBuild\n \n\n    tagging builds (\"host version\" ?)\n\n\n\n\npolicy data:\n same as for \ntagBuild\n, plus \nuser_id\n\n\nvm\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.winBuild\n \n\n    windows builds in a vm (\nwin-build\n command)\n\n\n\n\npolicy data:\n\n\n\n\n\n\ntag\n \n\n    the destination tag\n\n\n\n\n\n\nvm_name\n \n\n    the name of the vm\n\n\n\n\n\n\nExamples\n\n\nLet people with the \"build\" permission also add packages and build SRPMs\n\n\npackage_list = \n    has_perm admin :: allow\n    has_perm build \n match action add update :: allow\n    all :: deny\n\nbuild_from_srpm =\n    has_perm admin build :: allow\n    all :: deny\n\n\n\n\n\nPromotion policy for different teams\n\n\n\n\nSoftware team members can tag any package as testing/release.\n\n\nOperations team members can tag vo-clients as testing/release.\n\n\nSecurity team members can tag CA packages as testing/release.\n\n\n\n\n\n\n\npromotion =\n   has_perm software-team :: allow\n   has_perm operations-team \n package vo-client :: allow\n   has_perm security-team \n package *-ca-certs* :: allow\n   all :: deny\n\ntag =\n    has_perm admin :: allow\n    operation tag :: {\n        tag *testing *release* \n policy promotion :: allow\n        tag *testing *release* !! allow\n    }\n    operation untag :: {\n        fromtag *testing *release* \n policy promotion :: allow\n        fromtag *testing *release* !! allow\n    }\n    operation move :: {\n        tag *testing *release* \n policy promotion :: allow\n        fromtag *testing *release* \n policy promotion :: allow\n        tag *testing *release* !! {\n            fromtag *testing *release* !! allow\n        }\n    }\n    all :: deny", 
            "title": "Koji Policy Writing"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#koji-permissions-and-policy", 
            "text": "These are some notes I wrote on Koji ACLs/policy after doing some source diving in the Koji code.\nThe version of Koji was 1.6.0.\nI later found some documentation at  https://docs.pagure.org/koji/defining_hub_policies/ . Go read it, that page has better examples.", 
            "title": "Koji permissions and policy"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#default-policies-defined-in-hubkojixmlrpcpy", 
            "text": "build_from_srpm =\n       has_perm admin :: allow\n       all :: deny\nbuild_from_repo_id =\n       has_perm admin :: allow\n       all :: deny\nchannel =\n       has req_channel :: req\n       is_child_task :: parent\n       all :: use default\npackage_list =\n       has_perm admin :: allow\n       all :: deny\nvm =\n       has_perm admin win-admin :: allow\n       all :: deny  If  MissingPolicyOk  is true (default), then policies that do not exist default to \"allow\".", 
            "title": "Default policies (defined in hub/kojixmlrpc.py)"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#policy-syntax", 
            "text": "Policies are set in the  [policy]  section of  /etc/koji-hub/hub.conf  Each policy definition starts with  policy_name =  The rest of the definition is indented. The lines in the policy definition have the following format:  Simple tests:  test [params] [  test [params] ...] :: action-if-true\ntest [params] [  test [params] ...] !! action-if-false  Complex tests:  test [params [  ...]] :: {\n   test [params [  ...]] :: action\n   test [params [  ...]] :: {\n      ...\n      }\n}  The following generic tests are defined in  koji/policy.py :    true  /  all   \n    always true    false  /  none   \n    always false    has FIELD   \n    true if policy data contains a field called FIELD    bool FIELD   \n    true if FIELD is true    match FIELD PATTERN1 [PATTERN2 ...]   \n    true if FIELD matches any of the patterns (globs)    compare FIELD OP NUMBER   \n    compare FIELD against a number. OP can be  ,  ,  =,  =, =, !=    the following koji-specific tests are defined in  hub/kojihub.py :    buildtag PATTERN1 [PATTERN2 ...]   \n    true if the build tag of a build matches a pattern    fromtag PATTERN1 [PATTERN2 ...]   \n    true if the tag we're moving a package from matches a pattern    has_perm PATTERN1 [PATTERN2 ...]   \n    true if user has any matching permission    hastag TAG   \n    true if the build has the tag TAG    imported   \n    true if the build was imported    is_build_owner   \n    true if the user doing this task owns the build    is_child_task   \n    true if the task is a child of some other task    is_new_package   \n    true if the package being looked at is new (i.e. doesn't have an 'id' yet)    method PATTERN1 [PATTERN2 ...]   \n    true if the method matches a pattern    operation PATTERN1 [PATTERN2 ...]   \n    true if current operation matches any of the patterns    package PATTERN1 [PATTERN2 ...]   \n    true if the package name matches any of the patterns    policy POLICY   \n    true if the named policy is true    skip_tag   \n    true if the skip_tag option is true    source PATTERN1 [PATTERN2 ...]   \n    true if source matches patterns    tag PATTERN1 [PATTERN2 ...]   \n    true if the tag name matches any of the patterns    user PATTERN1 [PATTERN2 ...]   \n    true if username matches a pattern    user_in_group PATTERN1 [PATTERN2 ...]   \n    true if the user is in any matching group    vm_name PATTERN1 [PATTERN2 ...]   \n    true if vm name matches a pattern    The actions are:    allow   \n    allow the action    deny   \n    deny the action    req   \n    ?    parent   \n    ?    use default   \n    ?", 
            "title": "Policy syntax"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#default-permissions", 
            "text": "These are the permissions that people can be given in koji:   admin  build  repo  livecd  maven-import  win-import  win-admin  appliance   Additional permissions can be added with  grant-permission --new , see below.  The following permissions are checked by name in the koji command-line utility (i.e. policies are not used):    admin :   add-group ,  add-tag ,  add-target ,  clone-tag ,  edit-target ,  remove-tag ,  remove-target ,  wrapper-rpm    maven-import :   import-archive  with the  --type=maven  option    win-import :   import-archive  with the  --type=win  option    repo :   regen-repo    I haven't found out where some of the other permissions are used.", 
            "title": "Default permissions"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#adding-permissions", 
            "text": "You can give someone a permission that doesn't exist by doing:  osg-koji grant-permission --new  PERMISSION   USER   Doing so creates the permission as a side effect.\nI'm not aware of a way to create the permission without granting it, other than with database hackery.", 
            "title": "Adding permissions"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#where-policies-are-used-and-what-policy-data-is-passed-on", 
            "text": "", 
            "title": "Where policies are used and what policy data is passed on:"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#build95from95srpm", 
            "text": "source:   builder/kojid:BuildTask.handler   \n    used when source url points to an SRPM (as opposed to an scm) and the build is not a scratch build.   policy data:    user_id   \n    the owner of the task    source   \n    the url of the source file    task_id   \n    the id of the task    build_tag   \n    the id of the build tag    skip_tag   \n    true if we're not tagging this build ( --scratch  or  --skip-tag  passed on the command line)    target   \n    the build target (only if we have one?)    tag   \n    the destination tag (only if  skip_tag  is false)", 
            "title": "build_from_srpm"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#build95from95repo95id", 
            "text": "source:   builder/kojid:BuildTask.handler   \n    used when the  --repo-id  option is passed to  koji build   policy data:  same as  build_from_srpm", 
            "title": "build_from_repo_id"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#package95list", 
            "text": "source:   hub/kojihub.py:pkglist_add    add-pkg ,  block-pkg ,  set-pkg-arches ,  set-pkg-owner  commands   policy data:    action   \n    'add', 'update', 'block' depending on what is being done    force   \n    true if  --force  is passed on the command line    package   \n    package info (the id I think?)    tag   \n    the id of the tag we're trying to add the package to/package is in    source:   hub/kojihub.py:pkglist_remove   \n    used internally by the  koji clone-tag  command?   policy data:  same as above, except  action  is 'remove'  source:   hub/kojihub.py:pkglist_unblock    unblock-pkg  command   policy data:  same as above, except  action  is 'unblock'", 
            "title": "package_list"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#tag", 
            "text": "Note  RootExports is the class containing functions exported via XMLRPC. In general, each function corresponds to a koji task.   source:   hub/kojihub.py:RootExports.tagBuild   \n    tagging builds   policy data:    build   \n    the id of the build    fromtag   \n    the id of the tag we're moving the build from, if there is one    operation   \n    'tag' or 'move'    tag   \n    the id of the tag    source:   hub/kojihub.py:RootExports.untagBuild   \n    untagging builds   policy data:  same as above, except  operation  is 'untag', and  tag  is None  source:   hub/kojihub.py:RootExports.moveAllBuilds   \n    moving all builds of a package from tag1 to tag2   policy data:  same as for  tagBuild , except  operation  is 'move'. The policy is checked once for each build being moved.  source:   hub/kojihub.py:HostExports.tagBuild   \n    tagging builds (\"host version\" ?)   policy data:  same as for  tagBuild , plus  user_id", 
            "title": "tag"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#vm", 
            "text": "source:   hub/kojihub.py:RootExports.winBuild   \n    windows builds in a vm ( win-build  command)   policy data:    tag   \n    the destination tag    vm_name   \n    the name of the vm", 
            "title": "vm"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#let-people-with-the-build-permission-also-add-packages-and-build-srpms", 
            "text": "package_list = \n    has_perm admin :: allow\n    has_perm build   match action add update :: allow\n    all :: deny\n\nbuild_from_srpm =\n    has_perm admin build :: allow\n    all :: deny", 
            "title": "Let people with the \"build\" permission also add packages and build SRPMs"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#promotion-policy-for-different-teams", 
            "text": "Software team members can tag any package as testing/release.  Operations team members can tag vo-clients as testing/release.  Security team members can tag CA packages as testing/release.    promotion =\n   has_perm software-team :: allow\n   has_perm operations-team   package vo-client :: allow\n   has_perm security-team   package *-ca-certs* :: allow\n   all :: deny\n\ntag =\n    has_perm admin :: allow\n    operation tag :: {\n        tag *testing *release*   policy promotion :: allow\n        tag *testing *release* !! allow\n    }\n    operation untag :: {\n        fromtag *testing *release*   policy promotion :: allow\n        fromtag *testing *release* !! allow\n    }\n    operation move :: {\n        tag *testing *release*   policy promotion :: allow\n        fromtag *testing *release*   policy promotion :: allow\n        tag *testing *release* !! {\n            fromtag *testing *release* !! allow\n        }\n    }\n    all :: deny", 
            "title": "Promotion policy for different teams"
        }, 
        {
            "location": "/infrastructure/koji-user-management/", 
            "text": "Koji User Management\n\n\nAll of these operations require Koji admin privileges.\nYou can see what privileges you have by running \nosg-koji list-permissions --mine\n.\nIf you don't see \nadmin\n, you do not have Koji admin privileges.\n\n\nAs of March 2018, the people with \nadmin\n privileges are:\n\n\n\n\nMat Selmeci\n\n\nCarl Edquist\n\n\nBrian Lin\n\n\nTim Cartwright\n\n\nTim Theisen\n\n\n\n\nAdding a User\n\n\nAdding a user requires two pieces of information about the user:\n\n\n\n\nthe DN of the cert they're using\n\n\ntheir purpose for using Koji, e.g. what package(s) they are going to build, what repos they are going to build into,\n  or whether they should have promotion abilities for certain repos\n\n\n\n\nKoji only allows certs signed by one of the following CAs:\n\n\n\n\nESnet Root CA 1\n\n\nCERN Trusted Certification Authority\n\n\nCERN Grid Certification Authority\n\n\nCILogon Basic CA 1 (note: Fermi certs are signed by this)\n\n\nCILogon OSG CA 1\n\n\n\n\nThis is controlled by the \n/etc/pki/tls/CAs/allowed-cas-for-users.crt\n file,\nwhich contains the full cert chains for these CAs.\n\n\n\n\nNote\n\n\nThe file also contains the cert chains for the following two CAs, which are used for build hosts and automated users.\n\n\n\n\nkoji.chtc.wisc.edu\n\n\nInCommon RSA Server CA\n\n\n\n\n\n\nYou can usually guess which CA signed the user's cert from their DN.\nIf unsure, ask them for the output of:\n\n\n$\n openssl x509 -in \nCERTFILE\n -noout -issuer\n\n\n\n\n\nIf the CA is not one of the above ones, you will have to add the CA.\nGet permission to do this from the Software Manager, then follow the procedure on the \nkoji-hub setup page\n.\n\n\nThe user's Koji username will be the \nfirst\n Common Name (CN) of their certificate subject.\nFor example:\n\n\n$\n openssl x509 -in \nCERTFILE\n -noout -subject\n\nsubject= /DC=org/DC=cilogon/C=US/O=Fermi National Accelerator Laboratory/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n\n\n\n\n\n\nwill result in the Koji username \"Matyas Selmeci\".\nTo add the user, run\n\n\n$\n osg-koji add-user \nUSERNAME\n\n\n\n\n\n\nThis will create the user but will give them no permissions.\n\n\n\n\nWarning\n\n\nIt is effectively impossible to delete a user.\nIf you get the name wrong, follow the \ninstructions below for renaming a user\n below.\n\n\n\n\nTo give them permissions, use the command\n\n\n$\n osg-koji grant-permission \nPERMISSION\n \nUSERNAME\n\n\n\n\n\n\nGenerally, the permissions you should give are (more explanation below):\n\n\n\n\nmost people should be given \nbuild\n and \nrepo\n\n\nHCC people should also be given \nhcc-build\n\n\nS\nR people should also be given \nsoftware-team\n\n\nSecurity people should also be given \nsecurity-team\n\n\nOps people should also be given \noperations-team\n\n\nother software devs should also be given \nsoftware-contributor\n\n\nfew people should be given \nadmin\n\n\n\n\nTo revoke permissions, use the command\n\n\n$\n osg-koji revoke-permission \nPERMISSION\n \nUSERNAME\n\n\n\n\n\n\nPermissions details\n\n\nTo list the available permissions, run\n\n\n$\n osg-koji list-permissions\n\n\n\n\n\nThe important permissions are:\n\n\n\n\nrepo\n: the ability to run \nosg-koji regen-repo\n.\n  Should be given to anyone that builds or tests packages.\n\n\nbuild\n: the ability to build (but not tag!) any package.\n  Should be given to anyone that builds packages, including automated users.\n  This perm is necessary but \nnot\n sufficient for them to build into the development repos;\n  it \nis\n enough to let them do scratch builds.\n\n\nsoftware-contributor\n: the ability to build into \nany\n of the development repos \nand\n promote into the osg-testing repos (but not any other repos).\n  Should be given to people that build specific software but do not belong to the OSG S\nR team or HCC.\n\n\noperations-team\n: the ability to build into development and promote into testing the \nvo-client\n package.\n  Should be given to people in OSG Operations.\n\n\nsecurity-team\n: the ability to build into development and promote into testing the \n*-ca-certs*\n packages.\n  Should be given to people in OSG Security.\n\n\nhcc-build\n: the ability to build and promote into any of the \nhcc\n tags.\n  Should be given to people in Nebraska's Holland Computing Center.\n\n\nsoftware-team\n, \nrelease-team\n: the ability to build any package into any development tag, and promote to any tag.\n  Current policy does not distinguish between these two permissions.\n  Should be given to people on the S\nR team.\n\n\nadmin\n: the ability to do anything (short of direct database or config manipulation).\n  Should be given sparingly.\n\n\n\n\nThe \ngrant-permission\n command can also be used to create new permissions; doing so is beyond the scope of this document.\nFor further permission details, see the \npolicy writing doc\n\nand \n/etc/koji-hub/hub.conf\n on koji.chtc.wisc.edu.\n\n\nHandling User Cert Changes / Renaming a User\n\n\nKoji users are identified by the \nfirst\n Common Name (CN) of their X.509 certificate.\nIf this changes for a user (e.g. they get a cert from a new CA), someone with root access on koji.chtc.wisc.edu will\nneed to SSH there and do the following:\n\n\n$\n sudo /root/psql-kojidb\n\n\n\n\n\nkoji=\n \nBEGIN\n;\n\n\nkoji=\n \nSELECT\n \n*\n \nFROM\n \nusers\n \nWHERE\n \nname\n=\nOLDNAME\n;\n\n\n-- make sure you find one and exactly one person\n\n\nkoji=\n \nUPDATE\n \nusers\n \nSET\n \nname\n=\nNEWNAME\n \nWHERE\n \nname\n=\nOLDNAME\n;\n\n\n-- 1 row should have been updated\n\n\nkoji=\n \nSELECT\n \n*\n \nFROM\n \nusers\n;\n\n\n-- sanity check; if everything looks ok, commit\n\n\nkoji=\n \nCOMMIT\n;\n\n\nkoji=\n \n\\q\n\n\n\n\n\n\nIf you mess up, do \nROLLBACK; BEGIN;\n and try again.\n\n\nInform the user:\n\n\n\n\nthey can no longer use their old cert\n\n\nif they \naren't\n using a proxy for Koji auth, they must rerun \nosg-koji setup\n to fix their \nclient.crt\n files\n\n\nthey must import their new cert into their browsers\n\n\nthey must clear their browser cache and cookies for koji.chtc.wisc.edu before using the web interface\n  (or else they'll get a Python stack trace when they try to connect)\n\n\n\n\nDisabling and Enabling a User\n\n\nUsers cannot be deleted, but they can be disabled.\n\n\nTo disable a user, use the command:\n\n\n$\n osg-koji disable-user \nUSERNAME\n\n\n\n\n\n\nTo enable a user, use the command:\n\n\n$\n osg-koji enable-user \nUSERNAME", 
            "title": "Koji User Management"
        }, 
        {
            "location": "/infrastructure/koji-user-management/#koji-user-management", 
            "text": "All of these operations require Koji admin privileges.\nYou can see what privileges you have by running  osg-koji list-permissions --mine .\nIf you don't see  admin , you do not have Koji admin privileges.  As of March 2018, the people with  admin  privileges are:   Mat Selmeci  Carl Edquist  Brian Lin  Tim Cartwright  Tim Theisen", 
            "title": "Koji User Management"
        }, 
        {
            "location": "/infrastructure/koji-user-management/#adding-a-user", 
            "text": "Adding a user requires two pieces of information about the user:   the DN of the cert they're using  their purpose for using Koji, e.g. what package(s) they are going to build, what repos they are going to build into,\n  or whether they should have promotion abilities for certain repos   Koji only allows certs signed by one of the following CAs:   ESnet Root CA 1  CERN Trusted Certification Authority  CERN Grid Certification Authority  CILogon Basic CA 1 (note: Fermi certs are signed by this)  CILogon OSG CA 1   This is controlled by the  /etc/pki/tls/CAs/allowed-cas-for-users.crt  file,\nwhich contains the full cert chains for these CAs.   Note  The file also contains the cert chains for the following two CAs, which are used for build hosts and automated users.   koji.chtc.wisc.edu  InCommon RSA Server CA    You can usually guess which CA signed the user's cert from their DN.\nIf unsure, ask them for the output of:  $  openssl x509 -in  CERTFILE  -noout -issuer  If the CA is not one of the above ones, you will have to add the CA.\nGet permission to do this from the Software Manager, then follow the procedure on the  koji-hub setup page .  The user's Koji username will be the  first  Common Name (CN) of their certificate subject.\nFor example:  $  openssl x509 -in  CERTFILE  -noout -subject subject= /DC=org/DC=cilogon/C=US/O=Fermi National Accelerator Laboratory/OU=People/CN=Matyas Selmeci/CN=UID:matyas   will result in the Koji username \"Matyas Selmeci\".\nTo add the user, run  $  osg-koji add-user  USERNAME   This will create the user but will give them no permissions.   Warning  It is effectively impossible to delete a user.\nIf you get the name wrong, follow the  instructions below for renaming a user  below.   To give them permissions, use the command  $  osg-koji grant-permission  PERMISSION   USERNAME   Generally, the permissions you should give are (more explanation below):   most people should be given  build  and  repo  HCC people should also be given  hcc-build  S R people should also be given  software-team  Security people should also be given  security-team  Ops people should also be given  operations-team  other software devs should also be given  software-contributor  few people should be given  admin   To revoke permissions, use the command  $  osg-koji revoke-permission  PERMISSION   USERNAME", 
            "title": "Adding a User"
        }, 
        {
            "location": "/infrastructure/koji-user-management/#permissions-details", 
            "text": "To list the available permissions, run  $  osg-koji list-permissions  The important permissions are:   repo : the ability to run  osg-koji regen-repo .\n  Should be given to anyone that builds or tests packages.  build : the ability to build (but not tag!) any package.\n  Should be given to anyone that builds packages, including automated users.\n  This perm is necessary but  not  sufficient for them to build into the development repos;\n  it  is  enough to let them do scratch builds.  software-contributor : the ability to build into  any  of the development repos  and  promote into the osg-testing repos (but not any other repos).\n  Should be given to people that build specific software but do not belong to the OSG S R team or HCC.  operations-team : the ability to build into development and promote into testing the  vo-client  package.\n  Should be given to people in OSG Operations.  security-team : the ability to build into development and promote into testing the  *-ca-certs*  packages.\n  Should be given to people in OSG Security.  hcc-build : the ability to build and promote into any of the  hcc  tags.\n  Should be given to people in Nebraska's Holland Computing Center.  software-team ,  release-team : the ability to build any package into any development tag, and promote to any tag.\n  Current policy does not distinguish between these two permissions.\n  Should be given to people on the S R team.  admin : the ability to do anything (short of direct database or config manipulation).\n  Should be given sparingly.   The  grant-permission  command can also be used to create new permissions; doing so is beyond the scope of this document.\nFor further permission details, see the  policy writing doc \nand  /etc/koji-hub/hub.conf  on koji.chtc.wisc.edu.", 
            "title": "Permissions details"
        }, 
        {
            "location": "/infrastructure/koji-user-management/#handling-user-cert-changes-renaming-a-user", 
            "text": "Koji users are identified by the  first  Common Name (CN) of their X.509 certificate.\nIf this changes for a user (e.g. they get a cert from a new CA), someone with root access on koji.chtc.wisc.edu will\nneed to SSH there and do the following:  $  sudo /root/psql-kojidb  koji=   BEGIN ;  koji=   SELECT   *   FROM   users   WHERE   name = OLDNAME ;  -- make sure you find one and exactly one person  koji=   UPDATE   users   SET   name = NEWNAME   WHERE   name = OLDNAME ;  -- 1 row should have been updated  koji=   SELECT   *   FROM   users ;  -- sanity check; if everything looks ok, commit  koji=   COMMIT ;  koji=   \\q   If you mess up, do  ROLLBACK; BEGIN;  and try again.  Inform the user:   they can no longer use their old cert  if they  aren't  using a proxy for Koji auth, they must rerun  osg-koji setup  to fix their  client.crt  files  they must import their new cert into their browsers  they must clear their browser cache and cookies for koji.chtc.wisc.edu before using the web interface\n  (or else they'll get a Python stack trace when they try to connect)", 
            "title": "Handling User Cert Changes / Renaming a User"
        }, 
        {
            "location": "/infrastructure/koji-user-management/#disabling-and-enabling-a-user", 
            "text": "Users cannot be deleted, but they can be disabled.  To disable a user, use the command:  $  osg-koji disable-user  USERNAME   To enable a user, use the command:  $  osg-koji enable-user  USERNAME", 
            "title": "Disabling and Enabling a User"
        }, 
        {
            "location": "/infrastructure/madison-itb/", 
            "text": "pre em { color: red; font-weight: normal; font-style: normal; }\n.old { color: red; }\n.off { color: blue; }\n\n\n\n\nSoftware/Release Team ITB Site Design\n\n\nMadison ITB Machines\n\n\nAll physical hosts are located in 3370A in the VDT rack.\n\n\n\n\n\n\n\n\nHost\n\n\nPurpose\n\n\nOS\n\n\nCPU Model\n\n\nCPUs\n\n\nRAM\n\n\nStorage\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nitb-data1\n\n\nworker node\n\n\nSL 6.9\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as HDFS data node\n\n\n\n\n\n\nitb-data2\n\n\nworker node\n\n\nSL 6.9\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as HDFS data node\n\n\n\n\n\n\nitb-data3\n\n\nworker node\n\n\nSL 7.4\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as HDFS data node\n\n\n\n\n\n\nitb-data4\n\n\nworker node\n\n\nSL 6.9\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as XRootD data node\n\n\n\n\n\n\nitb-data5\n\n\nworker node\n\n\nSL 6.9\n\n\nXeon E3-1220 3.10GHz\n\n\n2 / 4\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as XRootD data node\n\n\n\n\n\n\nitb-data6\n\n\nworker node\n\n\nSL 7.4\n\n\nXeon E3-1220 3.10GHz\n\n\n2 / 4\n\n\n8 GB\n\n\n???\n\n\nplanned as XRootD data node\n\n\n\n\n\n\nitb-host-1\n\n\nKVM host\n\n\nSL 7.4\n\n\nXeon E5-2450 2.10GHz\n\n\n16 / 32\n\n\n64 GB\n\n\n1 TB \u00d7 4 (HW RAID 5)\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-ce1\n\n\nHTCondor-CE\n\n\nSL 6.9\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-ce2\n\n\nHTCondor-CE\n\n\nSL 6.9\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-cm\n\n\nHTCondor CM\n\n\nSL 7.4\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-glidein\n\n\nGlideinWMS VO frontend?\n\n\nSL 6.3\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-gums-rsv\n\n\nGUMS, RSV\n\n\nSL 6.3\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-hdfs-name1\n\n\n\u2014 (so far)\n\n\nSL ?\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-hdfs-name2\n\n\n\u2014 (so far)\n\n\nSL 6.3\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-se-hdfs\n\n\n\u2014 (so far)\n\n\nSL 6.3\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-se-xrootd\n\n\n\u2014 (so far)\n\n\nSL 6.3\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-submit\n\n\nHTCondor submit\n\n\nSL 6.9\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-xrootd\n\n\n\u2014 (so far)\n\n\nSL ?\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\nitb-host-2\n\n\nworker node\n\n\nSL 6.9\n\n\nXeon E5-2450 2.10GHz\n\n\n16 / 32\n\n\n64 GB\n\n\n352 GB in $(EXECUTE)\n\n\n\n\n\n\n\n\nitb-host-3\n\n\nworker node\n\n\nSL 7.4\n\n\nXeon E5-2450 2.10GHz\n\n\n16 / 32\n\n\n64 GB\n\n\n352 GB in $(EXECUTE)\n\n\n\n\n\n\n\n\n\n\n(Data last updated 2017-10-13 by Tim\u00a0C. \nRed\n indicates a host that has yet to be rebuilt; \nBlue\n is rebuilt but currently off.)\n\n\nITB Goals\n\n\nGoals for the Madison ITB site are now maintained in \na Google document\n.\n\n\nConfiguration\n\n\nBasic host configuration is handled by Ansible and a local Git repository of playbooks.\n\n\nGit Repository\n\n\nThe authoritative Git repository for Madison ITB configuration is \ngitolite@git.chtc.wisc.edu:osgitb\n.  Clone the\nrepository and push validated changes back to it.\n\n\nAnsible\n\n\nThe \nosghost\n machine has Ansible 2.3.1.0 installed via RPM.  Use other hosts and versions at your own risk.\n\n\nCommon Ansible commands\n\n\nNote:\n\n\n\n\nFor critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person\n\n\nAll commands below are meant to be run from the top directory of your \nosgitb\n Git repo (e.g. on \nosghost\n,\n  not on the target machine)\n\n\n\n\nTo run Ansible for the first time for a new machine (using the \nroot\n password for the target machine when prompted):\n\n\nansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN\n\n\nansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN\n\n\n\n\n\n\nThe \nHOST-PATTERN\n can be a glob-like pattern or a regular expression that matches host names in the inventory file; see\nAnsible documentation for details.\n\n\nAfter initial successful runs of both playbooks, subsequent runs should replace the \n-u root -k\n part with \n-bK\n to use\nyour own login and \nsudo\n \non the target machine\n.  For example:\n\n\nansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN\n\n\nansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ]\n\n\n\n\n\n\nOmit the \n-l\n option to apply configuration to all hosts.\n\n\nIf you have your own playbook to manage personal configuration, run it as follows:\n\n\nansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ]\n\n\n\n\n\n\nAdding host and other certificates\n\n\n(This is in very rough form, but the key bits are here.)\n\n\n\n\nAsk Mat to get new certificates\u00a0\u2014 be sure to think about \nhttp\n, \nrsv\n, and other service certificates\n\n\nWait for Mat to tell you that the new certificates are in \n/p/condor/home/certificates\n\n\nscp -p\n the certificate(s) (\n*cert.pem*\n and \n*key.pem\n) to your home directory on \nosghost\n or whatever machine you use for Ansible\n   Note that if you are renewing a certificate, only the \n*cert.pem\n will be updated and the \n*key.pem\n will remain the same.\n\n\nFind the corresponding certificate location(s) in the Ansible \nroles/certs/files\n directory\n\n\ncp -p\n the certificate files over the top of the existing Ansible ones (or create new, equivalent paths)\n\n\nRun \nansible-vault encrypt FILE(S)\n to encrypt the files\u00a0\u2014 get the Ansible vault password from Tim\u00a0C. if you need it\n   Note that only the \n*key.pem\n files need to be encrypted, as the \n*cert.pem\n files are meant to be public.\n   If the (unencrypted) \n*key.pem\n file is not getting updated, there is no need to re-encrypt a new copy.\n\n\nVerify permissions, contents (you can \ncat\n the encrypted files), etc.\n\n\nApply the files with something like \nansible-playbook secure.yml -i inventory -bK -f 20 -t certs\n\n\nCommit changes (now or after applying)\n\n\nPush changes to origin (\ngitolite@git.chtc.wisc.edu:osgitb\n)\n\n\n\n\nDoing yum updates\n\n\n\n\n\n\nCheck to see if updates are needed and, if so, what would be updated:\n\n\nansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a \nyum check-update\n\n\n\n\n\n\nYou can name a single \nHOST\n or an inventory \nGROUP\n (such as the handy \ncurrent\n group); with a group, you can\nfurther restrict the hosts with a \n-l\n option.\n\n\nNote:\n \nyum check-update\n exits with status code \n100\n when it succeeds in identifying packages to update;\ntherefore Ansible shows such results as failures.\n\n\n\n\n\n\nReview the package lists to be updated and decide whether to proceed with all updates or limited ones\n\n\n\n\n\n\nDo updates:\n\n\nansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a \nyum --assumeyes update\n [ -l LIMITS ]\n\n\n\n\n\n\n\n\n\n\nIf needed (and if unsure, ask a sysadmin), reboot machines:\n\n\nansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a \n/sbin/shutdown -r +1\n [ -l LIMITS ]\n\n\n\n\n\n\n\n\n\n\nUpdating HTCondor from Upcoming\n\n\nSomething like this:\n\n\nansible condordev -i inventory -bK -f 10 -m command -a \nyum --enablerepo=osg-upcoming --assumeyes update condor\n\n\n\n\n\n\nMonitoring\n\n\nHTCondor-CE View\n\n\nOnce we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available\n\nhere\n through HTCondor-CE View.\n\n\nTracking payload jobs via Kibana\n\n\nAt this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via \nKibana\n. To view all payload jobs that have run on our ITB site in the past week, use \nthis query\n.\n\n\nMaking a New Virtual Machine on itb-host-1\n\n\nFor this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins.\nIf you do not have an account, request one from the CHTC system administrators.\n\n\n\n\n\n\nIf this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure\n\n\n\n\nPick a MAC address, starting with \n00:16:3e:\n followed by three random octets (e.g., \n00:16:3e:f7:29:ee\n)\n\n\nEmail \n with a request for a new OSG ITB VM, including the chosen MAC address\n\n\nWait to receive the associated IP address for the new host\n\n\n\n\n\n\n\n\nCreate or edit the Cobbler system object for the host\n\n\n\n\nAccess \nhttps://cobbler-widmir.chtc.wisc.edu/cobbler_web\n\n\nIn the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link\n\n\nIf desired, filter (at the bottom) by \u201cname\u201d on something like \nitb-*.chtc.wisc.edu\n\n\nFor a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d\n\n\nFor a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry\n\n\nIn the \nfirst\n \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific_6_8_osg_vm\u201d or \u201cScientific_7_2_osg_vm\u201d\n\n\nIn the \nsecond\n \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox\n\n\nIn the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine\n\n\nIn the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host\n\n\nClick the \u201cSave\u201d button\n\n\nIn the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link\n\n\n\n\n\n\n\n\nLog in to \nitb-host-1\n and become \nroot\n\n\n\n\n\n\nCreate the libvirt definition file\n\n\n\n\nCreate a new XML file named after the desired hostname (e.g., \nitb-ce2.xml\n) and copy in the template below\n\n\nReplace \n{{ HOSTNAME }}\n with the fully qualified hostname of the new virtual host\n\n\nReplace \n{{ MAC_ADDRESS }}\n with the MAC address of the new virtual host (from above)\n\n\nIf desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed\n\n\nSave the XML file\n\n\n\n\nCreate a new, empty disk image for the virtual host, in its correct location (as specified in the XML file):\n\n\ntruncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd\n\n\nchown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd\n\n\n\n\n\n\n\n\n\n\nLoad the new host definition into libvirt:\n\n\nvirsh define XML-FILE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the new machine\n\n\n\n\n\n\nStart the virtual machine:\n\n\nvirsh start HOSTNAME\n\n\n\n\n\n\nAt this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS,\nthen reboot the now-installed machine.  The whole process typically takes 15\u201320 minutes.  You may be able to\n\nssh\n into the machine during the install process, but there is no need to monitor or interfere.\n\n\n\n\n\n\nOnce the machine is available (which you can only guess at), \nssh\n in and verify that the machine basically works\n\n\n\n\nImmediately run Ansible on the machine, first with the \nsecure.yml\n playbook, then the \nsite.yml\n one (see above)\n\n\nLog in to the machine and look around to make sure it seems OK\n\n\nWhen things look good, tell virsh to start the virtual machine when the host itself starts:\nvirsh autostart HOSTNAME\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLibvirt VM Template\n\n\ndomain\n \ntype=\nkvm\n\n\n  \nname\n{{ HOSTNAME }}\n/name\n\n\n  \nmemory\n \nunit=\nGiB\n6\n/memory\n\n  \nvcpu\n4\n/vcpu\n\n  \nos\n\n    \ntype\nhvm\n/type\n\n    \nboot\n \ndev=\nnetwork\n/\n\n    \nboot\n \ndev=\nhd\n/\n\n    \nbios\n \nuseserial=\nyes\n \nrebootTimeout=\n0\n/\n\n  \n/os\n\n  \nfeatures\n\n    \nacpi/\n\n    \napic/\n\n    \npae/\n\n  \n/features\n\n\n  \ndevices\n\n\n    \nemulator\n/usr/libexec/qemu-kvm\n/emulator\n\n\n    \ndisk\n \ntype=\nfile\n \ndevice=\ndisk\n\n      \nsource\n \nfile=\n/var/lib/libvirt/images/{{ HOSTNAME }}.dd\n/\n\n      \ntarget\n \ndev=\nvda\n \nbus=\nvirtio\n/\n\n    \n/disk\n\n\n    \ninterface\n \ntype=\nbridge\n\n      \nmac\n \naddress=\n{{ MAC_ADDRESS }}\n/\n\n      \nsource\n \nbridge=\nbr0\n/\n\n      \nmodel\n \ntype=\nvirtio\n/\n\n    \n/interface\n\n\n    \nserial\n \ntype=\npty\n\n      \ntarget\n \nport=\n0\n/\n\n    \n/serial\n\n    \nconsole\n \ntype=\npty\n\n      \ntarget\n \ntype=\nserial\n \nport=\n0\n/\n\n    \n/console\n\n\n    \ngraphics\n \ntype=\nvnc\n \nautoport=\nyes\n \nlisten=\n127.0.0.1\n/\n\n\n  \n/devices\n\n\n/domain", 
            "title": "Madison ITB"
        }, 
        {
            "location": "/infrastructure/madison-itb/#softwarerelease-team-itb-site-design", 
            "text": "", 
            "title": "Software/Release Team ITB Site Design"
        }, 
        {
            "location": "/infrastructure/madison-itb/#madison-itb-machines", 
            "text": "All physical hosts are located in 3370A in the VDT rack.     Host  Purpose  OS  CPU Model  CPUs  RAM  Storage  Notes      itb-data1  worker node  SL 6.9  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as HDFS data node    itb-data2  worker node  SL 6.9  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as HDFS data node    itb-data3  worker node  SL 7.4  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as HDFS data node    itb-data4  worker node  SL 6.9  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as XRootD data node    itb-data5  worker node  SL 6.9  Xeon E3-1220 3.10GHz  2 / 4  8 GB  750 GB \u00d7 2 (RAID?)  planned as XRootD data node    itb-data6  worker node  SL 7.4  Xeon E3-1220 3.10GHz  2 / 4  8 GB  ???  planned as XRootD data node    itb-host-1  KVM host  SL 7.4  Xeon E5-2450 2.10GHz  16 / 32  64 GB  1 TB \u00d7 4 (HW RAID 5)     \u00b7\u00a0 itb-ce1  HTCondor-CE  SL 6.9  VM  4  6 GB  192 GB     \u00b7\u00a0 itb-ce2  HTCondor-CE  SL 6.9  VM  4  6 GB  192 GB     \u00b7\u00a0 itb-cm  HTCondor CM  SL 7.4  VM  4  6 GB  192 GB     \u00b7\u00a0  itb-glidein  GlideinWMS VO frontend?  SL 6.3  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-gums-rsv  GUMS, RSV  SL 6.3  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-hdfs-name1  \u2014 (so far)  SL ?  VM  4  6 GB  192 GB     \u00b7\u00a0  itb-hdfs-name2  \u2014 (so far)  SL 6.3  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-se-hdfs  \u2014 (so far)  SL 6.3  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-se-xrootd  \u2014 (so far)  SL 6.3  VM  3  6 GB  50 GB     \u00b7\u00a0 itb-submit  HTCondor submit  SL 6.9  VM  4  6 GB  192 GB     \u00b7\u00a0  itb-xrootd  \u2014 (so far)  SL ?  VM  4  6 GB  192 GB     itb-host-2  worker node  SL 6.9  Xeon E5-2450 2.10GHz  16 / 32  64 GB  352 GB in $(EXECUTE)     itb-host-3  worker node  SL 7.4  Xeon E5-2450 2.10GHz  16 / 32  64 GB  352 GB in $(EXECUTE)      (Data last updated 2017-10-13 by Tim\u00a0C.  Red  indicates a host that has yet to be rebuilt;  Blue  is rebuilt but currently off.)", 
            "title": "Madison ITB Machines"
        }, 
        {
            "location": "/infrastructure/madison-itb/#itb-goals", 
            "text": "Goals for the Madison ITB site are now maintained in  a Google document .", 
            "title": "ITB Goals"
        }, 
        {
            "location": "/infrastructure/madison-itb/#configuration", 
            "text": "Basic host configuration is handled by Ansible and a local Git repository of playbooks.", 
            "title": "Configuration"
        }, 
        {
            "location": "/infrastructure/madison-itb/#git-repository", 
            "text": "The authoritative Git repository for Madison ITB configuration is  gitolite@git.chtc.wisc.edu:osgitb .  Clone the\nrepository and push validated changes back to it.", 
            "title": "Git Repository"
        }, 
        {
            "location": "/infrastructure/madison-itb/#ansible", 
            "text": "The  osghost  machine has Ansible 2.3.1.0 installed via RPM.  Use other hosts and versions at your own risk.", 
            "title": "Ansible"
        }, 
        {
            "location": "/infrastructure/madison-itb/#common-ansible-commands", 
            "text": "Note:   For critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person  All commands below are meant to be run from the top directory of your  osgitb  Git repo (e.g. on  osghost ,\n  not on the target machine)   To run Ansible for the first time for a new machine (using the  root  password for the target machine when prompted):  ansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN  ansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN   The  HOST-PATTERN  can be a glob-like pattern or a regular expression that matches host names in the inventory file; see\nAnsible documentation for details.  After initial successful runs of both playbooks, subsequent runs should replace the  -u root -k  part with  -bK  to use\nyour own login and  sudo   on the target machine .  For example:  ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN  ansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ]   Omit the  -l  option to apply configuration to all hosts.  If you have your own playbook to manage personal configuration, run it as follows:  ansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ]", 
            "title": "Common Ansible commands"
        }, 
        {
            "location": "/infrastructure/madison-itb/#adding-host-and-other-certificates", 
            "text": "(This is in very rough form, but the key bits are here.)   Ask Mat to get new certificates\u00a0\u2014 be sure to think about  http ,  rsv , and other service certificates  Wait for Mat to tell you that the new certificates are in  /p/condor/home/certificates  scp -p  the certificate(s) ( *cert.pem*  and  *key.pem ) to your home directory on  osghost  or whatever machine you use for Ansible\n   Note that if you are renewing a certificate, only the  *cert.pem  will be updated and the  *key.pem  will remain the same.  Find the corresponding certificate location(s) in the Ansible  roles/certs/files  directory  cp -p  the certificate files over the top of the existing Ansible ones (or create new, equivalent paths)  Run  ansible-vault encrypt FILE(S)  to encrypt the files\u00a0\u2014 get the Ansible vault password from Tim\u00a0C. if you need it\n   Note that only the  *key.pem  files need to be encrypted, as the  *cert.pem  files are meant to be public.\n   If the (unencrypted)  *key.pem  file is not getting updated, there is no need to re-encrypt a new copy.  Verify permissions, contents (you can  cat  the encrypted files), etc.  Apply the files with something like  ansible-playbook secure.yml -i inventory -bK -f 20 -t certs  Commit changes (now or after applying)  Push changes to origin ( gitolite@git.chtc.wisc.edu:osgitb )", 
            "title": "Adding host and other certificates"
        }, 
        {
            "location": "/infrastructure/madison-itb/#doing-yum-updates", 
            "text": "Check to see if updates are needed and, if so, what would be updated:  ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a  yum check-update   You can name a single  HOST  or an inventory  GROUP  (such as the handy  current  group); with a group, you can\nfurther restrict the hosts with a  -l  option.  Note:   yum check-update  exits with status code  100  when it succeeds in identifying packages to update;\ntherefore Ansible shows such results as failures.    Review the package lists to be updated and decide whether to proceed with all updates or limited ones    Do updates:  ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a  yum --assumeyes update  [ -l LIMITS ]     If needed (and if unsure, ask a sysadmin), reboot machines:  ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a  /sbin/shutdown -r +1  [ -l LIMITS ]", 
            "title": "Doing yum updates"
        }, 
        {
            "location": "/infrastructure/madison-itb/#updating-htcondor-from-upcoming", 
            "text": "Something like this:  ansible condordev -i inventory -bK -f 10 -m command -a  yum --enablerepo=osg-upcoming --assumeyes update condor", 
            "title": "Updating HTCondor from Upcoming"
        }, 
        {
            "location": "/infrastructure/madison-itb/#monitoring", 
            "text": "", 
            "title": "Monitoring"
        }, 
        {
            "location": "/infrastructure/madison-itb/#htcondor-ce-view", 
            "text": "Once we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available here  through HTCondor-CE View.", 
            "title": "HTCondor-CE View"
        }, 
        {
            "location": "/infrastructure/madison-itb/#tracking-payload-jobs-via-kibana", 
            "text": "At this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via  Kibana . To view all payload jobs that have run on our ITB site in the past week, use  this query .", 
            "title": "Tracking payload jobs via Kibana"
        }, 
        {
            "location": "/infrastructure/madison-itb/#making-a-new-virtual-machine-on-itb-host-1", 
            "text": "For this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins.\nIf you do not have an account, request one from the CHTC system administrators.    If this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure   Pick a MAC address, starting with  00:16:3e:  followed by three random octets (e.g.,  00:16:3e:f7:29:ee )  Email   with a request for a new OSG ITB VM, including the chosen MAC address  Wait to receive the associated IP address for the new host     Create or edit the Cobbler system object for the host   Access  https://cobbler-widmir.chtc.wisc.edu/cobbler_web  In the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link  If desired, filter (at the bottom) by \u201cname\u201d on something like  itb-*.chtc.wisc.edu  For a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d  For a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry  In the  first  \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific_6_8_osg_vm\u201d or \u201cScientific_7_2_osg_vm\u201d  In the  second  \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox  In the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine  In the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host  Click the \u201cSave\u201d button  In the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link     Log in to  itb-host-1  and become  root    Create the libvirt definition file   Create a new XML file named after the desired hostname (e.g.,  itb-ce2.xml ) and copy in the template below  Replace  {{ HOSTNAME }}  with the fully qualified hostname of the new virtual host  Replace  {{ MAC_ADDRESS }}  with the MAC address of the new virtual host (from above)  If desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed  Save the XML file   Create a new, empty disk image for the virtual host, in its correct location (as specified in the XML file):  truncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd  chown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd     Load the new host definition into libvirt:  virsh define XML-FILE       Install the new machine    Start the virtual machine:  virsh start HOSTNAME   At this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS,\nthen reboot the now-installed machine.  The whole process typically takes 15\u201320 minutes.  You may be able to ssh  into the machine during the install process, but there is no need to monitor or interfere.    Once the machine is available (which you can only guess at),  ssh  in and verify that the machine basically works   Immediately run Ansible on the machine, first with the  secure.yml  playbook, then the  site.yml  one (see above)  Log in to the machine and look around to make sure it seems OK  When things look good, tell virsh to start the virtual machine when the host itself starts: virsh autostart HOSTNAME", 
            "title": "Making a New Virtual Machine on itb-host-1"
        }, 
        {
            "location": "/infrastructure/madison-itb/#libvirt-vm-template", 
            "text": "domain   type= kvm \n\n   name {{ HOSTNAME }} /name \n\n   memory   unit= GiB 6 /memory \n   vcpu 4 /vcpu \n   os \n     type hvm /type \n     boot   dev= network / \n     boot   dev= hd / \n     bios   useserial= yes   rebootTimeout= 0 / \n   /os \n   features \n     acpi/ \n     apic/ \n     pae/ \n   /features \n\n   devices \n\n     emulator /usr/libexec/qemu-kvm /emulator \n\n     disk   type= file   device= disk \n       source   file= /var/lib/libvirt/images/{{ HOSTNAME }}.dd / \n       target   dev= vda   bus= virtio / \n     /disk \n\n     interface   type= bridge \n       mac   address= {{ MAC_ADDRESS }} / \n       source   bridge= br0 / \n       model   type= virtio / \n     /interface \n\n     serial   type= pty \n       target   port= 0 / \n     /serial \n     console   type= pty \n       target   type= serial   port= 0 / \n     /console \n\n     graphics   type= vnc   autoport= yes   listen= 127.0.0.1 / \n\n   /devices  /domain", 
            "title": "Libvirt VM Template"
        }, 
        {
            "location": "/documentation/writing-documentation/", 
            "text": "Writing OSG Software Documentation\n\n\nOSG software documentation is written in \nmarkdown\n, built using \nMkDocs\n, and served via \nGitHub Pages\n. To contribute documentation, submit a pull request to the relevant github repository:\n\n\n\n\nSite administrator documentation\n\n\nInternal OSG Technology Area documentation\n.\n\n\n\n\nThis document contains instructions, recommendations, and guidelines for writing OSG Software documentatation.\n\n\nDocument Layout\n\n\nThis section contains suggested layouts of externally-facing, site administrator documentation. The introduction is the only layout requirement for documents except for installation guides.\n\n\nIntroductions\n\n\nAll documents should start with an introduction that explains \nwhat\n the document contains, \nwhat\n the product does, and \nwhy\n someone may want to use it. In the past, document introductions were included in \nAbout this...\n sections due to the layout of the table of contents. Since the table of contents is included in the sidebar, introductions should go directly below the title header.\n\n\nThe \nHTCondor-CE installation guide\n is an example that meet all of the above criteria.\n\n\nInstallation guides\n\n\nIn addition to the introduction above, installation documents should have the following sections:\n\n\n\n\nBefore Starting:\n This section should contain information for any prepatory work that the site administrator should do or consider before proceeding with the installation (\nexample\n).\n\n\nInstallation:\n The (\nexample\n)\n\n\nValidation:\n How does the user make sure their installation is functional?\n\n\nHelp:\n Often just a link to the relevant \nhelp document\n as well as contact information for specific support groups, if applicable.\n\n\n\n\nOptionally, the following sections should be included as necessary.\n\n\n\n\nOverview:\n if the introduction becomes large and unwieldy, extract the details of \nwhat\n the product does into an overview section\n\n\nConfiguration:\n required configuration steps (\nexample\n) as well as a sub-section for optional configurations. For long optional configuration sections, consider creating a list of contents at the top of the sub-section (\nexample\n).\n\n\nTroubleshooting:\n common issues that users encounter and their fixes\n\n\nReference:\n Details about configuration and log files, unix users, certificates, networking, links to relevant upstream documentation, etc. (\nexample\n)\n\n\n\n\nIf any of the sections become too large, consider separating them out and linking to the new documents (\nexample\n).\n\n\nTips for Writing Procedural Instructions\n\n\n\n\n\n\nTitle the procedure with the user goal, usually starting with a gerund; e.g.:\n\n\nInstalling the Frobnosticator\n\n\n\n\n\n\nNumber all steps (as opposed to using bullets)\n\n\n\n\n\n\nList steps in order in which they are performed\n\n\n\n\n\n\nEach step should begin with a single-line instruction in plain English, in command form; e.g.:\n\n\n\n\nMake sure that the Frobnosticator configuration file is world-writable\n\n\n\n\n\n\n\n\nIf the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.:\n  \nchmod a+x /usr/share/frobnosticator/frob.conf\n\n\n\n\n\n\nPut clarifying information in separate paragraphs within the step\n\n\n\n\n\n\nPut critical information about the \nwhole\n procedure in one or more paragraphs before the numbered steps\n\n\n\n\n\n\nPut supplemental information about the \nwhole\n procedure in one or more paragraphs after the numbered steps\n\n\n\n\n\n\nAvoid pronouns when writing technical articles or documentation e.g., \ninstall foo\n rather than \ninstall it\n.\n\n\n\n\n\n\nContributing Documentation\n\n\nWe use the GitHub pull request model for accepting document contributions. To contribute to the OSG documentation, follow these steps:\n\n\n\n\nFork and clone\n the GitHub repository and choose one of the following:\n\n\nIf you are making changes to an existing document, make changes in your local clone and push them to your fork.\n\n\nIf you are contributing a new document:\n\n\nName the document. Document file names should be lowercase, \n-\n delimited, and concise but descriptive, e.g. \nmarkdown-migration.md\n or \ncutting-release.md\n\n\nPlace the document in the relevant sub-folder of the \ndocs/\n directory. If you are unsure of the appropriate location, note that in the description of the pull request.\n\n\nAdd the document to the \npages:\n section of \nmkdocs.yml\n in \ntitle case\n, e.g. \n- Migrating Documents to Markdown: 'software/markdown-migration.md'\n\n\n\n\n\n\n\n\n\n\nSubmit your changes as a pull request to the \nappropriate upstream repository\n\n\n\n\nDeploying ITB documentation\n\n\nIf you are a member of the OSG software and release team, you can preview large changes to the \nITB docs\n by pushing a branch that starts with an \nitb.\n prefix to the \nopensciencegrid/docs\n repo. For example:\n\n\n$\n git remote add upstream https://github.com/opensciencegrid/docs.git\n\n$\n git checkout new_docs\n\n$\n git push upstream new_docs:itb.new_docs\n\n\n\n\n\n\n\nNote\n\n\nSince there is only one ITB docs area, simultaneous new commits to different \nitb.*\n branches will overwrite each other's changes. To re-deploy your changes, find your \nTravis-CI build\n and restart it \nBUT\n coordinate with the author of the other commits to avoid conflicts.", 
            "title": "Writing Documentation"
        }, 
        {
            "location": "/documentation/writing-documentation/#writing-osg-software-documentation", 
            "text": "OSG software documentation is written in  markdown , built using  MkDocs , and served via  GitHub Pages . To contribute documentation, submit a pull request to the relevant github repository:   Site administrator documentation  Internal OSG Technology Area documentation .   This document contains instructions, recommendations, and guidelines for writing OSG Software documentatation.", 
            "title": "Writing OSG Software Documentation"
        }, 
        {
            "location": "/documentation/writing-documentation/#document-layout", 
            "text": "This section contains suggested layouts of externally-facing, site administrator documentation. The introduction is the only layout requirement for documents except for installation guides.", 
            "title": "Document Layout"
        }, 
        {
            "location": "/documentation/writing-documentation/#introductions", 
            "text": "All documents should start with an introduction that explains  what  the document contains,  what  the product does, and  why  someone may want to use it. In the past, document introductions were included in  About this...  sections due to the layout of the table of contents. Since the table of contents is included in the sidebar, introductions should go directly below the title header.  The  HTCondor-CE installation guide  is an example that meet all of the above criteria.", 
            "title": "Introductions"
        }, 
        {
            "location": "/documentation/writing-documentation/#installation-guides", 
            "text": "In addition to the introduction above, installation documents should have the following sections:   Before Starting:  This section should contain information for any prepatory work that the site administrator should do or consider before proceeding with the installation ( example ).  Installation:  The ( example )  Validation:  How does the user make sure their installation is functional?  Help:  Often just a link to the relevant  help document  as well as contact information for specific support groups, if applicable.   Optionally, the following sections should be included as necessary.   Overview:  if the introduction becomes large and unwieldy, extract the details of  what  the product does into an overview section  Configuration:  required configuration steps ( example ) as well as a sub-section for optional configurations. For long optional configuration sections, consider creating a list of contents at the top of the sub-section ( example ).  Troubleshooting:  common issues that users encounter and their fixes  Reference:  Details about configuration and log files, unix users, certificates, networking, links to relevant upstream documentation, etc. ( example )   If any of the sections become too large, consider separating them out and linking to the new documents ( example ).", 
            "title": "Installation guides"
        }, 
        {
            "location": "/documentation/writing-documentation/#tips-for-writing-procedural-instructions", 
            "text": "Title the procedure with the user goal, usually starting with a gerund; e.g.:  Installing the Frobnosticator    Number all steps (as opposed to using bullets)    List steps in order in which they are performed    Each step should begin with a single-line instruction in plain English, in command form; e.g.:   Make sure that the Frobnosticator configuration file is world-writable     If the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.:\n   chmod a+x /usr/share/frobnosticator/frob.conf    Put clarifying information in separate paragraphs within the step    Put critical information about the  whole  procedure in one or more paragraphs before the numbered steps    Put supplemental information about the  whole  procedure in one or more paragraphs after the numbered steps    Avoid pronouns when writing technical articles or documentation e.g.,  install foo  rather than  install it .", 
            "title": "Tips for Writing Procedural Instructions"
        }, 
        {
            "location": "/documentation/writing-documentation/#contributing-documentation", 
            "text": "We use the GitHub pull request model for accepting document contributions. To contribute to the OSG documentation, follow these steps:   Fork and clone  the GitHub repository and choose one of the following:  If you are making changes to an existing document, make changes in your local clone and push them to your fork.  If you are contributing a new document:  Name the document. Document file names should be lowercase,  -  delimited, and concise but descriptive, e.g.  markdown-migration.md  or  cutting-release.md  Place the document in the relevant sub-folder of the  docs/  directory. If you are unsure of the appropriate location, note that in the description of the pull request.  Add the document to the  pages:  section of  mkdocs.yml  in  title case , e.g.  - Migrating Documents to Markdown: 'software/markdown-migration.md'      Submit your changes as a pull request to the  appropriate upstream repository", 
            "title": "Contributing Documentation"
        }, 
        {
            "location": "/documentation/writing-documentation/#deploying-itb-documentation", 
            "text": "If you are a member of the OSG software and release team, you can preview large changes to the  ITB docs  by pushing a branch that starts with an  itb.  prefix to the  opensciencegrid/docs  repo. For example:  $  git remote add upstream https://github.com/opensciencegrid/docs.git $  git checkout new_docs $  git push upstream new_docs:itb.new_docs   Note  Since there is only one ITB docs area, simultaneous new commits to different  itb.*  branches will overwrite each other's changes. To re-deploy your changes, find your  Travis-CI build  and restart it  BUT  coordinate with the author of the other commits to avoid conflicts.", 
            "title": "Deploying ITB documentation"
        }, 
        {
            "location": "/documentation/reviewing-documentation/", 
            "text": "Review OSG Software Documentation\n\n\nTo maintain quality documentation, we must regularly review our documentation for clarity and correctness. We differentiate between two types of reviews, content and editorial, which you can think of as for correctness and clarity, respectively. If you are reviewing a document as part of the release process, perform a content review.\n\n\nAfter completing a review, update the appropriate columns in the \ndocument tracking spreadsheet\n. Speak to Brian Lin for write access.\n\n\nContent Review\n\n\nContent reviews are for validating the correctness of technical steps and details. To perform a content review, follow the instructions of the document to a tee as if you were an OSG neophyte. Things to note and/or fix:\n\n\n\n\nAfter completing the instructions in the document:\n\n\nDoes the document inform you how to use the product?\n\n\nDoes the document tell you how to verify that the product is functioning?\n\n\nDoes the product work?\n\n\n\n\n\n\nIncorrect or out of date instructions\n\n\nSteps that may be particularly conducive to software or default configuration as a solution\n\n\nLack of clarity or any other confusion you may have\n\n\n\n\nEditorial Review\n\n\nEditorial reviews are for ensuring docs meet our \nstyle\n and \nlayout\n guidelines; improving readability; and proofing spelling, grammar, and punctuation.", 
            "title": "Reviewing Documentation"
        }, 
        {
            "location": "/documentation/reviewing-documentation/#review-osg-software-documentation", 
            "text": "To maintain quality documentation, we must regularly review our documentation for clarity and correctness. We differentiate between two types of reviews, content and editorial, which you can think of as for correctness and clarity, respectively. If you are reviewing a document as part of the release process, perform a content review.  After completing a review, update the appropriate columns in the  document tracking spreadsheet . Speak to Brian Lin for write access.", 
            "title": "Review OSG Software Documentation"
        }, 
        {
            "location": "/documentation/reviewing-documentation/#content-review", 
            "text": "Content reviews are for validating the correctness of technical steps and details. To perform a content review, follow the instructions of the document to a tee as if you were an OSG neophyte. Things to note and/or fix:   After completing the instructions in the document:  Does the document inform you how to use the product?  Does the document tell you how to verify that the product is functioning?  Does the product work?    Incorrect or out of date instructions  Steps that may be particularly conducive to software or default configuration as a solution  Lack of clarity or any other confusion you may have", 
            "title": "Content Review"
        }, 
        {
            "location": "/documentation/reviewing-documentation/#editorial-review", 
            "text": "Editorial reviews are for ensuring docs meet our  style  and  layout  guidelines; improving readability; and proofing spelling, grammar, and punctuation.", 
            "title": "Editorial Review"
        }, 
        {
            "location": "/documentation/style-guide/", 
            "text": "Markdown Style Guide\n\n\nThis document contains markdown conventions that are used in OSG Software documentation.\n\n\nMeta\n\n\nWherever possible, prose should be limited to 120 characters wide.\nIn addition, using one line for each sentence is recommended since it makes update diffs easier to review.\n\n\nHeadings\n\n\nUse the following conventions for headings:\n\n\n\n\nThe title should be the only level 1 heading\n\n\nLevel 1 headings should use the \n====\n format\n\n\nLevel 2 headings should use the \n----\n format\n\n\nUse Title Case for level 1 and level 2 headings. Only capitalize the first word for all other headings.\n\n\nOther heading levels should use the appropriate number of \n#\n\n\nGo no deeper than of level 5 headings\n\n\nSpin-off a new document or re-organize the existing document if you find that you regularly need level 5 headings.\n\n\n\n\nLinks\n\n\nUse site-relative (\n/software/development-process\n) instead of document-relative (\n../software/development-process.md\n)\nlinks. This will allow us to easily search for links and move documents around in the future.\n\n\nSection links\n\n\nTo link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple\nsections with the same name you can link the subsequent sections by appending \n_N\n where \nN\n is the section's ordinal\nnumber minus one, e.g. append \n_1\n for the second section. For example, if you have three sections named \"Optional\nConfiguration\", link them like so:\n\n\n[1st section](#optional-configuration)\n[2nd section](#optional-configuration_1)\n[3rd section](#optional-configuration_2)\n\n\n\n\n\nCommand blocks and file snippets\n\n\nCommand blocks and file snippets outside of lists should be wrapped in three back-ticks (```) followed by an optional\ncode highlighting format:\n\n\n```console\n# stuff\n```\n\n\n\n\n\nFor command blocks and file snippets that inside of a list should use the appropriate number of spaces before three\ncolons followed by an optional code highlighting format:\n\n\n::: console\n\n\n#\n stuff\n\n\n\n\n\nSee the \nlists section\n for details on properly formatting command blocks within a list.\n\n\nWe use the \nPygments\n highlighting library for syntax; it knows about 100 different languages.\nThe Pygments website contains a live renderer if you want to see how your text will come out.  Please use the \nconsole\n\nlanguage for shell sessions.\n\n\nRoot and user prompts\n\n\nWhen specifying instructions for the command-line, indicate to users whether the commands can be run as root \n(\nroot@host #\n) or as an unprivileged user (\nuser@host $\n).\n\n\nFor example:\n\n\nroot@host #\n useradd -m osguser\n\nroot@host #\n su - osguser\n\nuser@host $\n whoami\n\nosguser\n\n\n\n\n\n\nIt can provide helpful context to use a more specific hostname in the prompt than \nhost\n.\nFor example, if you're writing a doc for setting up a Storage Element and a command is run as root on the SE, use \nroot@se #\n.\nOr if you're testing the SE from the client side and the command is run as a normal user on a client, use \nuser@client $\n.\n\n\nHighlighting user input\n\n\nUse descriptive, all-caps text wrapped in angle brackets to to highlight areas that users would have to insert text\nspecific to their site. You may also use TWiki-style color highlighting. For example:\n\n\nroot@host #\n condor_ce_trace -d \nCE HOSTNAME\n\n\n\n\n\n\nLists\n\n\nWhen constructing lists, use the following guidelines:\n\n\n\n\nUse \n1.\n for each item in a numbered list\n\n\nTo make sure the contents of code blocks, file snippets, and subsequent paragraphs are indented properly, use the\n  following formatting:\n\n\nFor code blocks or file snippets, add an empty line after any regular text, then insert \n(N+1)*4\n spaces at the\n  beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code\n  block with \n:::\nFORMAT\n; see \nthis page\n for\n  details, including possible highlighting formats.  For an example of formatting a code section inside a list, see\n  \nthe release series document\n.\n\n\nFor additional text (i.e. after a code block), insert \nN*4\n spaces at the beginning of each line, where N is the\n  level of the item in the list.\n\n\n\n\n\n\n\n\nFor example:\n\n\n1. Foo\n    - Bar\n\n            :::console\n            COMMAND\n            BLOCK\n        text associated with Bar\n\n    text associated with Foo\n\n1. Baz\n\n        FILE\n        SNIPPET\n\n\n\n\n\nThere are 12 spaces and 8 spaces in front of the command block and text associated with \nBar\n, respectively; 4 spaces in\nfront of the text associated with \nFoo\n; and 8 spaces in front of the file snippet associated with \nBaz\n.  The above\nblock is rendered below:\n\n\n\n\n\n\nFoo\n\n\n\n\nBar\nCOMMAND\n\n\nBLOCK\n\n\n\n\n\n\ntext associated with Bar\n\n\n\n\n\n\ntext associated with Foo\n\n\n\n\n\n\nBaz\n\n\nFILE\nSNIPPET\n\n\n\n\n\n\n\n\n\nNotes\n\n\nTo catch the user's attention for important items or pitfalls, we used \n%NOTE%\n TWiki macros, these can be replaced with\nadmonition-style notes and warnings:\n\n\n!!! note\n    things to note\n\n\n\n\n\nor\n\n\n!!! warning\n    if a user doesn\nt do this thing, bad stuff will happen\n\n\n\n\n\nThe above blocks are rendered below as an example.\n\n\n!!! note things to note\n\n\nand\n\n\n!!! warning if a user doesn't do this thing, bad stuff will happen\n\n\nFor a full list of admonition styles, see the documentation\n\nhere\n.", 
            "title": "Markdown Style Guide"
        }, 
        {
            "location": "/documentation/style-guide/#markdown-style-guide", 
            "text": "This document contains markdown conventions that are used in OSG Software documentation.", 
            "title": "Markdown Style Guide"
        }, 
        {
            "location": "/documentation/style-guide/#meta", 
            "text": "Wherever possible, prose should be limited to 120 characters wide.\nIn addition, using one line for each sentence is recommended since it makes update diffs easier to review.", 
            "title": "Meta"
        }, 
        {
            "location": "/documentation/style-guide/#headings", 
            "text": "Use the following conventions for headings:   The title should be the only level 1 heading  Level 1 headings should use the  ====  format  Level 2 headings should use the  ----  format  Use Title Case for level 1 and level 2 headings. Only capitalize the first word for all other headings.  Other heading levels should use the appropriate number of  #  Go no deeper than of level 5 headings  Spin-off a new document or re-organize the existing document if you find that you regularly need level 5 headings.", 
            "title": "Headings"
        }, 
        {
            "location": "/documentation/style-guide/#links", 
            "text": "Use site-relative ( /software/development-process ) instead of document-relative ( ../software/development-process.md )\nlinks. This will allow us to easily search for links and move documents around in the future.", 
            "title": "Links"
        }, 
        {
            "location": "/documentation/style-guide/#section-links", 
            "text": "To link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple\nsections with the same name you can link the subsequent sections by appending  _N  where  N  is the section's ordinal\nnumber minus one, e.g. append  _1  for the second section. For example, if you have three sections named \"Optional\nConfiguration\", link them like so:  [1st section](#optional-configuration)\n[2nd section](#optional-configuration_1)\n[3rd section](#optional-configuration_2)", 
            "title": "Section links"
        }, 
        {
            "location": "/documentation/style-guide/#command-blocks-and-file-snippets", 
            "text": "Command blocks and file snippets outside of lists should be wrapped in three back-ticks (```) followed by an optional\ncode highlighting format:  ```console\n# stuff\n```  For command blocks and file snippets that inside of a list should use the appropriate number of spaces before three\ncolons followed by an optional code highlighting format:  ::: console  #  stuff  See the  lists section  for details on properly formatting command blocks within a list.  We use the  Pygments  highlighting library for syntax; it knows about 100 different languages.\nThe Pygments website contains a live renderer if you want to see how your text will come out.  Please use the  console \nlanguage for shell sessions.", 
            "title": "Command blocks and file snippets"
        }, 
        {
            "location": "/documentation/style-guide/#root-and-user-prompts", 
            "text": "When specifying instructions for the command-line, indicate to users whether the commands can be run as root \n( root@host # ) or as an unprivileged user ( user@host $ ).  For example:  root@host #  useradd -m osguser root@host #  su - osguser user@host $  whoami osguser   It can provide helpful context to use a more specific hostname in the prompt than  host .\nFor example, if you're writing a doc for setting up a Storage Element and a command is run as root on the SE, use  root@se # .\nOr if you're testing the SE from the client side and the command is run as a normal user on a client, use  user@client $ .", 
            "title": "Root and user prompts"
        }, 
        {
            "location": "/documentation/style-guide/#highlighting-user-input", 
            "text": "Use descriptive, all-caps text wrapped in angle brackets to to highlight areas that users would have to insert text\nspecific to their site. You may also use TWiki-style color highlighting. For example:  root@host #  condor_ce_trace -d  CE HOSTNAME", 
            "title": "Highlighting user input"
        }, 
        {
            "location": "/documentation/style-guide/#lists", 
            "text": "When constructing lists, use the following guidelines:   Use  1.  for each item in a numbered list  To make sure the contents of code blocks, file snippets, and subsequent paragraphs are indented properly, use the\n  following formatting:  For code blocks or file snippets, add an empty line after any regular text, then insert  (N+1)*4  spaces at the\n  beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code\n  block with  ::: FORMAT ; see  this page  for\n  details, including possible highlighting formats.  For an example of formatting a code section inside a list, see\n   the release series document .  For additional text (i.e. after a code block), insert  N*4  spaces at the beginning of each line, where N is the\n  level of the item in the list.     For example:  1. Foo\n    - Bar\n\n            :::console\n            COMMAND\n            BLOCK\n        text associated with Bar\n\n    text associated with Foo\n\n1. Baz\n\n        FILE\n        SNIPPET  There are 12 spaces and 8 spaces in front of the command block and text associated with  Bar , respectively; 4 spaces in\nfront of the text associated with  Foo ; and 8 spaces in front of the file snippet associated with  Baz .  The above\nblock is rendered below:    Foo   Bar COMMAND  BLOCK   text associated with Bar    text associated with Foo    Baz  FILE\nSNIPPET", 
            "title": "Lists"
        }, 
        {
            "location": "/documentation/style-guide/#notes", 
            "text": "To catch the user's attention for important items or pitfalls, we used  %NOTE%  TWiki macros, these can be replaced with\nadmonition-style notes and warnings:  !!! note\n    things to note  or  !!! warning\n    if a user doesn t do this thing, bad stuff will happen  The above blocks are rendered below as an example.  !!! note things to note  and  !!! warning if a user doesn't do this thing, bad stuff will happen  For a full list of admonition styles, see the documentation here .", 
            "title": "Notes"
        }, 
        {
            "location": "/documentation/new-doc-area/", 
            "text": "Creating a New Area\n\n\nThis document contains instructions for creating a new OSG documentation area via \nGitHub Pages\n and deploying it automatically with \nTravis-CI\n. Before starting, make sure that you have the \ngit\n and \ngem\n tools installed.\n\n\n\n\n\n\nCreate a new repository in the \nopensciencegrid organization\n (referred to as \nREPO NAME\n in the rest of this document)\n\n\n\n\nCheck the box marked \nInitialize this repository with a README\n\n\nOnce created, add the \ndocumentation\n topic by clicking on the \"Add topics\" button\n\n\n\n\n\n\n\n\nClone the repository and \ncd\n into the directory:\n\n\ngit clone git@github.com:opensciencegrid/\nREPO NAME\n\ncd \nREPO NAME\n\n\n\n\n\n\n\n\n\n\nCreate a \ngh-pages\n branch in the GitHub repository:\n\n\ngit push origin master:gh-pages\n\n\n\n\n\n\n\n\n\nUpdate the contents of \nREADME.md\n and populate the \nLICENSE\n file with a \nCreative Commons Attribution 4.0 license\n:\n\n\nwget https://creativecommons.org/licenses/by/4.0/legalcode.txt \n LICENSE\n\n\n\n\n\n\n\n\n\nFollow \nthese instructions\n to add the \ndoc-ci-scripts\n sub-module\n\n\n\n\nCreate \nmkdocs.yml\n and a \ndocs\n directory\n\n\n\n\nCreate and encrypt the repository deploy key\n\n\n\n\n\n\nGenerate the repository deploy key:\n\n\nssh-keygen -t rsa -b 4096 -C \nhelp@opensciencegrid.org\n -f deploy-key\n\n\n\n\n\n\n\n\n\nInstall the \ntravis\n gem:\n\n\ngem install travis\n\n\n\n\n\n\n\n\n\nEncrypt the deploy key:\n\n\ntravis encrypt-file deploy-key\n\n\n\n\n\n\n\n\n\nAdd and commit your files:\n\n\ngit add LICENSE mkdocs.yml docs deploy-key.enc\ngit commit -m \nPrepare the repository for Travis-CI deployment\n\n\n\n\n\n\n\n\nDanger\n\n\nDo NOT commit the unencrypted \ndeploy-key\n!\n\n\n\n\n\n\n\n\nAdd \ndeploy-key.pub\n to your repository's list of \ndeploy keys\n.\n   Make sure to check \nAllow write access\n.\n\n\n\n\n\n\n\n\n\n\nPush local changes to the GitHub repository:\n\n\ngit push origin master\n\n\n\n\n\nYour documents should be shortly available at \nhttps://opensciencegrid.github.io/\nREPO NAME\n\n\n\n\n\n\nCreating an ITB Area\n\n\nThis section describes creating an ITB repository for a documentation area created in the \nprevious section\n\n\n\n\n\n\nCreate a new repository in the \nopensciencegrid organization\n and name it \nREPO NAME\n-itb\n.\n   For example, an ITB area for the \ndocs\n repository has a repository name of \ndocs-itb\n.\n   The ITB repository will be referred to as \nITB REPO NAME\n in the rest of this document.\n\n\n\n\nCheck the box marked \nInitialize this repository with a README\n\n\nOnce created, add the \ndocumentation\n topic by clicking on the \"Add topics\" button\n\n\n\n\n\n\n\n\nClone the repository and \ncd\n into the directory:\n\n\ngit clone git@github.com:opensciencegrid/\nITB REPO NAME\n\ncd \nITB REPO NAME\n\n\n\n\n\n\n\n\n\n\nCreate a \ngh-pages\n branch in the GitHub repository:\n\n\ngit push origin master:gh-pages\n\n\n\n\n\n\n\n\n\nUpdate the contents of \nREADME.md\n\n\n\n\n\n\nIn the non-ITB repository, create and encrypt the ITB repository deploy key\n\n\n\n\n\n\ncd\n into the non-ITB repository and generate the ITB deploy key\n\n\ncd \nREPO NAME\n\nssh-keygen -t rsa -b 4096 -C \nhelp@opensciencegrid.org\n -f deploy-itb\n\n\n\n\n\n\n\n\n\nInstall the \ntravis\n gem:\n\n\ngem install travis\n\n\n\n\n\n\n\n\n\nEncrypt the deploy key:\n\n\ntravis encrypt-file deploy-itb\n\n\n\n\n\n\n\n\n\nUpdate \n.travis.env\n with the appropriate ITB values\n\n\n\n\n\n\nAdd and commit your files:\n\n\ngit add .travis.env deploy-itb.enc\ngit commit -m \nAdd ITB deployment\n\n\n\n\n\n\n\n\nDanger\n\n\nDo NOT commit the unencrypted \ndeploy-itb\n!\n\n\n\n\n\n\n\n\n\n\n\n\nAdd \ndeploy-itb.pub\n to the \nITB\n repository's list of \ndeploy keys\n.\n   Make sure to check \nAllow write access\n.\n\n\n\n\n\n\nStill in the non-ITB repository, push your local changes to the GitHub repository\n\n\ngit push origin master\n\n\n\n\n\nYour documents should be shortly available at \nhttps://opensciencegrid.github.io/\nREPO NAME", 
            "title": "Creating a New Area"
        }, 
        {
            "location": "/documentation/new-doc-area/#creating-a-new-area", 
            "text": "This document contains instructions for creating a new OSG documentation area via  GitHub Pages  and deploying it automatically with  Travis-CI . Before starting, make sure that you have the  git  and  gem  tools installed.    Create a new repository in the  opensciencegrid organization  (referred to as  REPO NAME  in the rest of this document)   Check the box marked  Initialize this repository with a README  Once created, add the  documentation  topic by clicking on the \"Add topics\" button     Clone the repository and  cd  into the directory:  git clone git@github.com:opensciencegrid/ REPO NAME \ncd  REPO NAME     Create a  gh-pages  branch in the GitHub repository:  git push origin master:gh-pages    Update the contents of  README.md  and populate the  LICENSE  file with a  Creative Commons Attribution 4.0 license :  wget https://creativecommons.org/licenses/by/4.0/legalcode.txt   LICENSE    Follow  these instructions  to add the  doc-ci-scripts  sub-module   Create  mkdocs.yml  and a  docs  directory   Create and encrypt the repository deploy key    Generate the repository deploy key:  ssh-keygen -t rsa -b 4096 -C  help@opensciencegrid.org  -f deploy-key    Install the  travis  gem:  gem install travis    Encrypt the deploy key:  travis encrypt-file deploy-key    Add and commit your files:  git add LICENSE mkdocs.yml docs deploy-key.enc\ngit commit -m  Prepare the repository for Travis-CI deployment    Danger  Do NOT commit the unencrypted  deploy-key !     Add  deploy-key.pub  to your repository's list of  deploy keys .\n   Make sure to check  Allow write access .      Push local changes to the GitHub repository:  git push origin master  Your documents should be shortly available at  https://opensciencegrid.github.io/ REPO NAME", 
            "title": "Creating a New Area"
        }, 
        {
            "location": "/documentation/new-doc-area/#creating-an-itb-area", 
            "text": "This section describes creating an ITB repository for a documentation area created in the  previous section    Create a new repository in the  opensciencegrid organization  and name it  REPO NAME -itb .\n   For example, an ITB area for the  docs  repository has a repository name of  docs-itb .\n   The ITB repository will be referred to as  ITB REPO NAME  in the rest of this document.   Check the box marked  Initialize this repository with a README  Once created, add the  documentation  topic by clicking on the \"Add topics\" button     Clone the repository and  cd  into the directory:  git clone git@github.com:opensciencegrid/ ITB REPO NAME \ncd  ITB REPO NAME     Create a  gh-pages  branch in the GitHub repository:  git push origin master:gh-pages    Update the contents of  README.md    In the non-ITB repository, create and encrypt the ITB repository deploy key    cd  into the non-ITB repository and generate the ITB deploy key  cd  REPO NAME \nssh-keygen -t rsa -b 4096 -C  help@opensciencegrid.org  -f deploy-itb    Install the  travis  gem:  gem install travis    Encrypt the deploy key:  travis encrypt-file deploy-itb    Update  .travis.env  with the appropriate ITB values    Add and commit your files:  git add .travis.env deploy-itb.enc\ngit commit -m  Add ITB deployment    Danger  Do NOT commit the unencrypted  deploy-itb !       Add  deploy-itb.pub  to the  ITB  repository's list of  deploy keys .\n   Make sure to check  Allow write access .    Still in the non-ITB repository, push your local changes to the GitHub repository  git push origin master  Your documents should be shortly available at  https://opensciencegrid.github.io/ REPO NAME", 
            "title": "Creating an ITB Area"
        }, 
        {
            "location": "/documentation/markdown-migration/", 
            "text": "Migrating to Markdown\n\n\nAs part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to \nMarkdown\n. The following document outlines the conversion process and conventions.\n\n\nChoosing the git repository\n\n\nFirst you will need to choose which git repoository you will be working with:\n\n\n\n\n\n\n\n\nIf you are converting a document from...\n\n\nUse this github repository...\n\n\n\n\n\n\n\n\n\n\nSoftwareTeam\n\n\ntechnology\n\n\n\n\n\n\nRelease3\n\n\ndocs\n\n\n\n\n\n\n\n\nOnce you've chosen the target repository for your document, move onto the next section and pick your conversion method.\n\n\nAutomatic TWiki conversion\n\n\n\n\nNote\n\n\nIf you are only archiving the documents, skip to this \nsection\n.\n\n\n\n\nChoose one of the following methods for converting TWiki documents:\n\n\n\n\nUsing our own \ndocker conversion image\n (recommended)\n\n\nDirectly using pandoc and mkdocs \non your own machine\n\n\n\n\nUsing docker\n\n\nThe twiki-converter docker image can be used to preview the document tree via a \nmkdocs\n development server, archive TWiki documents, and convert documents to Markdown via \npandoc\n. The image is available on \nosghost\n, otherwise, it is availble on \ndockerhub\n.\n\n\nuser@host $\n docker pull opensciencegrid/docker-twiki-converter\n\n\n\n\n\nRequirements\n\n\nTo perform a document migration using docker, you will need the following tools and accounts:\n\n\n\n\nFork\n and \nclone\n the repository that you chose in the \nabove section\n\n\nA host with a running docker service\n\n\nsudo\n or membership in the \ndocker\n group\n\n\n\n\nIf you cannot install the above tools locally, they are available on \nosghost\n. Speak with Brian L for access.\n\n\nPreparing the git repository\n\n\n\n\ncd\n into your local git repository\n\n\n\n\nAdd \nopensciencegrid/technology\n as the upstream remote repository for merging upstream changes:\n\n\nuser@host $\n git remote add upstream https://www.github.com/opensciencegrid/\nREPOSITORY\n.git\n\n\n\n\n\n\n\n\n\nCreate a branch for the document you plan to convert:\n\n\nuser@host $\n git branch \nBRANCH NAME\n master\n\n\n\n\n\n\n\n\n\nChange to the branch you just created\n\n\nuser@host $\n git checkout \nBRANCH NAME\n\n\n\n\n\n\n\n\n\n\nPreviewing the document tree\n\n\nWhen starting a twiki-converter docker container, it expects your local github repository to be mounted in \n/source\n so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image:\n\n\n\n\n\n\nCreate a container from the image with the following command:\n\n\nuser@host $\n docker run -d -v \nPATH TO LOCAL GITHUB REPO\n:/source -p \n8000\n opensciencegrid/docker-twiki-converter\n\n\n\n\n\nThe above command should return the container ID, which will be used in subsequent commands. \n\n\n\n\nNote\n\n\nIf the docker container exits immediately, remove the \n-d\n option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode.\n\n\n\n\n\n\n\n\nTo find the port that your development server is listening on, use the container ID (you should only need the first few chars of the ID) returned from the previous command:\n\n\nuser@host $\n docker port \nCONTAINER ID\n\n\n\n\n\n\n\n\n\n\nAccess the development server in your browser via \nhttp://osghost.chtc.wisc.edu:\nPORT\n or \nlocalhost:\nPORT\n for containers run on \nosghost\n or locally, respectively. \nosghost\n has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the \nosghost\n port to a local port.\n\n\n\n\n\n\nConverting documents\n\n\nThe docker image contains a convenience script, \nconvert-twiki\n for saving archives and converting them to Markdown. To run the script in a running container, run the following command:\n\n\nuser@host $\n docker \nexec\n \nCONTAINER ID\n convert-twiki \nTWIKI URL\n\n\n\n\n\n\nWhere \n is the docker container ID and \n is the link to the TWiki document that you want to convert, e.g. https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc, \narchive/SoftwareDevelopmentProcess\n, in your local repo and a converted copy, \nSoftwareDevelopmentProcess.md\n, placed into the root of your local github repository.  If the twiki url is for a specific revision of the document, a \n.rNN\n will be included in the output filenames.\n\n\n\n\nWarning\n\n\nIf the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See \nTroubleshooting conversion\n for next steps.\n\n\n\n\nTo see the converted document in your browser:\n\n\n\n\nRename, move the converted document into a folder in \ndocs/\n.\n\n\nDocument file names should be lowercase, \n-\n delimited, and descriptive but concise, e.g. \nmarkdown-migration.md\n or \ncutting-release.md\n\n\nIt's not important to get the name/location correct on the first try as this can be discussed in the pull request\n\n\n\n\n\n\nsudo chown\n the archived and converted documents to be owned by you\n\n\nAdd the document to the \npages:\n section of \nmkdocs.yml\n in \ntitle case\n, e.g. \n- Migrating Documents to Markdown: 'software/markdown-migration.md'\n\n\nRefresh the document tree in your browser\n\n\n\n\nOnce you can view the converted document in your browser, move onto the \nnext section\n\n\nTroubleshooting conversion\n\n\nPandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion.\n\n\n\n\nCopy the archive of the document into the root of your git repository\n\n\n\n\nKill the process in the docker container:\n\n\nuser@host $\n docker \nexec\n \nCONTAINER ID\n pkill -9 pandoc\n\n\n\n\n\n\n\n\n\nRemove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy)\n\n\n\n\n\n\nRun pandoc manually:\n\n\nuser@host $\n docker \nexec\n \nCONTAINER ID\n pandoc -f twiki -t markdown_github \nARCHIVE COPY\n \n \nMARKDOWN FILE\n\n\n\n\n\n\n\n\n\n\nRepeat steps 2-4 until you've narrowed down the problematic section\n\n\n\n\nManually convert the offending section\n\n\n\n\nConversion without Docker\n\n\nIf you've already used the \ndocker method\n, skip to the section about \ncompleting the conversion\n. \n\n\nRequirements\n\n\nThis method requires the following:\n\n\n\n\nFork\n and \nclone\n the repository that you chose in the \nabove section\n\n\npandoc (\n 1.16)\n\n\nmkdocs\n\n\nMarkdownHighlight\n\n\npygments\n\n\n\n\nPreparing the git repository\n\n\n\n\ncd\n into your local git repository\n\n\n\n\nAdd \nopensciencegrid/technology\n as the upstream remote repository for merging upstream changes:\n\n\nuser@host $\n git remote add upstream https://www.github.com/opensciencegrid/\nREPOSITORY\n.git\n\n\n\n\n\n\n\n\n\nCreate a branch for the document you plan to convert:\n\n\nuser@host $\n git branch \nBRANCH NAME\n master\n\n\n\n\n\n\n\n\n\nChange to the branch you just created\n\n\nuser@host $\n git checkout \nBRANCH NAME\n\n\n\n\n\n\n\n\n\n\nArchiving the TWiki document\n\n\nFollow the instructions for \narchival\n then continue to the next section to convert the document with pandoc.\n\n\nInitial conversion with Pandoc\n\n\nPandoc\n is a tool that's useful for automated conversion of markdown languages. \nOnce installed\n (alternatively, run pandoc \nvia docker\n), run the following command to convert TWiki to Markdown:\n\n\n$\n pandoc -f twiki -t markdown_github \nTWIKI FILE\n \n \nMARKDOWN FILE\n\n\n\n\n\n\nWhere \nTWIKI FILE\n is the path to initial document in raw TWiki and \nMARKDOWN FILE\n is the path to the resulting document in GitHub Markdown.\n\n\n\n\nNote\n\n\nIf you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section.\n\n\n\n\nPreviewing your document(s) with Mkdocs\n\n\nMkdocs\n has a development mode that can be used to preview documents as you work on them and is available via package manager or \npip\n. \nOnce installed\n, add your document(s) to the \npages\n section of \nmkdocs.yml\n and launch the mkdocs server with the following command from the dir containing \nmkdocs.yml\n:\n\n\n$\n \nPYTHONPATH\n=\nsrc/ mkdocs serve\n\n\n\n\n\nAccess the server at \nhttp://127.0.0.1:8000\n and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two.\n\n\nCompleting the conversion\n\n\nManual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents.\n\n\nVisit the \nstyle guide\n to ensure that the document meets all style guidelines.\n\n\nArchiving Documents\n\n\nIf the document is slated for archival (check if it says \"yes\" in the  \"archived\" column of the spreadsheet), just download the document to the \narchive\n folder of your local git repository:\n\n\nuser@host $\n \ncd\n technology/\n\nuser@host $\n curl \nTWIKI URL\n?raw=text\n \n|\n iconv -f windows-1252 \n archive/\nTWIKI TITLE\n\n\n\n\n\n\nFor example:\n\n\nuser@host $\n \ncd\n technology\n\nuser@host $\n curl \nhttps://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text\n \n|\n iconv -f windows-1252 \n archive/SHA2Compliance\n\n\n\n\n\nAfter downloading the document, continue onto the next section to walk through pull request submission.\n\n\nSubmitting the pull request\n\n\n\n\n\n\nStage the archived raw TWiki (as well as the converted Markdown document(s) and \nmkdocs.yml\n if you are converting the document):\n\n\nuser@host $\n git add mkdocs.yml archive/\nTWIKI ARCHIVE\n \nPATH TO CONVERTED DOC\n\n\n\n\n\n\n\n\n\n\nCommit and push your changes to your GitHub repo:\n\n\nuser@host $\n git commit -m \nCOMMIT MSG\n\n\nuser@host $\n git push origin \nBRANCH NAME\n\n\n\n\n\n\n\n\n\n\nOpen your browser and navigate to your GitHub fork\n\n\n\n\n\n\nSubmit a pull request containing with the following body:\n\n\nLINK TO TWIKI DOCUMENT\n\n\n\n-\n \n[\n \n]\n \nEnter\n \ndate\n \ninto\n \nMigrated\n \ncolumn\n \nof\n \ngoogle\n \nsheet\n\n\n\n\n\n\n\n\n\n\nIf you are migrating a document, also add this task:\n\n\n- [ ] Add migration header to TWiki document\n\n\n\n\n\n\n\n\n\nIf you are archiving a document, add this task:\n\n\n- [ ] Move TWiki document to the trash\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee an example pull request \nhere\n.\n\n\nAfter the pull request\n\n\nAfter the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document:\n\n\ndiv\n \nstyle=\nborder: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600;\n\nThis document has been migrated to !GitHub (\nLINK\n \nTO\n \nGITHUB\n \nDOCUMENT\n). If you wish to see the old TWiki document, use the TWiki history below.\n\nBackground:\n\nAt the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below:\n\n   * Release3: https://opensciencegrid.github.io/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]])\n   * !SoftwareTeam: https://opensciencegrid.github.io/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]])\n\n/div\n\n\n\n\n\n\nIf you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document:\n\n\n- [X] Enter date into \nMigrated\n column of google sheet\n- [X] Add migration div to TWiki document\n\n\n\n\n\nCurrently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL.  This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub.  This advice may change in the future as we gain experience with this transition.\n\n\nReviewing pull requests\n\n\nTo review pull requests, \ncd\n into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's \ncut-sw-release\n branch of the technology repository:\n\n\n#\n Add the requester\ns repo as a remote if you haven\nt already\n\nuser@host $\n git remote add blin https://www.github.com/brianhlin/technology.git\n\nuser@host $\n git fetch --all\n\nuser@host $\n git checkout blin/cut-sw-release\n\n\n\n\n\nRefresh your browser and navigate to the document in the request.", 
            "title": "Migrating Documents to Markdown"
        }, 
        {
            "location": "/documentation/markdown-migration/#migrating-to-markdown", 
            "text": "As part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to  Markdown . The following document outlines the conversion process and conventions.", 
            "title": "Migrating to Markdown"
        }, 
        {
            "location": "/documentation/markdown-migration/#choosing-the-git-repository", 
            "text": "First you will need to choose which git repoository you will be working with:     If you are converting a document from...  Use this github repository...      SoftwareTeam  technology    Release3  docs     Once you've chosen the target repository for your document, move onto the next section and pick your conversion method.", 
            "title": "Choosing the git repository"
        }, 
        {
            "location": "/documentation/markdown-migration/#automatic-twiki-conversion", 
            "text": "Note  If you are only archiving the documents, skip to this  section .   Choose one of the following methods for converting TWiki documents:   Using our own  docker conversion image  (recommended)  Directly using pandoc and mkdocs  on your own machine", 
            "title": "Automatic TWiki conversion"
        }, 
        {
            "location": "/documentation/markdown-migration/#using-docker", 
            "text": "The twiki-converter docker image can be used to preview the document tree via a  mkdocs  development server, archive TWiki documents, and convert documents to Markdown via  pandoc . The image is available on  osghost , otherwise, it is availble on  dockerhub .  user@host $  docker pull opensciencegrid/docker-twiki-converter", 
            "title": "Using docker"
        }, 
        {
            "location": "/documentation/markdown-migration/#requirements", 
            "text": "To perform a document migration using docker, you will need the following tools and accounts:   Fork  and  clone  the repository that you chose in the  above section  A host with a running docker service  sudo  or membership in the  docker  group   If you cannot install the above tools locally, they are available on  osghost . Speak with Brian L for access.", 
            "title": "Requirements"
        }, 
        {
            "location": "/documentation/markdown-migration/#preparing-the-git-repository", 
            "text": "cd  into your local git repository   Add  opensciencegrid/technology  as the upstream remote repository for merging upstream changes:  user@host $  git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git    Create a branch for the document you plan to convert:  user@host $  git branch  BRANCH NAME  master    Change to the branch you just created  user@host $  git checkout  BRANCH NAME", 
            "title": "Preparing the git repository"
        }, 
        {
            "location": "/documentation/markdown-migration/#previewing-the-document-tree", 
            "text": "When starting a twiki-converter docker container, it expects your local github repository to be mounted in  /source  so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image:    Create a container from the image with the following command:  user@host $  docker run -d -v  PATH TO LOCAL GITHUB REPO :/source -p  8000  opensciencegrid/docker-twiki-converter  The above command should return the container ID, which will be used in subsequent commands.    Note  If the docker container exits immediately, remove the  -d  option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode.     To find the port that your development server is listening on, use the container ID (you should only need the first few chars of the ID) returned from the previous command:  user@host $  docker port  CONTAINER ID     Access the development server in your browser via  http://osghost.chtc.wisc.edu: PORT  or  localhost: PORT  for containers run on  osghost  or locally, respectively.  osghost  has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the  osghost  port to a local port.", 
            "title": "Previewing the document tree"
        }, 
        {
            "location": "/documentation/markdown-migration/#converting-documents", 
            "text": "The docker image contains a convenience script,  convert-twiki  for saving archives and converting them to Markdown. To run the script in a running container, run the following command:  user@host $  docker  exec   CONTAINER ID  convert-twiki  TWIKI URL   Where   is the docker container ID and   is the link to the TWiki document that you want to convert, e.g. https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc,  archive/SoftwareDevelopmentProcess , in your local repo and a converted copy,  SoftwareDevelopmentProcess.md , placed into the root of your local github repository.  If the twiki url is for a specific revision of the document, a  .rNN  will be included in the output filenames.   Warning  If the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See  Troubleshooting conversion  for next steps.   To see the converted document in your browser:   Rename, move the converted document into a folder in  docs/ .  Document file names should be lowercase,  -  delimited, and descriptive but concise, e.g.  markdown-migration.md  or  cutting-release.md  It's not important to get the name/location correct on the first try as this can be discussed in the pull request    sudo chown  the archived and converted documents to be owned by you  Add the document to the  pages:  section of  mkdocs.yml  in  title case , e.g.  - Migrating Documents to Markdown: 'software/markdown-migration.md'  Refresh the document tree in your browser   Once you can view the converted document in your browser, move onto the  next section", 
            "title": "Converting documents"
        }, 
        {
            "location": "/documentation/markdown-migration/#troubleshooting-conversion", 
            "text": "Pandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion.   Copy the archive of the document into the root of your git repository   Kill the process in the docker container:  user@host $  docker  exec   CONTAINER ID  pkill -9 pandoc    Remove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy)    Run pandoc manually:  user@host $  docker  exec   CONTAINER ID  pandoc -f twiki -t markdown_github  ARCHIVE COPY     MARKDOWN FILE     Repeat steps 2-4 until you've narrowed down the problematic section   Manually convert the offending section", 
            "title": "Troubleshooting conversion"
        }, 
        {
            "location": "/documentation/markdown-migration/#conversion-without-docker", 
            "text": "If you've already used the  docker method , skip to the section about  completing the conversion .", 
            "title": "Conversion without Docker"
        }, 
        {
            "location": "/documentation/markdown-migration/#requirements_1", 
            "text": "This method requires the following:   Fork  and  clone  the repository that you chose in the  above section  pandoc (  1.16)  mkdocs  MarkdownHighlight  pygments", 
            "title": "Requirements"
        }, 
        {
            "location": "/documentation/markdown-migration/#preparing-the-git-repository_1", 
            "text": "cd  into your local git repository   Add  opensciencegrid/technology  as the upstream remote repository for merging upstream changes:  user@host $  git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git    Create a branch for the document you plan to convert:  user@host $  git branch  BRANCH NAME  master    Change to the branch you just created  user@host $  git checkout  BRANCH NAME", 
            "title": "Preparing the git repository"
        }, 
        {
            "location": "/documentation/markdown-migration/#archiving-the-twiki-document", 
            "text": "Follow the instructions for  archival  then continue to the next section to convert the document with pandoc.", 
            "title": "Archiving the TWiki document"
        }, 
        {
            "location": "/documentation/markdown-migration/#initial-conversion-with-pandoc", 
            "text": "Pandoc  is a tool that's useful for automated conversion of markdown languages.  Once installed  (alternatively, run pandoc  via docker ), run the following command to convert TWiki to Markdown:  $  pandoc -f twiki -t markdown_github  TWIKI FILE     MARKDOWN FILE   Where  TWIKI FILE  is the path to initial document in raw TWiki and  MARKDOWN FILE  is the path to the resulting document in GitHub Markdown.   Note  If you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section.", 
            "title": "Initial conversion with Pandoc"
        }, 
        {
            "location": "/documentation/markdown-migration/#previewing-your-documents-with-mkdocs", 
            "text": "Mkdocs  has a development mode that can be used to preview documents as you work on them and is available via package manager or  pip .  Once installed , add your document(s) to the  pages  section of  mkdocs.yml  and launch the mkdocs server with the following command from the dir containing  mkdocs.yml :  $   PYTHONPATH = src/ mkdocs serve  Access the server at  http://127.0.0.1:8000  and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two.", 
            "title": "Previewing your document(s) with Mkdocs"
        }, 
        {
            "location": "/documentation/markdown-migration/#completing-the-conversion", 
            "text": "Manual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents.  Visit the  style guide  to ensure that the document meets all style guidelines.", 
            "title": "Completing the conversion"
        }, 
        {
            "location": "/documentation/markdown-migration/#archiving-documents", 
            "text": "If the document is slated for archival (check if it says \"yes\" in the  \"archived\" column of the spreadsheet), just download the document to the  archive  folder of your local git repository:  user@host $   cd  technology/ user@host $  curl  TWIKI URL ?raw=text   |  iconv -f windows-1252   archive/ TWIKI TITLE   For example:  user@host $   cd  technology user@host $  curl  https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text   |  iconv -f windows-1252   archive/SHA2Compliance  After downloading the document, continue onto the next section to walk through pull request submission.", 
            "title": "Archiving Documents"
        }, 
        {
            "location": "/documentation/markdown-migration/#submitting-the-pull-request", 
            "text": "Stage the archived raw TWiki (as well as the converted Markdown document(s) and  mkdocs.yml  if you are converting the document):  user@host $  git add mkdocs.yml archive/ TWIKI ARCHIVE   PATH TO CONVERTED DOC     Commit and push your changes to your GitHub repo:  user@host $  git commit -m  COMMIT MSG  user@host $  git push origin  BRANCH NAME     Open your browser and navigate to your GitHub fork    Submit a pull request containing with the following body:  LINK TO TWIKI DOCUMENT  -   [   ]   Enter   date   into   Migrated   column   of   google   sheet     If you are migrating a document, also add this task:  - [ ] Add migration header to TWiki document    If you are archiving a document, add this task:  - [ ] Move TWiki document to the trash      See an example pull request  here .", 
            "title": "Submitting the pull request"
        }, 
        {
            "location": "/documentation/markdown-migration/#after-the-pull-request", 
            "text": "After the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document:  div   style= border: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600; \nThis document has been migrated to !GitHub ( LINK   TO   GITHUB   DOCUMENT ). If you wish to see the old TWiki document, use the TWiki history below.\n\nBackground:\n\nAt the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below:\n\n   * Release3: https://opensciencegrid.github.io/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]])\n   * !SoftwareTeam: https://opensciencegrid.github.io/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]]) /div   If you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document:  - [X] Enter date into  Migrated  column of google sheet\n- [X] Add migration div to TWiki document  Currently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL.  This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub.  This advice may change in the future as we gain experience with this transition.", 
            "title": "After the pull request"
        }, 
        {
            "location": "/documentation/markdown-migration/#reviewing-pull-requests", 
            "text": "To review pull requests,  cd  into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's  cut-sw-release  branch of the technology repository:  #  Add the requester s repo as a remote if you haven t already user@host $  git remote add blin https://www.github.com/brianhlin/technology.git user@host $  git fetch --all user@host $  git checkout blin/cut-sw-release  Refresh your browser and navigate to the document in the request.", 
            "title": "Reviewing pull requests"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/", 
            "text": "Notes on Koji initial install\n\n\n\n\nNote\n\n\nThis document was written for the original install of koji/koji-hub version\n1.6 to \nkoji-hub.batlab.org\n.  The document is kept for historical\npurposes, as this setup is no longer accurate with newer versions of Koji,\nmachine moves and renames, cert authority changes, etc.\n\n\n\n\nMachine setup\n\n\nkoji is run on \nkoji-hub.batlab.org\n; right now a single machine was used for the hub (koji-hub), the web frontend (koji-web), repo generation (kojira), and el5 building (kojid). A second machine, \nkojibuilder1.batlab.org\n was used for el6 building.\n\n\nAs instructions for setting up the machine, I'm just going to take the Fedora guide at \nhttp://fedoraproject.org/wiki/Koji/ServerHowTo\n, and add modifications / comments to suit our setup.\n\n\nSections taken from the Fedora guide will be between dividers like this:\n\n\n\n\n\n\n\n\nFor an overview of yum, mock, Koji (and all its subcomponents), mash, and how they all work together, see the excellent slides put together by Steve Traylen at CERN \nhttp://indico.cern.ch/event/55091\n\n\n\n\n\n\n\n\nPackages to install\n\n\n\n\n\n\nOn the server (koji-hub/koji-web)\n\n\n\n\nhttpd\n\n\nmod_ssl\n\n\npostgresql-server\n\n\nmod_python (\n= 3.3.1 for Kerberos authentication)\n\n\n\n\nOn the builder (koji-builder)\n\n\n\n\nmock\n\n\nsetarch (for some archs you'll require a patched version)\n\n\nrpm-build\n\n\ncreaterepo\n\n\n\n\n\n\n\n\nWe used one machine for both these roles, so all of the above had to be installed.\n\n\nThe koji packages to install are:\n\n\n\n\nkoji-hub\n\n\nkoji-web\n\n\nkoji-builder\n\n\nkoji-utils\n\n\n\n\nFilesystems\n\n\n\n\n\n\nA note on filesystem space\n\n\nKoji will consume copious amounts of disk space under the primary KojiDir directory (as set in the kojihub.conf file).\n\n\n\n\n\n\nThis is \n/mnt/koji\n by default. Koji keeps \nall RPMs built\n in here so a complete history is kept. In addition, repositories are kept here. A repository is just a set of text files, but since a new one is generated for every build, the space will add up. Koji does clean up the repositories once they get old enough. For \nkoji-hub.batlab.org\n, we allocated a 120G partition for \n/mnt/koji\n.\n\n\n\n\n\n\nHowever, as koji makes use of mock on the backend to actually create build roots and perform the builds in those build roots, it might come to a surprise to users that a running koji server will consume large amounts of disk space under /var/lib/mock and /var/cache/mock as well.\n\n\n\n\n\n\nkojid\n (the builder daemon) will refuse to start a build if it does not have sufficient space under \n/var/lib/mock\n. By default, this is an 8G surplus, but we lowered it to 4G for \nkoji-hub.batlab.org\n. The build roots will be wiped after successful builds, but the build roots for failed builds will be kept around for 2 weeks (?); as a result, disk usage of \n/var/lib/mock\n can swing wildly. For \nkoji-hub.batlab.org\n, we have allocated 35G for \n/var/lib/mock\n. \n/var/cache/mock\n doesn't use up that much space, so we didn't make a separate partition for it, it just uses what's under \n/\n.\n\n\nAuthentication\n\n\nMost of the \"Koji Authentication Selection\" section can be skipped. We got our certs from DigiCert. \nNote\n that certs are sensitive to line ending issues -- you may have to run \ndos2unix\n on them before they would work.\n\n\nWe have 3 certs, with the following subjects:\n\n\n\n\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=koji-hub.batlab.org\n (for koji-hub, koji-web, and kojid)\n\n\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=kojira/koji-hub.batlab.org\n (for kojira)\n\n\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=kojibuilder1.batlab.org\n (for kojid on kojibuilder1)\n\n\n\n\nkojid needed a different cert than kojira because the two would connect to koji-hub at the same time and log each other off.\n\n\nCopies of the certs exist in AFS at \n/p/condor/home/certificates/koji-hub.batlab.org/\n and \n/p/condor/home/certificates/kojibuilder1.batlab.org/\n\n\nClients connecting to koji-hub need a file containing the CA certs; this is just a concatenation of the DigiCert Grid Root CA and the DigiCert Grid CA-1 certs, which can be obtained from DigiCert's website (get PEM format). We have a \ndigicert-chain.crt\n in \n/etc/pki/tls/certs/digicert-chain.crt\n on \nkoji-hub\n\n\nWe made a \nkojiadmin\n user, but we didn't make a cert for it so we just used user/pass authentication until we set up our own accounts and then disabled \nkojiadmin\n.\n\n\nUsing proxy certs\n\n\n/etc/sysconfig/httpd\n needed to be changed to include the following lines:\n\n\nOPENSSL_ALLOW_PROXY\n=\n1\n\n\nOPENSSL_ALLOW_PROXY_CERTS\n=\n1\n\n\n\nexport\n OPENSSL_ALLOW_PROXY\n\nexport\n OPENSSL_ALLOW_PROXY_CERTS\n\n\n\n\n\nThe user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.\n\n\nPostgres Database\n\n\n\n\nInstall postgres:\n\n\n[root@koji-hub]# yum install postgresql-server\n\n\nInit the db:\n\n\n[root@koji-hub]# su - postgres -c \"PGDATA=/var/lib/pgsql/data\" initdb\n\n\nStart the db:\n\n\n[root@koji-hub]# service postgresql start\n\n\nMake a koji account:\n\n\n[root@koji-hub]# useradd koji; passwd -d koji\n\n\n\n\n\n\n\n\n\n\n\nSetup PostgreSQL and populate schema:\n\n\nThe following commands will create the \nkoji\n user within PostgreSQL and will then create the koji database using the schema within the \n/usr/share/doc/koji*/docs/schema.sql\n directory\n\n\nroot@localhost$ su - postgres\npostgres@localhost$ createuser koji\nShall the new role be a superuser? (y/n) n\nShall the new role be allowed to create databases? (y/n) n\nShall the new role be allowed to create more new roles? (y/n) n\npostgres@localhost$ createdb -O koji koji\npostgres@localhost$ logout\nroot@localhost$ su - koji\nkoji@localhost$ psql koji koji \n /usr/share/doc/koji*/docs/schema.sql\nkoji@localhost$ exit\n\n\n\n\n\nNOTE:\n When issuing the command to import the psql schema into the new database it is important to ensure that the directory path \n/usr/share/doc/koji*/docs/schema.sql\n remains intact and is not resolved to a specific version of koji. In test it was discovered that when the path is resolved to a specific version of koji then not all of the tables were created correctly.\n\n\n\n\n\n\nTo authorize the koji-web and koji-hub resources, make the following additions to \n/var/lib/pgsql/data/pg_hba.conf\n (IP addresses will vary):\n\n\n# access for koji\nhost    koji        postgres    127.0.0.1/32          trust\nhost    koji        koji        127.0.0.1/32          trust\nhost    koji        koji        128.104.100.41/32     trust\nhost    koji        koji        ::1/128               trust\n\n\n\n\n\nNext, we'll need a koji admin user to run some commands. We'll need to add it to the database manually. Once again, we used user/pass authentication for the kojiadmin user until we disabled it. All database commands should be done as the \nkoji\n user:\n\n\n[root@koji-hub]#\n sudo -u koji psql\n\n\n\n\n\nkoji=\n  \ninsert\n \ninto\n \nusers\n \n(\nname\n,\n \npassword\n,\n \nstatus\n,\n \nusertype\n)\n \nvalues\n\n          \n(\nkojiadmin\n,\n \nsome-throwaway-admin-password-in-plain-text\n,\n \n0\n,\n \n0\n);\n\n\nkoji=\n  \ninsert\n \ninto\n \nuser_perms\n \n(\nuser_id\n,\n \nperm_id\n,\n \ncreator_id\n)\n \nvalues\n\n          \n((\nselect\n \nid\n \nfrom\n \nusers\n \nwhere\n \nname\n=\nkojiadmin\n),\n \n1\n,\n\n           \n(\nselect\n \nid\n \nfrom\n \nusers\n \nwhere\n \nname\n=\nkojiadmin\n));\n\n\n\n\n\n\nKoji-Hub\n\n\n\n\n\n\nKoji-hub is the center of all Koji operations. It is an XML-RPC server running under mod_python in Apache. koji-hub is passive in that it only receives XML-RPC calls and relies upon the build daemons and other components to initiate communication. Koji-hub is the only component that has direct access to the database and is one of the two components that have write access to the file system.\n\n\n\n\n\n\nConfig files we care about:\n\n\n\n\n/etc/httpd/conf/httpd.conf\n\n\n/etc/httpd/conf.d/kojihub.conf\n\n\n/etc/httpd/conf.d/ssl.conf\n\n\n/etc/koji-hub/hub.conf\n\n\n\n\nInstall the necessary packages.\n\n\n[root@koji-hub]#\n yum install koji-hub httpd mod_ssl mod_python\n\n\n\n\n\n\n\n\n\n/etc/httpd/conf/httpd.conf:\n\n\nThe apache web server has two places that it sets maximum requests a server will handle before the server restarts. The xmlrpc interface in kojihub is a python application, and mod_python can sometimes grow outrageously large when it doesn't reap memory often enough. As a result, it is strongly recommended that you set both instances of MaxRequestsPerChild in \nhttpd.conf\n to something reasonable in order to prevent the server from becoming overloaded and crashing (at 100 the \nhttpd\n processes will grow to about 75MB resident set size before respawning).\n\n\nIfModule\n \nprefork.c\n\n...\nMaxRequestsPerChild  100\n\n/IfModule\n\n\nIfModule\n \nworker.c\n\n...\nMaxRequestsPerChild  100\n\n/IfModule\n\n\n\n\n\n\n/etc/koji-hub/hub.conf:\n\n\nThis file contains the configuration information for the hub. You will need to edit this configuration to point Koji Hub to the database you are using and to setup Koji Hub to utilize the authentication scheme you selected in the beginning.\n\n\n\n\n\n\nWe made multiple changes to this config file.\n\n\nNotable changes:\n\n\n\n\nWe turned off \nLoginCreatesUser\n\n\nKojiWebURL\n is \nhttp://koji-hub.batlab.org/koji\n\n\nPluginPath\n is \n/usr/lib/koji-hub-plugins\n\n\nPlugins = sign\n since there is an RPM signing plugin (package name: \nkoji-plugin-sign\n) that we use\n\n\n\n\nOther notes:\n\n\n\n\nShould not be world-readable\n\n\nMust be readable by the \napache\n user though\n\n\n\n\nFinally, there is a \n[policy]\n section for controlling ACLs. The contents of that are described later in this document.\n\n\n\n\n\n\n/etc/httpd/conf.d/kojihub.conf:\n\n\nIf using SSL auth, uncomment these lines for kojiweb to allow logins.\n\n\nLocation\n \n/kojihub\n\nSSLOptions +StdEnvVars\n\n/Location\n\n\n\n\n\n\n\n\n\n\nThis is actually outdated information, useful for Koji \n 1.4.0. Instead, we add the following:\n\n\nLocation\n \n/kojihub/ssllogin\n\n        \nSSLVerifyClient\n require\n        \nSSLVerifyDepth\n  \n10\n\n        \nSSLOptions\n +StdEnvVars\n\n/Location\n\n\n\n\n\n\n\n\n\n\n/etc/httpd/conf.d/ssl.conf:\n\n\nIf using SSL you will also need to add the needed SSL options for apache. These options should point to where the certificates are located on the hub.\n\n\n\n\n\n\nThese are the config lines we use:\n\n\n## The host cert for koji-hub\n\n\nSSLCertificateFile\n \n/etc/pki/tls/certs/digicert_hostcert.crt\n\n\n\n## The private key for koji-hub\n\n\nSSLCertificateKeyFile\n \n/etc/pki/tls/private/digicert_hostkey.key\n\n\n\n## The concatenation of the DigiCert CA certs we made above.\n\n\nSSLCertificateChainFile\n \n/etc/pki/tls/certs/digicert-chain.crt\n\n\n\n## All the CA certs on the system.\n\n\nSSLCACertificateFile\n \n/etc/pki/tls/certs/ca-bundle.crt\n\n\n\nSSLVerifyClient\n require\n\nSSLVerifyDepth\n  \n10\n\n\n\n\n\n\nRestart \nhttpd\n after doing this.\n\n\nAdding a real admin user\n\n\nAt this point, SSL should be set up on Koji Hub. Create an account for yourself (your CN) and give yourself the admin bit:\n\n\n[kojiadmin@koji-hub]$\n koji add-user \nYour Name 123456\n\n\n[kojiadmin@koji-hub]$\n koji grant-permission admin \nYour Name 123456\n\n\n\n\n\n\nOnce you're done with all the setup\n, you should remove the admin bit from \nkojiadmin\n, block the user, and remove its password from the database.\n\n\n[you@koji-hub]$\n koji revoke-permission admin kojiadmin\n\n[you@koji-hub]$\n koji disable-user kojiadmin\n\n\n\n\n\nIn the database:\n\n\nkoji=\n  \nupdate\n \nusers\n \nset\n \npassword\n \n=\n \nnull\n \nwhere\n \nname\n=\nkojiadmin\n;\n\n\n\n\n\n\nKoji filesystem\n\n\nKojiDir\n is \n/mnt/koji\n, so the following tree should be created:\n\n\n[root@koji-hub]#\n mkdir -p /mnt/koji/\n{\npackages,repos,work,scratch\n}\n\n\n[root@koji-hub]#\n chown apache:apache *\n\n\n\n\n\nAccounts for kojira and kojid\n\n\n\n\n\n\nIt is important to note that the kojira component needs repo privileges, but if you just let the account get auto created the first time you run kojira, it won't have that privilege, so you should pre-create the account and grant it the repo privilege now.\n\n\nkojiadmin@koji-hub$ koji add-user kojira\nkojiadmin@koji-hub$ koji grant-permission repo kojira\n\n\n\n\n\nFor similar technical reasons, you need to add-host each build host prior to starting kojid on that host the first time and could also do that now.\n\n\n\n\n\n\nWe turned off auto account creation, so there won't be a problem with accounts with bad perms getting created. This means they have to be created manually. We have kojid running on koji-hub, so this is the command we used to add it:\n\n\n[you@koji-hub]$\n koji add-host koji-hub.batlab.org i386 x86_64\n\n\n\n\n\nKoji-Web, Kojid\n\n\nInstalling:\n\n\n[root@koji-hub]#\n yum install koji-web mod_ssl\n\n[root@koji-hub]#\n yum install koji-builder\n\n\n\n\n\nNote that we use a patched koji-builder, so be sure to install what's in the OSG repository. (The patch is not likely to be accepted upstream).\n\n\nSSL certs\n\n\nThe following files should be in the \n/etc/pki/tls\n tree:\n\n\n\n\n\n\n/etc/pki/tls/private/digicert_hostkey.key\n\n    the host key from DigiCert\n\n\n\n\n\n\n/etc/pki/tls/private/kojiweb.pem\n\n    digicert_hostcert.crt catted together with digicert_hostkey.key (public key first)\n\n\n\n\n\n\n/etc/pki/tls/private/kojira.key\n\n    kojira's private key from DigiCert\n\n\n\n\n\n\n/etc/pki/tls/private/kojira.pem\n\n    kojira.crt catted together with kojira.key (public key first)\n\n\n\n\n\n\n/etc/pki/tls/cert.pem\n\n    symlink to certs/ca-bundle.crt\n\n\n\n\n\n\n/etc/pki/tls/certs/ca-bundle.crt\n\n    all certs available on the system; the DigiCert Grid certs should be in here\n\n\n\n\n\n\n/etc/pki/tls/certs/kojira.crt\n\n    kojira's cert\n\n\n\n\n\n\n/etc/pki/tls/certs/digicert_hostcert.crt\n\n    the host cert from DigiCert\n\n\n\n\n\n\n/etc/pki/tls/certs/digicert-chain.crt\n\n    the DigiCert CAs catted together\n\n\n\n\n\n\nAdding koji-builder host to database\n\n\n[you@koji-hub]$\n koji add-host-to-channel koji-hub.batlab.org createrepo\n\n\n\n\n\nIn the database:\n\n\nkoji=\n#\n  \nupdate\n \nhost\n \nset\n \ncapacity\n \n=\n \n4\n \nwhere\n \nname\n \n=\n \nkoji-hub.batlab.org\n;\n\n\n\n\n\n\nStart kojid\n\n\n[root@koji-hub]#\n /sbin/service kojid start\n\n\n\n\n\nCheck \n/var/log/kojid.log\n to make sure everything started up.\n\n\nKojira\n\n\nInstalling:\n\n\n[root@koji-hub]#\n yum install koji-utils\n\n\n\n\n\nKojira also needs a user\n\n\n[you@koji-hub]$\n koji add-user kojira\n\n[you@koji-hub]$\n koji grant-permission repo kojira\n\n\n\n\n\nStart it up\n\n\n[root@koji-hub]#\n /sbin/service kojira start", 
            "title": "Koji Initial Install"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#notes-on-koji-initial-install", 
            "text": "Note  This document was written for the original install of koji/koji-hub version\n1.6 to  koji-hub.batlab.org .  The document is kept for historical\npurposes, as this setup is no longer accurate with newer versions of Koji,\nmachine moves and renames, cert authority changes, etc.", 
            "title": "Notes on Koji initial install"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#machine-setup", 
            "text": "koji is run on  koji-hub.batlab.org ; right now a single machine was used for the hub (koji-hub), the web frontend (koji-web), repo generation (kojira), and el5 building (kojid). A second machine,  kojibuilder1.batlab.org  was used for el6 building.  As instructions for setting up the machine, I'm just going to take the Fedora guide at  http://fedoraproject.org/wiki/Koji/ServerHowTo , and add modifications / comments to suit our setup.  Sections taken from the Fedora guide will be between dividers like this:     For an overview of yum, mock, Koji (and all its subcomponents), mash, and how they all work together, see the excellent slides put together by Steve Traylen at CERN  http://indico.cern.ch/event/55091", 
            "title": "Machine setup"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#packages-to-install", 
            "text": "On the server (koji-hub/koji-web)   httpd  mod_ssl  postgresql-server  mod_python ( = 3.3.1 for Kerberos authentication)   On the builder (koji-builder)   mock  setarch (for some archs you'll require a patched version)  rpm-build  createrepo     We used one machine for both these roles, so all of the above had to be installed.  The koji packages to install are:   koji-hub  koji-web  koji-builder  koji-utils", 
            "title": "Packages to install"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#filesystems", 
            "text": "", 
            "title": "Filesystems"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#a-note-on-filesystem-space", 
            "text": "Koji will consume copious amounts of disk space under the primary KojiDir directory (as set in the kojihub.conf file).    This is  /mnt/koji  by default. Koji keeps  all RPMs built  in here so a complete history is kept. In addition, repositories are kept here. A repository is just a set of text files, but since a new one is generated for every build, the space will add up. Koji does clean up the repositories once they get old enough. For  koji-hub.batlab.org , we allocated a 120G partition for  /mnt/koji .    However, as koji makes use of mock on the backend to actually create build roots and perform the builds in those build roots, it might come to a surprise to users that a running koji server will consume large amounts of disk space under /var/lib/mock and /var/cache/mock as well.    kojid  (the builder daemon) will refuse to start a build if it does not have sufficient space under  /var/lib/mock . By default, this is an 8G surplus, but we lowered it to 4G for  koji-hub.batlab.org . The build roots will be wiped after successful builds, but the build roots for failed builds will be kept around for 2 weeks (?); as a result, disk usage of  /var/lib/mock  can swing wildly. For  koji-hub.batlab.org , we have allocated 35G for  /var/lib/mock .  /var/cache/mock  doesn't use up that much space, so we didn't make a separate partition for it, it just uses what's under  / .", 
            "title": "A note on filesystem space"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#authentication", 
            "text": "Most of the \"Koji Authentication Selection\" section can be skipped. We got our certs from DigiCert.  Note  that certs are sensitive to line ending issues -- you may have to run  dos2unix  on them before they would work.  We have 3 certs, with the following subjects:   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=koji-hub.batlab.org  (for koji-hub, koji-web, and kojid)  /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=kojira/koji-hub.batlab.org  (for kojira)  /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=kojibuilder1.batlab.org  (for kojid on kojibuilder1)   kojid needed a different cert than kojira because the two would connect to koji-hub at the same time and log each other off.  Copies of the certs exist in AFS at  /p/condor/home/certificates/koji-hub.batlab.org/  and  /p/condor/home/certificates/kojibuilder1.batlab.org/  Clients connecting to koji-hub need a file containing the CA certs; this is just a concatenation of the DigiCert Grid Root CA and the DigiCert Grid CA-1 certs, which can be obtained from DigiCert's website (get PEM format). We have a  digicert-chain.crt  in  /etc/pki/tls/certs/digicert-chain.crt  on  koji-hub  We made a  kojiadmin  user, but we didn't make a cert for it so we just used user/pass authentication until we set up our own accounts and then disabled  kojiadmin .", 
            "title": "Authentication"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#using-proxy-certs", 
            "text": "/etc/sysconfig/httpd  needed to be changed to include the following lines:  OPENSSL_ALLOW_PROXY = 1  OPENSSL_ALLOW_PROXY_CERTS = 1  export  OPENSSL_ALLOW_PROXY export  OPENSSL_ALLOW_PROXY_CERTS  The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.", 
            "title": "Using proxy certs"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#postgres-database", 
            "text": "Install postgres:  [root@koji-hub]# yum install postgresql-server  Init the db:  [root@koji-hub]# su - postgres -c \"PGDATA=/var/lib/pgsql/data\" initdb  Start the db:  [root@koji-hub]# service postgresql start  Make a koji account:  [root@koji-hub]# useradd koji; passwd -d koji", 
            "title": "Postgres Database"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#setup-postgresql-and-populate-schema", 
            "text": "The following commands will create the  koji  user within PostgreSQL and will then create the koji database using the schema within the  /usr/share/doc/koji*/docs/schema.sql  directory  root@localhost$ su - postgres\npostgres@localhost$ createuser koji\nShall the new role be a superuser? (y/n) n\nShall the new role be allowed to create databases? (y/n) n\nShall the new role be allowed to create more new roles? (y/n) n\npostgres@localhost$ createdb -O koji koji\npostgres@localhost$ logout\nroot@localhost$ su - koji\nkoji@localhost$ psql koji koji   /usr/share/doc/koji*/docs/schema.sql\nkoji@localhost$ exit  NOTE:  When issuing the command to import the psql schema into the new database it is important to ensure that the directory path  /usr/share/doc/koji*/docs/schema.sql  remains intact and is not resolved to a specific version of koji. In test it was discovered that when the path is resolved to a specific version of koji then not all of the tables were created correctly.    To authorize the koji-web and koji-hub resources, make the following additions to  /var/lib/pgsql/data/pg_hba.conf  (IP addresses will vary):  # access for koji\nhost    koji        postgres    127.0.0.1/32          trust\nhost    koji        koji        127.0.0.1/32          trust\nhost    koji        koji        128.104.100.41/32     trust\nhost    koji        koji        ::1/128               trust  Next, we'll need a koji admin user to run some commands. We'll need to add it to the database manually. Once again, we used user/pass authentication for the kojiadmin user until we disabled it. All database commands should be done as the  koji  user:  [root@koji-hub]#  sudo -u koji psql  koji=    insert   into   users   ( name ,   password ,   status ,   usertype )   values \n           ( kojiadmin ,   some-throwaway-admin-password-in-plain-text ,   0 ,   0 );  koji=    insert   into   user_perms   ( user_id ,   perm_id ,   creator_id )   values \n           (( select   id   from   users   where   name = kojiadmin ),   1 , \n            ( select   id   from   users   where   name = kojiadmin ));", 
            "title": "Setup PostgreSQL and populate schema:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#koji-hub", 
            "text": "Koji-hub is the center of all Koji operations. It is an XML-RPC server running under mod_python in Apache. koji-hub is passive in that it only receives XML-RPC calls and relies upon the build daemons and other components to initiate communication. Koji-hub is the only component that has direct access to the database and is one of the two components that have write access to the file system.    Config files we care about:   /etc/httpd/conf/httpd.conf  /etc/httpd/conf.d/kojihub.conf  /etc/httpd/conf.d/ssl.conf  /etc/koji-hub/hub.conf   Install the necessary packages.  [root@koji-hub]#  yum install koji-hub httpd mod_ssl mod_python", 
            "title": "Koji-Hub"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etchttpdconfhttpdconf", 
            "text": "The apache web server has two places that it sets maximum requests a server will handle before the server restarts. The xmlrpc interface in kojihub is a python application, and mod_python can sometimes grow outrageously large when it doesn't reap memory often enough. As a result, it is strongly recommended that you set both instances of MaxRequestsPerChild in  httpd.conf  to something reasonable in order to prevent the server from becoming overloaded and crashing (at 100 the  httpd  processes will grow to about 75MB resident set size before respawning).  IfModule   prefork.c \n...\nMaxRequestsPerChild  100 /IfModule  IfModule   worker.c \n...\nMaxRequestsPerChild  100 /IfModule", 
            "title": "/etc/httpd/conf/httpd.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etckoji-hubhubconf", 
            "text": "This file contains the configuration information for the hub. You will need to edit this configuration to point Koji Hub to the database you are using and to setup Koji Hub to utilize the authentication scheme you selected in the beginning.    We made multiple changes to this config file.  Notable changes:   We turned off  LoginCreatesUser  KojiWebURL  is  http://koji-hub.batlab.org/koji  PluginPath  is  /usr/lib/koji-hub-plugins  Plugins = sign  since there is an RPM signing plugin (package name:  koji-plugin-sign ) that we use   Other notes:   Should not be world-readable  Must be readable by the  apache  user though   Finally, there is a  [policy]  section for controlling ACLs. The contents of that are described later in this document.", 
            "title": "/etc/koji-hub/hub.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etchttpdconfdkojihubconf", 
            "text": "If using SSL auth, uncomment these lines for kojiweb to allow logins.  Location   /kojihub \nSSLOptions +StdEnvVars /Location     This is actually outdated information, useful for Koji   1.4.0. Instead, we add the following:  Location   /kojihub/ssllogin \n         SSLVerifyClient  require\n         SSLVerifyDepth    10 \n         SSLOptions  +StdEnvVars /Location", 
            "title": "/etc/httpd/conf.d/kojihub.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etchttpdconfdsslconf", 
            "text": "If using SSL you will also need to add the needed SSL options for apache. These options should point to where the certificates are located on the hub.    These are the config lines we use:  ## The host cert for koji-hub  SSLCertificateFile   /etc/pki/tls/certs/digicert_hostcert.crt  ## The private key for koji-hub  SSLCertificateKeyFile   /etc/pki/tls/private/digicert_hostkey.key  ## The concatenation of the DigiCert CA certs we made above.  SSLCertificateChainFile   /etc/pki/tls/certs/digicert-chain.crt  ## All the CA certs on the system.  SSLCACertificateFile   /etc/pki/tls/certs/ca-bundle.crt  SSLVerifyClient  require SSLVerifyDepth    10   Restart  httpd  after doing this.", 
            "title": "/etc/httpd/conf.d/ssl.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#adding-a-real-admin-user", 
            "text": "At this point, SSL should be set up on Koji Hub. Create an account for yourself (your CN) and give yourself the admin bit:  [kojiadmin@koji-hub]$  koji add-user  Your Name 123456  [kojiadmin@koji-hub]$  koji grant-permission admin  Your Name 123456   Once you're done with all the setup , you should remove the admin bit from  kojiadmin , block the user, and remove its password from the database.  [you@koji-hub]$  koji revoke-permission admin kojiadmin [you@koji-hub]$  koji disable-user kojiadmin  In the database:  koji=    update   users   set   password   =   null   where   name = kojiadmin ;", 
            "title": "Adding a real admin user"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#koji-filesystem", 
            "text": "KojiDir  is  /mnt/koji , so the following tree should be created:  [root@koji-hub]#  mkdir -p /mnt/koji/ { packages,repos,work,scratch }  [root@koji-hub]#  chown apache:apache *", 
            "title": "Koji filesystem"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#accounts-for-kojira-and-kojid", 
            "text": "It is important to note that the kojira component needs repo privileges, but if you just let the account get auto created the first time you run kojira, it won't have that privilege, so you should pre-create the account and grant it the repo privilege now.  kojiadmin@koji-hub$ koji add-user kojira\nkojiadmin@koji-hub$ koji grant-permission repo kojira  For similar technical reasons, you need to add-host each build host prior to starting kojid on that host the first time and could also do that now.    We turned off auto account creation, so there won't be a problem with accounts with bad perms getting created. This means they have to be created manually. We have kojid running on koji-hub, so this is the command we used to add it:  [you@koji-hub]$  koji add-host koji-hub.batlab.org i386 x86_64", 
            "title": "Accounts for kojira and kojid"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#koji-web-kojid", 
            "text": "Installing:  [root@koji-hub]#  yum install koji-web mod_ssl [root@koji-hub]#  yum install koji-builder  Note that we use a patched koji-builder, so be sure to install what's in the OSG repository. (The patch is not likely to be accepted upstream).", 
            "title": "Koji-Web, Kojid"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#ssl-certs", 
            "text": "The following files should be in the  /etc/pki/tls  tree:    /etc/pki/tls/private/digicert_hostkey.key \n    the host key from DigiCert    /etc/pki/tls/private/kojiweb.pem \n    digicert_hostcert.crt catted together with digicert_hostkey.key (public key first)    /etc/pki/tls/private/kojira.key \n    kojira's private key from DigiCert    /etc/pki/tls/private/kojira.pem \n    kojira.crt catted together with kojira.key (public key first)    /etc/pki/tls/cert.pem \n    symlink to certs/ca-bundle.crt    /etc/pki/tls/certs/ca-bundle.crt \n    all certs available on the system; the DigiCert Grid certs should be in here    /etc/pki/tls/certs/kojira.crt \n    kojira's cert    /etc/pki/tls/certs/digicert_hostcert.crt \n    the host cert from DigiCert    /etc/pki/tls/certs/digicert-chain.crt \n    the DigiCert CAs catted together", 
            "title": "SSL certs"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#adding-koji-builder-host-to-database", 
            "text": "[you@koji-hub]$  koji add-host-to-channel koji-hub.batlab.org createrepo  In the database:  koji= #    update   host   set   capacity   =   4   where   name   =   koji-hub.batlab.org ;", 
            "title": "Adding koji-builder host to database"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#start-kojid", 
            "text": "[root@koji-hub]#  /sbin/service kojid start  Check  /var/log/kojid.log  to make sure everything started up.", 
            "title": "Start kojid"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#kojira", 
            "text": "Installing:  [root@koji-hub]#  yum install koji-utils  Kojira also needs a user  [you@koji-hub]$  koji add-user kojira [you@koji-hub]$  koji grant-permission repo kojira  Start it up  [root@koji-hub]#  /sbin/service kojira start", 
            "title": "Kojira"
        }, 
        {
            "location": "/projects/sha2-support/", 
            "text": "SHA-2 Compliance\n\n\nWhen a certificate authority signs a certificate, it uses one of several possible hash algorithms. \nHistorically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family.\nSHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by \nGoogle researchers\n.\n These days, the preferred hash algorithm family is SHA-2.\n\n\nThe certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2.\n\n\nThe table below denotes indicates the minimum releases necessary to support SHA-2 certificates.\n\n\n\n\n\n\n\n\nComponent\n\n\nVersion\n\n\nIn Release\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBeStMan 2\n\n\nbestman2-2.3.0-9.osg\n\n\n3.1.13\n\n\nSHA-2 support; also see jGlobus, below\n\n\n\n\n\n\ndCache SRM client\n\n\ndcache-srmclient-2.2.11.1-2.osg\n\n\n3.1.22\n\n\nMajor update includes SHA-2 support\n\n\n\n\n\n\nGlobus GRAM\n\n\nglobus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg\n\n\n3.1.9\n\n\nCritical bug fixes (not SHA-2 specific)\n\n\n\n\n\n\nGUMS\n\n\ngums-1.3.18.009-15.2.osg\n\n\n3.1.13\n\n\nSwitched to jGlobus 2 with SHA-2 support; also see jGlobus, below\n\n\n\n\n\n\njGlobus (for BeStMan 2)\n\n\njglobus-2.0.5-3.osg\n\n\n3.1.18\n\n\nFixed CRL refresh bug (not SHA-2 specific)\n\n\n\n\n\n\nVOMS\n\n\nvoms-2.0.8-1.5.osg\n\n\n3.1.17\n\n\nSHA-2 fix for voms-proxy-init\n\n\n\n\n\n\n\n\nIf a component does not appear in the above table, it already has SHA-2 support.", 
            "title": "SHA-2 Support"
        }, 
        {
            "location": "/projects/sha2-support/#sha-2-compliance", 
            "text": "When a certificate authority signs a certificate, it uses one of several possible hash algorithms. \nHistorically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family.\nSHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by  Google researchers .\n These days, the preferred hash algorithm family is SHA-2.  The certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2.  The table below denotes indicates the minimum releases necessary to support SHA-2 certificates.     Component  Version  In Release  Notes      BeStMan 2  bestman2-2.3.0-9.osg  3.1.13  SHA-2 support; also see jGlobus, below    dCache SRM client  dcache-srmclient-2.2.11.1-2.osg  3.1.22  Major update includes SHA-2 support    Globus GRAM  globus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg  3.1.9  Critical bug fixes (not SHA-2 specific)    GUMS  gums-1.3.18.009-15.2.osg  3.1.13  Switched to jGlobus 2 with SHA-2 support; also see jGlobus, below    jGlobus (for BeStMan 2)  jglobus-2.0.5-3.osg  3.1.18  Fixed CRL refresh bug (not SHA-2 specific)    VOMS  voms-2.0.8-1.5.osg  3.1.17  SHA-2 fix for voms-proxy-init     If a component does not appear in the above table, it already has SHA-2 support.", 
            "title": "SHA-2 Compliance"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/", 
            "text": "OSG Technology Area Meeting, 23 April 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n8 (+0) open tickets\n\n\n17 (+0) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n116\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n44\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n9\n\n\n+6\n\n\nReady for Testing\n\n\n\n\n\n\n3\n\n\n-24\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nRegister for a \nGGUS account\n\n\nIU transition  \n\n\nSite technical transition document\n\n\nSite office hours (Central Time): \nhttps://unl.zoom.us/j/277958559\n or Telephone: US: +1 408 638 0968  or +1 646 876 9923  or +1 669 900 6833  \n\n\nMondays, 4-5pm\n\n\nTuesdays, 1-3pm\n\n\nThursdays, 10-11am\n\n\n\n\n\n\n\n\n\n\nOSG 3.4.11  \n\n\nGlideinWMS 3.2.22.2 is ready for testing\n\n\nXRootD 4.8.3 should be ready for testing\n\n\n\n\n\n\n\n\nDiscussion\n\n\nNone this week  \n\n\nSupport Update\n\n\nCaltech (Marian) - set up and registered the LIGO StashCahe origin server\nLIGO (Edgar) - some sites cannot access their frame files\nFNAL (Derek) - Joel Snow wanted to use rsync against repo for a local mirror but it wasn't working. They're happily using the UNL repo now.\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.34\n\n\n\n\nBoth\n\n\n\n\n3.4.11\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n18\n\n\n+18\n\n\n18\n\n\n+18\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n1\n\n\n+0\n\n\n9\n\n\n+9\n\n\n10\n\n\n+8\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n8\n\n\n+8\n\n\n8\n\n\n+8\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+0\n\n\n0\n\n\n+0\n\n\n3\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n-1\n\n\n4\n\n\n+0\n\n\n35\n\n\n+35\n\n\n39\n\n\n+34\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nReady for Testing  \n\n\nOSG 3.4.11\n\n\nGlideinWMS 3.2.22\n\n\nRemove broken dependencies\n\n\nproxy renewal script properly process with the default FQAN\n\n\n\n\n\n\nGratia probes\n\n\nFixed Slurm bugs found at UFL\n\n\nHTCondor probe: projectname case insensitive comparison\n\n\nPBS probe: Handle raw seconds as well as hh:mm:ss for durations\n\n\nDrop GRAM and glexec probe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReady for Release  \n\n\nOSG 3.3.34\n\n\nxrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS\n\n\nfrontier-squid: fixed startup problem under SELinux\n\n\nxrootd-hdfs 2.0.2: improved write support\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nExpect a fix for the OSG 3.3 VMU tests this week\n\n\nOSG Investigations Team\n\n\nGOC Transition is going to dominate the Investigations Team's time over the next couple of weeks.   I want to give general status of the transitions we are participating in.  So, I will give a couple of states:  \n\n\n\n\nHave not started\n\n\nIn development - Could be provisioning hardware, adapting software, installing software\n\n\nReady for testing - In a state where it can be tested by the consumers\n\n\nDeployed\n\n\n\n\n\n\n\n\n\n\nService\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nOIM\n\n\nIn Development\n\n\n\n\n\n\nOASIS\n\n\nReady for testing\n\n\n\n\n\n\nCVMFS\n\n\nReady for testing\n\n\n\n\n\n\nSoftware Repo\n\n\nDeployed\n\n\n\n\n\n\nOSG Display\n\n\nReady for testing\n\n\n\n\n\n\nStashCache Redirector\n\n\nDeployed\n\n\n\n\n\n\nOSG Collector\n\n\nReady for testing\n\n\n\n\n\n\n\n\nChanges:  \n\n\n\n\nStashCache Redirector: Testing -\n Deployed.  Origins are reporting to the new redirector.\n\n\nSoftware Repo (waiting on DNS): Testing -\n Deployed.  External site is already using it as an rsync mirror (GOC one broke)\n\n\nOIM Projects and VOs are working for GRACC.  Topology is untested.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "April 23, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#osg-technology-area-meeting-23-april-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting, 23 April 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  8 (+0) open tickets  17 (+0) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#jira", 
            "text": "# of tickets   State      116  -3  Open    44  -1  In Progress    9  +6  Ready for Testing    3  -24  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#osg-software-team", 
            "text": "Register for a  GGUS account  IU transition    Site technical transition document  Site office hours (Central Time):  https://unl.zoom.us/j/277958559  or Telephone: US: +1 408 638 0968  or +1 646 876 9923  or +1 669 900 6833    Mondays, 4-5pm  Tuesdays, 1-3pm  Thursdays, 10-11am      OSG 3.4.11    GlideinWMS 3.2.22.2 is ready for testing  XRootD 4.8.3 should be ready for testing", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#discussion", 
            "text": "None this week", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#support-update", 
            "text": "Caltech (Marian) - set up and registered the LIGO StashCahe origin server\nLIGO (Edgar) - some sites cannot access their frame files\nFNAL (Derek) - Joel Snow wanted to use rsync against repo for a local mirror but it wasn't working. They're happily using the UNL repo now.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#osg-release-team", 
            "text": "3.3.34   Both   3.4.11   Total   Status      0  +0  0  +0  18  +18  18  +18  Open    0  -1  1  +0  9  +9  10  +8  In Progress    0  +0  0  +0  8  +8  8  +8  Ready for Testing    0  +0  3  +0  0  +0  3  +0  Ready for Release    0  -1  4  +0  35  +35  39  +34  Total      Ready for Testing    OSG 3.4.11  GlideinWMS 3.2.22  Remove broken dependencies  proxy renewal script properly process with the default FQAN    Gratia probes  Fixed Slurm bugs found at UFL  HTCondor probe: projectname case insensitive comparison  PBS probe: Handle raw seconds as well as hh:mm:ss for durations  Drop GRAM and glexec probe        Ready for Release    OSG 3.3.34  xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS  frontier-squid: fixed startup problem under SELinux  xrootd-hdfs 2.0.2: improved write support", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#discussions", 
            "text": "Expect a fix for the OSG 3.3 VMU tests this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#osg-investigations-team", 
            "text": "GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks.   I want to give general status of the transitions we are participating in.  So, I will give a couple of states:     Have not started  In development - Could be provisioning hardware, adapting software, installing software  Ready for testing - In a state where it can be tested by the consumers  Deployed      Service  Status      OIM  In Development    OASIS  Ready for testing    CVMFS  Ready for testing    Software Repo  Deployed    OSG Display  Ready for testing    StashCache Redirector  Deployed    OSG Collector  Ready for testing     Changes:     StashCache Redirector: Testing -  Deployed.  Origins are reporting to the new redirector.  Software Repo (waiting on DNS): Testing -  Deployed.  External site is already using it as an rsync mirror (GOC one broke)  OIM Projects and VOs are working for GRACC.  Topology is untested.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180423/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/", 
            "text": "OSG Technology Area Meeting, 16 April 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n8 (-2) open tickets\n\n\n17 (-4) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n113\n\n\n+6\n\n\nOpen\n\n\n\n\n\n\n44\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n-6\n\n\nReady for Testing\n\n\n\n\n\n\n27\n\n\n+17\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nIU transition  \n\n\nTransition tasks are the highest priority unless stated otherwise\n\n\nWe missed quite a few \n*.grid.iu.edu\n addresses\n\n\nSite technical transition document\n\n\nSite office hours (Central Time): Monday 4-5pm, Tuesday 1-3pm, Thursday 10-11am\n\n\n\n\n\n\nStatus update on GCT release\n\n\nNew Singularity breaks glideinWMS but is fixed in the newest glideinWMS version so they must be released together\n\n\nNext doc focus 2018-04-19 1-5pm Central\n\n\n\n\nDiscussion\n\n\n\n\nOperations support staff will become unavailable starting this Thursday\n\n\n(Action item: Suchandra) Freshdesk accounts are needed for all members of the Technology Area\n\n\n(Action item: BrianL) Send instructions for GGUS account registration\n\n\n(Action item: BrianL) Transition active Footprints tickets\n\n\n\n\nSupport Update\n\n\n\n\nPurdue (Carl): needs to speak to Derek about their APEL reporting issues\n\n\nCaltech (Marian): Helped LIGO set up their new StashCahce origin\n\n\nLCMAPS VOMS transition (Suchandra): MWT2 and UCR have effectively completed their transitions\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.34\n\n\n\n\nBoth\n\n\n\n\n3.4.10\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n-12\n\n\n1\n\n\n-12\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n1\n\n\n+0\n\n\n9\n\n\n-15\n\n\n11\n\n\n-14\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n-1\n\n\n2\n\n\n-5\n\n\n2\n\n\n-6\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+2\n\n\n24\n\n\n+15\n\n\n27\n\n\n+17\n\n\nReady for Release\n\n\n\n\n\n\n1\n\n\n+1\n\n\n4\n\n\n+1\n\n\n36\n\n\n-17\n\n\n41\n\n\n-15\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nReady for Testing\n\n\nSingularity 2.4.6 (waiting on 2.4.7)\n\n\nAdding cigetcert to tarball (waiting on tarball build)\n\n\n\n\n\n\nReady for Release\n\n\nHTCondor 8.6.10\n\n\nxrootd-lcmaps fix crashes on EL6 with HTTPS requests\n\n\nxrootd-hdfs improved write support\n\n\nUpcoming: HTCondor 8.7.7\n\n\nRelease cigetcert in OSG (from fermi)\n\n\nBLAHP\n\n\nSave debugging dir when blahp job submission fails\n\n\nVerify input existance\n\n\n\n\n\n\nxrootd-hdfs 2.0.1\n\n\nFix SELinux configuration for frontier-squid\n\n\nUpdate Internet2 packages\n\n\nHTCondor CE - Accept InCommon certificates in default mapfile\n\n\nosg-configure - varies small fixes\n\n\nPackage maintenance\n\n\nfold osg-gridftp-hdfs into osg-gridftp\n\n\nosg-se-hadoop\n\n\n\n\n\n\nosg-build - internal tool maintenance\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\n(Action item: TimT) Assign gWMS testing to Edgar. Edgar will try to test it against new/old versions of Singularity\n\n\n(Action item: Derek) S\nR team members need access to the new repo host. Each S\nR member will need to send their SSH public keys to Derek for access.\n\n\n(Action item: Carl) Some work still needs to be done in osg-test to fix the EL7 3.3 failures\n\n\n\n\nOSG Investigations Team\n\n\nGOC Transition is going to dominate the Investigations Team's time over the next couple of weeks.   I want to give general status of the transitions we are participating in.  So, I will give a couple of states:  \n\n\n\n\nHave not started\n\n\nIn development - Could be provisioning hardware, adapting software, installing software\n\n\nReady for testing - In a state where it can be tested by the consumers\n\n\nDeployed\n\n\n\n\n\n\n\n\n\n\nService\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nOIM\n\n\nIn Development\n\n\n\n\n\n\nOASIS\n\n\nReady for testing\n\n\n\n\n\n\nCVMFS\n\n\nReady for testing\n\n\n\n\n\n\nSoftware Repo\n\n\nReady for testing\n\n\n\n\n\n\nOSG Display\n\n\nReady for testing\n\n\n\n\n\n\nStashCache Redirector\n\n\nReady for testing\n\n\n\n\n\n\nOSG Collector\n\n\nReady for testing\n\n\n\n\n\n\n\n\nChanges:\n- OASIS: Development -\n Testing.  Installation was completed and starting to move things over.\n- CVMFS: Development -\n Testing.  Keys where exchanged, so now we can sign repos.\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "April 16, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#osg-technology-area-meeting-16-april-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting, 16 April 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  8 (-2) open tickets  17 (-4) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#jira", 
            "text": "# of tickets   State      113  +6  Open    44  -1  In Progress    3  -6  Ready for Testing    27  +17  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#osg-software-team", 
            "text": "IU transition    Transition tasks are the highest priority unless stated otherwise  We missed quite a few  *.grid.iu.edu  addresses  Site technical transition document  Site office hours (Central Time): Monday 4-5pm, Tuesday 1-3pm, Thursday 10-11am    Status update on GCT release  New Singularity breaks glideinWMS but is fixed in the newest glideinWMS version so they must be released together  Next doc focus 2018-04-19 1-5pm Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#discussion", 
            "text": "Operations support staff will become unavailable starting this Thursday  (Action item: Suchandra) Freshdesk accounts are needed for all members of the Technology Area  (Action item: BrianL) Send instructions for GGUS account registration  (Action item: BrianL) Transition active Footprints tickets", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#support-update", 
            "text": "Purdue (Carl): needs to speak to Derek about their APEL reporting issues  Caltech (Marian): Helped LIGO set up their new StashCahce origin  LCMAPS VOMS transition (Suchandra): MWT2 and UCR have effectively completed their transitions", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#osg-release-team", 
            "text": "3.3.34   Both   3.4.10   Total   Status      0  +0  0  +0  1  -12  1  -12  Open    1  +1  1  +0  9  -15  11  -14  In Progress    0  +0  0  -1  2  -5  2  -6  Ready for Testing    0  +0  3  +2  24  +15  27  +17  Ready for Release    1  +1  4  +1  36  -17  41  -15  Total      Ready for Testing  Singularity 2.4.6 (waiting on 2.4.7)  Adding cigetcert to tarball (waiting on tarball build)    Ready for Release  HTCondor 8.6.10  xrootd-lcmaps fix crashes on EL6 with HTTPS requests  xrootd-hdfs improved write support  Upcoming: HTCondor 8.7.7  Release cigetcert in OSG (from fermi)  BLAHP  Save debugging dir when blahp job submission fails  Verify input existance    xrootd-hdfs 2.0.1  Fix SELinux configuration for frontier-squid  Update Internet2 packages  HTCondor CE - Accept InCommon certificates in default mapfile  osg-configure - varies small fixes  Package maintenance  fold osg-gridftp-hdfs into osg-gridftp  osg-se-hadoop    osg-build - internal tool maintenance", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#discussions", 
            "text": "(Action item: TimT) Assign gWMS testing to Edgar. Edgar will try to test it against new/old versions of Singularity  (Action item: Derek) S R team members need access to the new repo host. Each S R member will need to send their SSH public keys to Derek for access.  (Action item: Carl) Some work still needs to be done in osg-test to fix the EL7 3.3 failures", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#osg-investigations-team", 
            "text": "GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks.   I want to give general status of the transitions we are participating in.  So, I will give a couple of states:     Have not started  In development - Could be provisioning hardware, adapting software, installing software  Ready for testing - In a state where it can be tested by the consumers  Deployed      Service  Status      OIM  In Development    OASIS  Ready for testing    CVMFS  Ready for testing    Software Repo  Ready for testing    OSG Display  Ready for testing    StashCache Redirector  Ready for testing    OSG Collector  Ready for testing     Changes:\n- OASIS: Development -  Testing.  Installation was completed and starting to move things over.\n- CVMFS: Development -  Testing.  Keys where exchanged, so now we can sign repos.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180416/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/", 
            "text": "OSG Technology Area Meeting,  9 April 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Derek, Marian, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Mat\n\n\n10 (+0) open tickets\n\n\n21 (-6) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n107\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n45\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n9\n\n\n-6\n\n\nReady for Testing\n\n\n\n\n\n\n10\n\n\n+9\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nServices being retired or transitioned at IU\n\n\nVOMS install failures on EL6? \nhttp://vdt.cs.wisc.edu/tests/20180408-0423/results.html\n\n\nSingularity 2.4.6 built, needs testing\n\n\nNo word on GlideinWMS/XRootD updates\n\n\nNext doc focus 2018-04-19 1-5pm Central\n\n\n\n\nDiscussion\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nWicsonsin (Marian) - they're happy with the testing versions of xrootd-hdfs and xrootd-lcmaps\n\n\nMWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.34\n\n\n\n\nBoth\n\n\n\n\n3.4.10\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n13\n\n\n-1\n\n\n13\n\n\n-1\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n-1\n\n\n24\n\n\n+2\n\n\n25\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n7\n\n\n-6\n\n\n8\n\n\n-6\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n9\n\n\n+8\n\n\n10\n\n\n+9\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+0\n\n\n53\n\n\n+3\n\n\n56\n\n\n+3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nReady for Testing  \n\n\nBLAHP  \n\n\nSave debugging dir when blahp job submission fails\n\n\nVerify input existance\n\n\n\n\n\n\nxrootd-hdfs 2.0.1\n\n\nUpdate Internet2 packages\n\n\nPackage maintenance  \n\n\nfold osg-gridftp-hdfs into osg-gridftp\n\n\nosg-se-hadoop\n\n\n\n\n\n\nAdd cigetcert to AFS tarball\n\n\n\n\n\n\nReady for Release  \n\n\nHTCondor 8.6.10\n\n\nUpcoming: HTCondor 8.7.7\n\n\nRelease cigetcert in OSG (from fermi)\n\n\nosg-build - internal tool maintenance\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nGOC Transition is going to dominate the Investigations Team's time over the next couple of weeks.   I want to give general status of the transitions we are participating in.  So, I will give a couple of states:\n    1. Have not started\n    1. In development - Could be provisioning hardware, adapting software, installing software...\n    1. Ready for testing - In a state where it can be tested by the consumers\n    1. Deployed\n\n\n\n\n\n\n\n\nService\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nOIM\n\n\nIn Development\n\n\n\n\n\n\nOASIS\n\n\nIn Development\n\n\n\n\n\n\nCVMFS\n\n\nIn Development\n\n\n\n\n\n\nSoftware Repo\n\n\nReady for testing\n\n\n\n\n\n\nOSG Display\n\n\nReady for testing\n\n\n\n\n\n\nStashCache Redirector\n\n\nReady for testing\n\n\n\n\n\n\nOSG Collector\n\n\nReady for testing\n\n\n\n\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "April 9, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#osg-technology-area-meeting-9-april-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Derek, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  9 April 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#triage-duty", 
            "text": "This week: BrianL  Next week: Mat  10 (+0) open tickets  21 (-6) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#jira", 
            "text": "# of tickets   State      107  +0  Open    45  +1  In Progress    9  -6  Ready for Testing    10  +9  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#osg-software-team", 
            "text": "Services being retired or transitioned at IU  VOMS install failures on EL6?  http://vdt.cs.wisc.edu/tests/20180408-0423/results.html  Singularity 2.4.6 built, needs testing  No word on GlideinWMS/XRootD updates  Next doc focus 2018-04-19 1-5pm Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#discussion", 
            "text": "None this week", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#support-update", 
            "text": "Wicsonsin (Marian) - they're happy with the testing versions of xrootd-hdfs and xrootd-lcmaps  MWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#osg-release-team", 
            "text": "3.3.34   Both   3.4.10   Total   Status      0  +0  0  +0  13  -1  13  -1  Open    0  +0  1  -1  24  +2  25  +1  In Progress    0  +0  1  +0  7  -6  8  -6  Ready for Testing    0  +0  1  +1  9  +8  10  +9  Ready for Release    0  +0  3  +0  53  +3  56  +3  Total      Ready for Testing    BLAHP    Save debugging dir when blahp job submission fails  Verify input existance    xrootd-hdfs 2.0.1  Update Internet2 packages  Package maintenance    fold osg-gridftp-hdfs into osg-gridftp  osg-se-hadoop    Add cigetcert to AFS tarball    Ready for Release    HTCondor 8.6.10  Upcoming: HTCondor 8.7.7  Release cigetcert in OSG (from fermi)  osg-build - internal tool maintenance", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#osg-investigations-team", 
            "text": "GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks.   I want to give general status of the transitions we are participating in.  So, I will give a couple of states:\n    1. Have not started\n    1. In development - Could be provisioning hardware, adapting software, installing software...\n    1. Ready for testing - In a state where it can be tested by the consumers\n    1. Deployed     Service  Status      OIM  In Development    OASIS  In Development    CVMFS  In Development    Software Repo  Ready for testing    OSG Display  Ready for testing    StashCache Redirector  Ready for testing    OSG Collector  Ready for testing", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180409/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/", 
            "text": "OSG Technology Area Meeting,  2 April 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: BrianL\n\n\n11 (+0) open tickets\n\n\n27 (-9) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n107\n\n\n-1\n\n\nOpen\n\n\n\n\n\n\n44\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n15\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nNew vo-client package incoming\n\n\nXRootD and GlideinWMS still don't have release builds\n\n\nDoc focus:  \n\n\nNext doc focus this month\n\n\nReview the hadoop overview document\n (Suchandra to address review comments)\n\n\nUpdate current documentation to reflect current release practice\n (TimT to address review comments)\n\n\n\n\n\n\nLCMAPS VOMS transition: ~2/3 of sites have completed or effectively completed the transition\n\n\n\n\nDiscussion\n\n\nData release and possibly a software release yet this week\n\n\nSupport Update\n\n\n\n\nWicsonsin (Marian) - HTTPS transfer to XRootD causes it to crash\n\n\nMWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete\n\n\nVanderbilt (BrianL) - Routed gridmanager jobs still remain. New versions of HTCondor (available in testing) may fix this\n\n\n\n\nOSG Release Team\n\n\n\n\nData Release this week\n\n\nIGTF 1.90\n\n\nVO Package v78\n\n\nUpdated ATLAS VO default mappings\n\n\nvo-client-lcmaps-voms requiers vo-client\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.34\n\n\n\n\nBoth\n\n\n\n\n3.4.10\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n14\n\n\n-13\n\n\n14\n\n\n-13\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n2\n\n\n+2\n\n\n22\n\n\n+1\n\n\n24\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n13\n\n\n+12\n\n\n14\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\n50\n\n\n+1\n\n\n53\n\n\n+4\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nReady for Testing  \n\n\nHTCondor 8.6.10\n\n\nUpcoming: HTCondor 8.7.7\n\n\nBLAHP\n\n\nSave debugging dir when blahp job submission fails\n\n\nVerify input existance\n\n\n\n\n\n\nxrootd-hdfs 2.0.1\n\n\nPackage maintenance\n\n\nfold osg-gridftp-hdfs into osg-gridftp\n\n\nosg-se-hadoop\n\n\n\n\n\n\nosg-build - internal tool maintenance\n\n\nRelease cigetcert in OSG (from fermi)\n\n\n\n\n\n\nReady for Release  \n\n\nNothing yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nReview of the flexible release model will happen some time after the All Hands Meeting  \n\n\nOSG Investigations Team\n\n\n\n\nlot of code updates (xrootd) to StashCache for better monitoring, testing xrootd RC3 candidate as well as newer build in HCC repo with latest code changes\n\n\nwork with perfSonar team sending data from ps-collectors also to GRACC Elasticsearch\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "April 2, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#osg-technology-area-meeting-2-april-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting,  2 April 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#triage-duty", 
            "text": "This week: Carl  Next week: BrianL  11 (+0) open tickets  27 (-9) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#jira", 
            "text": "# of tickets   State      107  -1  Open    44  +3  In Progress    15  +0  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#osg-software-team", 
            "text": "New vo-client package incoming  XRootD and GlideinWMS still don't have release builds  Doc focus:    Next doc focus this month  Review the hadoop overview document  (Suchandra to address review comments)  Update current documentation to reflect current release practice  (TimT to address review comments)    LCMAPS VOMS transition: ~2/3 of sites have completed or effectively completed the transition", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#discussion", 
            "text": "Data release and possibly a software release yet this week", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#support-update", 
            "text": "Wicsonsin (Marian) - HTTPS transfer to XRootD causes it to crash  MWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete  Vanderbilt (BrianL) - Routed gridmanager jobs still remain. New versions of HTCondor (available in testing) may fix this", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#osg-release-team", 
            "text": "Data Release this week  IGTF 1.90  VO Package v78  Updated ATLAS VO default mappings  vo-client-lcmaps-voms requiers vo-client          3.3.34   Both   3.4.10   Total   Status      0  +0  0  +0  14  -13  14  -13  Open    0  +0  2  +2  22  +1  24  +3  In Progress    0  +0  1  +1  13  +12  14  +13  Ready for Testing    0  +0  0  +0  1  +1  1  +1  Ready for Release    0  +0  3  +3  50  +1  53  +4  Total      Ready for Testing    HTCondor 8.6.10  Upcoming: HTCondor 8.7.7  BLAHP  Save debugging dir when blahp job submission fails  Verify input existance    xrootd-hdfs 2.0.1  Package maintenance  fold osg-gridftp-hdfs into osg-gridftp  osg-se-hadoop    osg-build - internal tool maintenance  Release cigetcert in OSG (from fermi)    Ready for Release    Nothing yet", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#discussions", 
            "text": "Review of the flexible release model will happen some time after the All Hands Meeting", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#osg-investigations-team", 
            "text": "lot of code updates (xrootd) to StashCache for better monitoring, testing xrootd RC3 candidate as well as newer build in HCC repo with latest code changes  work with perfSonar team sending data from ps-collectors also to GRACC Elasticsearch", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180402/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/", 
            "text": "OSG Technology Area Meeting, 12 March 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\n\n\nOSG All Hands\n next week!\n\n\nNext two meetings canceled, resuming on 2018-04-02\n\n\nMarian on vacation this week\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n11 (+2) open tickets\n\n\n36 (+9) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n114\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n41\n\n\n+7\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n-9\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-5\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nOSG All Hands\n\n\nSoftware training session on Thursday morning\n\n\nSplit up between CMS/ATLAS/FIFE on Monday\n\n\n\n\n\n\nEnthusiastic LCMAPS VOMS transition support: let's aim for \n 24 hr response times and pinging users at least twice a week\n\n\nDoc focus wrap-up:\n\n\nReview the hadoop overview document\n (BrianL to review)\n\n\nUpdate current documentation to reflect current release practice\n (TimT to address review comments)\n\n\n\n\n\n\n\n\nDiscussion\n\n\nNone this week\n\n\nSupport Update\n\n\nNone this week\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.10\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n27\n\n\n+27\n\n\nOpen\n\n\n\n\n\n\n21\n\n\n+21\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n49\n\n\n+49\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.10\n\n\nReady for Testing\n\n\nxrootd-hdfs 2.0.1\n\n\n\n\n\n\nReady for Release\n\n\nNothing yet\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nReview of the flexible release model will happen some time after the All Hands Meeting\n\n\nOSG Investigations Team\n\n\nNo updates: investigations team unavailable\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "March 12, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#osg-technology-area-meeting-12-march-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 12 March 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#announcements", 
            "text": "OSG All Hands  next week!  Next two meetings canceled, resuming on 2018-04-02  Marian on vacation this week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  11 (+2) open tickets  36 (+9) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#jira", 
            "text": "# of tickets   State      114  -3  Open    41  +7  In Progress    3  -9  Ready for Testing    0  -5  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#osg-software-team", 
            "text": "OSG All Hands  Software training session on Thursday morning  Split up between CMS/ATLAS/FIFE on Monday    Enthusiastic LCMAPS VOMS transition support: let's aim for   24 hr response times and pinging users at least twice a week  Doc focus wrap-up:  Review the hadoop overview document  (BrianL to review)  Update current documentation to reflect current release practice  (TimT to address review comments)", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#discussion", 
            "text": "None this week", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#support-update", 
            "text": "None this week", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#osg-release-team", 
            "text": "3.4.10   Status      27  +27  Open    21  +21  In Progress    1  +1  Ready for Testing    0  +0  Ready for Release    49  +49  Total      3.4.10  Ready for Testing  xrootd-hdfs 2.0.1    Ready for Release  Nothing yet", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#discussions", 
            "text": "Review of the flexible release model will happen some time after the All Hands Meeting", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#osg-investigations-team", 
            "text": "No updates: investigations team unavailable", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180312/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/", 
            "text": "OSG Technology Area Meeting,  5 March 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nRegister for \nOSG All Hands\n!  \n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n9 (-2) open tickets\n\n\n27 (+0) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n117\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n34\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n12\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n6\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nEnthusiastic LCMAPS VOMS transition support: if users have questions, let's aim for \n 24 hr response times\n\n\nOSG 3.4.9/3.3.33: osg-se-hadoop progress in upcoming?\n\n\nDoc focus wrap-up:  \n\n\nReplace GUMS references in install-xrootd with lcmaps-voms auth\n (BrianL to address review comments)\n\n\nReview the hadoop overview document\n (BrianL to review)\n\n\nReview content of HTCondor-CE installation doc\n (Carl to review)\n\n\nUpdate current documentation to reflect current release practice\n (TimT to address review comments)\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\nFor the LCMAPS VOMS transitions, also aim to ping users at least twice a week\n\n\nSuchandra to finish osg-se-hadoop changes today\n\n\nUtah Hosted-CEs up and running but some troubleshooting items remain\n\n\n\n\nSupport Update\n\n\n\n\nFNAL (BrianL): \ncondor_ce_reconfig\n causes CollectorLog to be output to ToolLog (fixed in 8.6.10)\n\n\nMWT2 (Suchandra): Finishing the LCMAPS VOMS transition (including dCache)\n\n\nOSC (Mat): waiting for a response from Ops concerning the potential remaining Gratia collector\n\n\nUSCD (Edgar): Issues with XRootD 4.8.0 and the developers assure us it is fixed in 4.8.1\n\n\nUTA (BrianL): In some cases, routed jobs can't be removed without \ncondor_ce_rm -forcex\n (\nhttps://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=6586,0\n, fixed in 8.6.10)\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.9/3.3.33\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n-19\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n-14\n\n\nIn Progress\n\n\n\n\n\n\n11\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n6\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n21\n\n\n-31\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.9  \n\n\nReady for Testing  \n\n\nXRootD 4.8.1\n\n\nXRootD 4.8.0-2 in 3.3\n\n\nGlideinWMS 3.2.21-2\n\n\nauto proxy renewal\n\n\n\n\n\n\nRSV 3.17.0-1\n\n\nosg-release 3.4-3\n\n\n\n\n\n\nReady for Release  \n\n\nFrontier Squid 3.4.27-3.1\n\n\nosg-test 2.1.0-1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nAssigning XRootD 4.8.1 to Terrence Martin at UCSD since it may fix an issue they're having. TimT will find testers for the XRootD tmpfiles fix and the RSV GRACC collector query.\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nStarted better monitoring for StashCache, per directory monitoring.  Still much to do to make it production.  \n\n\nhttps://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh\n (it broke over the weekend.  see not production)\n\n\nNeed to debug the GEO-Location of StashCache CVMFS.  Some numbers don't add up.\n\n\n\n\n\n\nPefsonar data import is ongoing.\n\n\nCVMFS Stratum 1 backup at Nebraska is growing very fast, need to allocate more storage... working on it.\n\n\n(ongoing) Fix GRACC bug with projects. \nhttps://ticket.opensciencegrid.org/35943\n\n\n(ongoing) Help push belle2 usage to EGI / APEL.  \nhttps://ticket.opensciencegrid.org/35943\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "March 5, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#osg-technology-area-meeting-5-march-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting,  5 March 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#announcements", 
            "text": "Register for  OSG All Hands !", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  9 (-2) open tickets  27 (+0) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#jira", 
            "text": "# of tickets   State      117  -2  Open    34  -1  In Progress    12  +2  Ready for Testing    6  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#osg-software-team", 
            "text": "Enthusiastic LCMAPS VOMS transition support: if users have questions, let's aim for   24 hr response times  OSG 3.4.9/3.3.33: osg-se-hadoop progress in upcoming?  Doc focus wrap-up:    Replace GUMS references in install-xrootd with lcmaps-voms auth  (BrianL to address review comments)  Review the hadoop overview document  (BrianL to review)  Review content of HTCondor-CE installation doc  (Carl to review)  Update current documentation to reflect current release practice  (TimT to address review comments)", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#discussion", 
            "text": "For the LCMAPS VOMS transitions, also aim to ping users at least twice a week  Suchandra to finish osg-se-hadoop changes today  Utah Hosted-CEs up and running but some troubleshooting items remain", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#support-update", 
            "text": "FNAL (BrianL):  condor_ce_reconfig  causes CollectorLog to be output to ToolLog (fixed in 8.6.10)  MWT2 (Suchandra): Finishing the LCMAPS VOMS transition (including dCache)  OSC (Mat): waiting for a response from Ops concerning the potential remaining Gratia collector  USCD (Edgar): Issues with XRootD 4.8.0 and the developers assure us it is fixed in 4.8.1  UTA (BrianL): In some cases, routed jobs can't be removed without  condor_ce_rm -forcex  ( https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=6586,0 , fixed in 8.6.10)", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#osg-release-team", 
            "text": "3.4.9/3.3.33   Status      1  -19  Open    3  -14  In Progress    11  +2  Ready for Testing    6  +0  Ready for Release    21  -31  Total      3.4.9    Ready for Testing    XRootD 4.8.1  XRootD 4.8.0-2 in 3.3  GlideinWMS 3.2.21-2  auto proxy renewal    RSV 3.17.0-1  osg-release 3.4-3    Ready for Release    Frontier Squid 3.4.27-3.1  osg-test 2.1.0-1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#discussions", 
            "text": "Assigning XRootD 4.8.1 to Terrence Martin at UCSD since it may fix an issue they're having. TimT will find testers for the XRootD tmpfiles fix and the RSV GRACC collector query.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#last-week", 
            "text": "Started better monitoring for StashCache, per directory monitoring.  Still much to do to make it production.    https://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh  (it broke over the weekend.  see not production)  Need to debug the GEO-Location of StashCache CVMFS.  Some numbers don't add up.    Pefsonar data import is ongoing.  CVMFS Stratum 1 backup at Nebraska is growing very fast, need to allocate more storage... working on it.  (ongoing) Fix GRACC bug with projects.  https://ticket.opensciencegrid.org/35943  (ongoing) Help push belle2 usage to EGI / APEL.   https://ticket.opensciencegrid.org/35943", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180305/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/", 
            "text": "OSG Technology Area Meeting, 26 February 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nRegister for \nOSG All Hands\n and book your hotel!\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Mat\n\n\n11\n (+0) open tickets\n\n\n27 (+0) open LCMAPS VOMS tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n119\n\n\n+13\n\n\nOpen\n\n\n\n\n\n\n35\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n10\n\n\n-6\n\n\nReady for Testing\n\n\n\n\n\n\n6\n\n\n+6\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nVMU tests going held due to HTCondor bugs + new CHTC memory policy\n\n\nXRootD tmpfiles fix RFT in OSG 3.3 (developer tests still required for 3.4)\n\n\nDoc Focus Frenzy III: 2018-03-01, 1PM Central\n\n\n\n\nDiscussion\n\n\nDropping software support from the LCMAPS VOMS tickets. Separate out the LCMAPS VOMS tickets in the weekly meetings\n\n\nSupport Update\n\n\n\n\ncondor_ssh_to_job\n (Edgar) - Having back and forth with ToddT to get it working in the OSG\n\n\nUConn (BrianL) - jobs going over memory and ignoring \nPREEMPT_IF\n memory expression due to the bulk of the usage being in shared memory. \nCGROUP_MEMORY_LIMIT_POLICY = hard\n should enforce RSS + shared mem\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.9/3.3.33\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n20\n\n\n-1\n\n\nOpen\n\n\n\n\n\n\n17\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n9\n\n\n-3\n\n\nReady for Testing\n\n\n\n\n\n\n6\n\n\n+4\n\n\nReady for Release\n\n\n\n\n\n\n52\n\n\n-1\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.9\n\n\nReady for Testing\n\n\nRSV 3.17.0-1\n\n\n\n\n\n\nReady for Release\n\n\nFrontier Squid 3.4.27-3.1\n\n\nosg-test 2.1.0-1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nStarted better monitoring for StashCache, per directory monitoring.  Still much to do to make it production.\n\n\nhttps://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh\n (it broke over the weekend.  see not production)\n\n\n\n\n\n\n(ongoing) Fix GRACC bug with projects. \nhttps://ticket.opensciencegrid.org/35943\n\n\n(ongoing) Help push belle2 usage to EGI / APEL.  \nhttps://ticket.opensciencegrid.org/35943\n\n\n(Suchandra) Poked Utah again, still do not want OSG due to meltdown bug (not fully fixed?)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "February 26, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#osg-technology-area-meeting-26-february-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting, 26 February 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#announcements", 
            "text": "Register for  OSG All Hands  and book your hotel!", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#triage-duty", 
            "text": "This week: BrianL  Next week: Mat  11  (+0) open tickets  27 (+0) open LCMAPS VOMS tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#jira", 
            "text": "# of tickets   State      119  +13  Open    35  +1  In Progress    10  -6  Ready for Testing    6  +6  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#osg-software-team", 
            "text": "VMU tests going held due to HTCondor bugs + new CHTC memory policy  XRootD tmpfiles fix RFT in OSG 3.3 (developer tests still required for 3.4)  Doc Focus Frenzy III: 2018-03-01, 1PM Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#discussion", 
            "text": "Dropping software support from the LCMAPS VOMS tickets. Separate out the LCMAPS VOMS tickets in the weekly meetings", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#support-update", 
            "text": "condor_ssh_to_job  (Edgar) - Having back and forth with ToddT to get it working in the OSG  UConn (BrianL) - jobs going over memory and ignoring  PREEMPT_IF  memory expression due to the bulk of the usage being in shared memory.  CGROUP_MEMORY_LIMIT_POLICY = hard  should enforce RSS + shared mem", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#osg-release-team", 
            "text": "3.4.9/3.3.33   Status      20  -1  Open    17  -1  In Progress    9  -3  Ready for Testing    6  +4  Ready for Release    52  -1  Total      3.4.9  Ready for Testing  RSV 3.17.0-1    Ready for Release  Frontier Squid 3.4.27-3.1  osg-test 2.1.0-1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#last-week", 
            "text": "Started better monitoring for StashCache, per directory monitoring.  Still much to do to make it production.  https://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh  (it broke over the weekend.  see not production)    (ongoing) Fix GRACC bug with projects.  https://ticket.opensciencegrid.org/35943  (ongoing) Help push belle2 usage to EGI / APEL.   https://ticket.opensciencegrid.org/35943  (Suchandra) Poked Utah again, still do not want OSG due to meltdown bug (not fully fixed?)", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180226/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/", 
            "text": "OSG Technology Area Meeting, 19 February 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\n\n\nRegister for \nOSG All Hands\n!\n\n\nBrianL at Fermilab Wednesday and UChicago Thursday\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: BrianL\n\n\n39 (+11) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n106\n\n\n-5\n\n\nOpen\n\n\n\n\n\n\n34\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n16\n\n\n+4\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nOSG 3.4.9 and 3.3.33: GlideinWMS and XRootD nearly ready; possible release as early as next week\n\n\nLCMAPS VOMS  \n\n\nAnyone know anything about KIT (GermanGrid)?\n\n\nSupport instructions\n\n\n\n\n\n\nDoc Focus Frenzy III: 2018-03-01, 1PM Central\n\n\n\n\nDiscussion\n\n\nDerek will investigate KIT contact information\n\n\nSupport Update\n\n\n\n\nATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure\n\n\nNebraska (Derek) - Carl is taking on a lot of the LCMAPS VOMS transition support. Derek will forward support instructions and Brian will assist if necessary\n\n\nPurdue (Derek) - Purdue is having issues with APEL reporting of CPU Duration.  If I had to guess, I would blame PBS, but maybe log format changed and our very outdated PBS probe failed.\n\n\nMWT2 (Suchandra) - transitioning off of LCMAPS VOMS, will talk to Bob Ball about dCache configuration\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.9/3.3.33\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n21\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n12\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n53\n\n\n+6\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.9\n\n\nReady for Testing\n\n\nRSV 3.17.0-1\n\n\nosg-test 2.1.0-1\n\n\n\n\n\n\nReady for Release\n\n\nFrontier Squid 3.4.27-3.1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\n\n\nNew PS service coming to GRACC  \n\n\nCurrently running.  Still need to redirect to GRACC's ES.\n\n\n\n\n\n\nHTCondor xfer stats\n  \n\n\nAnother round of data integrity on GRACC  \n\n\nFirst focus was \"Unknown\" records in site records.\n\n\nOpened Ticket \n#35943\n\n\n\n\n\n\n\n\n\n\nThis week\n\n\n\n\nImproved StashCache monitoring coming, using xrootd fstream monitoring!\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "February 19, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#osg-technology-area-meeting-19-february-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting, 19 February 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#announcements", 
            "text": "Register for  OSG All Hands !  BrianL at Fermilab Wednesday and UChicago Thursday", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#triage-duty", 
            "text": "This week: Carl  Next week: BrianL  39 (+11) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#jira", 
            "text": "# of tickets   State      106  -5  Open    34  +4  In Progress    16  +4  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#osg-software-team", 
            "text": "OSG 3.4.9 and 3.3.33: GlideinWMS and XRootD nearly ready; possible release as early as next week  LCMAPS VOMS    Anyone know anything about KIT (GermanGrid)?  Support instructions    Doc Focus Frenzy III: 2018-03-01, 1PM Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#discussion", 
            "text": "Derek will investigate KIT contact information", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#support-update", 
            "text": "ATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure  Nebraska (Derek) - Carl is taking on a lot of the LCMAPS VOMS transition support. Derek will forward support instructions and Brian will assist if necessary  Purdue (Derek) - Purdue is having issues with APEL reporting of CPU Duration.  If I had to guess, I would blame PBS, but maybe log format changed and our very outdated PBS probe failed.  MWT2 (Suchandra) - transitioning off of LCMAPS VOMS, will talk to Bob Ball about dCache configuration", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#osg-release-team", 
            "text": "3.4.9/3.3.33   Status      21  -3  Open    18  +6  In Progress    12  +1  Ready for Testing    2  +2  Ready for Release    53  +6  Total      3.4.9  Ready for Testing  RSV 3.17.0-1  osg-test 2.1.0-1    Ready for Release  Frontier Squid 3.4.27-3.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#osg-investigations-team", 
            "text": "New PS service coming to GRACC    Currently running.  Still need to redirect to GRACC's ES.    HTCondor xfer stats     Another round of data integrity on GRACC    First focus was \"Unknown\" records in site records.  Opened Ticket  #35943", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#this-week", 
            "text": "Improved StashCache monitoring coming, using xrootd fstream monitoring!", 
            "title": "This week"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180219/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/", 
            "text": "OSG Technology Area Meeting, 12 February 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra\n\n\nAnnouncements\n\n\n\n\nRegister for OSG All Hands!\n\n\nTimT out of office, returning Feb 19\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Carl\n\n\n28 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n111\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n30\n\n\n-4\n\n\nIn Progress\n\n\n\n\n\n\n12\n\n\n+9\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nLCMAPS VOMS tickets  \n\n\nGUMS helper script and \nusage instructions\n\n\ndCache: can't use LCMAPS but grid-vomapfile has a similar format to the default voms-mapfile (FNAL using their own auth method)\n\n\nXRootD instructions currently a WIP (ATLAS sites use xrootd-voms-plugin a.k.a vomsxrd)\n\n\n\n\n\n\nDoc Focus Frenzy III: 2018-03-01, 1PM Central\n\n\n\n\nDiscussion\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure\n\n\ndisplay.opensciencegrid.org (Carl) - new build deployed on display-itb\n\n\nOU (Matyas) - LCMAPS VOMS + XRootD: Horst gave some suggestions for the LCMAPS VOMS docs\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.9/3.3.33\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n24\n\n\n+24\n\n\nOpen\n\n\n\n\n\n\n12\n\n\n+12\n\n\nIn Progress\n\n\n\n\n\n\n11\n\n\n+11\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n47\n\n\n+47\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.9  \n\n\nReady for Testing  \n\n\nFrontier Squid 3.4.27-3.1\n\n\nRSV 3.17.0-1\n\n\nosg-test 2.1.0-1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\n\n\nNew PS service coming to GRACC  \n\n\nCurrently running.  Still need to redirect to GRACC's ES.\n\n\n\n\n\n\nHTCondor xfer stats\n  \n\n\nUpdate with domain metrics.\n\n\n\n\n\n\nAnother round of data integrity on GRACC  \n\n\nFirst focus was \"Unknown\" records in site records.\n\n\nOpened Ticket \n#35943\n\n\n\n\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "February 12, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#osg-technology-area-meeting-12-february-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra", 
            "title": "OSG Technology Area Meeting, 12 February 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#announcements", 
            "text": "Register for OSG All Hands!  TimT out of office, returning Feb 19", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#triage-duty", 
            "text": "This week: Edgar  Next week: Carl  28 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#jira", 
            "text": "# of tickets   State      111  -3  Open    30  -4  In Progress    12  +9  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#osg-software-team", 
            "text": "LCMAPS VOMS tickets    GUMS helper script and  usage instructions  dCache: can't use LCMAPS but grid-vomapfile has a similar format to the default voms-mapfile (FNAL using their own auth method)  XRootD instructions currently a WIP (ATLAS sites use xrootd-voms-plugin a.k.a vomsxrd)    Doc Focus Frenzy III: 2018-03-01, 1PM Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#discussion", 
            "text": "None this week", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#support-update", 
            "text": "ATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure  display.opensciencegrid.org (Carl) - new build deployed on display-itb  OU (Matyas) - LCMAPS VOMS + XRootD: Horst gave some suggestions for the LCMAPS VOMS docs", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#osg-release-team", 
            "text": "3.4.9/3.3.33   Status      24  +24  Open    12  +12  In Progress    11  +11  Ready for Testing    0  +0  Ready for Release    47  +47  Total      3.4.9    Ready for Testing    Frontier Squid 3.4.27-3.1  RSV 3.17.0-1  osg-test 2.1.0-1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#osg-investigations-team", 
            "text": "New PS service coming to GRACC    Currently running.  Still need to redirect to GRACC's ES.    HTCondor xfer stats     Update with domain metrics.    Another round of data integrity on GRACC    First focus was \"Unknown\" records in site records.  Opened Ticket  #35943", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180212/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/", 
            "text": "OSG Technology Area Meeting,  5 February 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, TimC\n\n\nAnnouncements\n\n\n\n\nOSG All Hands registration is open\n\n\nTimT out of office, returning Feb 19\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: BrianL (?)\n\n\n28 (+5) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n114\n\n\n-9\n\n\nOpen\n\n\n\n\n\n\n34\n\n\n+11\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n-21\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-5\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nTriage/LCMAPS VOMS tickets  \n\n\nMake sure to update the \"Next Action Deadline\" after you or GOC staff have updated the ticket\n\n\nAfter a site transitions verify that pilots are still authenticating with factory ops\n\n\n\n\n\n\nDoc Focus  \n\n\nMove tickets that haven't been started out of the sprint (\nhttps://jira.opensciencegrid.org/secure/RapidBoard.jspa?rapidView=7\nprojectKey=SOFTWARE\n)\n\n\nUpdate the doc tracking spreadsheet (\nhttps://docs.google.com/spreadsheets/d/1b3_9WqjUVlszu_tM23ehaOPQVAKb5OSRvmtuLx0u8Go/edit?usp=sharing\n)\n\n    after you've reviewed a doc and changes have been merged in\n\n\nDoc Focus Frenzy III: 2018-03-01, 1PM Central\n\n\n\n\n\n\n\n\nDiscussion\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\ndisplay.opensciencegrid.org (Carl) - able to reproduce the problem and has a fix\n\n\nFIU (BrianL) - completed LCMAPS VOMS transition\n\n\nNebraska (Derek) - low CPU utilization fixed by downgrade to HTCondor 8.6.x\n\n\nOSC (Matyas) - Gave Trey an RSV patch to test\n\n\nSBGrid (Edgar) - assisting with a 3.2 -\n 3.4 upgrade. Looking to transition off of their VOMS Admin server\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.8/3.3.32\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n1\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.8/3.3.32  \n\n\nGlideinWMS frontend bug fix\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\n\n\nCMS WLCG cpu utilization debugging\n\n\nSite issue with Nebraska.  Fixed with update to HTCondor.  \n\n\nCondor Ticket \n#6426\n\n\nNot going to worry about current records.  Consulted with WLCG accounting team.\n\n\n\n\n\n\nOSG Docker images are getting a bit big, working with user support team to audit their contents\n\n\nMultiple Cuda versions were installed in single image.\n\n\n\n\n\n\nDoc Focus this week!\n\n\nSuccess!\n\n\n\n\n\n\nNew PS service coming to GRACC\n\n\nCurrently running.  Still need to redirect to GRACC's ES.\n\n\n\n\n\n\nHTCondor xfer stats\n\n\nUpdate with domain metrics.\n\n\n\n\n\n\nLigo is going to run a StashCache origin server.\n\n\nAnother round of data integrity on GRACC\n\n\nFirst focus was \"Unknown\" records in site records.\n\n\nOpened Ticket \n#35943\n\n\n\n\n\n\nFZU updated stashcache cache.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nBrianL will follow up with Scott T to update HTCondor to a version that supports xfer stats", 
            "title": "February 5, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#osg-technology-area-meeting-5-february-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, TimC", 
            "title": "OSG Technology Area Meeting,  5 February 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#announcements", 
            "text": "OSG All Hands registration is open  TimT out of office, returning Feb 19", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#triage-duty", 
            "text": "This week: Carl  Next week: BrianL (?)  28 (+5) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#jira", 
            "text": "# of tickets   State      114  -9  Open    34  +11  In Progress    3  -21  Ready for Testing    0  -5  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#osg-software-team", 
            "text": "Triage/LCMAPS VOMS tickets    Make sure to update the \"Next Action Deadline\" after you or GOC staff have updated the ticket  After a site transitions verify that pilots are still authenticating with factory ops    Doc Focus    Move tickets that haven't been started out of the sprint ( https://jira.opensciencegrid.org/secure/RapidBoard.jspa?rapidView=7 projectKey=SOFTWARE )  Update the doc tracking spreadsheet ( https://docs.google.com/spreadsheets/d/1b3_9WqjUVlszu_tM23ehaOPQVAKb5OSRvmtuLx0u8Go/edit?usp=sharing ) \n    after you've reviewed a doc and changes have been merged in  Doc Focus Frenzy III: 2018-03-01, 1PM Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#discussion", 
            "text": "None this week", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#support-update", 
            "text": "display.opensciencegrid.org (Carl) - able to reproduce the problem and has a fix  FIU (BrianL) - completed LCMAPS VOMS transition  Nebraska (Derek) - low CPU utilization fixed by downgrade to HTCondor 8.6.x  OSC (Matyas) - Gave Trey an RSV patch to test  SBGrid (Edgar) - assisting with a 3.2 -  3.4 upgrade. Looking to transition off of their VOMS Admin server", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#osg-release-team", 
            "text": "3.4.8/3.3.32   Status      0  +0  Open    0  +0  In Progress    1  +1  Ready for Testing    0  +0  Ready for Release    1  +1  Total      3.4.8/3.3.32    GlideinWMS frontend bug fix", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#osg-investigations-team", 
            "text": "CMS WLCG cpu utilization debugging  Site issue with Nebraska.  Fixed with update to HTCondor.    Condor Ticket  #6426  Not going to worry about current records.  Consulted with WLCG accounting team.    OSG Docker images are getting a bit big, working with user support team to audit their contents  Multiple Cuda versions were installed in single image.    Doc Focus this week!  Success!    New PS service coming to GRACC  Currently running.  Still need to redirect to GRACC's ES.    HTCondor xfer stats  Update with domain metrics.    Ligo is going to run a StashCache origin server.  Another round of data integrity on GRACC  First focus was \"Unknown\" records in site records.  Opened Ticket  #35943    FZU updated stashcache cache.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180205/#discussions_1", 
            "text": "BrianL will follow up with Scott T to update HTCondor to a version that supports xfer stats", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/", 
            "text": "OSG Technology Area Meeting, 29 January 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT  \n\n\nAnnouncements\n\n\n\n\nSuchandra will only be available in the afternoons until Feb 5\n\n\nTimT will be gone starting next Monday, returning Feb 19\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: Carl\n\n\n23 (+13) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n123\n\n\n-11\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n-10\n\n\nIn Progress\n\n\n\n\n\n\n24\n\n\n+6\n\n\nReady for Testing\n\n\n\n\n\n\n5\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\n15 tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication  \n\n\nThere are already instructions for edg-mkgridmap to transition.\n\n    The timeline we'd like to see is that edg-mkgridmap sites transition by the end of March.\n\n\nGUMS sites are trickier:\n\n\nIf they attach their GUMS config, MAKE SURE THEIR DB USERNAME/PASS ARE REDACTED\n\n\nFind out what GUMS clients they may have: CE, gridftp, xrootd, dcache, bestman. The first three all support LCMAPS VOMS auth, dcache and bestman2 do not. They cannot turn off their GUMS until they transition off of dcache/bestman2.\n\n\nLet Carl know of any attached GUMS configs and the output of `rpm -q osg-gums-config`.\n\n\n\n\n\n\n\n\n\n\nGCT: Frank Scheiner registered Gitbook/ReadTheDocs orgs but hasn't worked on the documentation other than that. Let's get started on making the transition from Globus to GCF/GCT. I can help setup GitHub docs.\n\n\nOSG 3.4.7  \n\n\nHDFS tested at Nebraska, looks good over the weekend\n\n\nCan we sneak HTCondor into this release?\n\n\n\n\n\n\nGOC tickets should be closed in favor of JIRA tickets if the user agrees to it (\nGOC vs JIRA\n):\n\n\nhttps://ticket.grid.iu.edu/34406\n\n\nhttps://ticket.grid.iu.edu/35448\n\n\nhttps://ticket.grid.iu.edu/35735\n\n\n\n\n\n\nDoc Focus 2 is scheduled for 1pm CST on Feb 1\n\n\n\n\nDiscussion\n\n\nEdgar would like notes on how to determine what auth clients are using for help with the LCMAPS VOMS transition  \n\n\nSupport Update\n\n\n\n\ndisplay.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate\n\n\nNebraska/Purdue (Derek) - Issues with CPU usage being reported to the WLCG.\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.7\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-7\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-16\n\n\nIn Progress\n\n\n\n\n\n\n25\n\n\n+11\n\n\nReady for Testing\n\n\n\n\n\n\n5\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n30\n\n\n-11\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.7  \n\n\nHDFS 2.6 and supporting packages\n\n\nxrootd-hdfs\n\n\ngridftp-hdfs\n\n\n\n\n\n\nHTCondor 8.6.9\n\n\nHTCondor 8.7.6\n\n\nSingularity 2.4.2: Major upgrade from 2.3.2\n\n\nfrontier-squid 3.5.27-2.1\n\n\nPegasus 4.8.1\n\n\nosg-pki-tools: fix to osg-user-cert-renew\n\n\nPerfSonar tools meta-package  \n\n\nowamp\n\n\nnuttcp\n\n\nbwctl\n\n\n\n\n\n\nRemove dependencies on osg-version\n\n\nosg-ca-generator 1.3.0  \n\n\nbackups should be created if using \nforce\n\n\nGenerate LSC file\n\n\n\n\n\n\nRemove Slurm from osg-contrib\n\n\n\n\n\n\n\n\nDiscussions\n\n\nExpect TimT's proposal for making documentation review part of the release process this week  \n\n\nOSG Investigations Team\n\n\n\n\nCMS WLCG cpu utilization debugging\n\n\nOSG Docker images are getting a bit big, working with user support team to audit their contents\n\n\nDoc Focus this week!\n\n\nNew PS service coming to GRACC\n\n\nHTCondor xfer stats\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nDerek will follow up with Scott T to update HTCondor to a version that supports xfer stats", 
            "title": "January 29, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#osg-technology-area-meeting-29-january-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 29 January 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#announcements", 
            "text": "Suchandra will only be available in the afternoons until Feb 5  TimT will be gone starting next Monday, returning Feb 19", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#triage-duty", 
            "text": "This week: Suchandra  Next week: Carl  23 (+13) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#jira", 
            "text": "# of tickets   State      123  -11  Open    23  -10  In Progress    24  +6  Ready for Testing    5  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#osg-software-team", 
            "text": "15 tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication    There are already instructions for edg-mkgridmap to transition. \n    The timeline we'd like to see is that edg-mkgridmap sites transition by the end of March.  GUMS sites are trickier:  If they attach their GUMS config, MAKE SURE THEIR DB USERNAME/PASS ARE REDACTED  Find out what GUMS clients they may have: CE, gridftp, xrootd, dcache, bestman. The first three all support LCMAPS VOMS auth, dcache and bestman2 do not. They cannot turn off their GUMS until they transition off of dcache/bestman2.  Let Carl know of any attached GUMS configs and the output of `rpm -q osg-gums-config`.      GCT: Frank Scheiner registered Gitbook/ReadTheDocs orgs but hasn't worked on the documentation other than that. Let's get started on making the transition from Globus to GCF/GCT. I can help setup GitHub docs.  OSG 3.4.7    HDFS tested at Nebraska, looks good over the weekend  Can we sneak HTCondor into this release?    GOC tickets should be closed in favor of JIRA tickets if the user agrees to it ( GOC vs JIRA ):  https://ticket.grid.iu.edu/34406  https://ticket.grid.iu.edu/35448  https://ticket.grid.iu.edu/35735    Doc Focus 2 is scheduled for 1pm CST on Feb 1", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#discussion", 
            "text": "Edgar would like notes on how to determine what auth clients are using for help with the LCMAPS VOMS transition", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#support-update", 
            "text": "display.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate  Nebraska/Purdue (Derek) - Issues with CPU usage being reported to the WLCG.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#osg-release-team", 
            "text": "3.4.7   Status      0  -7  Open    0  -16  In Progress    25  +11  Ready for Testing    5  +1  Ready for Release    30  -11  Total      3.4.7    HDFS 2.6 and supporting packages  xrootd-hdfs  gridftp-hdfs    HTCondor 8.6.9  HTCondor 8.7.6  Singularity 2.4.2: Major upgrade from 2.3.2  frontier-squid 3.5.27-2.1  Pegasus 4.8.1  osg-pki-tools: fix to osg-user-cert-renew  PerfSonar tools meta-package    owamp  nuttcp  bwctl    Remove dependencies on osg-version  osg-ca-generator 1.3.0    backups should be created if using  force  Generate LSC file    Remove Slurm from osg-contrib", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#discussions", 
            "text": "Expect TimT's proposal for making documentation review part of the release process this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#osg-investigations-team", 
            "text": "CMS WLCG cpu utilization debugging  OSG Docker images are getting a bit big, working with user support team to audit their contents  Doc Focus this week!  New PS service coming to GRACC  HTCondor xfer stats", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180129/#discussions_1", 
            "text": "Derek will follow up with Scott T to update HTCondor to a version that supports xfer stats", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/", 
            "text": "OSG Technology Area Meeting, 22 January 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Mat, TimC, TimT  \n\n\nAnnouncements\n\n\nSuchandra will only be available in the afternoons until Feb 5  \n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n10 (+2) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n134\n\n\n-8\n\n\nOpen\n\n\n\n\n\n\n33\n\n\n-5\n\n\nIn Progress\n\n\n\n\n\n\n18\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n4\n\n\n+3\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nTickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication  \n\n\nInstructions need to be improved for sites still on 3.3 (\nhttps://ticket.grid.iu.edu/35830\n)\n\n\ngWMS site tester\n results\n\n\n\n\n\n\nGCT: Seeing some external contributions and we're ramping up on the first release. Globus references and docs need to be fixed.\n\n\nOSG 3.4.7:  \n\n\nHTCondor issues with 8.6.9 and 8.7.6, will await the next release\n\n\nStatus on RSV? It's nearly ready, there's one ticket that needs to review before being built\n\n\n\n\n\n\nDoc Focus 2 is scheduled for 1pm CST on Feb 1\n\n\n\n\nDiscussion\n\n\n\n\ngWMS site tester timing out because it competes with actual job. Edgar to run a slightly different probe using Condor-G (or condor\nping\n?)\n\n\nWe need to communicate with the rest of the GCF that we will be taking point on documentation\n\n\nDerek and Edgar will attend the doc focus afternoon (Edgar will start and end ~1hr earlier)\n\n\n\n\nSupport Update\n\n\n\n\ndisplay.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate\n\n\nOK (Marian) - working on XrootD PID file placement.\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.7\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n7\n\n\n-8\n\n\nOpen\n\n\n\n\n\n\n16\n\n\n-8\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n+9\n\n\nReady for Testing\n\n\n\n\n\n\n4\n\n\n+4\n\n\nReady for Release\n\n\n\n\n\n\n41\n\n\n-3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nData Release\n\n\nIGTF 1.89\n\n\nSHA256\n\n\n\n\n\n\n3.4.7\n\n\nHDFS 2.6 and supporting packages\n\n\nSingularity 2.4.2: Major upgrade from 2.3.2\n\n\nfrontier-squid 3.5.27-2.1\n\n\nPegasus 4.8.1\n\n\nosg-pki-tools: fix to osg-user-cert-renew\n\n\nPerfSonar tools meta-package\n\n\nowamp\n\n\nnuttcp\n\n\nbwctl\n\n\n\n\n\n\nRemove dependencies on osg-version\n\n\nosg-ca-generator 1.3.0  \n\n\nbackups should be created if using \nforce\n\n\nGenerate LSC file\n\n\n\n\n\n\nRemove Slurm from osg-contrib\n\n\n\n\n\n\n\n\nDiscussions\n\n\nExpect TimT's proposal for making documentation review part of the release process this week  \n\n\nOSG Investigations Team\n\n\n\n\nNew XRootD Release Candidate available.  Will build in OSG\n\n\n3 HTCondor XFer nodes:\n\n\nOSG-Connect\n\n\nXD-Login\n\n\nWisc\n\n\n(upcoming) CSIU\n\n\n\n\n\n\nNew PS service coming to GRACC\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "January 22, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#osg-technology-area-meeting-22-january-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Derek, Edgar, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 22 January 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#announcements", 
            "text": "Suchandra will only be available in the afternoons until Feb 5", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  10 (+2) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#jira", 
            "text": "# of tickets   State      134  -8  Open    33  -5  In Progress    18  +7  Ready for Testing    4  +3  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#osg-software-team", 
            "text": "Tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication    Instructions need to be improved for sites still on 3.3 ( https://ticket.grid.iu.edu/35830 )  gWMS site tester  results    GCT: Seeing some external contributions and we're ramping up on the first release. Globus references and docs need to be fixed.  OSG 3.4.7:    HTCondor issues with 8.6.9 and 8.7.6, will await the next release  Status on RSV? It's nearly ready, there's one ticket that needs to review before being built    Doc Focus 2 is scheduled for 1pm CST on Feb 1", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#discussion", 
            "text": "gWMS site tester timing out because it competes with actual job. Edgar to run a slightly different probe using Condor-G (or condor ping ?)  We need to communicate with the rest of the GCF that we will be taking point on documentation  Derek and Edgar will attend the doc focus afternoon (Edgar will start and end ~1hr earlier)", 
            "title": "Discussion"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#support-update", 
            "text": "display.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate  OK (Marian) - working on XrootD PID file placement.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#osg-release-team", 
            "text": "3.4.7   Status      7  -8  Open    16  -8  In Progress    14  +9  Ready for Testing    4  +4  Ready for Release    41  -3  Total      Data Release  IGTF 1.89  SHA256    3.4.7  HDFS 2.6 and supporting packages  Singularity 2.4.2: Major upgrade from 2.3.2  frontier-squid 3.5.27-2.1  Pegasus 4.8.1  osg-pki-tools: fix to osg-user-cert-renew  PerfSonar tools meta-package  owamp  nuttcp  bwctl    Remove dependencies on osg-version  osg-ca-generator 1.3.0    backups should be created if using  force  Generate LSC file    Remove Slurm from osg-contrib", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#discussions", 
            "text": "Expect TimT's proposal for making documentation review part of the release process this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#osg-investigations-team", 
            "text": "New XRootD Release Candidate available.  Will build in OSG  3 HTCondor XFer nodes:  OSG-Connect  XD-Login  Wisc  (upcoming) CSIU    New PS service coming to GRACC", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180122/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/", 
            "text": "OSG Technology Area Meeting,  8 January 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: TimT\n\n\n8 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n150\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n36\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n8\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nGlideinWMS release candidate expected today\n\n\nTWiki expected to be turned down in the next few days\n\n\nAddress PR comments from last month's doc focus\n\n\nThe Madison team is deciding on a day for the next doc focus. Other team members are welcome to join the chosen date+time or set aside their own 4 hour focus time.\n\n\nThoughts/feedback on the material theme used for the technology area vs the readthedocs theme used for the docs area? \n\n\n\n\nDiscussions\n\n\n\n\nThe material theme content width is too narrow. Request a configuration knob or pointers on how to adjust it.\n\n\nAdditionally, TimC would like configuration to easily disable previous/next page links in the footer\n\n\n\n\nSupport Update\n\n\n\n\nAGLT2 (BrianL): Bad UK eScience CA update may be the cause of a user's gfal-copy failures to AGLT2's SRM\n\n\nOklahoma (Marian): /var/run/xrootd doesn't get recreated on EL7 upon reboot. Passing this information onto the xrootd developers.\n\n\nUtah (Suchandra, Derek): Adding lonepeak cluster to the OSG via hosted CE\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.7\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n15\n\n\n+2\n\n\nOpen\n\n\n\n\n\n\n24\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n5\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n44\n\n\n+6\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.7\n\n\nfrontier-squid 3.5.27-2.1\n\n\nosg-ca-generator 1.3.0\n\n\nbackups should be created if using --force\n\n\nGenerate LSC file\n\n\n\n\n\n\nRemove Slurm from osg-contrib\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\n\n\nGRACC is now collecting HTCondor transfer records.  \nAll the Graphs\n (URL likely to change)\n\n\nGRACC is now running on a new frontend (web facing node).  Check for broken things.\n\n\nNew GRACC HTCondor xfer nodes coming online\n\n\nNew PS service coming to GRACC\n\n\nStashCache monitoring / accounting in inadaquate.  We always knew this, but now we are getting requests for reports.  Need to add features to XRootD for better monitoring.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "January 8, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#osg-technology-area-meeting-8-january-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  8 January 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#triage-duty", 
            "text": "This week: Edgar  Next week: TimT  8 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#jira", 
            "text": "# of tickets   State      150  -3  Open    36  +0  In Progress    8  +2  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#osg-software-team", 
            "text": "GlideinWMS release candidate expected today  TWiki expected to be turned down in the next few days  Address PR comments from last month's doc focus  The Madison team is deciding on a day for the next doc focus. Other team members are welcome to join the chosen date+time or set aside their own 4 hour focus time.  Thoughts/feedback on the material theme used for the technology area vs the readthedocs theme used for the docs area?", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#discussions", 
            "text": "The material theme content width is too narrow. Request a configuration knob or pointers on how to adjust it.  Additionally, TimC would like configuration to easily disable previous/next page links in the footer", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#support-update", 
            "text": "AGLT2 (BrianL): Bad UK eScience CA update may be the cause of a user's gfal-copy failures to AGLT2's SRM  Oklahoma (Marian): /var/run/xrootd doesn't get recreated on EL7 upon reboot. Passing this information onto the xrootd developers.  Utah (Suchandra, Derek): Adding lonepeak cluster to the OSG via hosted CE", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#osg-release-team", 
            "text": "3.4.7   Status      15  +2  Open    24  +2  In Progress    5  +2  Ready for Testing    0  +0  Ready for Release    44  +6  Total      3.4.7  frontier-squid 3.5.27-2.1  osg-ca-generator 1.3.0  backups should be created if using --force  Generate LSC file    Remove Slurm from osg-contrib", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#osg-investigations-team", 
            "text": "GRACC is now collecting HTCondor transfer records.   All the Graphs  (URL likely to change)  GRACC is now running on a new frontend (web facing node).  Check for broken things.  New GRACC HTCondor xfer nodes coming online  New PS service coming to GRACC  StashCache monitoring / accounting in inadaquate.  We always knew this, but now we are getting requests for reports.  Need to add features to XRootD for better monitoring.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180108/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/", 
            "text": "OSG Technology Area Meeting,  2 January 2018\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Brian L, Marian, Mat, Suchandra, Tim C, Tim T\n\n\nAnnouncements\n\n\nPlease look at triage schedule and see if it works for you.\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Carl (?)\n\n\n7 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n153\n\n\n+1\n\n\nOpen\n\n\n\n\n\n\n37\n\n\n+7\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n-21\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-16\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nAddress PR comments from last month's doc focus\n\n\nExpect an e-mail this week about the January doc focus frenzy\n\n\n\n\nDiscussions\n\n\n\n\nGCT build system stuff waiting for PR review\n\n\nXRootD: \n/var/run/xrootd\n not created in reboot\n\n\n\n\nSupport Update\n\n\nNone this week\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.4.7\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n13\n\n\n+13\n\n\nOpen\n\n\n\n\n\n\n22\n\n\n+22\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n38\n\n\n+38\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n3.4.7\n\n\nosg-ca-generator 1.3.0\n\n\nbackups should be created if using --force\n\n\nGenerate LSC file\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\n\n\nGRACC is now collecting HTCondor transfer records.  Graphs incoming.\n\n\nGRACC is now running on a new frontend (web facing node).  Check for broken things.\n\n\nEdgar wrote documentation on how to send transfer records: https://opensciencegrid.github.io/docs/other/schedd-filebeats/\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "January 2, 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#osg-technology-area-meeting-2-january-2018", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Brian L, Marian, Mat, Suchandra, Tim C, Tim T", 
            "title": "OSG Technology Area Meeting,  2 January 2018"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#announcements", 
            "text": "Please look at triage schedule and see if it works for you.", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#triage-duty", 
            "text": "This week: BrianL  Next week: Carl (?)  7 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#jira", 
            "text": "# of tickets   State      153  +1  Open    37  +7  In Progress    6  -21  Ready for Testing    0  -16  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#osg-software-team", 
            "text": "Address PR comments from last month's doc focus  Expect an e-mail this week about the January doc focus frenzy", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#discussions", 
            "text": "GCT build system stuff waiting for PR review  XRootD:  /var/run/xrootd  not created in reboot", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#support-update", 
            "text": "None this week", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#osg-release-team", 
            "text": "3.4.7   Status      13  +13  Open    22  +22  In Progress    3  +3  Ready for Testing    0  +0  Ready for Release    38  +38  Total      3.4.7  osg-ca-generator 1.3.0  backups should be created if using --force  Generate LSC file", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#osg-investigations-team", 
            "text": "GRACC is now collecting HTCondor transfer records.  Graphs incoming.  GRACC is now running on a new frontend (web facing node).  Check for broken things.  Edgar wrote documentation on how to send transfer records: https://opensciencegrid.github.io/docs/other/schedd-filebeats/", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2018/TechArea20180102/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/", 
            "text": "OSG Technology Area Meeting, 18 December 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Carl, BrianL, Derek, Edgar, Marian, Mat, TimC, TimT\n\n\nAnnouncements\n\n\n\n\nSee the OSG Software google calendar for outages\n\n\nNext meeting 2018-01-02\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: TimT\n\n\n7 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n152\n\n\n+4\n\n\nOpen\n\n\n\n\n\n\n30\n\n\n-4\n\n\nIn Progress\n\n\n\n\n\n\n27\n\n\n+14\n\n\nReady for Testing\n\n\n\n\n\n\n16\n\n\n+13\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nDocumentation\n\n\nPreview of the documentation area using the material theme: \nhttps://opensciencegrid.github.io/docs-itb/\n\n\nFuture focus frenzies will give more flexibility to team members outside of Madison\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nCVMFS 2.4.2 is showing issues, reported by Lincoln Bryant and Ken Herner; may wish to pull from release, but talk to Dave Dykstra first\n\n\n\n\nSupport Update\n\n\n\n\nOkalahoma (BrianL): osg-user-cert-renew is broken due different formatting of CILogon vs DigiCert serial numbers in the OIM DB\n\n\nfix won't make the December release\n\n\n\n\n\n\nNERSC (Derek): delaying caching work until after the New Year\n\n\nDerek, BrianL: on ticket for UConn (Richard Jones) -- might need HTCondor Team help (TJ or Greg)\n\n\nMarian (Florida): XRootD errors turned out to be sporadic\n\n\n\n\nOSG Release Team\n\n\n\n\nVO Package Release expected this week (VO Package v77)\n\n\nRelease estimated to go out week of December 18th\n\n\nBetter aligns with XRootD 4.8.0 release\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.31\n\n\n\n\nBoth\n\n\n\n\n3.4.6\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-1\n\n\n3\n\n\n-4\n\n\n4\n\n\n-1\n\n\n7\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n2\n\n\n+2\n\n\n12\n\n\n+1\n\n\n9\n\n\n+1\n\n\n23\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+0\n\n\n10\n\n\n+0\n\n\n0\n\n\n+0\n\n\n11\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+1\n\n\n28\n\n\n-4\n\n\n12\n\n\n+1\n\n\n44\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nRelease Glideinwms v3.2.20+\n\n\nDrop globus-gram-client-tools requirement from glideinwms\n\n\n\n\n\n\nInclude the gfal2 http plugins as part of the osg-wn-client\n\n\nOSG PKI Tools\n\n\nDefault to using HTTPS\n\n\nUpdated help information\n\n\nLeave old keys in place if new keys cannot be fetched\n\n\nUnhandled exception when network unreachable\n\n\nUpdate to help@opensciencegrid.org\n\n\n\n\n\n\nosg-gridftp: add osg-configure-gratia\n\n\nMinor bug fix to BLAHP\n\n\nInclude cvmfs upstream bug fix for losing singularity bind mounts\n\n\nAllow tarballs to be updated by different users (internal)\n\n\n\n\n\n\n3.4.6\n\n\nNothing yet\n\n\n\n\n\n\n3.3.31\n\n\nGridFTP-HDFS: fix potential crash related to CVMFS checksums\n\n\n\n\n\n\n\n\nDiscussions\n\n\nProposal coming this week for making doc review part of the release process\n\n\nOSG Investigations Team\n\n\n\n\nAdding GPU support and transfer logs to GRACC.\n\n\n\n\nGRACC nodes getting updated; frontend node getting moved. This should require zero downtime.\n\n\n\n\n\n\nGRACC caught up from the backlog.\n\n\n\n\nPerfsonar data now flowing smoothly through the message broker.\n\n\nNERSC using CMS XRootD Cache.\n\n\nLook for changes in the gratia probe for GPU detection.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "December 18, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#osg-technology-area-meeting-18-december-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Carl, BrianL, Derek, Edgar, Marian, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 18 December 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#announcements", 
            "text": "See the OSG Software google calendar for outages  Next meeting 2018-01-02", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#triage-duty", 
            "text": "This week: Edgar  Next week: TimT  7 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#jira", 
            "text": "# of tickets   State      152  +4  Open    30  -4  In Progress    27  +14  Ready for Testing    16  +13  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#osg-software-team", 
            "text": "Documentation  Preview of the documentation area using the material theme:  https://opensciencegrid.github.io/docs-itb/  Future focus frenzies will give more flexibility to team members outside of Madison", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#discussions", 
            "text": "CVMFS 2.4.2 is showing issues, reported by Lincoln Bryant and Ken Herner; may wish to pull from release, but talk to Dave Dykstra first", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#support-update", 
            "text": "Okalahoma (BrianL): osg-user-cert-renew is broken due different formatting of CILogon vs DigiCert serial numbers in the OIM DB  fix won't make the December release    NERSC (Derek): delaying caching work until after the New Year  Derek, BrianL: on ticket for UConn (Richard Jones) -- might need HTCondor Team help (TJ or Greg)  Marian (Florida): XRootD errors turned out to be sporadic", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#osg-release-team", 
            "text": "VO Package Release expected this week (VO Package v77)  Release estimated to go out week of December 18th  Better aligns with XRootD 4.8.0 release        3.3.31   Both   3.4.6   Total   Status      0  -1  3  -4  4  -1  7  -6  Open    2  +2  12  +1  9  +1  23  +4  In Progress    1  +0  10  +0  0  +0  11  +0  Ready for Testing    0  +0  3  +3  0  +0  3  +3  Ready for Release    3  +1  28  -4  12  +1  44  +1  Total      Both  Release Glideinwms v3.2.20+  Drop globus-gram-client-tools requirement from glideinwms    Include the gfal2 http plugins as part of the osg-wn-client  OSG PKI Tools  Default to using HTTPS  Updated help information  Leave old keys in place if new keys cannot be fetched  Unhandled exception when network unreachable  Update to help@opensciencegrid.org    osg-gridftp: add osg-configure-gratia  Minor bug fix to BLAHP  Include cvmfs upstream bug fix for losing singularity bind mounts  Allow tarballs to be updated by different users (internal)    3.4.6  Nothing yet    3.3.31  GridFTP-HDFS: fix potential crash related to CVMFS checksums", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#discussions_1", 
            "text": "Proposal coming this week for making doc review part of the release process", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#osg-investigations-team", 
            "text": "Adding GPU support and transfer logs to GRACC.   GRACC nodes getting updated; frontend node getting moved. This should require zero downtime.    GRACC caught up from the backlog.   Perfsonar data now flowing smoothly through the message broker.  NERSC using CMS XRootD Cache.  Look for changes in the gratia probe for GPU detection.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171218/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/", 
            "text": "OSG Technology Area Meeting, 11 December 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Edgar\n\n\n7 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n148\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n34\n\n\n+9\n\n\nIn Progress\n\n\n\n\n\n\n13\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nAll tickets for the December release must be RFR by the end of the week\n\n\nDoc Focus Afternoon (Thursday 12/14 1PM Central)  \n\n\nWork list\n  \n\n\nPlease review tickets assigned to you in the sprint before Thursday\n\n\nTackle items assigned to you in the sprint\n\n\nIf you finish items assigned to you in the sprint, pick up items from the backlog (organized in priority order) and move them into the sprint.\n\n\n\n\n\n\nUse the #osg-software slack channel for discussions\n\n\n\n\n\n\n\n\nDiscussions\n\n\nTimC gave us a summary of Rucio from last Friday's blueprint meeting  \n\n\nSupport Update\n\n\n\n\nGRACC (Suchandra): some leftover GRACC tickets from last week's slowdown\n\n\nVanderbilt (Edgar): added GPU factory entries\n\n\nNERSC (Derek): Debugging XRootD for StashCache\n\n\n\n\nOSG Release Team\n\n\n\n\nVO Package Release expected this week (VO Package v77)\n\n\nRelease estimated to go out week of December 18th  \n\n\nBetter aligns with XRootD 4.8.0 release\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.31\n\n\n\n\nBoth\n\n\n\n\n3.4.6\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-1\n\n\n3\n\n\n-4\n\n\n4\n\n\n-1\n\n\n7\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n2\n\n\n+2\n\n\n12\n\n\n+1\n\n\n9\n\n\n+1\n\n\n23\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+0\n\n\n10\n\n\n+0\n\n\n0\n\n\n+0\n\n\n11\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+1\n\n\n28\n\n\n-4\n\n\n12\n\n\n+1\n\n\n44\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nRelease Glideinwms v3.2.20+  \n\n\nDrop globus-gram-client-tools requirement from glideinwms\n\n\n\n\n\n\nInclude the gfal2 http plugins as part of the osg-wn-client\n\n\nOSG PKI Tools  \n\n\nDefault to using HTTPS\n\n\nUpdated help information\n\n\nLeave old keys in place if new keys cannot be fetched\n\n\nUnhandled exception when network unreachable\n\n\nUpdate to help@opensciencegrid.org\n\n\n\n\n\n\nosg-gridftp: add osg-configure-gratia\n\n\nMinor bug fix to BLAHP\n\n\nInclude cvmfs upstream bug fix for losing singularity bind mounts\n\n\nAllow tarballs to be updated by different users (internal)\n\n\n\n\n\n\n3.4.6  \n\n\nNothing yet\n\n\n\n\n\n\n3.3.31  \n\n\nGridFTP-HDFS: fix potential crash related to CVMFS checksums\n\n\n\n\n\n\n\n\nDiscussions\n\n\nProposal coming this week for making doc review part of the release process  \n\n\nOSG Investigations Team\n\n\n\n\nGRACC caught up from the backlog.\n\n\nPerfsonar data now flowing smoothly through the message broker.\n\n\nNERSC using CMS XRootD Cache.\n\n\nLook for changes in the gratia probe for GPU detection.\n\n\nworking on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "December 11, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#osg-technology-area-meeting-11-december-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 11 December 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#triage-duty", 
            "text": "This week: Mat  Next week: Edgar  7 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#jira", 
            "text": "# of tickets   State      148  -6  Open    34  +9  In Progress    13  +1  Ready for Testing    3  +3  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#osg-software-team", 
            "text": "All tickets for the December release must be RFR by the end of the week  Doc Focus Afternoon (Thursday 12/14 1PM Central)    Work list     Please review tickets assigned to you in the sprint before Thursday  Tackle items assigned to you in the sprint  If you finish items assigned to you in the sprint, pick up items from the backlog (organized in priority order) and move them into the sprint.    Use the #osg-software slack channel for discussions", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#discussions", 
            "text": "TimC gave us a summary of Rucio from last Friday's blueprint meeting", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#support-update", 
            "text": "GRACC (Suchandra): some leftover GRACC tickets from last week's slowdown  Vanderbilt (Edgar): added GPU factory entries  NERSC (Derek): Debugging XRootD for StashCache", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#osg-release-team", 
            "text": "VO Package Release expected this week (VO Package v77)  Release estimated to go out week of December 18th    Better aligns with XRootD 4.8.0 release        3.3.31   Both   3.4.6   Total   Status      0  -1  3  -4  4  -1  7  -6  Open    2  +2  12  +1  9  +1  23  +4  In Progress    1  +0  10  +0  0  +0  11  +0  Ready for Testing    0  +0  3  +3  0  +0  3  +3  Ready for Release    3  +1  28  -4  12  +1  44  +1  Total      Both    Release Glideinwms v3.2.20+    Drop globus-gram-client-tools requirement from glideinwms    Include the gfal2 http plugins as part of the osg-wn-client  OSG PKI Tools    Default to using HTTPS  Updated help information  Leave old keys in place if new keys cannot be fetched  Unhandled exception when network unreachable  Update to help@opensciencegrid.org    osg-gridftp: add osg-configure-gratia  Minor bug fix to BLAHP  Include cvmfs upstream bug fix for losing singularity bind mounts  Allow tarballs to be updated by different users (internal)    3.4.6    Nothing yet    3.3.31    GridFTP-HDFS: fix potential crash related to CVMFS checksums", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#discussions_1", 
            "text": "Proposal coming this week for making doc review part of the release process", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#osg-investigations-team", 
            "text": "GRACC caught up from the backlog.  Perfsonar data now flowing smoothly through the message broker.  NERSC using CMS XRootD Cache.  Look for changes in the gratia probe for GPU detection.  working on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana)", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171211/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/", 
            "text": "OSG Technology Area Meeting,  4 December 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: Mat\n\n\n7 (+2) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n154\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n25\n\n\n-4\n\n\nIn Progress\n\n\n\n\n\n\n12\n\n\n+6\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nSpoke with XRootD team about static analysis (they run coverity). They were open to making static analysis part of their release process.\n\n\nDoc Focus Afternoon  \n\n\nDoc priority list\n \n JIRA tickets incoming\n\n\nYou may have noticed shuffling ticket ownership for doc tickets, I'm just balancing plates for the upcoming afternoon\n\n\n\n\n\n\n\n\nDon't forget to update your \nstale software tickets\n  \n\n\n\n\n\n\n\n\nAssignee\n\n\n# of stale tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n7\n\n\n\n\n\n\nBrianL\n\n\n3\n\n\n\n\n\n\nCarl\n\n\n3\n\n\n\n\n\n\nTimT\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nSuggestions to make to the XRootD team: Perform coverity diffs to easily spot new issues; dynamic analysis\n\n\nWe will go back to labeling specific releases instead of release series to better track work in the next release\n\n\n\n\nSupport Update\n\n\n\n\nGRACC (Carl, Derek, Edgar): site gratia probe timeouts due to RSV perfsonar clogging up the hosted RabbitMQ (problem seen at NET2, \nGOC-35513\n)\n\n\nNERSC (Derek, Marian): Continued work to set up non-root XRootD caching for CMS workflow, deploying service certificate to manage data access\n\n\nSyracuse (Edgar): path issue turned out to be a CE configuration issue (\nSOFTWARE-3035\n)\n\n\nTexas A\nM (Carl): PBS probe reprocesses empty logs, causing slowdowns\n\n\n\n\nOSG Release Team\n\n\n\n\nVO Package Release expected this week (VO Package v77)\n\n\nRelease estimated to go out week of December 18th\n\n\nBetter aligns with XRootD 4.8.0 release\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.31\n\n\n\n\nBoth\n\n\n\n\n3.4.6\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+1\n\n\n7\n\n\n-8\n\n\n5\n\n\n-2\n\n\n13\n\n\n-9\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n11\n\n\n-1\n\n\n8\n\n\n+3\n\n\n19\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+0\n\n\n10\n\n\n+5\n\n\n0\n\n\n+0\n\n\n11\n\n\n+5\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n+0\n\n\n28\n\n\n-4\n\n\n12\n\n\n+1\n\n\n43\n\n\n-3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nRelease Glideinwms v3.2.20+\n\n\nDrop globus-gram-client-tools requirement from glideinwms\n\n\n\n\n\n\nInclude the gfal2 http plugins as part of the osg-wn-client\n\n\nOSG PKI Tools\n\n\nDefault to using HTTPS\n\n\nUpdated help information\n\n\nLeave old keys in place if new keys cannot be fetched\n\n\n\n\n\n\nosg-gridftp: add osg-configure-gratia\n\n\nMinor bug fix to BLAHP\n\n\nInclude cvmfs upstream bug fix for losing singularity bind mounts\n\n\nAllow tarballs to be updated by different users (internal)\n\n\n\n\n\n\n3.4.6  \n\n\nNothing yet\n\n\n\n\n\n\n3.3.31  \n\n\nGridFTP-HDFS: fix potential crash related to CVMFS checksums\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nWe will go back to labeling for the specific release to better track work\n\n\n\n\nOSG Investigations Team\n\n\n\n\nGRACC is now using a hosted RabbitMQ service. Better alarms.\n\n\nRC1 release of XRootD v4.8.0 in \nosg-development\n (StashCache at Nebrasking under testing)\n\n\nworking on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "December 4, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#osg-technology-area-meeting-4-december-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  4 December 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#triage-duty", 
            "text": "This week: Suchandra  Next week: Mat  7 (+2) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#jira", 
            "text": "# of tickets   State      154  -2  Open    25  -4  In Progress    12  +6  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#osg-software-team", 
            "text": "Spoke with XRootD team about static analysis (they run coverity). They were open to making static analysis part of their release process.  Doc Focus Afternoon    Doc priority list    JIRA tickets incoming  You may have noticed shuffling ticket ownership for doc tickets, I'm just balancing plates for the upcoming afternoon     Don't forget to update your  stale software tickets        Assignee  # of stale tickets      Mat  7    BrianL  3    Carl  3    TimT  1", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#discussions", 
            "text": "Suggestions to make to the XRootD team: Perform coverity diffs to easily spot new issues; dynamic analysis  We will go back to labeling specific releases instead of release series to better track work in the next release", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#support-update", 
            "text": "GRACC (Carl, Derek, Edgar): site gratia probe timeouts due to RSV perfsonar clogging up the hosted RabbitMQ (problem seen at NET2,  GOC-35513 )  NERSC (Derek, Marian): Continued work to set up non-root XRootD caching for CMS workflow, deploying service certificate to manage data access  Syracuse (Edgar): path issue turned out to be a CE configuration issue ( SOFTWARE-3035 )  Texas A M (Carl): PBS probe reprocesses empty logs, causing slowdowns", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#osg-release-team", 
            "text": "VO Package Release expected this week (VO Package v77)  Release estimated to go out week of December 18th  Better aligns with XRootD 4.8.0 release        3.3.31   Both   3.4.6   Total   Status      1  +1  7  -8  5  -2  13  -9  Open    0  -1  11  -1  8  +3  19  +1  In Progress    1  +0  10  +5  0  +0  11  +5  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    2  +0  28  -4  12  +1  43  -3  Total      Both    Release Glideinwms v3.2.20+  Drop globus-gram-client-tools requirement from glideinwms    Include the gfal2 http plugins as part of the osg-wn-client  OSG PKI Tools  Default to using HTTPS  Updated help information  Leave old keys in place if new keys cannot be fetched    osg-gridftp: add osg-configure-gratia  Minor bug fix to BLAHP  Include cvmfs upstream bug fix for losing singularity bind mounts  Allow tarballs to be updated by different users (internal)    3.4.6    Nothing yet    3.3.31    GridFTP-HDFS: fix potential crash related to CVMFS checksums", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#discussions_1", 
            "text": "We will go back to labeling for the specific release to better track work", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#osg-investigations-team", 
            "text": "GRACC is now using a hosted RabbitMQ service. Better alarms.  RC1 release of XRootD v4.8.0 in  osg-development  (StashCache at Nebrasking under testing)  working on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana)", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171204/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/", 
            "text": "OSG Technology Area Meeting, 27 November 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: Suchandra\n\n\n5 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n-5\n\n\nOpen\n\n\n\n\n\n\n29\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-2\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nXRootD 4.8.0 release candidate available. Mainly bug fixes, can go directly into the release repo.\n\n\nDon't forget to update your \nstale software tickets\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nIceCube (Derek): GPU code wasn't working due to GLOW defaulting to running in Singularity containers\n\n\nNERSC (Derek): Continued work to set up non-root XRootD caching for CMS workflow\n\n\nPurdue (BrianL): condor_ce_trace fails for IPv6-enabled hosts\n\n\nTaiwan (BrianL): Continued assistance with job router misconfiguration\n\n\nSyracuse (Edgar): Glideins couldn't find Singularity in their expected path\n\n\n\n\nOSG Release Team\n\n\n\n\nData Release this week (IGTF 1.88) w/ SHA2 checksum (may take a little longer)\n\n\n\n\n\n\n\n\n\n\n3.3\n\n\n\n\nBoth\n\n\n\n\n3.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n15\n\n\n-5\n\n\n7\n\n\n+2\n\n\n22\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+0\n\n\n12\n\n\n+3\n\n\n5\n\n\n+1\n\n\n18\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+0\n\n\n5\n\n\n+4\n\n\n0\n\n\n-1\n\n\n6\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n+0\n\n\n32\n\n\n+2\n\n\n12\n\n\n+2\n\n\n46\n\n\n+4\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nRelease Glideinwms v3.2.20+\n\n\nDrop globus-gram-client-tools requirement from glideinwms\n\n\nInclude the gfal2 http plugins as part of the osg-wn-client\n\n\nInclude cvmfs upstream bug fix for losing singularity bind mounts\n\n\nAllow tarballs to be updated by different users\n\n\n\n\n\n\n3.4.6  \n\n\nNothing yet\n\n\n\n\n\n\n3.3.31  \n\n\nRelease GridFTP-HDFS 1.1.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\n\n\nGRACC is now using a hosted RabbitMQ service.  Better alarms.\n\n\nPerfSonar data now flowing through hosted RabbitMQ.\n\n\nBug fixes in perfsonar data collection\n\n\n\n\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "November 27, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#osg-technology-area-meeting-27-november-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 27 November 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#triage-duty", 
            "text": "This week: Carl  Next week: Suchandra  5 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#jira", 
            "text": "# of tickets   State      156  -5  Open    29  +3  In Progress    6  +1  Ready for Testing    0  -2  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#osg-software-team", 
            "text": "XRootD 4.8.0 release candidate available. Mainly bug fixes, can go directly into the release repo.  Don't forget to update your  stale software tickets", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#support-update", 
            "text": "IceCube (Derek): GPU code wasn't working due to GLOW defaulting to running in Singularity containers  NERSC (Derek): Continued work to set up non-root XRootD caching for CMS workflow  Purdue (BrianL): condor_ce_trace fails for IPv6-enabled hosts  Taiwan (BrianL): Continued assistance with job router misconfiguration  Syracuse (Edgar): Glideins couldn't find Singularity in their expected path", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#osg-release-team", 
            "text": "Data Release this week (IGTF 1.88) w/ SHA2 checksum (may take a little longer)      3.3   Both   3.4   Total   Status      0  +0  15  -5  7  +2  22  -3  Open    1  +0  12  +3  5  +1  18  +4  In Progress    1  +0  5  +4  0  -1  6  +3  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    2  +0  32  +2  12  +2  46  +4  Total      Both    Release Glideinwms v3.2.20+  Drop globus-gram-client-tools requirement from glideinwms  Include the gfal2 http plugins as part of the osg-wn-client  Include cvmfs upstream bug fix for losing singularity bind mounts  Allow tarballs to be updated by different users    3.4.6    Nothing yet    3.3.31    Release GridFTP-HDFS 1.1.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#osg-investigations-team", 
            "text": "GRACC is now using a hosted RabbitMQ service.  Better alarms.  PerfSonar data now flowing through hosted RabbitMQ.  Bug fixes in perfsonar data collection", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171127/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/", 
            "text": "OSG Technology Area Meeting, 20 November 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nThanksgiving on Thursday\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Carl\n\n\n5 (-1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n161\n\n\n-19\n\n\nOpen\n\n\n\n\n\n\n26\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n5\n\n\n+4\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n-41\n\n\nReady for Release\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nDocumentation focus afternoon 12/14\n\n\nDon't forget to update your \nstale software tickets\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCTSC (BrianL): Force GSI authentication for locally submitted jobs\n\n\nTaiwan (BrianL): Assisted with job router misconfiguration\n\n\n\n\nOSG Release Team\n\n\n\n\nData Release today: VO Package v76\n\n\n\n\n\n\n\n\n\n\n3.3\n\n\n\n\nBoth\n\n\n\n\n3.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n20\n\n\n+20\n\n\n5\n\n\n+5\n\n\n25\n\n\n+25\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n9\n\n\n+9\n\n\n4\n\n\n+4\n\n\n14\n\n\n+14\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n+2\n\n\n30\n\n\n+30\n\n\n10\n\n\n+0\n\n\n42\n\n\n+42\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nInclude cvmfs upstream bug fix for losing singularity bind mounts\n\n\n\n\n\n\n3.4.6\n\n\nUpgrade to singularity 2.4+ (On hold for January release)\n\n\n\n\n\n\n3.3.31\n\n\nRelease GridFTP-HDFS 1.1.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\n\n\nGRACC Updates.  Better data!\n\n\nStashCache writes implemented in production on OSG-Connect, more or less\n\n\nLogging for CVMFS-Singularity-Sync now going into GRACC (need to implement removal of old logs)\n\n\nOSG BIBM workshop was a success! http://sbbi-panda.unl.edu/bibm2017/program.html\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "November 20, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#osg-technology-area-meeting-20-november-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:  BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 20 November 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#announcements", 
            "text": "Thanksgiving on Thursday", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#triage-duty", 
            "text": "This week: BrianL  Next week: Carl  5 (-1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#jira", 
            "text": "# of tickets   State      161  -19  Open    26  +3  In Progress    5  +4  Ready for Testing    2  -41  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#osg-software-team", 
            "text": "Documentation focus afternoon 12/14  Don't forget to update your  stale software tickets", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#support-update", 
            "text": "CTSC (BrianL): Force GSI authentication for locally submitted jobs  Taiwan (BrianL): Assisted with job router misconfiguration", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#osg-release-team", 
            "text": "Data Release today: VO Package v76      3.3   Both   3.4   Total   Status      0  +0  20  +20  5  +5  25  +25  Open    1  +1  9  +9  4  +4  14  +14  In Progress    1  +1  1  +1  1  +1  3  +3  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    2  +2  30  +30  10  +0  42  +42  Total      Both  Include cvmfs upstream bug fix for losing singularity bind mounts    3.4.6  Upgrade to singularity 2.4+ (On hold for January release)    3.3.31  Release GridFTP-HDFS 1.1.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#osg-investigations-team", 
            "text": "GRACC Updates.  Better data!  StashCache writes implemented in production on OSG-Connect, more or less  Logging for CVMFS-Singularity-Sync now going into GRACC (need to implement removal of old logs)  OSG BIBM workshop was a success! http://sbbi-panda.unl.edu/bibm2017/program.html", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171120/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/", 
            "text": "OSG Technology Area Meeting, 13 November 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nBrian B and Derek at conferences this week\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n6 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n180\n\n\n+18\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n-39\n\n\nReady for Testing\n\n\n\n\n\n\n43\n\n\n+40\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\n3.4.5 and 3.3.30 will be the last releases on the regular schedule. Subsequent releases will use the \nflexible release schedule\n.  \n\n\nOSG Software Team\n\n\n\n\nJIRA ticket 3000 created!\n\n\nDocumentation updates  \n\n\nITB documentation: \nhttps://opensciencegrid.github.io/docs-itb/\n (push a branch to opensciencegrid/docs that starts with \"itb.\")\n\n\nMonthly (?) documentation focus afternoons (incoming doodle poll).\n\n\n\n\n\n\nGlobus Toolkit replacement = Grid Community Toolkit (\nhttps://gridcf.org/\n)\n\n\nHDFS 2++: Initial Hadoop 2.6 (from Cloudera) built but gridftp-hdfs and xrootd-hdfs [[http://vdt.cs.wisc.edu/tests/20171110-1421/results.html][tests are failing]]\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCTSC (BrianL): assisted Sanjay (UW Madison student) with certificate chain and CE questions\n\n\nTaiwan (BrianL): issues with condor daemon security because their host DN didn't match anything in the \ncondor_mapfile\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa will handle the November Release\n\n\n\n\n\n\n\n\n\n\n3.3.30\n\n\n\n\nBoth\n\n\n\n\n3.4.5\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-4\n\n\n0\n\n\n-31\n\n\n0\n\n\n-3\n\n\n0\n\n\n-38\n\n\nReady for Testing\n\n\n\n\n\n\n5\n\n\n+4\n\n\n33\n\n\n+32\n\n\n5\n\n\n+3\n\n\n43\n\n\n+39\n\n\nReady for Release\n\n\n\n\n\n\n5\n\n\n+0\n\n\n33\n\n\n+1\n\n\n5\n\n\n+0\n\n\n43\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nDefault to authenticated request and retrieval of certificates  \n\n\nDrop PKI tool quota verification\n\n\n\n\n\n\nCVMFS 2.4.2\n\n\nXRootD 4.7.1\n\n\nUpdate osg-system profiler\n\n\nglobus-gridftp-server-control 6.0\n\n\nlcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options\n\n\nDrop SOFTWARE-1853 patch from globus-ftp-client\n\n\nFix EPSV response for IPv4-mapped IPv6 addresses\n\n\ngratia probes updates\n\n\nBug fixes for BLAHP: Slurm memory parsing, Unicode decode error\n\n\nRSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported\n\n\nMaintenance on osg-test\n\n\n\n\n\n\n3.4.5  \n\n\nosg-configure 2.2.2\n\n\ntweak comments in 10-misc.ini\n\n\n\n\n\n\n3.3.30  \n\n\nosg-configure 1.10.2\n\n\nGridFTP-HDFS 1.1\n\n\nGUMS: software.grid.iu.edu -\n repo.grid.iu.edu\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nTechnology area out at conferences  \n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "November 13, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#osg-technology-area-meeting-13-november-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin    Attending:", 
            "title": "OSG Technology Area Meeting, 13 November 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#announcements", 
            "text": "Brian B and Derek at conferences this week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  6 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#jira", 
            "text": "# of tickets   State      180  +18  Open    23  +6  In Progress    1  -39  Ready for Testing    43  +40  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     3.4.5 and 3.3.30 will be the last releases on the regular schedule. Subsequent releases will use the  flexible release schedule .", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#osg-software-team", 
            "text": "JIRA ticket 3000 created!  Documentation updates    ITB documentation:  https://opensciencegrid.github.io/docs-itb/  (push a branch to opensciencegrid/docs that starts with \"itb.\")  Monthly (?) documentation focus afternoons (incoming doodle poll).    Globus Toolkit replacement = Grid Community Toolkit ( https://gridcf.org/ )  HDFS 2++: Initial Hadoop 2.6 (from Cloudera) built but gridftp-hdfs and xrootd-hdfs [[http://vdt.cs.wisc.edu/tests/20171110-1421/results.html][tests are failing]]", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#support-update", 
            "text": "CTSC (BrianL): assisted Sanjay (UW Madison student) with certificate chain and CE questions  Taiwan (BrianL): issues with condor daemon security because their host DN didn't match anything in the  condor_mapfile", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#osg-release-team", 
            "text": "Suchandra Thapa will handle the November Release      3.3.30   Both   3.4.5   Total   Status      0  +0  0  +0  0  +0  0  +0  Open    0  +0  0  +0  0  +0  0  +0  In Progress    0  -4  0  -31  0  -3  0  -38  Ready for Testing    5  +4  33  +32  5  +3  43  +39  Ready for Release    5  +0  33  +1  5  +0  43  +1  Total      Both    Default to authenticated request and retrieval of certificates    Drop PKI tool quota verification    CVMFS 2.4.2  XRootD 4.7.1  Update osg-system profiler  globus-gridftp-server-control 6.0  lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options  Drop SOFTWARE-1853 patch from globus-ftp-client  Fix EPSV response for IPv4-mapped IPv6 addresses  gratia probes updates  Bug fixes for BLAHP: Slurm memory parsing, Unicode decode error  RSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported  Maintenance on osg-test    3.4.5    osg-configure 2.2.2  tweak comments in 10-misc.ini    3.3.30    osg-configure 1.10.2  GridFTP-HDFS 1.1  GUMS: software.grid.iu.edu -  repo.grid.iu.edu", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#osg-investigations-team", 
            "text": "Technology area out at conferences", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171113/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/", 
            "text": "OSG Technology Area Meeting, 11 November 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Brian B, Carl, Derek, Edgar, Marian, Mat, Suchandra, Tim C, Tim T\n\n\nAnnouncements\n\n\nBrianL out Thu-Mon (11/2-11/6)\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n6 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n162\n\n\n+11\n\n\nOpen\n\n\n\n\n\n\n17\n\n\n-20\n\n\nIn Progress\n\n\n\n\n\n\n40\n\n\n+29\n\n\nReady for Testing\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\n3.4.5 and 3.3.30 will be the last releases on the regular schedule.  Subsequent releases will use the \nflexible release schedule\n.\n\n\nOSG Software Team\n\n\nDiscussions\n\n\nXRootd 4.7.1 has a bug that affects StashCache servers, but is not user-facing\n\n\nSupport Update\n\n\n\n\nUCSD Tier 2 (Edgar) - GPU jobs from IceCube wre causing GPU nodes to lock up and crash.\n    Might be hardware issue since we haven't seen anything like that at Nebraska.\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa will handle the November Release\n\n\n\n\n\n\n\n\n\n\n3.3.30\n\n\n\n\nBoth\n\n\n\n\n3.4.5\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-5\n\n\n0\n\n\n-1\n\n\n9\n\n\n-7\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-20\n\n\n0\n\n\n-3\n\n\n0\n\n\n-24\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n+2\n\n\n31\n\n\n+23\n\n\n3\n\n\n+1\n\n\n38\n\n\n+26\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\n4\n\n\n+4\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+1\n\n\n32\n\n\n-1\n\n\n5\n\n\n-1\n\n\n42\n\n\n-1\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nDefault to authenticated request and retrieval of certificates\n\n\nDrop PKI tool quota verification\n\n\n\n\n\n\nCVMFS 2.4.2\n\n\nXRootD 4.7.1\n\n\nUpdate osg-system profiler\n\n\nglobus-gridftp-server-control 6.0\n\n\nlcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options\n\n\nDrop SOFTWARE-1853 patch from globus-ftp-client\n\n\nFix EPSV response for IPv4-mapped IPv6 addresses\n\n\ngratia probes updates\n\n\nBug fixes for BLAHP: Slurm memory parsing, Unicode decode error\n\n\nRSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported\n\n\nMaintenance on osg-test\n\n\n\n\n\n\n3.4.5\n\n\nosg-configure 2.2.2\n\n\ntweak comments in 10-misc.ini\n\n\n\n\n\n\n3.3.30\n\n\nosg-configure 1.10.2\n\n\nGridFTP-HDFS 1.1\n\n\nGUMS: software.grid.iu.edu -\n repo.grid.iu.edu\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nNeed testing help\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC changes\n\n\nChanges to dashboard based on comments from e.g. LIGO and Minerva\n\n\nOverview of changes https://djw8605.github.io/2017/11/06/cleaning-up-gracc/\n\n\n\n\n\n\nStashCache changes\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC changes\n\n\nHelp NERSC with StashCache\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "November 6, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#osg-technology-area-meeting-11-november-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Brian B, Carl, Derek, Edgar, Marian, Mat, Suchandra, Tim C, Tim T", 
            "title": "OSG Technology Area Meeting, 11 November 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#announcements", 
            "text": "BrianL out Thu-Mon (11/2-11/6)", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  6 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#jira", 
            "text": "# of tickets   State      162  +11  Open    17  -20  In Progress    40  +29  Ready for Testing    3  +3  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     3.4.5 and 3.3.30 will be the last releases on the regular schedule.  Subsequent releases will use the  flexible release schedule .", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#discussions", 
            "text": "XRootd 4.7.1 has a bug that affects StashCache servers, but is not user-facing", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#support-update", 
            "text": "UCSD Tier 2 (Edgar) - GPU jobs from IceCube wre causing GPU nodes to lock up and crash.\n    Might be hardware issue since we haven't seen anything like that at Nebraska.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#osg-release-team", 
            "text": "Suchandra Thapa will handle the November Release      3.3.30   Both   3.4.5   Total   Status      0  -1  0  -5  0  -1  9  -7  Open    0  -1  0  -20  0  -3  0  -24  In Progress    4  +2  31  +23  3  +1  38  +26  Ready for Testing    1  +1  1  +1  2  +2  4  +4  Ready for Release    4  +1  32  -1  5  -1  42  -1  Total      Both  Default to authenticated request and retrieval of certificates  Drop PKI tool quota verification    CVMFS 2.4.2  XRootD 4.7.1  Update osg-system profiler  globus-gridftp-server-control 6.0  lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options  Drop SOFTWARE-1853 patch from globus-ftp-client  Fix EPSV response for IPv4-mapped IPv6 addresses  gratia probes updates  Bug fixes for BLAHP: Slurm memory parsing, Unicode decode error  RSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported  Maintenance on osg-test    3.4.5  osg-configure 2.2.2  tweak comments in 10-misc.ini    3.3.30  osg-configure 1.10.2  GridFTP-HDFS 1.1  GUMS: software.grid.iu.edu -  repo.grid.iu.edu", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#discussions_1", 
            "text": "Need testing help", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#last-week", 
            "text": "GRACC changes  Changes to dashboard based on comments from e.g. LIGO and Minerva  Overview of changes https://djw8605.github.io/2017/11/06/cleaning-up-gracc/    StashCache changes", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#this-week", 
            "text": "More GRACC changes  Help NERSC with StashCache", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171106/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/", 
            "text": "OSG Technology Area Meeting, 30 October 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nBrianL out Thu-Mon (11/2-11/6)\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n6 (-3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n151\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n37\n\n\n+11\n\n\nIn Progress\n\n\n\n\n\n\n11\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\nNumber of 3.4.5/3.3.30 still open or in progress\n\n\n\n\n\n\n\n\nOwner\n\n\n# not RFT\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n13\n\n\n\n\n\n\nMat\n\n\n6\n\n\n\n\n\n\nEdgar\n\n\n6\n\n\n\n\n\n\nCarl\n\n\n4\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\nDaveD\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible releases\n mean no more dev/testing deadlines. To make sure your assigned tickets are progressing, please use the \nstale Software ticket filter\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCTSC (BrianL) - Assisted Sanjay with writing submit files that he could use to submit to his test CE\n\n\nJINR (BrianL) - fixed blahp failures (use StringIO.StringIO vs io.StringIO in Python2)\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa will handle the November Release\n\n\n\n\n\n\n\n\n\n\n3.3.30\n\n\n\n\nBoth\n\n\n\n\n3.4.5\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+0\n\n\n5\n\n\n-5\n\n\n1\n\n\n+0\n\n\n7\n\n\n-5\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+0\n\n\n20\n\n\n+10\n\n\n3\n\n\n+1\n\n\n24\n\n\n+11\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+1\n\n\n8\n\n\n+1\n\n\n2\n\n\n+0\n\n\n12\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+1\n\n\n33\n\n\n+6\n\n\n6\n\n\n+1\n\n\n43\n\n\n+8\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nUpdate osg-system profiler\n\n\nglobus-gridftp-server-control 6.0\n\n\nlcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options\n\n\nDrop SOFTWARE-1853 patch from globus-ftp-client\n\n\nFix EPSV response for IPv4-mapped IPv6 addresses\n\n\n\n\n\n\n3.4.5  \n\n\nosg-configure 2.2.2\n\n\ntweak comments in 10-misc.ini\n\n\n\n\n\n\n3.3.30  \n\n\nosg-configure 1.10.2\n\n\nGridFTP-HDFS 1.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nInstalled CMS data cache at NERSC in userspace.  Will cache CMS data destined at NERSC.\n\n\nXRootD v4.7.1-rc2 packaging for StashCache testing\n\n\nEnabled Starter and StartD logs for GRACC.  Caused issues at factories, but now figured out.\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes  \n\n\nGRACC-reporting tweaks\n\n\nfully automated ES backups\n\n\n\n\n\n\nDraft GRACC SLA proposal\n\n\nWritable StashCache. Enabling access for initial testers.  \n\n\nFinalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska\n\n\nGRACC focus day on Friday.\n\n\nPuppetizing\n\n\nOPS across various services\n\n\ncheck_mk improvements\n\n\n\n\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "October 30, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#osg-technology-area-meeting-30-october-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 30 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#announcements", 
            "text": "BrianL out Thu-Mon (11/2-11/6)", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  6 (-3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#jira", 
            "text": "# of tickets   State      151  -3  Open    37  +11  In Progress    11  +1  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#osg-software-team", 
            "text": "Number of 3.4.5/3.3.30 still open or in progress     Owner  # not RFT      BrianL  13    Mat  6    Edgar  6    Carl  4    Marian  1    DaveD  1       Flexible releases  mean no more dev/testing deadlines. To make sure your assigned tickets are progressing, please use the  stale Software ticket filter", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#support-update", 
            "text": "CTSC (BrianL) - Assisted Sanjay with writing submit files that he could use to submit to his test CE  JINR (BrianL) - fixed blahp failures (use StringIO.StringIO vs io.StringIO in Python2)", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#osg-release-team", 
            "text": "Suchandra Thapa will handle the November Release      3.3.30   Both   3.4.5   Total   Status      1  +0  5  -5  1  +0  7  -5  Open    1  +0  20  +10  3  +1  24  +11  In Progress    2  +1  8  +1  2  +0  12  +2  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    4  +1  33  +6  6  +1  43  +8  Total      Both    Update osg-system profiler  globus-gridftp-server-control 6.0  lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options  Drop SOFTWARE-1853 patch from globus-ftp-client  Fix EPSV response for IPv4-mapped IPv6 addresses    3.4.5    osg-configure 2.2.2  tweak comments in 10-misc.ini    3.3.30    osg-configure 1.10.2  GridFTP-HDFS 1.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#last-week", 
            "text": "Installed CMS data cache at NERSC in userspace.  Will cache CMS data destined at NERSC.  XRootD v4.7.1-rc2 packaging for StashCache testing  Enabled Starter and StartD logs for GRACC.  Caused issues at factories, but now figured out.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#this-week", 
            "text": "Finalizing GRACC changes    GRACC-reporting tweaks  fully automated ES backups    Draft GRACC SLA proposal  Writable StashCache. Enabling access for initial testers.    Finalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska  GRACC focus day on Friday.  Puppetizing  OPS across various services  check_mk improvements", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171030/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/", 
            "text": "OSG Technology Area Meeting, 23 October 2017\n\n\nCoordinates:\n Conference: +1 669 900 6833  or +1 408 638 0968  or +1 646 876 9923, PIN: 735 282 244; \nhttps://nebraskaextension.zoom.us/j/735282244\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nDerek will be giving a short talk about querying pilot logs in Elasticsearch\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Mat\n\n\n9 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n154\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n26\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n10\n\n\n+6\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDocumentation migration complete!\n\n\nNovember release development freeze - 10/30\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nSupport Update\n\n\n\n\nCTSC (BrianL) - Worked locally with student to set up test CE for testing the blahp\n\n\nJINR (BrianL) - blahp failures due to issue with ASCII chars, finally got a stacktrace\n\n\nLBNL (BrianL) - issues with the blahp may have been due to misconfiguration\n\n\nTaiwan (BrianL) - idle jobs on CE, likely a basic configuration issue\n\n\nUCSD (BrianL) - helped Terrence troubleshoot condor not starting after issues with a full disk (resolved by restarting and waiting)\n\n\nGLOW (Edgar \n Derek) - Helping figure out why GLOW is not running as much as OSG. (mostly edgar)\n\n\nCMS (Derek) - Installation of StashCache at NERSC for CMS data.\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa will handle the \nNovember 14th\n release\n\n\n\n\n\n\n\n\n\n\n3.3.30\n\n\n\n\nBoth\n\n\n\n\n3.4.5\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+0\n\n\n10\n\n\n+1\n\n\n1\n\n\n+1\n\n\n12\n\n\n+2\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+0\n\n\n10\n\n\n-2\n\n\n2\n\n\n+0\n\n\n13\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+1\n\n\n7\n\n\n+3\n\n\n2\n\n\n+2\n\n\n10\n\n\n+6\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+1\n\n\n27\n\n\n+2\n\n\n5\n\n\n+3\n\n\n35\n\n\n+6\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate osg-system profiler\n\n\nglobus-gridftp-server-control 6.0\n\n\nlcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options\n\n\nDrop SOFTWARE-1853 patch from globus-ftp-client\n\n\n\n\n\n\n3.4.5\n\n\nosg-configure 2.2.2\n\n\ntweak comments in 10-misc.ini\n\n\n\n\n\n\n3.3.30\n\n\nosg-configure 1.10.2\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nFlexible release model being announced. November release follows the existing process\n\n\nMiron wants to have documentation updates as part of the release process\n\n\nWill be working on a mechanism to accomplish this\n\n\nTimC suggested looking at the overall documentation updates\n\n\nEngage the end users of the documentation\n\n\nRespond quickly to tickets filed by the end users\n\n\n\n\n\n\n\n\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nMore work on writable stashcache.  Began working with user support for testing.\n\n\nInstalled CMS data cache at NERSC in userspace.  Will cache CMS data destined at NERSC.\n\n\nReview papers for OSG Bioinformatic workshop.\n\n\nXRootD v4.7.1-rc2 packaging for StashCache testing\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes\n\n\nGRACC-reporting tweaks\n\n\nfully automated ES backups\n\n\n\n\n\n\nDraft GRACC SLA proposal\n\n\nWritable StashCache. Enabling access for initial testers.\n\n\nfinalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska\n\n\n\n\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nDuring the production call, David from CMS has mentioned a potential issue with GRACC hours for some CMS sites", 
            "title": "October 23, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#osg-technology-area-meeting-23-october-2017", 
            "text": "Coordinates:  Conference: +1 669 900 6833  or +1 408 638 0968  or +1 646 876 9923, PIN: 735 282 244;  https://nebraskaextension.zoom.us/j/735282244  Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 23 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#announcements", 
            "text": "Derek will be giving a short talk about querying pilot logs in Elasticsearch", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#triage-duty", 
            "text": "This week: Edgar  Next week: Mat  9 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#jira", 
            "text": "# of tickets   State      154  -6  Open    26  +0  In Progress    10  +6  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#osg-software-team", 
            "text": "Documentation migration complete!  November release development freeze - 10/30", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#support-update", 
            "text": "CTSC (BrianL) - Worked locally with student to set up test CE for testing the blahp  JINR (BrianL) - blahp failures due to issue with ASCII chars, finally got a stacktrace  LBNL (BrianL) - issues with the blahp may have been due to misconfiguration  Taiwan (BrianL) - idle jobs on CE, likely a basic configuration issue  UCSD (BrianL) - helped Terrence troubleshoot condor not starting after issues with a full disk (resolved by restarting and waiting)  GLOW (Edgar   Derek) - Helping figure out why GLOW is not running as much as OSG. (mostly edgar)  CMS (Derek) - Installation of StashCache at NERSC for CMS data.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#osg-release-team", 
            "text": "Suchandra Thapa will handle the  November 14th  release      3.3.30   Both   3.4.5   Total   Status      1  +0  10  +1  1  +1  12  +2  Open    1  +0  10  -2  2  +0  13  -2  In Progress    1  +1  7  +3  2  +2  10  +6  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    3  +1  27  +2  5  +3  35  +6  Total      Both  Update osg-system profiler  globus-gridftp-server-control 6.0  lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options  Drop SOFTWARE-1853 patch from globus-ftp-client    3.4.5  osg-configure 2.2.2  tweak comments in 10-misc.ini    3.3.30  osg-configure 1.10.2", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#discussions_1", 
            "text": "Flexible release model being announced. November release follows the existing process  Miron wants to have documentation updates as part of the release process  Will be working on a mechanism to accomplish this  TimC suggested looking at the overall documentation updates  Engage the end users of the documentation  Respond quickly to tickets filed by the end users", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#last-week", 
            "text": "More work on writable stashcache.  Began working with user support for testing.  Installed CMS data cache at NERSC in userspace.  Will cache CMS data destined at NERSC.  Review papers for OSG Bioinformatic workshop.  XRootD v4.7.1-rc2 packaging for StashCache testing", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#this-week", 
            "text": "Finalizing GRACC changes  GRACC-reporting tweaks  fully automated ES backups    Draft GRACC SLA proposal  Writable StashCache. Enabling access for initial testers.  finalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171023/#discussions_2", 
            "text": "During the production call, David from CMS has mentioned a potential issue with GRACC hours for some CMS sites", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/", 
            "text": "OSG Technology Area Meeting, 16 October 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Derek, Edgar, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nLIGO announced neutron star collision detection!\n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Edgar\n\n\n9 (-3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n160\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n26\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n+4\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-17\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDocumentation migration remaining items  \n\n\nSoftwareTeam migrations\n\n\nRelease3 and SoftwareTeam archivals\n\n\nTWiki migration headers and archive doc deletions\n\n\n\n\n\n\nGlobus EOL: Actively discussing repo destination with EGI and WLCG\n\n\nNovember release development freeze - 11/30\n\n\n\n\nDiscussions\n\n\n\n\nPer-page migration headers are preferred but a per-web migration would be easier\n\n\nWe need to search and replace twiki references in software\n\n\n\n\nSupport Update\n\n\n\n\nColorado (BrianL) - BeStMan not accepting RFC-compliant proxies\n\n\nJINR (BrianL) - blahp failures due to some issue with ASCII chars\n\n\nLBNL (BrianL) - issues with the blahp may have been due to misconfiguration\n\n\nTaiwan (BrianL) - idle jobs on CE, likely a basic configuration issue\n\n\nGLOW (Derek, Edgar) - GLOW isn't running on many OSG sites we would expect.  Edgar is leading the investigation.\n\n\nCMS / NERSC (Derek) - Add userspace StashCache for CMS data caching.  Ongoing.\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen will handle the next release if the new procedure is used\n\n\n\n\n\n\n\n\n\n\n3.3.30\n\n\n\n\nBoth\n\n\n\n\n3.4.5\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+1\n\n\n9\n\n\n+9\n\n\n0\n\n\n+0\n\n\n10\n\n\n+10\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n12\n\n\n+12\n\n\n2\n\n\n+2\n\n\n15\n\n\n+15\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n4\n\n\n+4\n\n\n0\n\n\n+0\n\n\n4\n\n\n+4\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n+2\n\n\n25\n\n\n+25\n\n\n2\n\n\n+2\n\n\n29\n\n\n+29\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate osg-system profiler\n\n\n\n\n\n\n3.4.5\n\n\n3.3.30\n\n\n\n\nDiscussions\n\n\n\n\nCA Certificate update handled in 2 days\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nWorked on cleaning up GRACC data.\n\n\nMajor update of GRACC-APEL reports to reflect non-zero CpuDuration in \nWLCG reports\n\n\nWorked on getting data from GRACC for press release (requested by BrianB/FKW)\n\n\nMore work on writable stashcache.  \n\n\nInstalled CMS data cache at NERSC in userspace.  Will cache CMS data destined at NERSC.\n\n\nLots of CVMFS changes on OSG hosted repos (except oasis).\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes\n\n\nDraft GRACC SLA proposal\n\n\nWritable StashCache. Enabling access for initial testers.\n\n\nImprove CMS cache at NERSC, productionize\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "October 16, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#osg-technology-area-meeting-16-october-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Derek, Edgar, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 16 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#announcements", 
            "text": "LIGO announced neutron star collision detection!", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#triage-duty", 
            "text": "This week: Derek  Next week: Edgar  9 (-3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#jira", 
            "text": "# of tickets   State      160  -6  Open    26  +1  In Progress    4  +4  Ready for Testing    0  -17  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#osg-software-team", 
            "text": "Documentation migration remaining items    SoftwareTeam migrations  Release3 and SoftwareTeam archivals  TWiki migration headers and archive doc deletions    Globus EOL: Actively discussing repo destination with EGI and WLCG  November release development freeze - 11/30", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#discussions", 
            "text": "Per-page migration headers are preferred but a per-web migration would be easier  We need to search and replace twiki references in software", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#support-update", 
            "text": "Colorado (BrianL) - BeStMan not accepting RFC-compliant proxies  JINR (BrianL) - blahp failures due to some issue with ASCII chars  LBNL (BrianL) - issues with the blahp may have been due to misconfiguration  Taiwan (BrianL) - idle jobs on CE, likely a basic configuration issue  GLOW (Derek, Edgar) - GLOW isn't running on many OSG sites we would expect.  Edgar is leading the investigation.  CMS / NERSC (Derek) - Add userspace StashCache for CMS data caching.  Ongoing.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#osg-release-team", 
            "text": "Tim Theisen will handle the next release if the new procedure is used      3.3.30   Both   3.4.5   Total   Status      1  +1  9  +9  0  +0  10  +10  Open    1  +1  12  +12  2  +2  15  +15  In Progress    0  +0  4  +4  0  +0  4  +4  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    2  +2  25  +25  2  +2  29  +29  Total      Both  Update osg-system profiler    3.4.5  3.3.30", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#discussions_1", 
            "text": "CA Certificate update handled in 2 days", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#last-week", 
            "text": "Worked on cleaning up GRACC data.  Major update of GRACC-APEL reports to reflect non-zero CpuDuration in  WLCG reports  Worked on getting data from GRACC for press release (requested by BrianB/FKW)  More work on writable stashcache.    Installed CMS data cache at NERSC in userspace.  Will cache CMS data destined at NERSC.  Lots of CVMFS changes on OSG hosted repos (except oasis).", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#this-week", 
            "text": "Finalizing GRACC changes  Draft GRACC SLA proposal  Writable StashCache. Enabling access for initial testers.  Improve CMS cache at NERSC, productionize", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171016/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/", 
            "text": "OSG Technology Area Meeting,  9 October 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Derek\n\n\n12 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n166\n\n\n+7\n\n\nOpen\n\n\n\n\n\n\n25\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n-11\n\n\nReady for Testing\n\n\n\n\n\n\n17\n\n\n+15\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nDocumentation transition on temporary hold as we improve the migration process  \n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\nNone last week  \n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the October Release\n\n\nVO Package v75 needs testing by CMS\n\n\nIGTF Released today\n\n\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-6\n\n\n0\n\n\n+0\n\n\n0\n\n\n-7\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-2\n\n\n0\n\n\n+5\n\n\n0\n\n\n-4\n\n\n0\n\n\n-11\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\n10\n\n\n+9\n\n\n5\n\n\n+4\n\n\n17\n\n\n+15\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n-1\n\n\n10\n\n\n-4\n\n\n5\n\n\n+0\n\n\n17\n\n\n-5\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\nDon't use mirrors for goc repos\n\n\nosg-ca-scripts: require wget\n\n\nosg-configure: Detect when fetch-crl missing\n\n\nosg-configure: don't use condor_config_val -expand\n\n\ngsi-openssh-server update for LIGO\n\n\n\n\n\n\n3.4.4\n\n\nUpdate to HTCondor 8.6.6 in OSG 3.4\n\n\nUpdate to HTCondor 8.7.3 in Upcoming\n\n\nUpdate to singularity-2.3.2+\n\n\nAdd singularity to osg-tested-internal\n\n\nosg-configure: Release 2.2.1\n\n\n\n\n\n\n3.3.29\n\n\nRelease voms-admin-server-2.7.0-1.23+\n\n\nosg-configure: Release 1.10.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nDerek and Marian will get a tester from CMS for the vo-client package\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nWorked on cleaning up GRACC data\n\n\nComparing cleaned GRACC Data\n\n\nMore work on writable stashcache, still waiting on bugfix\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes\n\n\nHope to have writeable StashCache avail. for early testers; waiting for Condor bugfix\n\n\nOSG - CVMFS focus day to improve the \n*.osgstorage\n and \nsingularity.opensciencegrid.org\n repo stability.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "October 9, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-technology-area-meeting-9-october-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  9 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#triage-duty", 
            "text": "This week: BrianL  Next week: Derek  12 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#jira", 
            "text": "# of tickets   State      166  +7  Open    25  +2  In Progress    1  -11  Ready for Testing    17  +15  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-software-team", 
            "text": "Documentation transition on temporary hold as we improve the migration process", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#support-update", 
            "text": "None last week", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-release-team", 
            "text": "Tim Theisen is handling the October Release  VO Package v75 needs testing by CMS  IGTF Released today      3.3.29   Both   3.4.4   Total   Status      0  +0  0  -2  0  +0  0  -2  Open    0  -1  0  -6  0  +0  0  -7  In Progress    0  -2  0  +5  0  -4  0  -11  Ready for Testing    2  +2  10  +9  5  +4  17  +15  Ready for Release    2  -1  10  -4  5  +0  17  -5  Total      Both  Update globus-gridftp-server-control to 5.2  Don't use mirrors for goc repos  osg-ca-scripts: require wget  osg-configure: Detect when fetch-crl missing  osg-configure: don't use condor_config_val -expand  gsi-openssh-server update for LIGO    3.4.4  Update to HTCondor 8.6.6 in OSG 3.4  Update to HTCondor 8.7.3 in Upcoming  Update to singularity-2.3.2+  Add singularity to osg-tested-internal  osg-configure: Release 2.2.1    3.3.29  Release voms-admin-server-2.7.0-1.23+  osg-configure: Release 1.10.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#discussions_1", 
            "text": "Derek and Marian will get a tester from CMS for the vo-client package", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#last-week", 
            "text": "Worked on cleaning up GRACC data  Comparing cleaned GRACC Data  More work on writable stashcache, still waiting on bugfix", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#this-week", 
            "text": "Finalizing GRACC changes  Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix  OSG - CVMFS focus day to improve the  *.osgstorage  and  singularity.opensciencegrid.org  repo stability.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/", 
            "text": "OSG Technology Area Meeting, 2 October 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Mat, Carl, Edgar, Suchandra, Derek, Marian, BrianB\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: BrianL\n\n\n10 (-1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n159\n\n\n+4\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n12\n\n\n+4\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\n\n\nOpen JIRA tickets\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets not RFT\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n5\n\n\n\n\n\n\nMat\n\n\n4\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nCarl\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware and release teams merged. Effort #s updated \nhere\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/opensciencegrid/docs/pulse#new-issues\n\n\nhttps://github.com/opensciencegrid/technology/pulse#new-issues\n\n\n\n\nHold off on these for now\n\n\n\n\nDiscussions\n\n\nSupport Update\n\n\n\n\nStill debugging XRootD issues at Florida; possibly found leaks?\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-3\n\n\n2\n\n\n-10\n\n\n0\n\n\n-3\n\n\n2\n\n\n-16\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n6\n\n\n-2\n\n\n0\n\n\n-2\n\n\n7\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+2\n\n\n5\n\n\n+4\n\n\n4\n\n\n+4\n\n\n11\n\n\n+10\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+0\n\n\n14\n\n\n-7\n\n\n5\n\n\n+0\n\n\n22\n\n\n-7\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\nDon't use mirrors for goc repos\n\n\nosg-ca-scripts: require wget\n\n\nosg-configure: Detect when fetch-crl missing\n\n\nosg-configure: don't use condor_config_val -expand\n\n\n\n\n\n\n3.4.4\n\n\nUpdate to HTCondor 8.6.6 in OSG 3.4\n\n\nUpdate to HTCondor 8.7.3 in Upcoming\n\n\nUpdate to singularity-2.3.2+\n\n\nAdd singularity to osg-tested-internal\n\n\nosg-configure: Release 2.2.1\n\n\n\n\n\n\n3.3.29\n\n\nRelease voms-admin-server-2.7.0-1.23+\n\n\nosg-configure: Release 1.10.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nWorked on cleaning up GRACC data\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes\n\n\nHope to have writeable StashCache avail. for early testers; waiting for Condor bugfix\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions", 
            "title": "October 2, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-technology-area-meeting-2-october-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Mat, Carl, Edgar, Suchandra, Derek, Marian, BrianB", 
            "title": "OSG Technology Area Meeting, 2 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#triage-duty", 
            "text": "This week: Carl  Next week: BrianL  10 (-1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#jira", 
            "text": "# of tickets   State      159  +4  Open    23  +3  In Progress    12  +4  Ready for Testing    2  +2  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-software-team", 
            "text": "Open JIRA tickets     Owner  # tickets not RFT      BrianL  5    Mat  4    Edgar  2    Carl  0       Software and release teams merged. Effort #s updated  here", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#documentation", 
            "text": "https://github.com/opensciencegrid/docs/pulse#new-issues  https://github.com/opensciencegrid/technology/pulse#new-issues   Hold off on these for now", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#discussions", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#support-update", 
            "text": "Still debugging XRootD issues at Florida; possibly found leaks?", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-release-team", 
            "text": "3.3.29   Both   3.4.4   Total   Status      0  -3  2  -10  0  -3  2  -16  Open    1  +1  6  -2  0  -2  7  -3  In Progress    2  +2  5  +4  4  +4  11  +10  Ready for Testing    0  +0  1  +1  1  +1  2  +2  Ready for Release    3  +0  14  -7  5  +0  22  -7  Total      Both  Update globus-gridftp-server-control to 5.2  Don't use mirrors for goc repos  osg-ca-scripts: require wget  osg-configure: Detect when fetch-crl missing  osg-configure: don't use condor_config_val -expand    3.4.4  Update to HTCondor 8.6.6 in OSG 3.4  Update to HTCondor 8.7.3 in Upcoming  Update to singularity-2.3.2+  Add singularity to osg-tested-internal  osg-configure: Release 2.2.1    3.3.29  Release voms-admin-server-2.7.0-1.23+  osg-configure: Release 1.10.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#last-week", 
            "text": "Worked on cleaning up GRACC data", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#this-week", 
            "text": "Finalizing GRACC changes  Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#discussions_2", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/", 
            "text": "OSG Technology Area Meeting, 25 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Tim C, Brian L, Suchandra, Marian, Edgar, Mat, Derek, Carl\n\n\nAnnouncements\n\n\nOSG All Hands 2018 at University of Utah Mar 19 - 22, 2018\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n11 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n155\n\n\n-7\n\n\nOpen\n\n\n\n\n\n\n20\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n8\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\n\n\nOpen JIRA tickets\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets not RFT\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n12\n\n\n\n\n\n\nCarl\n\n\n4\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nBrianL\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware and release teams merged. Effort #s updated \nhere\n\n\n\n\nIt sounds like HDFS 3 isn't expected until at least April 2018\n\n\nShip the newest version of HDFS 2 in OSG 3.4\n\n\nAim for November, but don't know how much work it will take yet\n\n\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/opensciencegrid/docs/pulse#new-issues\n\n\nhttps://github.com/opensciencegrid/technology/pulse#new-issues\n\n\n\n\nOnly 3 docs fully migrated, ~7 docs awaiting review\n\n\nRelease3: ~45 docs remaining\n\n\nSoftwareTeam: ~50 docs remaining\n\n\n\n\nDiscussions\n\n\n\n\nSoftware/Release merger finalized -- see front page of tech area docs\n\n\nSoftware folks should expect to do more testing; release folks might have to help out with software updates\n\n\n\n\nSupport Update\n\n\n\n\nLBNL (BrianL): Strange Gridmanager errors showing up intermittently; asked them to update to condor 8.6\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-3\n\n\n2\n\n\n-10\n\n\n0\n\n\n-3\n\n\n2\n\n\n-16\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n6\n\n\n-2\n\n\n0\n\n\n-2\n\n\n7\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+2\n\n\n5\n\n\n+4\n\n\n4\n\n\n+4\n\n\n11\n\n\n+10\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+0\n\n\n14\n\n\n-7\n\n\n5\n\n\n+0\n\n\n22\n\n\n-7\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\nDon't use mirrors for goc repos\n\n\nosg-ca-scripts: require wget\n\n\nosg-configure: Detect when fetch-crl missing\n\n\nosg-configure: don't use condor_config_val -expand\n\n\n\n\n\n\n3.4.4\n\n\nUpdate to HTCondor 8.6.6 in OSG 3.4\n\n\nUpdate to HTCondor 8.7.3 in Upcoming\n\n\nUpdate to singularity-2.3.2+\n\n\nAdd singularity to osg-tested-internal\n\n\nosg-configure: Release 2.2.1\n\n\n\n\n\n\n3.3.29\n\n\nRelease voms-admin-server-2.7.0-1.23+\n\n\nosg-configure: Release 1.10.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nEnabling writeback of Stash\n\n\nExtra backups made of GRACC records\n\n\nAPEL report issues related to firewall; issues were fixed and a check_mk report was added\n\n\n\n\nThis Week\n\n\n\n\nStill working on cleaning up GRACC data\n\n\nFinish writeback of Stash\n\n\nBetter monitoring of APEL uploads\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nStarting to setup time for Kibana walkthrough for Glidein Logs", 
            "title": "September 25, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-technology-area-meeting-25-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Tim C, Brian L, Suchandra, Marian, Edgar, Mat, Derek, Carl", 
            "title": "OSG Technology Area Meeting, 25 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#announcements", 
            "text": "OSG All Hands 2018 at University of Utah Mar 19 - 22, 2018", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  11 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#jira", 
            "text": "# of tickets   State      155  -7  Open    20  +1  In Progress    8  +7  Ready for Testing    0  0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-software-team", 
            "text": "Open JIRA tickets     Owner  # tickets not RFT      Mat  12    Carl  4    Edgar  2    BrianL  1       Software and release teams merged. Effort #s updated  here   It sounds like HDFS 3 isn't expected until at least April 2018  Ship the newest version of HDFS 2 in OSG 3.4  Aim for November, but don't know how much work it will take yet", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#documentation", 
            "text": "https://github.com/opensciencegrid/docs/pulse#new-issues  https://github.com/opensciencegrid/technology/pulse#new-issues   Only 3 docs fully migrated, ~7 docs awaiting review  Release3: ~45 docs remaining  SoftwareTeam: ~50 docs remaining", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#discussions", 
            "text": "Software/Release merger finalized -- see front page of tech area docs  Software folks should expect to do more testing; release folks might have to help out with software updates", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#support-update", 
            "text": "LBNL (BrianL): Strange Gridmanager errors showing up intermittently; asked them to update to condor 8.6", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-release-team", 
            "text": "3.3.29   Both   3.4.4   Total   Status      0  -3  2  -10  0  -3  2  -16  Open    1  +1  6  -2  0  -2  7  -3  In Progress    2  +2  5  +4  4  +4  11  +10  Ready for Testing    0  +0  1  +1  1  +1  2  +2  Ready for Release    3  +0  14  -7  5  +0  22  -7  Total      Both  Update globus-gridftp-server-control to 5.2  Don't use mirrors for goc repos  osg-ca-scripts: require wget  osg-configure: Detect when fetch-crl missing  osg-configure: don't use condor_config_val -expand    3.4.4  Update to HTCondor 8.6.6 in OSG 3.4  Update to HTCondor 8.7.3 in Upcoming  Update to singularity-2.3.2+  Add singularity to osg-tested-internal  osg-configure: Release 2.2.1    3.3.29  Release voms-admin-server-2.7.0-1.23+  osg-configure: Release 1.10.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#last-week", 
            "text": "Enabling writeback of Stash  Extra backups made of GRACC records  APEL report issues related to firewall; issues were fixed and a check_mk report was added", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#this-week", 
            "text": "Still working on cleaning up GRACC data  Finish writeback of Stash  Better monitoring of APEL uploads", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#discussions_2", 
            "text": "Starting to setup time for Kibana walkthrough for Glidein Logs", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/", 
            "text": "OSG Technology Area Meeting, 18 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n10 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n162\n\n\n+9\n\n\nOpen\n\n\n\n\n\n\n19\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n-5\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-20\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nosghost downtime starts tomorrow at 2pm Central  \n\n\nDocumentation\n\n\n\n\nhttps://github.com/opensciencegrid/docs/pulse#new-issues\n  \n\n\n7 docs fully migrated (many more awaiting review!), \n100 docs archived\n\n\nRelease3: 17 high-priority and ~30 low-priority docs remaining\n\n\nSoftwareTeam: ~56 docs remaining\n\n\n\n\n\n\nCarl's mass migration and archival wrapper tools are robust enough to be used by other areas\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nClemson (BrianL): Blahp segfaults appeared to be due to overloaded NFS server\n\n\nCTSC (BrianL): Attended meeting to discuss HTCondor issues encountered\n\n\nLBNL (BrianL): Strange Gridmanager errors showing up intermittently\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n3\n\n\n+3\n\n\n12\n\n\n+12\n\n\n3\n\n\n+3\n\n\n18\n\n\n+18\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n8\n\n\n+8\n\n\n2\n\n\n+2\n\n\n10\n\n\n+10\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+3\n\n\n21\n\n\n+21\n\n\n5\n\n\n+5\n\n\n29\n\n\n+29\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\n\n\n\n\n3.4.4\n\n\n3.3.29\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nFirst step of creating ES snapshots for backup up GRACC - Ongoing.\n\n\nFixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs.  Changing indexes to relefect the changes.\n\n\n\n\nThis Week\n\n\n\n\nInitiate backups of ES snapshots\n\n\nGRACC-ITB work\n\n\nStart indexing GOC server status in GRACC ES\n\n\nStash Writeback hack-a-thon on Tuesday \n Wednesday (Derek, Brian, Lincoln, Marian)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nStarting to setup time for Kibana walkthrough for Glidein Logs.", 
            "title": "September 18, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-technology-area-meeting-18-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:", 
            "title": "OSG Technology Area Meeting, 18 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  10 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#jira", 
            "text": "# of tickets   State      162  +9  Open    19  +2  In Progress    1  -5  Ready for Testing    0  -20  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-software-team", 
            "text": "osghost downtime starts tomorrow at 2pm Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#documentation", 
            "text": "https://github.com/opensciencegrid/docs/pulse#new-issues     7 docs fully migrated (many more awaiting review!),  100 docs archived  Release3: 17 high-priority and ~30 low-priority docs remaining  SoftwareTeam: ~56 docs remaining    Carl's mass migration and archival wrapper tools are robust enough to be used by other areas", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#support-update", 
            "text": "Clemson (BrianL): Blahp segfaults appeared to be due to overloaded NFS server  CTSC (BrianL): Attended meeting to discuss HTCondor issues encountered  LBNL (BrianL): Strange Gridmanager errors showing up intermittently", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-release-team", 
            "text": "3.3.29   Both   3.4.4   Total   Status      3  +3  12  +12  3  +3  18  +18  Open    0  +0  8  +8  2  +2  10  +10  In Progress    0  +0  1  +1  0  +0  1  +1  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    3  +3  21  +21  5  +5  29  +29  Total      Both  Update globus-gridftp-server-control to 5.2    3.4.4  3.3.29", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#last-week", 
            "text": "First step of creating ES snapshots for backup up GRACC - Ongoing.  Fixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs.  Changing indexes to relefect the changes.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#this-week", 
            "text": "Initiate backups of ES snapshots  GRACC-ITB work  Start indexing GOC server status in GRACC ES  Stash Writeback hack-a-thon on Tuesday   Wednesday (Derek, Brian, Lincoln, Marian)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#discussions_2", 
            "text": "Starting to setup time for Kibana walkthrough for Glidein Logs.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/", 
            "text": "OSG Technology Area Meeting, 11 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT  \n\n\nAnnouncements\n\n\nWiFi issues at UW-Madison today so availability may be spotty  \n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Suchandra\n\n\n10 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n153\n\n\n-1\n\n\nOpen\n\n\n\n\n\n\n17\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n-15\n\n\nReady for Testing\n\n\n\n\n\n\n20\n\n\n+19\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n3.4.3/3.3.28\n\n\n\n\nTickets not marked RFT\n\n\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n10\n\n\n\n\n\n\nBrian\n\n\n7\n\n\n\n\n\n\nCarl\n\n\n5\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\n\n\nDocumentation\n\n\n\n\nCarl has written a doc migration/archival wrapper for Software  \n\n\nThere were concerns about pandoc hanging so it's not yet foolproof as an area mass migration tool\n\n\nCarl will work on robustness so that other areas can benefit\n\n\n\n\n\n\nDerek to speak to his two students this week about their progress with migrating docs\n\n\n\n\nITB\n\n\n\n\nHTCondor 8.6.6 pre-release is installed on an ITB CE and some worker nodes\n\n\nBrianL will contact factory ops to add Madison ITB entries to the ITB factory\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nSupport Update\n\n\n\n\nBaylor (BrianL): Fixed issues with edg-mkgridmap -\n LCMAPS VOMS transition due to an old version of HTCondor-CE. Docs to be updated.\n\n\nClemson (BrianL): Blahp using too much CPU and segfaulting - need to talk to Jaime\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nSeptember 12th\n release\n\n\nRelease tomorrow???\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n+0\n\n\n1\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-6\n\n\n0\n\n\n-2\n\n\n0\n\n\n-9\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n-4\n\n\n3\n\n\n-7\n\n\n0\n\n\n-2\n\n\n6\n\n\n-15\n\n\nReady for Testing\n\n\n\n\n\n\n5\n\n\n+5\n\n\n11\n\n\n+10\n\n\n4\n\n\n+4\n\n\n20\n\n\n+19\n\n\nReady for Release\n\n\n\n\n\n\n9\n\n\n+0\n\n\n14\n\n\n-5\n\n\n4\n\n\n-2\n\n\n27\n\n\n-7\n\n\nTotal\n\n\n\n\n\n\n\n\nTickets needing attention\n\n\n\n\nBoth\n\n\nRelease StashCache metapackage 0.8+ (\nSOFTWARE-2873\n)\n\n\nMigrate transfer limit code from HDFS to generic plugin (\nSOFTWARE-2512\n)\n\n\nPackage and release xrootd-lcmaps 1.3.4 (\nSOFTWARE-2847\n)\n\n\n\n\n\n\n3.4.3\n\n\n3.3.28\n\n\nLCMAPS VOMS plugin and xrootd-lcmaps  (\nSOFTWARE-2848\n)\n\n\nUpdate to xrootd-hdfs 1.9.2 (\nSOFTWARE-2853\n)\n\n\nMigrate GridFTP-HDFS from Globus-Toolkit back to OSG (\nSOFTWARE-2856\n)\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nSome GRACC support of a \nslurm_meter\n issue, still debugging.\n\n\nIndexing Glidein Logs in GRACC's ES - Ongoing.  We now have worker node hostnames!\n\n\nFirst step of creating ES snapshots for backup up GRACC - Ongoing.\n\n\nFixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs\n\n\n\n\nThis Week\n\n\n\n\nInitiate backups of ES snapshots\n\n\nGRACC-ITB work\n\n\nStart indexing GOC server status in GRACC ES\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "September 11, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-technology-area-meeting-11-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 11 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#announcements", 
            "text": "WiFi issues at UW-Madison today so availability may be spotty", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#triage-duty", 
            "text": "This week: BrianL  Next week: Suchandra  10 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#jira", 
            "text": "# of tickets   State      153  -1  Open    17  -2  In Progress    6  -15  Ready for Testing    20  +19  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#3433328", 
            "text": "Tickets not marked RFT      Owner  # tickets      Mat  10    Brian  7    Carl  5    Marian  1", 
            "title": "3.4.3/3.3.28"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#documentation", 
            "text": "Carl has written a doc migration/archival wrapper for Software    There were concerns about pandoc hanging so it's not yet foolproof as an area mass migration tool  Carl will work on robustness so that other areas can benefit    Derek to speak to his two students this week about their progress with migrating docs", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#itb", 
            "text": "HTCondor 8.6.6 pre-release is installed on an ITB CE and some worker nodes  BrianL will contact factory ops to add Madison ITB entries to the ITB factory", 
            "title": "ITB"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#support-update", 
            "text": "Baylor (BrianL): Fixed issues with edg-mkgridmap -  LCMAPS VOMS transition due to an old version of HTCondor-CE. Docs to be updated.  Clemson (BrianL): Blahp using too much CPU and segfaulting - need to talk to Jaime", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-release-team", 
            "text": "Tim Theisen is handling the  September 12th  release  Release tomorrow???      3.3.28   Both   3.4.3   Total   Status      1  +0  0  -2  0  +0  1  -2  Open    0  -1  0  -6  0  -2  0  -9  In Progress    3  -4  3  -7  0  -2  6  -15  Ready for Testing    5  +5  11  +10  4  +4  20  +19  Ready for Release    9  +0  14  -5  4  -2  27  -7  Total     Tickets needing attention   Both  Release StashCache metapackage 0.8+ ( SOFTWARE-2873 )  Migrate transfer limit code from HDFS to generic plugin ( SOFTWARE-2512 )  Package and release xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 )    3.4.3  3.3.28  LCMAPS VOMS plugin and xrootd-lcmaps  ( SOFTWARE-2848 )  Update to xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )  Migrate GridFTP-HDFS from Globus-Toolkit back to OSG ( SOFTWARE-2856 )", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#last-week", 
            "text": "Some GRACC support of a  slurm_meter  issue, still debugging.  Indexing Glidein Logs in GRACC's ES - Ongoing.  We now have worker node hostnames!  First step of creating ES snapshots for backup up GRACC - Ongoing.  Fixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#this-week", 
            "text": "Initiate backups of ES snapshots  GRACC-ITB work  Start indexing GOC server status in GRACC ES", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/", 
            "text": "OSG Technology Area Meeting, 5 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Edgar, Derek, Tim C, Tim T, Carl, Suchandra\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Brian Lin\n\n\n9 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n152\n\n\n-4\n\n\nOpen\n\n\n\n\n\n\n19\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n21\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n3.4.3/3.3.28\n\n\n\n\nTickets not marked RFT\n\n\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n7\n\n\n\n\n\n\nBrian\n\n\n1\n\n\n\n\n\n\nCarl\n\n\n3\n\n\n\n\n\n\nTim\n\n\n1\n\n\n\n\n\n\n\n\nDocumentation\n\n\nDiscussions\n\n\nNew 8.7 Condor build might have to be punted to October.  We might still release a new 8.6 this month -- have to look at OSG's demands to see if it's worth releasing.\n\n\nSupport Update\n\n\nSome issues with Florida -- Derek to investigate.\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nSeptember 12th\n release\n\n\nPackage Freeze today!\n\n\nTimT may call upon software team members for testing assistance\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n-1\n\n\n2\n\n\n-7\n\n\n0\n\n\n+0\n\n\n3\n\n\n-8\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n-2\n\n\n6\n\n\n-2\n\n\n2\n\n\n+1\n\n\n9\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n7\n\n\n+2\n\n\n10\n\n\n+4\n\n\n4\n\n\n+1\n\n\n21\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n9\n\n\n-1\n\n\n19\n\n\n-4\n\n\n6\n\n\n+2\n\n\n34\n\n\n-3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nStashCache 0.8 (\nSOFTWARE-2873\n)\n\n\nosg-ca-scripts 1.1.7 (\nSOFTWARE-2834\n)\n\n\nxrootd-lcmaps 1.3.4 (\nSOFTWARE-2847\n)\n\n\n\n\n\n\n3.4.3  \n\n\nSingularity 2.3 (\nSOFTWARE-2755\n)\n\n\nCVMFS 2.4.1 (\nSOFTWARE-2858\n)\n\n\nosg-configure 2.2.0 (\nSOFTWARE-2864\n)\n\n\n\n\n\n\n3.3.28  \n\n\nosg-configure 1.10.0 (\nSOFTWARE-2865\n)\n\n\nxrootd-hdfs 1.9.2 (\nSOFTWARE-2853\n)\n\n\n\n\n\n\n\n\nDiscussions\n\n\nWill need extra help for testing.  Nebraska will help test the Globus update for 3.4, but need a 3.3 site to test the 3.3 updates -- should contact osg-sites.\n\n\nSuchandra may test gridftp-hdfs but Matyas still needs to debug \n promote.\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC backup of Dashboards\n\n\nInitiate backups of ES snapshots\n\n\nStart indexing GOC server status in GRACC ES\n\n\nWork no Glidein logs in ES\n\n\nFix naming issues in GRACC related to explosion of \"Fake\" sites in records.\n\n\n\n\nThis Week\n\n\n\n\nSome corruption in GRACC\n\n\nTesting GRACC update\n\n\nStashcache XRootD update going smoothly\n\n\nSome minor issues at Syracuse\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions", 
            "title": "September 5, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-technology-area-meeting-5-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Edgar, Derek, Tim C, Tim T, Carl, Suchandra", 
            "title": "OSG Technology Area Meeting, 5 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#triage-duty", 
            "text": "This week: Derek  Next week: Brian Lin  9 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#jira", 
            "text": "# of tickets   State      152  -4  Open    19  -1  In Progress    21  +7  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#3433328", 
            "text": "Tickets not marked RFT      Owner  # tickets      Mat  7    Brian  1    Carl  3    Tim  1", 
            "title": "3.4.3/3.3.28"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#discussions", 
            "text": "New 8.7 Condor build might have to be punted to October.  We might still release a new 8.6 this month -- have to look at OSG's demands to see if it's worth releasing.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#support-update", 
            "text": "Some issues with Florida -- Derek to investigate.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  September 12th  release  Package Freeze today!  TimT may call upon software team members for testing assistance      3.3.28   Both   3.4.3   Total   Status      1  -1  2  -7  0  +0  3  -8  Open    1  -2  6  -2  2  +1  9  -3  In Progress    7  +2  10  +4  4  +1  21  +7  Ready for Testing    0  +0  1  +1  0  +0  1  +1  Ready for Release    9  -1  19  -4  6  +2  34  -3  Total      Both    StashCache 0.8 ( SOFTWARE-2873 )  osg-ca-scripts 1.1.7 ( SOFTWARE-2834 )  xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 )    3.4.3    Singularity 2.3 ( SOFTWARE-2755 )  CVMFS 2.4.1 ( SOFTWARE-2858 )  osg-configure 2.2.0 ( SOFTWARE-2864 )    3.3.28    osg-configure 1.10.0 ( SOFTWARE-2865 )  xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#discussions_1", 
            "text": "Will need extra help for testing.  Nebraska will help test the Globus update for 3.4, but need a 3.3 site to test the 3.3 updates -- should contact osg-sites.  Suchandra may test gridftp-hdfs but Matyas still needs to debug   promote.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#last-week", 
            "text": "GRACC backup of Dashboards  Initiate backups of ES snapshots  Start indexing GOC server status in GRACC ES  Work no Glidein logs in ES  Fix naming issues in GRACC related to explosion of \"Fake\" sites in records.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#this-week", 
            "text": "Some corruption in GRACC  Testing GRACC update  Stashcache XRootD update going smoothly  Some minor issues at Syracuse", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#discussions_2", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/", 
            "text": "OSG Technology Area Meeting, 28 August 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Suchandra, TimC\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Derek\n\n\n9 (-3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n154\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n20\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n+12\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n3.4.3/3.3.28\n\n\n\n\nTickets not marked RFT\n\n\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n10\n\n\n\n\n\n\nBrian\n\n\n7\n\n\n\n\n\n\nCarl\n\n\n5\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\n\n\nDocumentation\n\n\n\n\n6 documents migrated and meeting-related pages archived last week:  \n\n\nhttps://github.com/opensciencegrid/technology/pulse\n\n\nhttps://github.com/opensciencegrid/docs/pulse\n\n\nIf you're short on time, pick shorter documents to migrate or archive the ones marked as such in the spreadsheet\n\n\n\n\n\n\nDerek has two students that are beginning to familiarize themselves with the migration process\n\n\nWe have enough experience with the process that we can start focusing on high-priority Release3 documents\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nClemson (BrianL, Marian): expired WN CRLs and possibly an HTCondor IPv6 bug\n\n\nJINR (BrianL): CE jobs held (\"Error parsing classad or job not found\") due to missing \npbs_pro\n in \n/etc/blah.config\n\n\nIssues with BLAHP at some of the BOSCO-CE's.  Mats and Derek are continueing to investigate.  Some issue with a sub-directory of the Bosco sandbox not being populated with the X509 certificate.\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nSeptember 12th\n release\n\n\nDevelopment Freeze today!\n\n\nTimT may call upon software team members for testing assistance\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n2\n\n\n-3\n\n\n9\n\n\n-2\n\n\n0\n\n\n-1\n\n\n11\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n+1\n\n\n8\n\n\n+2\n\n\n1\n\n\n+0\n\n\n12\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n5\n\n\n+5\n\n\n6\n\n\n+5\n\n\n3\n\n\n+3\n\n\n14\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n10\n\n\n+3\n\n\n22\n\n\n+4\n\n\n4\n\n\n+2\n\n\n37\n\n\n+10\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nStashCache 0.8 (\nSOFTWARE-2873\n)\n\n\nosg-ca-scripts 1.1.7 (\nSOFTWARE-2834\n)\n\n\nxrootd-lcmaps 1.3.4 (\nSOFTWARE-2847\n)\n\n\n\n\n\n\n3.4.3  \n\n\nSingularity 2.3 (\nSOFTWARE-2755\n)\n\n\nCVMFS 2.4.1 (\nSOFTWARE-2858\n)\n\n\nosg-configure 2.2.0 (\nSOFTWARE-2864\n)\n\n\n\n\n\n\n3.3.28  \n\n\nosg-configure 1.10.0 (\nSOFTWARE-2865\n)\n\n\nxrootd-hdfs 1.9.2 (\nSOFTWARE-2853\n)\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a \nslurm_meter\n issue, still debugging.\n\n\nIndexing Glidein Logs in GRACC's ES - Ongoing\n\n\nBackup Reports are now running every Monday (or, Sunday night, because GRACC node is in UTC time)\n\n\nFirst step of creating ES snapshots for backup up GRACC - Ongoing.\n\n\n\n\nThis Week\n\n\n\n\nGRACC backup of Dashboards\n\n\nInitiate backups of ES snapshots\n\n\nStart indexing GOC server status in GRACC ES\n\n\nWork no Glidein logs in ES\n\n\nFix naming issues in GRACC related to explosion of \"Fake\" sites in records.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nXRootD developers have moved to a release model where they seek explicit sign-off from stakeholders", 
            "title": "August 28, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-technology-area-meeting-28-august-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Suchandra, TimC", 
            "title": "OSG Technology Area Meeting, 28 August 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#triage-duty", 
            "text": "This week: Edgar  Next week: Derek  9 (-3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#jira", 
            "text": "# of tickets   State      154  -3  Open    20  -1  In Progress    14  +12  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#3433328", 
            "text": "Tickets not marked RFT      Owner  # tickets      Mat  10    Brian  7    Carl  5    Marian  1", 
            "title": "3.4.3/3.3.28"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#documentation", 
            "text": "6 documents migrated and meeting-related pages archived last week:    https://github.com/opensciencegrid/technology/pulse  https://github.com/opensciencegrid/docs/pulse  If you're short on time, pick shorter documents to migrate or archive the ones marked as such in the spreadsheet    Derek has two students that are beginning to familiarize themselves with the migration process  We have enough experience with the process that we can start focusing on high-priority Release3 documents", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#support-update", 
            "text": "Clemson (BrianL, Marian): expired WN CRLs and possibly an HTCondor IPv6 bug  JINR (BrianL): CE jobs held (\"Error parsing classad or job not found\") due to missing  pbs_pro  in  /etc/blah.config  Issues with BLAHP at some of the BOSCO-CE's.  Mats and Derek are continueing to investigate.  Some issue with a sub-directory of the Bosco sandbox not being populated with the X509 certificate.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  September 12th  release  Development Freeze today!  TimT may call upon software team members for testing assistance      3.3.28   Both   3.4.3   Total   Status      2  -3  9  -2  0  -1  11  -6  Open    3  +1  8  +2  1  +0  12  +3  In Progress    5  +5  6  +5  3  +3  14  +13  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    10  +3  22  +4  4  +2  37  +10  Total      Both    StashCache 0.8 ( SOFTWARE-2873 )  osg-ca-scripts 1.1.7 ( SOFTWARE-2834 )  xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 )    3.4.3    Singularity 2.3 ( SOFTWARE-2755 )  CVMFS 2.4.1 ( SOFTWARE-2858 )  osg-configure 2.2.0 ( SOFTWARE-2864 )    3.3.28    osg-configure 1.10.0 ( SOFTWARE-2865 )  xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a  slurm_meter  issue, still debugging.  Indexing Glidein Logs in GRACC's ES - Ongoing  Backup Reports are now running every Monday (or, Sunday night, because GRACC node is in UTC time)  First step of creating ES snapshots for backup up GRACC - Ongoing.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#this-week", 
            "text": "GRACC backup of Dashboards  Initiate backups of ES snapshots  Start indexing GOC server status in GRACC ES  Work no Glidein logs in ES  Fix naming issues in GRACC related to explosion of \"Fake\" sites in records.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#discussions_2", 
            "text": "XRootD developers have moved to a release model where they seek explicit sign-off from stakeholders", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/", 
            "text": "OSG Technology Area Meeting, 14 August 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nNo meeting next week due to outages  \n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Carl\n\n\n12 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n160\n\n\n-4\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-28\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovemeber\n\n\n3.4.5, 3.3.29\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n13 documents migrated last week, 3 more awaiting review\n\n\nGUMS and Gratia failures in the RHEL7 3.3 nightlies due to new SELinux policy\n\n\nSoftware team members track effot percentage numbers in shared google spreadsheet\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nSeptember 12th\n release\n\n\nDevelopement Freeze in 2 weeks.\n\n\nData release today (IGTF 1.85)\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n5\n\n\n+5\n\n\n11\n\n\n+11\n\n\n1\n\n\n+1\n\n\n17\n\n\n+17\n\n\nOpen\n\n\n\n\n\n\n2\n\n\n+2\n\n\n6\n\n\n+6\n\n\n1\n\n\n+1\n\n\n9\n\n\n+9\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n7\n\n\n+7\n\n\n18\n\n\n+18\n\n\n2\n\n\n+2\n\n\n27\n\n\n+27\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate gsi-openssh-server\n\n\n\n\n\n\n3.4.3\n\n\nNothing yet\n\n\n\n\n\n\n3.3.28\n\n\nNothing yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a `slurm\nmeter\n` issue, still debugging\n\n\nIndexing Glidein Logs in GRACC's ES\n\n\nStarted backup reports\n\n\nFirst step of creating ES snapshots for backing up GRACC\n\n\n\n\nThis Week\n\n\n\n\nFinish GRACC Backup reports\n\n\nGRACC backup of Dashboards\n\n\nInitiate backups of ES snapshots\n\n\nStart indexing GOC service status in GRACC ES.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\n\n\nOnce format of Glidein Logs in GRACC ES is finalized, light documentation will be written for the benefit of internal teams\n\n\nDerek offered to give a short presentation on ES queries", 
            "title": "August 14, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-technology-area-meeting-14-august-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 14 August 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#announcements", 
            "text": "No meeting next week due to outages", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#triage-duty", 
            "text": "This week: Mat  Next week: Carl  12 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#jira", 
            "text": "# of tickets   State      160  -4  Open    18  +4  In Progress    2  +0  Ready for Testing    0  -28  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     Novemeber  3.4.5, 3.3.29  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-software-team", 
            "text": "13 documents migrated last week, 3 more awaiting review  GUMS and Gratia failures in the RHEL7 3.3 nightlies due to new SELinux policy  Software team members track effot percentage numbers in shared google spreadsheet", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  September 12th  release  Developement Freeze in 2 weeks.  Data release today (IGTF 1.85)      3.3.28   Both   3.4.3   Total   Status      5  +5  11  +11  1  +1  17  +17  Open    2  +2  6  +6  1  +1  9  +9  In Progress    0  +0  1  +1  0  +0  1  +1  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    7  +7  18  +18  2  +2  27  +27  Total      Both  Update gsi-openssh-server    3.4.3  Nothing yet    3.3.28  Nothing yet", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a `slurm meter ` issue, still debugging  Indexing Glidein Logs in GRACC's ES  Started backup reports  First step of creating ES snapshots for backing up GRACC", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#this-week", 
            "text": "Finish GRACC Backup reports  GRACC backup of Dashboards  Initiate backups of ES snapshots  Start indexing GOC service status in GRACC ES.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#discussions_2", 
            "text": "Once format of Glidein Logs in GRACC ES is finalized, light documentation will be written for the benefit of internal teams  Derek offered to give a short presentation on ES queries", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/", 
            "text": "OSG Technology Area Meeting,  7 August 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: Mat\n\n\n12 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n164\n\n\n+6\n\n\nOpen\n\n\n\n\n\n\n14\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n-23\n\n\nReady for Testing\n\n\n\n\n\n\n28\n\n\n+27\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\nGUMS, Gratia, and GSI OpenSSH failures in the 3.3 nightlies\n\n\nNew SELinux policies for HTCondor require rebuilds in 3.3, 3.4, and upcoming\n\n\nList Globus package dependencies to prepare for EOL\n\n\nSoftware team members track effot percentage numbers in shared google spreadsheet\n\n\nDocumentation transition goal: migrate ~2 documents per week. Details incoming.\n\n\n\n\nDiscussions\n\n\n\n\nTWiki doc contents should be completely replaced with header after migration\n\n\nInternal docs should be migrated first\n\n\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\nosg-connect (BrianL) - Submit wrapper issue with new \n-nobatch\n option\n\n\nVanderbilt (Derek) - HTCondor-CE issues\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nRelease tomorrow\n\n\nData Release this week (IGTF 1.85)\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-4\n\n\n0\n\n\n-13\n\n\n0\n\n\n-8\n\n\n0\n\n\n-25\n\n\nReady for Testing\n\n\n\n\n\n\n4\n\n\n+4\n\n\n13\n\n\n+13\n\n\n10\n\n\n+10\n\n\n27\n\n\n+27\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+0\n\n\n14\n\n\n+0\n\n\n10\n\n\n+0\n\n\n28\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nRed Hat 7.4 update breaks HTCondor\n\n\nCondor-CE: Do not hold running jobs with expired proxy\n\n\ncondor-cron: add a way for users to override condor_ids\n\n\nosg-configure: Fix logging in ensure_valid_user_vo_file\n\n\nosg-configure does not warn/error in -v\n\n\nCEView VO tab throws 500 error on inital installation\n\n\nHTCondor-CE: only warn about configuration if osg-configure is present\n\n\nosg-configure: Configure GUMS before running gums-host-cron\n\n\nAdd blahp configuration to differentiate PBS flavors\n\n\ncondor-cron: disable gsi authz\n\n\nRelease condor-cron 1.1.3\n\n\nAdd gsi-openssh packages to osg-tested-internal\n\n\nFix selinux issues with GSI OpenSSH in EL7 nightly tests\n\n\nosg-configure: Make exception usage consistent\n\n\n3.4.2\n\n\nhtcondor collector python plugin has undefined symbols\n\n\nAdd pilot payload auditing\n\n\nUpdate to HTCondor 8.6.5 in OSG 3.4\n\n\nMerge osg-ce packages\n\n\nosg-configure: Remove unused test configs\n\n\nRelease htcondor-ce-3.0.0-1+\n\n\nAdd osg-gridftp back to osg-tested-internal\n\n\nRelease osg-configure 2.1.1 (OSG 3.4)\n\n\nRelease osg-tested-internal-3.4-3+\n\n\nUpcoming: Patch HTCondor 8.7.2 to work with Python Collector plugins\n\n\n3.3.27\n\n\nJGlobus incorrectly refuses proxies with key usage\n\n\nUpdate to HTCondor 8.4.12 in OSG 3.3\n\n\nRelease htcondor-ce-2.2.2-1+\n\n\nRelease osg-configure 1.9.1 (OSG 3.3)\n\n\n\n\nDiscussions\n\n\n\n\nEdgar was a big help to the release effort, thanks!\n\n\nOU will help with testing the late-breaking condor packaging changes\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a `slurm\nmeter\n` issue, still debugging.\n\n\nCVMFS-Sync RPM finished line.\n\n\nIndexing Glidein Logs in GRACC's ES.\n\n\n\n\nThis Week\n\n\n\n\nGRACC Backup reports\n\n\nGRACC backup of Dashboards\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "August 7, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-technology-area-meeting-7-august-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:", 
            "title": "OSG Technology Area Meeting,  7 August 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#triage-duty", 
            "text": "This week: TimT  Next week: Mat  12 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#jira", 
            "text": "# of tickets   State      164  +6  Open    14  +0  In Progress    2  -23  Ready for Testing    28  +27  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-software-team", 
            "text": "GUMS, Gratia, and GSI OpenSSH failures in the 3.3 nightlies  New SELinux policies for HTCondor require rebuilds in 3.3, 3.4, and upcoming  List Globus package dependencies to prepare for EOL  Software team members track effot percentage numbers in shared google spreadsheet  Documentation transition goal: migrate ~2 documents per week. Details incoming.", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#discussions", 
            "text": "TWiki doc contents should be completely replaced with header after migration  Internal docs should be migrated first", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool  osg-connect (BrianL) - Submit wrapper issue with new  -nobatch  option  Vanderbilt (Derek) - HTCondor-CE issues", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Release tomorrow  Data Release this week (IGTF 1.85)      3.3.27   Both   3.4.2   Total   Status      0  +0  1  +0  0  +0  1  +0  Open    0  +0  0  +0  0  -2  0  -2  In Progress    0  -4  0  -13  0  -8  0  -25  Ready for Testing    4  +4  13  +13  10  +10  27  +27  Ready for Release    4  +0  14  +0  10  +0  28  +0  Total      Both  Red Hat 7.4 update breaks HTCondor  Condor-CE: Do not hold running jobs with expired proxy  condor-cron: add a way for users to override condor_ids  osg-configure: Fix logging in ensure_valid_user_vo_file  osg-configure does not warn/error in -v  CEView VO tab throws 500 error on inital installation  HTCondor-CE: only warn about configuration if osg-configure is present  osg-configure: Configure GUMS before running gums-host-cron  Add blahp configuration to differentiate PBS flavors  condor-cron: disable gsi authz  Release condor-cron 1.1.3  Add gsi-openssh packages to osg-tested-internal  Fix selinux issues with GSI OpenSSH in EL7 nightly tests  osg-configure: Make exception usage consistent  3.4.2  htcondor collector python plugin has undefined symbols  Add pilot payload auditing  Update to HTCondor 8.6.5 in OSG 3.4  Merge osg-ce packages  osg-configure: Remove unused test configs  Release htcondor-ce-3.0.0-1+  Add osg-gridftp back to osg-tested-internal  Release osg-configure 2.1.1 (OSG 3.4)  Release osg-tested-internal-3.4-3+  Upcoming: Patch HTCondor 8.7.2 to work with Python Collector plugins  3.3.27  JGlobus incorrectly refuses proxies with key usage  Update to HTCondor 8.4.12 in OSG 3.3  Release htcondor-ce-2.2.2-1+  Release osg-configure 1.9.1 (OSG 3.3)", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#discussions_1", 
            "text": "Edgar was a big help to the release effort, thanks!  OU will help with testing the late-breaking condor packaging changes", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a `slurm meter ` issue, still debugging.  CVMFS-Sync RPM finished line.  Indexing Glidein Logs in GRACC's ES.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#this-week", 
            "text": "GRACC Backup reports  GRACC backup of Dashboards", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/", 
            "text": "OSG Technology Area Meeting, 31 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Derek, Edgar, Marian, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n12 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n158\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n14\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n25\n\n\n+8\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nhtcondor collector python plugin broken in 8.6.4; fixed in 8.6.5 \n(SOFTWARE-2816)\n which will be released tomorrow morning\n\n\n\n\nDiscussions\n\n\n\n\nAlejandro at CERN wants a new fix for JGlobus to add the SSL option DontInsertEmptyFragments to fix a Bestman issue; Mat will take a look\n\n\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\nUConn (BrianL): CE auth issues likely an internal OSG issue at this point\n\n\nUWash (BrianL): jobs held due to \"non-existent route or job route limit\"\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nPackage freeze today\n\n\nNeed testing help\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n-2\n\n\n1\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-5\n\n\n2\n\n\n+0\n\n\n2\n\n\n-6\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n+1\n\n\n13\n\n\n+4\n\n\n8\n\n\n+3\n\n\n25\n\n\n+8\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+0\n\n\n14\n\n\n-1\n\n\n10\n\n\n+1\n\n\n28\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nFix selinux issues with GSI OpenSSH in EL7 nightly tests\n\n\nUpdate gsi-openssh-server\n\n\nAdd gsi-openssh packages to osg-tested-internal\n\n\nosg-configure does not warn/error in -v\n\n\ncondor-cron: disable gsi authz\n\n\nRelease condor-cron 1.1.3\n\n\ncondor-cron: add a way for users to override condor_ids\n\n\nosg-configure: Configure GUMS before running gums-host-cron\n\n\nosg-configure: Fix logging in ensure_valid_user_vo_file\n\n\nAdd blahp configuration to differentiate PBS flavors\n\n\nCEView VO tab throws 500 error on inital installation\n\n\nCondor-CE: Do not hold running jobs with expired proxy\n\n\nHTCondor-CE: only warn about configuration if osg-configure is present\n\n\nosg-configure: Make exception usage consistent\n\n\n3.4.2\n\n\nFix selinux issues with GSI OpenSSH in EL7 nightly tests\n\n\nUpdate gsi-openssh-server\n\n\nAdd gsi-openssh packages to osg-tested-internal\n\n\nosg-configure does not warn/error in -v\n\n\ncondor-cron: disable gsi authz\n\n\nRelease condor-cron 1.1.3\n\n\ncondor-cron: add a way for users to override condor_ids\n\n\nosg-configure: Configure GUMS before running gums-host-cron\n\n\nosg-configure: Fix logging in ensure_valid_user_vo_file\n\n\nAdd blahp configuration to differentiate PBS flavors\n\n\nCEView VO tab throws 500 error on inital installation\n\n\nCondor-CE: Do not hold running jobs with expired proxy\n\n\nHTCondor-CE: only warn about configuration if osg-configure is present\n\n\nosg-configure: Make exception usage consistent\n\n\n3.2.27\n\n\nJGlobus incorrectly refuses proxies with key usage\n\n\nRelease osg-configure 1.9.1 (OSG 3.3)\n\n\nUpdate to HTCondor 8.4.12 in OSG 3.3\n\n\nRelease htcondor-ce-2.2.2-1+\n\n\n\n\nDiscussions\n\n\n\n\n\n\nWill need software team's help testing\n\n\n\n\n\n\nFrom the planning retreat:\n\n\n\n\nRelease team might go to a \"community testing\" model where we will ask the stakeholders to test changes\n\n\nMaybe might go to a \"rolling release\" model instead of point releases; may be not until 3.5; Tim T to write proposal\n\n\n\n\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a \nslurm_meter\n issue, still debugging.\n\n\n\n\nThis Week\n\n\n\n\nGRACC Backup reports\n\n\nCVMFS-Sync RPM across finish line.\n\n\nGRACC backup of Dashboards\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 31, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-technology-area-meeting-31-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Derek, Edgar, Marian, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting, 31 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  12 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#jira", 
            "text": "# of tickets   State      158  -3  Open    14  -1  In Progress    25  +8  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-software-team", 
            "text": "htcondor collector python plugin broken in 8.6.4; fixed in 8.6.5  (SOFTWARE-2816)  which will be released tomorrow morning", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#discussions", 
            "text": "Alejandro at CERN wants a new fix for JGlobus to add the SSL option DontInsertEmptyFragments to fix a Bestman issue; Mat will take a look", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool  UConn (BrianL): CE auth issues likely an internal OSG issue at this point  UWash (BrianL): jobs held due to \"non-existent route or job route limit\"", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Package freeze today  Need testing help      3.3.27   Both   3.4.2   Total   Status      0  +0  1  +0  0  -2  1  -2  Open    0  -1  0  -5  2  +0  2  -6  In Progress    4  +1  13  +4  8  +3  25  +8  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    4  +0  14  -1  10  +1  28  +0  Total      Both  Fix selinux issues with GSI OpenSSH in EL7 nightly tests  Update gsi-openssh-server  Add gsi-openssh packages to osg-tested-internal  osg-configure does not warn/error in -v  condor-cron: disable gsi authz  Release condor-cron 1.1.3  condor-cron: add a way for users to override condor_ids  osg-configure: Configure GUMS before running gums-host-cron  osg-configure: Fix logging in ensure_valid_user_vo_file  Add blahp configuration to differentiate PBS flavors  CEView VO tab throws 500 error on inital installation  Condor-CE: Do not hold running jobs with expired proxy  HTCondor-CE: only warn about configuration if osg-configure is present  osg-configure: Make exception usage consistent  3.4.2  Fix selinux issues with GSI OpenSSH in EL7 nightly tests  Update gsi-openssh-server  Add gsi-openssh packages to osg-tested-internal  osg-configure does not warn/error in -v  condor-cron: disable gsi authz  Release condor-cron 1.1.3  condor-cron: add a way for users to override condor_ids  osg-configure: Configure GUMS before running gums-host-cron  osg-configure: Fix logging in ensure_valid_user_vo_file  Add blahp configuration to differentiate PBS flavors  CEView VO tab throws 500 error on inital installation  Condor-CE: Do not hold running jobs with expired proxy  HTCondor-CE: only warn about configuration if osg-configure is present  osg-configure: Make exception usage consistent  3.2.27  JGlobus incorrectly refuses proxies with key usage  Release osg-configure 1.9.1 (OSG 3.3)  Update to HTCondor 8.4.12 in OSG 3.3  Release htcondor-ce-2.2.2-1+", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#discussions_1", 
            "text": "Will need software team's help testing    From the planning retreat:   Release team might go to a \"community testing\" model where we will ask the stakeholders to test changes  Maybe might go to a \"rolling release\" model instead of point releases; may be not until 3.5; Tim T to write proposal", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a  slurm_meter  issue, still debugging.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#this-week", 
            "text": "GRACC Backup reports  CVMFS-Sync RPM across finish line.  GRACC backup of Dashboards", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/", 
            "text": "OSG Technology Area Meeting, 24 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n   \n\n\nAnnouncements\n\n\n\n\nVaibhav no longer with UCSD\n\n\nOSG Annual Planning Retreat Tue - Wed\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n12 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n161\n\n\n+6\n\n\nOpen\n\n\n\n\n\n\n15\n\n\n-8\n\n\nIn Progress\n\n\n\n\n\n\n17\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nOpen tickets:\n\n\n\n\n\n\n\n\n\n\nDeveloper\n\n\n# non-RFT\n\n\n\n\n\n\n\n\n\n\nBrian L\n\n\n8\n\n\n\n\n\n\nDave D\n\n\n2\n\n\n\n\n\n\nMat S\n\n\n1\n\n\n\n\n\n\n\n\n\n\nNeed volunteers for building HTCondor 8.6.5 and developer test Singularity 2.3\n\n\nVMU tests are unavailable due to Gluster maintenance. Expected to be functional by EOB Tuesday  \n\n\nIn the meantime, use \nTravis-CI\n\n\n\n\n\n\nMysterious gsi-openssh-server failures in the nightlies on EL7, 3.3. testing\n\n\n\n\nDiscussions\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\nUConn (BrianL): CE auth issues likely an internal OSG issue at this point\n\n\nUWash (BrianL): jobs held due to \"non-existent route or job route limit\"\n\n\nFlorida (Derek): Issues with slurm probe not reporting records.  Caught in WLCG report. (ongoing)\n\n\nWisconsin (Derek): Routes for RHEL6 and RHEL7 nodes (ongoing)\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nDev freeze today\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\n3\n\n\n+3\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n5\n\n\n+5\n\n\n2\n\n\n+2\n\n\n8\n\n\n+8\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\n9\n\n\n+9\n\n\n5\n\n\n+5\n\n\n17\n\n\n+17\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+4\n\n\n15\n\n\n+15\n\n\n9\n\n\n+9\n\n\n28\n\n\n+28\n\n\nTotal\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nNot much going on with GRACC, which is good for a production service.\n\n\nDerek at OSG User School\n\n\nOSG User School used StashCP with great success!  No issues found with the infrastructure or tools with any of the 60 students using it.\n\n\n\n\nThis Week\n\n\n\n\nGRACC Backup reports\n\n\nCVMFS-Sync RPM across finish line.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 24, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-technology-area-meeting-24-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:", 
            "title": "OSG Technology Area Meeting, 24 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#announcements", 
            "text": "Vaibhav no longer with UCSD  OSG Annual Planning Retreat Tue - Wed", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  12 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#jira", 
            "text": "# of tickets   State      161  +6  Open    15  -8  In Progress    17  +13  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-software-team", 
            "text": "Open tickets:      Developer  # non-RFT      Brian L  8    Dave D  2    Mat S  1      Need volunteers for building HTCondor 8.6.5 and developer test Singularity 2.3  VMU tests are unavailable due to Gluster maintenance. Expected to be functional by EOB Tuesday    In the meantime, use  Travis-CI    Mysterious gsi-openssh-server failures in the nightlies on EL7, 3.3. testing", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#discussions", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool  UConn (BrianL): CE auth issues likely an internal OSG issue at this point  UWash (BrianL): jobs held due to \"non-existent route or job route limit\"  Florida (Derek): Issues with slurm probe not reporting records.  Caught in WLCG report. (ongoing)  Wisconsin (Derek): Routes for RHEL6 and RHEL7 nodes (ongoing)", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Dev freeze today      3.3.27   Both   3.4.2   Total   Status      0  +0  1  +1  2  +2  3  +3  Open    1  +1  5  +5  2  +2  8  +8  In Progress    3  +3  9  +9  5  +5  17  +17  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    4  +4  15  +15  9  +9  28  +28  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#last-week", 
            "text": "Not much going on with GRACC, which is good for a production service.  Derek at OSG User School  OSG User School used StashCP with great success!  No issues found with the infrastructure or tools with any of the 60 students using it.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#this-week", 
            "text": "GRACC Backup reports  CVMFS-Sync RPM across finish line.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/", 
            "text": "OSG Technology Area Meeting, 17 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n\nCarl, Edgar, Marian, Mat, Suchandra, Tim T\n\n\nAnnouncements\n\n\nOSG User School this week\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Mat\n\n\n11 (+6) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n155\n\n\n1\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+12\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n25\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\nDiscussions\n\n\n\n\nNew GFAL tools issue causing problems for XENON1T, on CVMFS. -- need to check if it's a 3.4 issue only\n\n\nNew Condor 8.6.5 release this week to fix IPv6 bugs, but the new release breaks Dave Dykstra's Python modules so we're debugging that\n\n\n\n\nSupport Update\n\n\nnone\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nDevelopment Freeze next week\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+0\n\n\n4\n\n\n+0\n\n\n2\n\n\n+0\n\n\n7\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n10\n\n\n+0\n\n\n4\n\n\n+0\n\n\n14\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+0\n\n\n1\n\n\n+0\n\n\n1\n\n\n+0\n\n\n4\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+0\n\n\n15\n\n\n+0\n\n\n7\n\n\n+0\n\n\n25\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nReady for Testing\n\n\nBoth\n\n\nUpdate gsi-openssh-server\n\n\n\n\n\n\n3.4.2\n\n\nMerge osg-ce packages\n\n\n\n\n\n\nBoth\n\n\nUpdate to HTCondor 8.4.12 in OSG 3.3\n\n\nJGlobus incorrectly refuses proxies with key usage\n\n\n\n\n\n\n\n\nDiscussions\n\n\nnone\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nDerek at PEARC\n\n\n\n\nThis Week\n\n\n\n\nDerek at OSG school\n\n\nStash origin maintenance at UChicago over the weekend so StashCache inaccessible over the weekend\n\n\nGRACC accounting issue with Florida revealed that Florida wasn't correctly running Gratia probes\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 17, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-technology-area-meeting-17-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending: \nCarl, Edgar, Marian, Mat, Suchandra, Tim T", 
            "title": "OSG Technology Area Meeting, 17 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#announcements", 
            "text": "OSG User School this week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#triage-duty", 
            "text": "This week: Edgar  Next week: Mat  11 (+6) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#jira", 
            "text": "# of tickets   State      155  1  Open    23  +12  In Progress    4  2  Ready for Testing    0  25  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#discussions", 
            "text": "New GFAL tools issue causing problems for XENON1T, on CVMFS. -- need to check if it's a 3.4 issue only  New Condor 8.6.5 release this week to fix IPv6 bugs, but the new release breaks Dave Dykstra's Python modules so we're debugging that", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#support-update", 
            "text": "none", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Development Freeze next week      3.3.27   Both   3.4.2   Total   Status      1  +0  4  +0  2  +0  7  +0  Open    0  +0  10  +0  4  +0  14  +0  In Progress    2  +0  1  +0  1  +0  4  +0  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    3  +0  15  +0  7  +0  25  +0  Total      Ready for Testing  Both  Update gsi-openssh-server    3.4.2  Merge osg-ce packages    Both  Update to HTCondor 8.4.12 in OSG 3.3  JGlobus incorrectly refuses proxies with key usage", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#discussions_1", 
            "text": "none", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#last-week", 
            "text": "Derek at PEARC", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#this-week", 
            "text": "Derek at OSG school  Stash origin maintenance at UChicago over the weekend so StashCache inaccessible over the weekend  GRACC accounting issue with Florida revealed that Florida wasn't correctly running Gratia probes", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/", 
            "text": "OSG Technology Area Meeting, 10 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianB, BrianL, Carl, Edgar, Marian, Mat, TimT, Vaibhav\n\n\nAnnouncements\n\n\nOSG User School next week  \n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Edgar\n\n\n5 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n+7\n\n\nOpen\n\n\n\n\n\n\n11\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n21\n\n\nReady for Testing\n\n\n\n\n\n\n25\n\n\n+23\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nTwo weeks until August development freeze\n\n\nMajor LCMAPS VOMS plugin issues plaguing multiple sites, causing extensive downtime\n\n\n\n\nDiscussions\n\n\n\n\nThe root cause of the LCMAPS VOMS segfaults appear to be due to memory issues in the proxy verification module. BrianB hopes to have a fix by the end of the day.\n\n\n\n\nSupport Update\n\n\n\n\nClemson, IIT, UConn (BrianL) - LCMAPS VOMS plugin issue above\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJuly 11th\n release\n\n\nRelease tomorrow\n\n\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-2\n\n\n18\n\n\n-18\n\n\n0\n\n\n-3\n\n\n0\n\n\n-23\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\n19\n\n\n+18\n\n\n4\n\n\n+4\n\n\n25\n\n\n+24\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n+0\n\n\n19\n\n\n+0\n\n\n4\n\n\n+1\n\n\n25\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nOperations will be adopting a more flexible release schedule, at least for changes transparent to the user\n\n\nCert update incoming; Edgar will help test it while Suchandra is at PEARC. In the future, it may make more sense to test cert packages in an automated fashion \\minus; perhaps with \nrpmdiff\n.\n\n\n\n\nOSG Investigations Team\n\n\nThis Week\n\n\n\n\nDerek at PEARC this week and the OSG User School next week\n\n\nContributing to CVMFS and looking at he LIGO use case again\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 10, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-technology-area-meeting-10-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianB, BrianL, Carl, Edgar, Marian, Mat, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting, 10 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#announcements", 
            "text": "OSG User School next week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#triage-duty", 
            "text": "This week: Derek  Next week: Edgar  5 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#jira", 
            "text": "# of tickets   State      156  +7  Open    11  +0  In Progress    2  21  Ready for Testing    25  +23  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-software-team", 
            "text": "Two weeks until August development freeze  Major LCMAPS VOMS plugin issues plaguing multiple sites, causing extensive downtime", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#discussions", 
            "text": "The root cause of the LCMAPS VOMS segfaults appear to be due to memory issues in the proxy verification module. BrianB hopes to have a fix by the end of the day.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#support-update", 
            "text": "Clemson, IIT, UConn (BrianL) - LCMAPS VOMS plugin issue above", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-release-team", 
            "text": "Tim Theisen is handling the  July 11th  release  Release tomorrow      3.3.26   Both   3.4.1   Total   Status      0  +0  0  +0  0  +0  0  +0  Open    0  +0  0  +0  0  +0  0  +0  In Progress    0  -2  18  -18  0  -3  0  -23  Ready for Testing    2  +2  19  +18  4  +4  25  +24  Ready for Release    2  +0  19  +0  4  +1  25  +1  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#discussions_1", 
            "text": "Operations will be adopting a more flexible release schedule, at least for changes transparent to the user  Cert update incoming; Edgar will help test it while Suchandra is at PEARC. In the future, it may make more sense to test cert packages in an automated fashion \\minus; perhaps with  rpmdiff .", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#this-week", 
            "text": "Derek at PEARC this week and the OSG User School next week  Contributing to CVMFS and looking at he LIGO use case again", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/", 
            "text": "OSG Technology Area Meeting, 3 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianB, Carl, Derek, Mat, Marian, Suchandra, Vaibhav\n\n\nAnnouncements\n\n\nNone\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: Derek\n\n\n6 (-2) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n149\n\n\n(+0)\n\n\nOpen\n\n\n\n\n\n\n11\n\n\n(-7)\n\n\nIn Progress\n\n\n\n\n\n\n23\n\n\n(+17)\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n(+1)\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\nPackage freeze today\n\n\nAll tickets ready for testing, one ready for release\n\n\n\n\nSupport Update\n\n\n\n\nUtah (Derek) - working with Utah, still debugging. Proxy is trying to be read by the pilot and the proxy doesn't exist in the expected directory. Only happening on one cluster\n\n\nIIT, Clemson (BrianL) - Stacktraces when authenticating some proxies\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n0\n\n\n1\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n6\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+1\n\n\n18\n\n\n+12\n\n\n3\n\n\n+0\n\n\n23\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n2\n\n\n+1\n\n\n19\n\n\n+6\n\n\n3\n\n\n+0\n\n\n24\n\n\n+7\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nShort testing week due to holiday\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nOSG-Connect wanted some project renames, done, but waiting on verification.\n\n\n\n\nThis Week\n\n\n\n\nGRACC host cert expired, but that only affects backups so everything is still online\n\n\nWCLG wants some info on missing accounting, but they are slow on finalizing their own accounting.  But it looks like it is now finished.\n\n\nDerek will be at PEARC next week and will be presenting\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 3, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-technology-area-meeting-3-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianB, Carl, Derek, Mat, Marian, Suchandra, Vaibhav", 
            "title": "OSG Technology Area Meeting, 3 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#announcements", 
            "text": "None", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#triage-duty", 
            "text": "This week: Carl  Next week: Derek  6 (-2) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#jira", 
            "text": "# of tickets   State      149  (+0)  Open    11  (-7)  In Progress    23  (+17)  Ready for Testing    2  (+1)  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-software-team", 
            "text": "Package freeze today  All tickets ready for testing, one ready for release", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#support-update", 
            "text": "Utah (Derek) - working with Utah, still debugging. Proxy is trying to be read by the pilot and the proxy doesn't exist in the expected directory. Only happening on one cluster  IIT, Clemson (BrianL) - Stacktraces when authenticating some proxies", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-release-team", 
            "text": "3.3.26   Both   3.4.1   Total   Status      0  +0  0  1  0   0  1  Open    0  +0  0  +0  0  +0  0  6  In Progress    2  +1  18  +12  3  +0  23  +13  Ready for Testing    0  +0  1  +0  0  +0  1  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    2  +1  19  +6  3  +0  24  +7  Total      Short testing week due to holiday", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#last-week", 
            "text": "OSG-Connect wanted some project renames, done, but waiting on verification.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#this-week", 
            "text": "GRACC host cert expired, but that only affects backups so everything is still online  WCLG wants some info on missing accounting, but they are slow on finalizing their own accounting.  But it looks like it is now finished.  Derek will be at PEARC next week and will be presenting", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/", 
            "text": "OSG Technology Area Meeting, 26 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Marian, Mat, Suchandra, TimC, Vaibhav  \n\n\nAnnouncements\n\n\n\n\nTimT, Edgar out until next week\n\n\nDerek will be 50% OSG starting July 1, down from 75%\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Carl\n\n\n8 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n149\n\n\n7\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+5\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\nDev freeze today:  \n\n\n\n\n\n\n\n\nOwner\n\n\n# open tickets\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n3\n\n\n\n\n\n\nCarl\n\n\n2\n\n\n\n\n\n\nMat\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\nSuchandra: How's the LCMAPS VOMS transition going?\n\n\n\n\nHTCondor-CE whole node accounting + memory request issues\n\n\nVMU tests: Erin investigating network issues with new RHEL VMs using the dev subscription. Dakota was brought up to speed on image generation/troubleshooting.\n\n\nITB progress: New pool configs sprayed out for separated CM and an additional CE for testing pre-release\n\n\nInternal doc migration started and lives \nhere\n (formerly https://github.com/brianhlin/technology/tree/internal_migration)\n\n\nxrootd-cmstfc\n (contrib) fails to build for EL7, need help with CMake to place libs in \n/usr/lib64\n rather than \n/usr/lib/\n\n\n\n\nDiscussions\n\n\n\n\nRSV JIRA ticket incoming to fix querying \ncondor_q\n output\n\n\nIncoming Gratia probe changes for whole node accounting issues by Derek, Carl to review\n\n\nHosted CE LCMAPS VOMS transition complete\n\n\n\n\nSupport Update\n\n\n\n\nFIT (BrianL) - Jobs held, passed on troubleshooting doc  \n\n\nTAMU (BrianL) - SAM tests are better with updates to their scheduler config. Jobs submitted to their backend condor are being held with \"job not found\"\n\n\nUNL (Derek) - Reprocessed Accounting records for whole node jobs.  Pull request for HTCondor-CE (https://github.com/opensciencegrid/htcondor-ce/pull/151) and Gratia-Probes (https://github.com/opensciencegrid/gratia-probe/pull/18)\n\n\nSyracuse \n UCSD (Derek) - GPU nodes are starting up.  Small amount of support last week, but this week I expect production use to start, so possibly some user support.\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJuly 11th\n release\n\n\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n5\n\n\n1\n\n\n17\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n1\n\n\n6\n\n\n+4\n\n\n0\n\n\n1\n\n\n6\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+1\n\n\n6\n\n\n+3\n\n\n3\n\n\n+3\n\n\n10\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n1\n\n\n1\n\n\n13\n\n\n4\n\n\n3\n\n\n3\n\n\n17\n\n\n8\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nCondor 8.6.4 and 8.7.2 in 3.4 and upcoming, respectively. Including af ix for HTCondor-CE-Bosco without certs\n\n\nblahp fix for multicore requests for SLURM batch systems\n\n\nNew package, \ngridftp-dsi-posix\n, to replace \nxrootd-dsi\n\n\nFix for GridFTP startup to use the correct plugin configuration\n\n\nFix for CVMFS client failing to mount when very large groups exist\n\n\nAdded ability to include arbitrary ClassAd attributes in Gratia records\n\n\nAdded osg-configure-misc dependency to osg-gridftp\n\n\nFix for HDFS NameNode infinite loop\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLots of vacation from Investigations team this week.  Not much to update.  \n\n\nLast Week\n\n\n\n\nReprocess underreported sites and upload new APEL report to WLCG\n\n\nDebug Nova CVMFS repo, will need to redo repo with chunking.\n\n\nPackaging of CVMFS-Sync and configurations. https://github.com/bbockelm/cvmfs-sync/pull/1\n\n\nInvestigate backups of GRACC peripheral services\n\n\n\n\nThis Week\n\n\n\n\nWill need to redo nova repo with chunking.\n\n\nDocker'ification of GRACC Agents\n\n\nStart backups of grafana configurations (dashboards and datasources)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 26, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-technology-area-meeting-26-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Marian, Mat, Suchandra, TimC, Vaibhav", 
            "title": "OSG Technology Area Meeting, 26 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#announcements", 
            "text": "TimT, Edgar out until next week  Derek will be 50% OSG starting July 1, down from 75%", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#triage-duty", 
            "text": "This week: BrianL  Next week: Carl  8 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#jira", 
            "text": "# of tickets   State      149  7  Open    18  +5  In Progress    6  +3  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-software-team", 
            "text": "Dev freeze today:       Owner  # open tickets      BrianL  3    Carl  2    Mat  2       Suchandra: How's the LCMAPS VOMS transition going?   HTCondor-CE whole node accounting + memory request issues  VMU tests: Erin investigating network issues with new RHEL VMs using the dev subscription. Dakota was brought up to speed on image generation/troubleshooting.  ITB progress: New pool configs sprayed out for separated CM and an additional CE for testing pre-release  Internal doc migration started and lives  here  (formerly https://github.com/brianhlin/technology/tree/internal_migration)  xrootd-cmstfc  (contrib) fails to build for EL7, need help with CMake to place libs in  /usr/lib64  rather than  /usr/lib/", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#discussions", 
            "text": "RSV JIRA ticket incoming to fix querying  condor_q  output  Incoming Gratia probe changes for whole node accounting issues by Derek, Carl to review  Hosted CE LCMAPS VOMS transition complete", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#support-update", 
            "text": "FIT (BrianL) - Jobs held, passed on troubleshooting doc    TAMU (BrianL) - SAM tests are better with updates to their scheduler config. Jobs submitted to their backend condor are being held with \"job not found\"  UNL (Derek) - Reprocessed Accounting records for whole node jobs.  Pull request for HTCondor-CE (https://github.com/opensciencegrid/htcondor-ce/pull/151) and Gratia-Probes (https://github.com/opensciencegrid/gratia-probe/pull/18)  Syracuse   UCSD (Derek) - GPU nodes are starting up.  Small amount of support last week, but this week I expect production use to start, so possibly some user support.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-release-team", 
            "text": "Tim Theisen is handling the  July 11th  release      3.3.26   Both   3.4.1   Total   Status      0   1  1  0  5  1  17  Open    0  1  6  +4  0  1  6  +2  In Progress    1  +1  6  +3  3  +3  10  +7  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    1  1  13  4  3  3  17  8  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#ready-for-testing", 
            "text": "Condor 8.6.4 and 8.7.2 in 3.4 and upcoming, respectively. Including af ix for HTCondor-CE-Bosco without certs  blahp fix for multicore requests for SLURM batch systems  New package,  gridftp-dsi-posix , to replace  xrootd-dsi  Fix for GridFTP startup to use the correct plugin configuration  Fix for CVMFS client failing to mount when very large groups exist  Added ability to include arbitrary ClassAd attributes in Gratia records  Added osg-configure-misc dependency to osg-gridftp  Fix for HDFS NameNode infinite loop", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-investigations-team", 
            "text": "Lots of vacation from Investigations team this week.  Not much to update.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#last-week", 
            "text": "Reprocess underreported sites and upload new APEL report to WLCG  Debug Nova CVMFS repo, will need to redo repo with chunking.  Packaging of CVMFS-Sync and configurations. https://github.com/bbockelm/cvmfs-sync/pull/1  Investigate backups of GRACC peripheral services", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#this-week", 
            "text": "Will need to redo nova repo with chunking.  Docker'ification of GRACC Agents  Start backups of grafana configurations (dashboards and datasources)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/", 
            "text": "OSG Technology Area Meeting, 19 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n8 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n8\n\n\nOpen\n\n\n\n\n\n\n13\n\n\n5\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n1\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n41\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDev freeze next Monday\n\n\nStarted transitioning hosted CEs to LCMAPS VOMS plugin last week\n\n\nRHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription\n\n\nxrootd-cmstfc\n (contrib) fails to build for EL7, need help with CMake to place libs in \n/usr/lib64\n rather than \n/usr/lib/\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCTSC (BrianL) - Provided rough size estimate of OSG CE packaging\n\n\nFIT (BrianL) - Assisting transition from GRAM to HTCondor-CE\n\n\nTAMU (BrianL) - SAM tests are still getting held with \"Job not found\"\n\n\nSyracuse (Derek) - Implemented GPU submission on their HTCondor-CE.  Fermilab is creating an entry on the ITB GlideinWMS factories to utilize and test this new GPU capability.  OSG will follow suite once singularity configuration is developed.\n\n\nUCSD (Derek) - Gave some assistance to job route configuration for GPUs.\n\n\nNebraska (Derek) - Assisted in debugging missing hours in WLCG report.  Turns out that removed jobs were not being accounted correctly.  Also, Multi-CPU jobs and whole node jobs where not being accounted.\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJuly 11th\n release\n\n\nData Release Coming: IGTF Update, VO Package\n\n\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+1\n\n\n12\n\n\n+12\n\n\n5\n\n\n+5\n\n\n18\n\n\n+18\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\n1\n\n\n+1\n\n\n4\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n2\n\n\n+2\n\n\n17\n\n\n+0\n\n\n6\n\n\n+6\n\n\n25\n\n\n+25\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nFix for CVMFS client failing to mount when very large groups exist\n\n\nAdded ability to include arbitrary ClassAd attributes in Gratia records\n\n\nAdded osg-configure-misc dependency to osg-gridftp\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLots of vacation from Investigations team this week.  Not much to update.  \n\n\nLast Week\n\n\n\n\nGRACC debugging of Underreported sites\n\n\nDebug Nova CVMFS repo, will need to redo repo with chunking.\n\n\nBlog post on StashCache: https://djw8605.github.io/2017/06/14/stashcache/ and on Planet OSG http://blogs.grid.iu.edu/\n\n\n\n\nThis Week\n\n\n\n\nReprocess underreported sites and upload new APEL report to WLCG\n\n\nDocker'ification of GRACC Agents\n\n\nPackaging of CVMFS-Sync and configurations.\n\n\nInvestigate backups of GRACC peripheral services\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 19, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-technology-area-meeting-19-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:", 
            "title": "OSG Technology Area Meeting, 19 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  8 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#jira", 
            "text": "# of tickets   State      156  8  Open    13  5  In Progress    3  1  Ready for Testing    1  41  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-software-team", 
            "text": "Dev freeze next Monday  Started transitioning hosted CEs to LCMAPS VOMS plugin last week  RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription  xrootd-cmstfc  (contrib) fails to build for EL7, need help with CMake to place libs in  /usr/lib64  rather than  /usr/lib/", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#support-update", 
            "text": "CTSC (BrianL) - Provided rough size estimate of OSG CE packaging  FIT (BrianL) - Assisting transition from GRAM to HTCondor-CE  TAMU (BrianL) - SAM tests are still getting held with \"Job not found\"  Syracuse (Derek) - Implemented GPU submission on their HTCondor-CE.  Fermilab is creating an entry on the ITB GlideinWMS factories to utilize and test this new GPU capability.  OSG will follow suite once singularity configuration is developed.  UCSD (Derek) - Gave some assistance to job route configuration for GPUs.  Nebraska (Derek) - Assisted in debugging missing hours in WLCG report.  Turns out that removed jobs were not being accounted correctly.  Also, Multi-CPU jobs and whole node jobs where not being accounted.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-release-team", 
            "text": "Tim Theisen is handling the  July 11th  release  Data Release Coming: IGTF Update, VO Package      3.3.26   Both   3.4.1   Total   Status      1  +1  12  +12  5  +5  18  +18  Open    1  +1  2  +2  1  +1  4  +4  In Progress    0  +0  3  +3  0  +0  3  +3  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    2  +2  17  +0  6  +6  25  +25  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#ready-for-testing", 
            "text": "Fix for CVMFS client failing to mount when very large groups exist  Added ability to include arbitrary ClassAd attributes in Gratia records  Added osg-configure-misc dependency to osg-gridftp", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-investigations-team", 
            "text": "Lots of vacation from Investigations team this week.  Not much to update.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#last-week", 
            "text": "GRACC debugging of Underreported sites  Debug Nova CVMFS repo, will need to redo repo with chunking.  Blog post on StashCache: https://djw8605.github.io/2017/06/14/stashcache/ and on Planet OSG http://blogs.grid.iu.edu/", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#this-week", 
            "text": "Reprocess underreported sites and upload new APEL report to WLCG  Docker'ification of GRACC Agents  Packaging of CVMFS-Sync and configurations.  Investigate backups of GRACC peripheral services", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/", 
            "text": "OSG Technology Area Meeting, 12 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT, Vaibhav  \n\n\nAnnouncements\n\n\n\n\nVaibhav at UW Madison this week\n\n\nMat out this week\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n9 (+) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n157\n\n\n8\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n29\n\n\nReady for Testing\n\n\n\n\n\n\n42\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.0 / 3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nLCMAPS VOMS plugin bug\n: all FQANs are considered instead of the first one. Blocker for June release.\n\n\nPre-release tests failed due to install/update failures \n required \nosg-koji regen-repo osg-3.4-el{6,7}-prerelease\n\n\nNeed to start transitioning hosted CEs to LCMAPS VOMS plugin this week, after the release\n\n\nRHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription\n\n\nHorst completed the edg-mkgridmap -\n LCMAPS VOMS transition successfully last week\n\n\nxrootd-cmstfc\n (contrib) fails to build for EL7, need help with CMake to place libs in \n/usr/lib64\n rather than \n/usr/lib/\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nFIT (BrianL) - Investing CE that doesn't appear to be running\n\n\nTAMU (BrianL) - SAM tests failing intermittently due to transient issues submitting to the CE\n\n\nUFL (BrianL) - Bockjoo found a blahp bug that resulted in incorrect multicore job requests\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJune 13th\n release\n\n\nSoftware Release tomorrow\n\n\nData Release Coming: IGTF Update, VO Package\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n-1\n\n\n0\n\n\n-1\n\n\n1\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-4\n\n\n0\n\n\n-10\n\n\n0\n\n\n-19\n\n\n0\n\n\n-33\n\n\nReady for Testing\n\n\n\n\n\n\n4\n\n\n+4\n\n\n15\n\n\n+14\n\n\n22\n\n\n+22\n\n\n41\n\n\n+40\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n+0\n\n\n17\n\n\n+2\n\n\n22\n\n\n+2\n\n\n43\n\n\n+4\n\n\nTotal\n\n\n\n\n\n\n\n\nLate breaking update\n\n\n\n\nlcmaps-plugins-voms maps all FQANs\n\n\n\n\nReady for Release\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\nRelease voms-admin-server-2.7.0-1.22+\n\n\nRelease osg-configure 1.8.1\n\n\nEnable JSP implementation for tomcat webapps\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nChange software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts\n\n\nAdd ability to request whole node jobs\n\n\nosg-configure: reject empty allowed_vos in subclusters\n\n\nunnecessary check for OSG_APP and OSG_DATA in osg-configure\n\n\nRelease xrootd-lcmaps-1.3.2-2 +\n\n\nUpdate to XRootD to 4.6.1\n\n\nRelease StashCache metapackage 0.7+\n\n\nosg-configure: Get default allowed_vos with lcmaps voms plugin\n\n\nAdd OSG VOMS mapfile to osg-ce\n\n\nlcmaps-plugins-voms maps all FQANs\n\n\nDocument configuration of lcmaps-voms-plugin\n\n\nRelease Glideinwms v3.2.19+\n\n\nRelease osg-build 1.10.0\n\n\nosg-build: drop vdt-build\n\n\nosg-build: drop ~/.osg-build.ini\n\n\nAdd vo-client-lcmaps-voms dependency to osg-gridftp\n\n\n\n\n\n\nOSG 3.4.0\n\n\nDrop conflicts from cvmfs-config-osg\n\n\nUpdate to HTCondor 8.6.3+ in OSG 3.4\n\n\nRelease osg-ce-3.4-1+\n\n\nDrop conflicts from globus-gridftp-osg-extensions\n\n\nRemove requirements for packages dropped in 3.4 in osg-tested-internal\n\n\nosg-configure: Drop glexec support for 3.4\n\n\nRelease osg-configure 2.0.0\n\n\nPrepare lcmaps for 3.4\n\n\nDrop conflicts from HTCondor-CE packaging\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\nosg-configure: Drop managedfork and network config from 2.0.0\n\n\nRemove gridftp from the CE metapackages\n\n\nosg-configure: Drop osg-cleanup options from 10-misc.ini\n\n\nosg-configure: Deprecate GUMS support\n\n\nDrop client tools from osg-ce metapackages\n\n\nosg-configure: Disable GRAM configuration (2.0.0)\n\n\nosg-configure: Drop 'rsv is not installed' warning\n\n\nDrop glexec and java from osg-wn-client\n\n\nosg-configure: Remove \"configure-osg\" alias\n\n\nDrop edg-mkgridmap from OSG 3.4\n\n\nDrop bestman2 from OSG 3.4\n\n\nDrop GUMS from 3.4\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLots of vacation from Investigations team this week.  Not much to update.\n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter StashCache Cache Alerting\n\n\nDocker'ification of GRACC Agents\n\n\n\n\nThis Week\n\n\n\n\nContinue to dockerify GRACC agents and services.  Next on the list is gracc-summary.\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\nWrite StashCache article for user support team.\n\n\nSome BLAHP work.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 12, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-technology-area-meeting-12-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting, 12 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#announcements", 
            "text": "Vaibhav at UW Madison this week  Mat out this week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  9 (+) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#jira", 
            "text": "# of tickets   State      157  8  Open    18  +4  In Progress    14  29  Ready for Testing    42  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.0 / 3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-software-team", 
            "text": "LCMAPS VOMS plugin bug : all FQANs are considered instead of the first one. Blocker for June release.  Pre-release tests failed due to install/update failures   required  osg-koji regen-repo osg-3.4-el{6,7}-prerelease  Need to start transitioning hosted CEs to LCMAPS VOMS plugin this week, after the release  RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription  Horst completed the edg-mkgridmap -  LCMAPS VOMS transition successfully last week  xrootd-cmstfc  (contrib) fails to build for EL7, need help with CMake to place libs in  /usr/lib64  rather than  /usr/lib/", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#support-update", 
            "text": "FIT (BrianL) - Investing CE that doesn't appear to be running  TAMU (BrianL) - SAM tests failing intermittently due to transient issues submitting to the CE  UFL (BrianL) - Bockjoo found a blahp bug that resulted in incorrect multicore job requests", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-release-team", 
            "text": "Tim Theisen is handling the  June 13th  release  Software Release tomorrow  Data Release Coming: IGTF Update, VO Package      3.3.25   Both   3.4.0   Total   Status      0  +0  0  -2  0  +0  0  -2  Open    0  +0  1  -1  0  -1  1  -2  In Progress    0  -4  0  -10  0  -19  0  -33  Ready for Testing    4  +4  15  +14  22  +22  41  +40  Ready for Release    0  +0  1  +1  0  +0  1  +1  Closed    4  +0  17  +2  22  +2  43  +4  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#late-breaking-update", 
            "text": "lcmaps-plugins-voms maps all FQANs", 
            "title": "Late breaking update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#ready-for-release", 
            "text": "OSG 3.3.25  Drop timeout_close.patch in globus-xio  Release voms-admin-server-2.7.0-1.22+  Release osg-configure 1.8.1  Enable JSP implementation for tomcat webapps    Both  osg-update-vos: clean yum cache before downloading vo-client  Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts  Add ability to request whole node jobs  osg-configure: reject empty allowed_vos in subclusters  unnecessary check for OSG_APP and OSG_DATA in osg-configure  Release xrootd-lcmaps-1.3.2-2 +  Update to XRootD to 4.6.1  Release StashCache metapackage 0.7+  osg-configure: Get default allowed_vos with lcmaps voms plugin  Add OSG VOMS mapfile to osg-ce  lcmaps-plugins-voms maps all FQANs  Document configuration of lcmaps-voms-plugin  Release Glideinwms v3.2.19+  Release osg-build 1.10.0  osg-build: drop vdt-build  osg-build: drop ~/.osg-build.ini  Add vo-client-lcmaps-voms dependency to osg-gridftp    OSG 3.4.0  Drop conflicts from cvmfs-config-osg  Update to HTCondor 8.6.3+ in OSG 3.4  Release osg-ce-3.4-1+  Drop conflicts from globus-gridftp-osg-extensions  Remove requirements for packages dropped in 3.4 in osg-tested-internal  osg-configure: Drop glexec support for 3.4  Release osg-configure 2.0.0  Prepare lcmaps for 3.4  Drop conflicts from HTCondor-CE packaging  Drop bestman2 and globus*run RSV metrics  osg-configure: Drop managedfork and network config from 2.0.0  Remove gridftp from the CE metapackages  osg-configure: Drop osg-cleanup options from 10-misc.ini  osg-configure: Deprecate GUMS support  Drop client tools from osg-ce metapackages  osg-configure: Disable GRAM configuration (2.0.0)  osg-configure: Drop 'rsv is not installed' warning  Drop glexec and java from osg-wn-client  osg-configure: Remove \"configure-osg\" alias  Drop edg-mkgridmap from OSG 3.4  Drop bestman2 from OSG 3.4  Drop GUMS from 3.4", 
            "title": "Ready for Release"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-investigations-team", 
            "text": "Lots of vacation from Investigations team this week.  Not much to update.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better StashCache Cache Alerting  Docker'ification of GRACC Agents", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#this-week", 
            "text": "Continue to dockerify GRACC agents and services.  Next on the list is gracc-summary.  Improve StashCache docs even more through feedback from sites. (hopefully we get some)  Write StashCache article for user support team.  Some BLAHP work.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/", 
            "text": "OSG Technology Area Meeting,  5 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT, Vaibhav  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n8 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n165\n\n\n+5\n\n\nOpen\n\n\n\n\n\n\n14\n\n\n23\n\n\nIn Progress\n\n\n\n\n\n\n33\n\n\n+27\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.0 / 3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nSingularity build\n in EPEL stable so we can exclude it from OSG 3.4.0\n\n\nBob Ball completed GUMS -\n LCMAPS VOMS transition on one host last week, Horst will start the edg-mkgridmap -\n LCMAPS VOMS transition this week\n\n\nGridFTP/XRootD docs need updating for us with the LCMAPS VOMS plugin, any ideas on how to track down TWiki section usage?\n\n\nRHEL VMU tests working for the being even though our RHEL subscription ended. Moate working on a RHEL7 VM exec host using his developer license.\n\n\nHadoop + Ganglia and GridFTP + umask questions sitting in osg-software list\n\n\n\n\nDiscussions\n\n\n\n\nOSG 3.4 singularity policy needs to be clear and set\n\n\nDocs need to specify that the LCMAPS VOMS plugin is the preferred authentication method\n\n\nKyle should be able to help search for TWiki %STARTSECTION% usage\n\n\nMat will follow up with AGLT2 about a bug with the testing version of osg-configure that affects CEs\n\n\nBrianL will find other edg-mkgridmap sites and coordinate with Suchandra to transition hosted CEs to LCMAPS VOMS plugin\n\n\nBrianL will find owners for software mailing list issues\n\n\nTimC and BrianL to discuss the future of the many OSG Software mailing lists\n\n\n\n\nSupport Update\n\n\n\n\nPurdue used OSG_WN_TMP configuration in osg-configure to set it to condor's scratch directory (execute directory?) (Derek)\n\n\nUtah renewed certificate, and is now working (Derek).\n\n\nAPEL update from ALICE usage (Derek).\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJune 13th\n release\n\n\nPackage Freeze today\n\n\nData Release Coming: IGTF Update, VO Package??\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n2\n\n\n-2\n\n\n0\n\n\n-1\n\n\n2\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-3\n\n\n2\n\n\n-6\n\n\n1\n\n\n-15\n\n\n3\n\n\n-24\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n+3\n\n\n10\n\n\n+7\n\n\n19\n\n\n+17\n\n\n33\n\n\n+27\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n+0\n\n\n15\n\n\n-1\n\n\n20\n\n\n+1\n\n\n39\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\nRelease voms-admin-server-2.7.0-1.22+\n\n\nRelease osg-configure 1.8.1\n\n\nEnable JSP implementation for tomcat webapps\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nChange software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts\n\n\nAdd ability to request whole node jobs\n\n\nosg-configure: reject empty allowed_vos in subclusters\n\n\nunnecessary check for OSG_APP and OSG_DATA in osg-configure\n\n\nxrootd-lcmaps-1.3.2-2 build fails for EL6\n\n\nUpdate to XRootD to 4.6.1\n\n\nRelease StashCache metapackage 0.7+\n\n\nosg-configure: Get default allowed_vos with lcmaps voms plugin\n\n\nAdd OSG VOMS mapfile to osg-ce\n\n\n\n\n\n\nOSG 3.4.0\n\n\nDrop conflicts from cvmfs-config-osg\n\n\nUpdate to HTCondor 8.6.3+ in OSG 3.4\n\n\nRelease osg-ce-3.4-1+\n\n\nDrop conflicts from globus-gridftp-osg-extensions\n\n\nRemove requirements for packages dropped in 3.4 in osg-tested-internal\n\n\nosg-configure: Drop glexec support for 3.4\n\n\nRelease osg-configure 2.0.0\n\n\nPrepare lcmaps for 3.4\n\n\nDrop conflicts from HTCondor-CE packaging\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\nosg-configure: Drop managedfork and network config from 2.0.0\n\n\nRemove gridftp from the CE metapackages\n\n\nosg-configure: Drop osg-cleanup options from 10-misc.ini\n\n\nosg-configure: Deprecate GUMS support\n\n\nDrop client tools from osg-ce metapackages\n\n\nosg-configure: Disable GRAM configuration (2.0.0)\n\n\nosg-configure: Drop 'rsv is not installed' warning\n\n\nDrop glexec and java from osg-wn-client\n\n\nosg-configure: Remove \"configure-osg\" alias\n\n\n\n\n\n\nUpcoming\n\n\nNothing\n\n\n\n\n\n\n\n\nReady for Release\n\n\n\n\nOSG 3.3.25\n\n\nBoth\n\n\nUpdate to rsv-perfsonar 1.3.1+\n\n\n\n\n\n\nOSG 3.4.0\n\n\nUpcoming\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter GRACC Alerting\n\n\nBetter StashCache Cache Alerting\n\n\nDocker'ification of GRACC Agents\n\n\nFix unknown projectnames\n\n\n\n\nThis Week\n\n\n\n\nContinue to improve StashCache alerting\n\n\nContinue to dockerify GRACC agents and services.\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 6, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-technology-area-meeting-5-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting,  5 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  8 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#jira", 
            "text": "# of tickets   State      165  +5  Open    14  23  In Progress    33  +27  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.0 / 3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-software-team", 
            "text": "Singularity build  in EPEL stable so we can exclude it from OSG 3.4.0  Bob Ball completed GUMS -  LCMAPS VOMS transition on one host last week, Horst will start the edg-mkgridmap -  LCMAPS VOMS transition this week  GridFTP/XRootD docs need updating for us with the LCMAPS VOMS plugin, any ideas on how to track down TWiki section usage?  RHEL VMU tests working for the being even though our RHEL subscription ended. Moate working on a RHEL7 VM exec host using his developer license.  Hadoop + Ganglia and GridFTP + umask questions sitting in osg-software list", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#discussions", 
            "text": "OSG 3.4 singularity policy needs to be clear and set  Docs need to specify that the LCMAPS VOMS plugin is the preferred authentication method  Kyle should be able to help search for TWiki %STARTSECTION% usage  Mat will follow up with AGLT2 about a bug with the testing version of osg-configure that affects CEs  BrianL will find other edg-mkgridmap sites and coordinate with Suchandra to transition hosted CEs to LCMAPS VOMS plugin  BrianL will find owners for software mailing list issues  TimC and BrianL to discuss the future of the many OSG Software mailing lists", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#support-update", 
            "text": "Purdue used OSG_WN_TMP configuration in osg-configure to set it to condor's scratch directory (execute directory?) (Derek)  Utah renewed certificate, and is now working (Derek).  APEL update from ALICE usage (Derek).", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-release-team", 
            "text": "Tim Theisen is handling the  June 13th  release  Package Freeze today  Data Release Coming: IGTF Update, VO Package??      3.3.25   Both   3.4.0   Total   Status      0  +0  2  -2  0  -1  2  -3  Open    0  -3  2  -6  1  -15  3  -24  In Progress    4  +3  10  +7  19  +17  33  +27  Ready for Testing    0  +0  1  +0  0  +0  1  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    4  +0  15  -1  20  +1  39  +0  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#ready-for-testing", 
            "text": "OSG 3.3.25  Drop timeout_close.patch in globus-xio  Release voms-admin-server-2.7.0-1.22+  Release osg-configure 1.8.1  Enable JSP implementation for tomcat webapps    Both  osg-update-vos: clean yum cache before downloading vo-client  Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts  Add ability to request whole node jobs  osg-configure: reject empty allowed_vos in subclusters  unnecessary check for OSG_APP and OSG_DATA in osg-configure  xrootd-lcmaps-1.3.2-2 build fails for EL6  Update to XRootD to 4.6.1  Release StashCache metapackage 0.7+  osg-configure: Get default allowed_vos with lcmaps voms plugin  Add OSG VOMS mapfile to osg-ce    OSG 3.4.0  Drop conflicts from cvmfs-config-osg  Update to HTCondor 8.6.3+ in OSG 3.4  Release osg-ce-3.4-1+  Drop conflicts from globus-gridftp-osg-extensions  Remove requirements for packages dropped in 3.4 in osg-tested-internal  osg-configure: Drop glexec support for 3.4  Release osg-configure 2.0.0  Prepare lcmaps for 3.4  Drop conflicts from HTCondor-CE packaging  Drop bestman2 and globus*run RSV metrics  osg-configure: Drop managedfork and network config from 2.0.0  Remove gridftp from the CE metapackages  osg-configure: Drop osg-cleanup options from 10-misc.ini  osg-configure: Deprecate GUMS support  Drop client tools from osg-ce metapackages  osg-configure: Disable GRAM configuration (2.0.0)  osg-configure: Drop 'rsv is not installed' warning  Drop glexec and java from osg-wn-client  osg-configure: Remove \"configure-osg\" alias    Upcoming  Nothing", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#ready-for-release", 
            "text": "OSG 3.3.25  Both  Update to rsv-perfsonar 1.3.1+    OSG 3.4.0  Upcoming", 
            "title": "Ready for Release"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better GRACC Alerting  Better StashCache Cache Alerting  Docker'ification of GRACC Agents  Fix unknown projectnames", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#this-week", 
            "text": "Continue to improve StashCache alerting  Continue to dockerify GRACC agents and services.  Improve StashCache docs even more through feedback from sites. (hopefully we get some)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/", 
            "text": "OSG Technology Area Meeting, 30 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT, Vaibhav  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Mat\n\n\n9 (+3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n160\n\n\n+4\n\n\nOpen\n\n\n\n\n\n\n37\n\n\n+5\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.25 / 3.4.0\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26 / 3.4.1\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.3.27 / 3.4.2\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\n\n\nDeveloper\n\n\nTickets not RFT\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n17\n\n\n\n\n\n\nBrian\n\n\n10\n\n\n\n\n\n\nMarian\n\n\n2\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nCarl\n\n\n1\n\n\n\n\n\n\n\n\n\n\nSingularity build\n in EPEL testing looks good, we should be able to drop it from 3.4.0\n\n\nBob Ball starting GUMS -\n LCMAPS VOMS transition this week, Horst will start the edg-mkgridmap -\n LCMAPS VOMS transition next week\n\n\nGlobus Toolkit support ends Jan 2018\n\n\n\n\nDiscussions\n\n\nSupport Update\n\n\n\n\nHosted CE GSISSH meeting today; Suchandra, BrianL, and possibly Jaime Frey will attend\n\n\nSuchandra had questions about running multiple CE services on a single host to limit IPv4 addresses. Will try IPv6 instead and coordinate with Edgar for Factory integration.\n\n\n\n\nSupport Update\n\n\n\n\nMIT (BrianL) - Investigated open UDP ports for GridFTP: turned out to be an old version of GridFTP\n\n\nUFL (BrianL) - Assisted Bockjoo with Slurm timeout issues\n\n\nPurdue (Derek) - Assist in configuration of OSG_WN_TMP for their cluster.\n\n\nALICE (Derek) - Alice VO had issues with submitting usage to GRACC.  So far, seems like it's the probe.\n\n\nFermilab (Suchandra) - disabling SSLv2/SSLv3 for BeStMan, BrianB to speak to them about transitioning them to another storage solution\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJune 13th\n release\n\n\nDevelopment Freeze today\n\n\nData Release Coming: IGTF Update, VO Package??\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n2\n\n\n4\n\n\n12\n\n\n1\n\n\n6\n\n\n5\n\n\n20\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n+2\n\n\n8\n\n\n1\n\n\n16\n\n\n+6\n\n\n27\n\n\n+7\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+0\n\n\n3\n\n\n+0\n\n\n2\n\n\n+2\n\n\n6\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n1\n\n\n0\n\n\n+0\n\n\n0\n\n\n1\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n+0\n\n\n16\n\n\n13\n\n\n19\n\n\n+2\n\n\n39\n\n\n11\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nChange software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts\n\n\n\n\n\n\nOSG 3.4.0\n\n\nDrop conflicts from cvmfs-config-osg\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\n\n\n\n\nUpcoming\n\n\nUpdate to HTCondor 8.6.3+ in Upcoming (labeled for both releases but in Upcoming)\n\n\n\n\n\n\n\n\nReady for Release\n\n\n\n\nOSG 3.3.25\n\n\nBoth\n\n\nUpdate to rsv-perfsonar 1.3.1+\n\n\n\n\n\n\nOSG 3.4.0\n\n\nUpcoming\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nFocused StashCache effort is over.\n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter GRACC Alerting - Now alert on all the things!\n\n\nBetter StashCache Cache Alerting - HTTP Accesses\n\n\nGRACC daemons no longer leak memory like it's their job\n\n\nPython tar file objects keep the metadata for every object added to the tar file, be sure to clear it!\n\n\n\n\n\n\nCVMFS syncing with Stash now works, increased time out.\n\n\nPackaging of GRACC Agent daemons in Docker\n\n\n\n\nThis Week\n\n\n\n\nComplete packaging of GRACC agents in docker.\n\n\nHelp debug HTTP stalls on XrootD\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "May 30, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-technology-area-meeting-30-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting, 30 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#triage-duty", 
            "text": "This week: Edgar  Next week: Mat  9 (+3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#jira", 
            "text": "# of tickets   State      160  +4  Open    37  +5  In Progress    6  +2  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.25 / 3.4.0  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26 / 3.4.1  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.3.27 / 3.4.2  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-software-team", 
            "text": "Developer  Tickets not RFT      Mat  17    Brian  10    Marian  2    Edgar  2    Carl  1      Singularity build  in EPEL testing looks good, we should be able to drop it from 3.4.0  Bob Ball starting GUMS -  LCMAPS VOMS transition this week, Horst will start the edg-mkgridmap -  LCMAPS VOMS transition next week  Globus Toolkit support ends Jan 2018", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#discussions", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#support-update", 
            "text": "Hosted CE GSISSH meeting today; Suchandra, BrianL, and possibly Jaime Frey will attend  Suchandra had questions about running multiple CE services on a single host to limit IPv4 addresses. Will try IPv6 instead and coordinate with Edgar for Factory integration.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#support-update_1", 
            "text": "MIT (BrianL) - Investigated open UDP ports for GridFTP: turned out to be an old version of GridFTP  UFL (BrianL) - Assisted Bockjoo with Slurm timeout issues  Purdue (Derek) - Assist in configuration of OSG_WN_TMP for their cluster.  ALICE (Derek) - Alice VO had issues with submitting usage to GRACC.  So far, seems like it's the probe.  Fermilab (Suchandra) - disabling SSLv2/SSLv3 for BeStMan, BrianB to speak to them about transitioning them to another storage solution", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-release-team", 
            "text": "Tim Theisen is handling the  June 13th  release  Development Freeze today  Data Release Coming: IGTF Update, VO Package??      3.3.25   Both   3.4.0   Total   Status      0  2  4  12  1  6  5  20  Open    3  +2  8  1  16  +6  27  +7  In Progress    1  +0  3  +0  2  +2  6  +2  Ready for Testing    0  +0  1  +1  0  +0  1  +1  Ready for Release    0  +0  0  1  0  +0  0  1  Closed    4  +0  16  13  19  +2  39  11  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#ready-for-testing", 
            "text": "OSG 3.3.25  Drop timeout_close.patch in globus-xio    Both  osg-update-vos: clean yum cache before downloading vo-client  Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts    OSG 3.4.0  Drop conflicts from cvmfs-config-osg  Drop bestman2 and globus*run RSV metrics    Upcoming  Update to HTCondor 8.6.3+ in Upcoming (labeled for both releases but in Upcoming)", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#ready-for-release", 
            "text": "OSG 3.3.25  Both  Update to rsv-perfsonar 1.3.1+    OSG 3.4.0  Upcoming", 
            "title": "Ready for Release"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-investigations-team", 
            "text": "Focused StashCache effort is over.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better GRACC Alerting - Now alert on all the things!  Better StashCache Cache Alerting - HTTP Accesses  GRACC daemons no longer leak memory like it's their job  Python tar file objects keep the metadata for every object added to the tar file, be sure to clear it!    CVMFS syncing with Stash now works, increased time out.  Packaging of GRACC Agent daemons in Docker", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#this-week", 
            "text": "Complete packaging of GRACC agents in docker.  Help debug HTTP stalls on XrootD  Improve StashCache docs even more through feedback from sites. (hopefully we get some)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/", 
            "text": "OSG Technology Area Meeting, 22 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nMemorial day next Monday, meeting on Tuesday instead.\n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Edgar\n\n\n6 (0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n7\n\n\nOpen\n\n\n\n\n\n\n32\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nOSG 3.4.0 targeted for June\n\n\nDiscussions\n\n\nSingularity may be updated in EPEL, Derek will test it this week and if successful, this may be able to be dropped from 3.4.\n\n\nSupport Update\n\n\n\n\nFIT (BrianL) - Assisting with GRAM -\n HTCondor-CE transition\n\n\nMIT (BrianL) - Investigating multiple GridFTP processes spawning on the real server when starting keepalived on the load balancing node\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nJune 13th\n release\n\n\n\n\nDevelopment Freeze next Tuesday\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n2\n\n\n18\n\n\n16\n\n\n15\n\n\n7\n\n\n4\n\n\n25\n\n\n7\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n6\n\n\n9\n\n\n9\n\n\n10\n\n\n4\n\n\n20\n\n\n7\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n0\n\n\n3\n\n\n3\n\n\n0\n\n\n0\n\n\n4\n\n\n3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n25\n\n\n29\n\n\n28\n\n\n17\n\n\n0\n\n\n50\n\n\n3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nUpdate to rsv-perfsonar 1.3.1+\n\n\n\n\n\n\nOSG 3.4.0\n\n\nUpcoming\n\n\nUpdate to HTCondor 8.6.3+ in Upcoming\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nWeek 3 of StashCache focus.  Effort decreasing...  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.  \n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter GRACC Alerting\n\n\nBetter StashCache Cache Alerting\n\n\n\n\nThis Week\n\n\n\n\nContinue to improve StashCache alerting\n\n\nHelp debug HTTP stalls on XrootD\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\nGRACC improvements to some memory leaky daemons\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "May 22, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-technology-area-meeting-22-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 22 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#announcements", 
            "text": "Memorial day next Monday, meeting on Tuesday instead.", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#triage-duty", 
            "text": "This week: Derek  Next week: Edgar  6 (0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#jira", 
            "text": "# of tickets   State      156  7  Open    32  +6  In Progress    4  0  Ready for Testing    0  0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-software-team", 
            "text": "OSG 3.4.0 targeted for June", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#discussions", 
            "text": "Singularity may be updated in EPEL, Derek will test it this week and if successful, this may be able to be dropped from 3.4.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#support-update", 
            "text": "FIT (BrianL) - Assisting with GRAM -  HTCondor-CE transition  MIT (BrianL) - Investigating multiple GridFTP processes spawning on the real server when starting keepalived on the load balancing node", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  June 13th  release   Development Freeze next Tuesday     3.3.25   Both   3.4.0   Total   Status      2  18  16  15  7  4  25  7  Open    1  6  9  9  10  4  20  7  In Progress    1  0  3  3  0  0  4  3  Ready for Testing    0  0  0  0  0  0  0  0  Ready for Release    0  1  1  1  0  0  1  0  Closed    4  25  29  28  17  0  50  3  Total       Ready for Testing   OSG 3.3.25  Drop timeout_close.patch in globus-xio    Both  osg-update-vos: clean yum cache before downloading vo-client  Update to rsv-perfsonar 1.3.1+    OSG 3.4.0  Upcoming  Update to HTCondor 8.6.3+ in Upcoming", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-investigations-team", 
            "text": "Week 3 of StashCache focus.  Effort decreasing...  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better GRACC Alerting  Better StashCache Cache Alerting", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#this-week", 
            "text": "Continue to improve StashCache alerting  Help debug HTTP stalls on XrootD  Improve StashCache docs even more through feedback from sites. (hopefully we get some)  GRACC improvements to some memory leaky daemons", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/", 
            "text": "OSG Technology Area Meeting, 15 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: Derek\n\n\n6 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n163\n\n\n+2\n\n\nOpen\n\n\n\n\n\n\n26\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n16\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nOSG 3.4 targeted for June, will revisit progress at Software Freeze\n\n\n\n\nDiscussions\n\n\n\n\n3.4.0 testing strategy: mostly automated testing with strategic manual testing, particularly with Globus-dependent software\n\n\nEdgar training new UCSD staff on OSG basics this week\n\n\n\n\nSupport Update\n\n\n\n\nOU (Derek) - slurm gratia-probe not reporting for a few days.  Then suddenly re-appeared.  Going to take some slurm magic to figure it out.  But it's working now.\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nJune 13th\n release\n\n\nDevelopment Release in two weeks\n\n\n\n\nVO Package v73 - release tomorrow\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n20\n\n\n20\n\n\n1\n\n\n1\n\n\n11\n\n\n11\n\n\n32\n\n\n32\n\n\nOpen\n\n\n\n\n\n\n7\n\n\n7\n\n\n0\n\n\n0\n\n\n6\n\n\n6\n\n\n13\n\n\n13\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\nClosed\n\n\n\n\n\n\n29\n\n\n29\n\n\n1\n\n\n1\n\n\n17\n\n\n17\n\n\n47\n\n\n47\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\nOSG 3.3.25\n\n\n\n\nReady for Testing\n\n\nUpcoming: HTCondor 8.6.3\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nWeek 2 of StashCache focus.  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.  \n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter monitoring for CVMFS StashCache\n\n\nStashCache documentation for admins of caches \n origins \nhttps://opensciencegrid.github.io/StashCache/\n\n\n\n\nThis Week\n\n\n\n\nImprove docs even more through feedback from sites.\n\n\nLots of StashCache authenticated packaging.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "May 15, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-technology-area-meeting-15-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 15 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#triage-duty", 
            "text": "This week: Carl  Next week: Derek  6 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#jira", 
            "text": "# of tickets   State      163  +2  Open    26  +6  In Progress    4  0  Ready for Testing    0  16  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-software-team", 
            "text": "OSG 3.4 targeted for June, will revisit progress at Software Freeze", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#discussions", 
            "text": "3.4.0 testing strategy: mostly automated testing with strategic manual testing, particularly with Globus-dependent software  Edgar training new UCSD staff on OSG basics this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#support-update", 
            "text": "OU (Derek) - slurm gratia-probe not reporting for a few days.  Then suddenly re-appeared.  Going to take some slurm magic to figure it out.  But it's working now.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  June 13th  release  Development Release in two weeks   VO Package v73 - release tomorrow     3.3.25   Both   3.4.0   Total   Status      20  20  1  1  11  11  32  32  Open    7  7  0  0  6  6  13  13  In Progress    1  1  0  0  0  0  1  1  Ready for Testing    0  0  0  0  0  0  0  0  Ready for Release    1  1  0  0  0  0  1  1  Closed    29  29  1  1  17  17  47  47  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-3325", 
            "text": "Ready for Testing  Upcoming: HTCondor 8.6.3", 
            "title": "OSG 3.3.25"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-investigations-team", 
            "text": "Week 2 of StashCache focus.  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better monitoring for CVMFS StashCache  StashCache documentation for admins of caches   origins  https://opensciencegrid.github.io/StashCache/", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#this-week", 
            "text": "Improve docs even more through feedback from sites.  Lots of StashCache authenticated packaging.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/", 
            "text": "OSG Technology Area Meeting,  8 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Carl\n\n\n7 (\n3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n161\n\n\n3\n\n\nOpen\n\n\n\n\n\n\n20\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n14\n\n\nReady for Testing\n\n\n\n\n\n\n16\n\n\n+15\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDropped unused JIRA ticket types; still need to summarize remaining types\n\n\nOSG 3.4 preparation in full force\n\n\nTWiki -\n GH doc transition needs to begin\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCHTC (BrianL/Jeff) - Asssisted Moate with bringing their frontend back up; turned out to just be a dead httpd service\n\n\nUW-Madison (Carl) - Working on discrepancies with APEL reporting\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nMay 9th\n release\n\n\nRelease tomorrow\n\n\nVO Package v73 - release this week (Wednesday or Thursday)\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-14\n\n\nReady for Testing\n\n\n\n\n\n\n17\n\n\n+16\n\n\nReady for Release\n\n\n\n\n\n\n17\n\n\n-3\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nReady for Release  \n\n\nosg-configure 1.7.0\n\n\nEdit lcmaps.db to use the VOMS plugin\n\n\nAdd template lcmaps.db files\n\n\n\n\n\n\nMake all attributes relating to the defunct BDII service optional\n\n\nDon't error out if user-vo-map missing, issue warning with suggestion\n\n\n\n\n\n\nCVMFS X.509 helper - fix for running inside a container\n\n\ngsissh in tarballs\n\n\nFix HTCondor Gratia probe to not call .eval() if not present\n\n\nosg-build 1.9.0\n\n\nSplit osg-build into subpackages\n\n\nadd supprt for git repos in .source files (for HCC)\n\n\nosg-build notes default options\n\n\nadd support for 3.4   \n\n\n\n\n\n\nUpcoming: HTCondor 8.6.2\n\n\nUpcoming: GlideinWMS 3.3.2\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\n\n\n\nOSG Investigations Team\n\n\nInvestigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.\n\n\nLast Week\n\n\n\n\nDebug some GRACC issues with new changes to OIM VO Field of Science\n\n\nBLAHP work, lots of little changes here and there.\n\n\nStashCache documentation for admins of caches \n origins https://opensciencegrid.github.io/StashCache/\n\n\nGather investigation publications for Tim et. al.\n\n\n\n\nThis Week\n\n\n\n\nHopefully limited GRACC transition.\n\n\nLots of StashCache authenticated packaging.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project", 
            "title": "May 8, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-technology-area-meeting-8-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  8 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#triage-duty", 
            "text": "This week: BrianL  Next week: Carl  7 ( 3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#jira", 
            "text": "# of tickets   State      161  3  Open    20  +2  In Progress    4  14  Ready for Testing    16  +15  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-software-team", 
            "text": "Dropped unused JIRA ticket types; still need to summarize remaining types  OSG 3.4 preparation in full force  TWiki -  GH doc transition needs to begin", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#support-update", 
            "text": "CHTC (BrianL/Jeff) - Asssisted Moate with bringing their frontend back up; turned out to just be a dead httpd service  UW-Madison (Carl) - Working on discrepancies with APEL reporting", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-release-team", 
            "text": "Tim Theisen is handling the  May 9th  release  Release tomorrow  VO Package v73 - release this week (Wednesday or Thursday)      3.3.24   Status      0  -2  Open    0  -3  In Progress    0  -14  Ready for Testing    17  +16  Ready for Release    17  -3  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-3324", 
            "text": "Ready for Release    osg-configure 1.7.0  Edit lcmaps.db to use the VOMS plugin  Add template lcmaps.db files    Make all attributes relating to the defunct BDII service optional  Don't error out if user-vo-map missing, issue warning with suggestion    CVMFS X.509 helper - fix for running inside a container  gsissh in tarballs  Fix HTCondor Gratia probe to not call .eval() if not present  osg-build 1.9.0  Split osg-build into subpackages  add supprt for git repos in .source files (for HCC)  osg-build notes default options  add support for 3.4       Upcoming: HTCondor 8.6.2  Upcoming: GlideinWMS 3.3.2", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-investigations-team", 
            "text": "Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#last-week", 
            "text": "Debug some GRACC issues with new changes to OIM VO Field of Science  BLAHP work, lots of little changes here and there.  StashCache documentation for admins of caches   origins https://opensciencegrid.github.io/StashCache/  Gather investigation publications for Tim et. al.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#this-week", 
            "text": "Hopefully limited GRACC transition.  Lots of StashCache authenticated packaging.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/", 
            "text": "OSG Technology Area Meeting,  1 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Marian, Mat, TimT  \n\n\nAnnouncements\n\n\n\n\nHTCondor Week Tuesday - Friday\n\n\nSuchandra on vacation until 5/5\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n10 (+6) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n167\n\n\n4\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n5\n\n\nIn Progress\n\n\n\n\n\n\n18\n\n\n+15\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nPackage freeze today\n\n\nUtilizing JIRA ticket type and dropping unused types:\n\n\nAccess\n\n\nChange\n\n\nFault\n\n\nIT Help\n\n\nOngoing\n\n\nPurchase\n\n\nRequest\n\n\nStory\n\n\nSub-task-ongoing\n\n\nTechnical task\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nXRootD 4.6.1 release candidate has CMS/OSG approval (Marian) but no new version cut yet. Punting to the next release.\n\n\nVOMS Admin Server also punted to next release.\n\n\nBrianL will start removing above unused JIRA ticket types next week and writing a summary for remaining ticket types\n\n\n\n\nSupport Update\n\n\n\n\nUMich (BrianL) - Bob Ball's security handshakes were due to a local network issue\n\n\nUW-Madison (Carl) - Working on discrepancies with APEL reporting\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nMay 9th\n release\n\n\nPackage freeze today\n\n\nVO Package v73\n\n\nNeed help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week this week\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n2\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n-5\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n+11\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n20\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nReady for Testing  \n\n\nosg-configure 1.7.0\n\n\nEdit lcmaps.db to use the VOMS plugin\n\n\nAdd template lcmaps.db files\n\n\n\n\n\n\ndrop unused BDII data\n\n\nDon't error out if user-vo-map missing, issue warning with suggestion\n\n\n\n\n\n\nFix HTCondor Gratia probe to not call .eval() if not present\n\n\nCVMFS X.509 helper - fix for running inside a container\n\n\ngsissh in tarballs\n\n\nHTCondor 8.6.2 in Upcoming\n\n\nosg-build 1.9.0\n\n\nSplit osg-build into subpackages\n\n\nadd supprt for git repos in .source files (for HCC)\n\n\nosg-build notes default options\n\n\nadd support for 3.4   \n\n\n\n\n\n\n\n\n\n\nReady for Release  \n\n\nUpcoming: GlideinWMS 3.3.2\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nTop priority\n is the APEL reports from GRACC.  They need to debugged this week! \nGRACC-19\n\n\nLast Week\n\n\n\n\nLots of GRACC work, still taking some time\n\n\nBLAHP work, lots of little changes here and there.\n\n\nStashcp to handle multiple origins\n\n\nStashCache documentation for admins of caches \n origins\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC transition.\n\n\nFinish up BLAHP initial pull request\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project", 
            "title": "May 1, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-technology-area-meeting-1-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Marian, Mat, TimT", 
            "title": "OSG Technology Area Meeting,  1 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#announcements", 
            "text": "HTCondor Week Tuesday - Friday  Suchandra on vacation until 5/5", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  10 (+6) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#jira", 
            "text": "# of tickets   State      167  4  Open    18  5  In Progress    18  +15  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-software-team", 
            "text": "Package freeze today  Utilizing JIRA ticket type and dropping unused types:  Access  Change  Fault  IT Help  Ongoing  Purchase  Request  Story  Sub-task-ongoing  Technical task", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#discussions", 
            "text": "XRootD 4.6.1 release candidate has CMS/OSG approval (Marian) but no new version cut yet. Punting to the next release.  VOMS Admin Server also punted to next release.  BrianL will start removing above unused JIRA ticket types next week and writing a summary for remaining ticket types", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#support-update", 
            "text": "UMich (BrianL) - Bob Ball's security handshakes were due to a local network issue  UW-Madison (Carl) - Working on discrepancies with APEL reporting", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-release-team", 
            "text": "Tim Theisen is handling the  May 9th  release  Package freeze today  VO Package v73  Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week this week      3.3.24   Status      2  -6  Open    3  -5  In Progress    14  +11  Ready for Testing    1  +1  Ready for Release    20  +1  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-3324", 
            "text": "Ready for Testing    osg-configure 1.7.0  Edit lcmaps.db to use the VOMS plugin  Add template lcmaps.db files    drop unused BDII data  Don't error out if user-vo-map missing, issue warning with suggestion    Fix HTCondor Gratia probe to not call .eval() if not present  CVMFS X.509 helper - fix for running inside a container  gsissh in tarballs  HTCondor 8.6.2 in Upcoming  osg-build 1.9.0  Split osg-build into subpackages  add supprt for git repos in .source files (for HCC)  osg-build notes default options  add support for 3.4         Ready for Release    Upcoming: GlideinWMS 3.3.2", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-investigations-team", 
            "text": "Top priority  is the APEL reports from GRACC.  They need to debugged this week!  GRACC-19", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#last-week", 
            "text": "Lots of GRACC work, still taking some time  BLAHP work, lots of little changes here and there.  Stashcp to handle multiple origins  StashCache documentation for admins of caches   origins", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#this-week", 
            "text": "More GRACC transition.  Finish up BLAHP initial pull request", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#ongoing", 
            "text": "GRACC Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/", 
            "text": "OSG Technology Area Meeting, 24 April 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Jeff, Marian, Mat, TimC, TimT\n\n\nAnnouncements\n\n\n\n\nBrianL on vacation Wednesday through Friday\n\n\nSuchandra on vacation until 5/5\n\n\nCondor week starts next Tuesday, 5/2\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: TimT\n\n\n4 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n171\n\n\n+5\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\nDev freeze today, tickets not RFT by owner:  \n\n\n\n\n\n\n\n\nDeveloper\n\n\n#\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n12\n\n\n\n\n\n\nMarian\n\n\n2\n\n\n\n\n\n\nBrianL\n\n\n1\n\n\n\n\n\n\nCarl\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nosg-build does not yet support building packages for 3.4. Expected support available in May.\n\n\nSupport Update\n\n\n\n\nUMich (BrianL/Jeff/Marian) - Bob Ball's rebuilt CE is experiencing security handshake timeouts. Also had an issue where pilots were running even though his site was in downtime\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n8\n\n\n-9\n\n\nOpen\n\n\n\n\n\n\n8\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n19\n\n\n-4\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nTim Theisen is handling the \nMay 9th\n release\n\n\nDevelopment freeze today\n\n\nVO Package ??\n\n\n\n\nNeed help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week next week\n\n\n\n\n\n\nReady for Testing  \n\n\n\n\nCVMFS X.509 Helper 1.0\n\n\ngsissh in tarballs\n\n\nUpcoming: GlideinWMS 3.3.2 (Needs factory testing)\n\n\n\n\n\n\nReady for Release  \n\n\nNone yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC operations transition.  Lots of fires\n\n\nCloud provider is adding more storage, and reconfiguring storage.  Causes IO timeouts for VMs\n\n\n\n\n\n\nBlahp merge work continues. Now on to getting binaries in the correct areas.\n\n\nStash CVMFS repo had to be built from scratch due to bug in CVMFS's auto-catalog (only affects stratum 0's)\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC Operations transition\n\n\nMore BLAHP merge\n\n\nStashCP work to work with multiple origins.\n\n\n\n\nOngoing\n\n\n\n\nGratia V2: Derek will be working on this.  Jira project: \nGRACC\n.  Project documentation located at \nhttps://opensciencegrid.github.io/gracc\n.\n\n\nNew StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "April 24, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-technology-area-meeting-24-april-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Edgar, Jeff, Marian, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 24 April 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#announcements", 
            "text": "BrianL on vacation Wednesday through Friday  Suchandra on vacation until 5/5  Condor week starts next Tuesday, 5/2", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#triage-duty", 
            "text": "This week: Mat  Next week: TimT  4 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#jira", 
            "text": "# of tickets   State      171  +5  Open    23  +6  In Progress    3  +3  Ready for Testing    0  0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-software-team", 
            "text": "Dev freeze today, tickets not RFT by owner:       Developer  #      Mat  12    Marian  2    BrianL  1    Carl  1", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#discussions", 
            "text": "osg-build does not yet support building packages for 3.4. Expected support available in May.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#support-update", 
            "text": "UMich (BrianL/Jeff/Marian) - Bob Ball's rebuilt CE is experiencing security handshake timeouts. Also had an issue where pilots were running even though his site was in downtime", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-release-team", 
            "text": "3.3.24   Status      8  -9  Open    8  +2  In Progress    3  +3  Ready for Testing    0  0  Ready for Release    19  -4  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-3324", 
            "text": "Tim Theisen is handling the  May 9th  release  Development freeze today  VO Package ??   Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week next week    Ready for Testing     CVMFS X.509 Helper 1.0  gsissh in tarballs  Upcoming: GlideinWMS 3.3.2 (Needs factory testing)    Ready for Release    None yet", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#last-week", 
            "text": "GRACC operations transition.  Lots of fires  Cloud provider is adding more storage, and reconfiguring storage.  Causes IO timeouts for VMs    Blahp merge work continues. Now on to getting binaries in the correct areas.  Stash CVMFS repo had to be built from scratch due to bug in CVMFS's auto-catalog (only affects stratum 0's)", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#this-week", 
            "text": "More GRACC Operations transition  More BLAHP merge  StashCP work to work with multiple origins.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#ongoing", 
            "text": "Gratia V2: Derek will be working on this.  Jira project:  GRACC .  Project documentation located at  https://opensciencegrid.github.io/gracc .  New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/", 
            "text": "OSG Technology Area Meeting, 17 April 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: Mat\n\n\n5 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n166\n\n\n+14\n\n\nOpen\n\n\n\n\n\n\n17\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n15\n\n\n15\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDev freeze next week but limited testing time due to HTCondor week and Suchandra's vacation\n\n\nXRootD 4.6.1 release candidate testing at UNL\n\n\n\n\nDiscussions\n\n\n\n\nMarian will speak with Mat about proper XRootD release candidate versioning name\n\n\nBrianL will look into system dashboard solution for missing project summary page\n\n\nFor testing gratia-probe/GRACC interaction, there is a GRACC testing interface that doesn't store the records but provides the proper responses to the probes\n\n\nUPITT (Marian) - XRootD assistance (\nhttps://ticket.grid.iu.edu/32605\n)\n\n\n\n\nSupport Update\n\n\n\n\nBNL (BrianL/Derek) - Worked with Xin to investigate missing CE ScheddAd from central collector caused by \nthis\n bug\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n17\n\n\n+17\n\n\nOpen\n\n\n\n\n\n\n6\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n23\n\n\n+23\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nReady for Testing  \n\n\nNone yet\n\n\n\n\n\n\nReady for Release  \n\n\nNone yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nIf XRootD 4.6.1 isn't ready by the package freeze, we will revisit the stability of the release candidates\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC operations transition\n\n\nXRootD bugs in caching and HTTPS connections. Seems to be fixed.\n\n\nBlahp merge work continues.  Now on to getting binaries in the correct areas.\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC Operations transition\n\n\nMore BLAHP merge\n\n\nmove \n*.osgstorage.org\n CVMFS repos to new host\n\n\n\n\nOngoing\n\n\n\n\nGratia V2: Derek will be working on this.  Jira project: \nGRACC\n.  Project documentation located at \nhttps://opensciencegrid.github.io/gracc\n.\n\n\nNew StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "April 17, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-technology-area-meeting-17-april-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:", 
            "title": "OSG Technology Area Meeting, 17 April 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#triage-duty", 
            "text": "This week: Suchandra  Next week: Mat  5 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#jira", 
            "text": "# of tickets   State      166  +14  Open    17  +1  In Progress    0  0  Ready for Testing    15  15  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-software-team", 
            "text": "Dev freeze next week but limited testing time due to HTCondor week and Suchandra's vacation  XRootD 4.6.1 release candidate testing at UNL", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#discussions", 
            "text": "Marian will speak with Mat about proper XRootD release candidate versioning name  BrianL will look into system dashboard solution for missing project summary page  For testing gratia-probe/GRACC interaction, there is a GRACC testing interface that doesn't store the records but provides the proper responses to the probes  UPITT (Marian) - XRootD assistance ( https://ticket.grid.iu.edu/32605 )", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#support-update", 
            "text": "BNL (BrianL/Derek) - Worked with Xin to investigate missing CE ScheddAd from central collector caused by  this  bug", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-release-team", 
            "text": "3.3.24   Status      17  +17  Open    6  +6  In Progress    0  0  Ready for Testing    0  0  Ready for Release    23  +23  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-3324", 
            "text": "Ready for Testing    None yet    Ready for Release    None yet", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#discussions_1", 
            "text": "If XRootD 4.6.1 isn't ready by the package freeze, we will revisit the stability of the release candidates", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#last-week", 
            "text": "GRACC operations transition  XRootD bugs in caching and HTTPS connections. Seems to be fixed.  Blahp merge work continues.  Now on to getting binaries in the correct areas.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#this-week", 
            "text": "More GRACC Operations transition  More BLAHP merge  move  *.osgstorage.org  CVMFS repos to new host", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#ongoing", 
            "text": "Gratia V2: Derek will be working on this.  Jira project:  GRACC .  Project documentation located at  https://opensciencegrid.github.io/gracc .  New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/", 
            "text": "OSG Technology Area Meeting, 27 February 2017\n\n\nCoordinates:\n Conference: 857-216-4999, PIN: 32390; \nhttps://www.uberconference.com/osgcat\n  \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\n\n\nNo meeting next week, OSG All Hands\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n6 (0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n141\n\n\n(\n17)\n\n\nOpen\n\n\n\n\n\n\n33\n\n\n(+9)\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n(+2)\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n(\n12)\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.22\n\n\n2017-02-27\n\n\n2017-03-06\n\n\n2017-03-14\n\n\n\n\n\n\n\n\n3.3.23\n\n\n2017-03-27\n\n\n2017-04-03\n\n\n2017-04-11\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nDiscussions\n\n\nSoftware freeze today  \n\n\n\n\n\n\n\n\nAssignee\n\n\nTickets Not RFT\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n7\n\n\n\n\n\n\nMat\n\n\n7\n\n\n\n\n\n\nCarl\n\n\n4\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nDerek\n\n\n2\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\n\n\nSupport Update\n\n\n\n\nBaylor (BrianL) - CE installation help, troubleshooting mapping and blahp issues\n\n\nClemson (BrianL) - blahp segfault, held jobs due to bad folder ownership\n\n\nGeorgia Tech (BrianL) - accounting records reporting start date as 1/1/1970\n\n\n\n\nOSG Release Team\n\n\n\n\nMarch 11th Release - OSG 3.3.22  \n\n\nDevelopment Freeze 2/27\n\n\n\n\n\n\nData Release - IGTF 1.80\n\n\n\n\n\n\n\n\n\n\n3.3.22\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n15\n\n\n(+15)\n\n\nOpen\n\n\n\n\n\n\n11\n\n\n(+11)\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n(\n2)\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n(\n3)\n\n\nReady for Release\n\n\n\n\n\n\n28\n\n\n(+16)\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.22\n\n\n\n\nTesting  \n\n\nXRootD 4.6.0\n\n\nfrontier-squid 3.5.24-1.1 in Upcoming\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC Transfer Summaries\n\n\nSyracuse StashCache running now.\n\n\nPackaged Auth StashCache is coming from Marian.\n\n\n\n\nThis Week\n\n\n\n\nFinish GRACC Tickets\n\n\nBlahp Merge\n\n\nBegin PEARC paper\n\n\n\n\nOngoing\n\n\n\n\nGratia V2: Derek will be working on this.  Jira project: \nGRACC\n.  Project documentation located at \nhttps://opensciencegrid.github.io/gracc\n.\n\n\nNew StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.\n\n\nUNL setting up authenticated StashCache as well\n\n\nSee Support Update section - StashCache troubleshooting at BNL, host migration to el7 caused some odd xrootd behavior, investigating", 
            "title": "February 27, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-technology-area-meeting-27-february-2017", 
            "text": "Coordinates:  Conference: 857-216-4999, PIN: 32390;  https://www.uberconference.com/osgcat     Attending:", 
            "title": "OSG Technology Area Meeting, 27 February 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#announcements", 
            "text": "No meeting next week, OSG All Hands", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  6 (0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#jira", 
            "text": "# of tickets   State      141  ( 17)  Open    33  (+9)  In Progress    4  (+2)  Ready for Testing    0  ( 12)  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.22  2017-02-27  2017-03-06  2017-03-14     3.3.23  2017-03-27  2017-04-03  2017-04-11     3.3.24  2017-04-25  2017-05-01  2017-05-09      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#discussions", 
            "text": "Software freeze today       Assignee  Tickets Not RFT      BrianL  7    Mat  7    Carl  4    Edgar  2    Derek  2    Marian  1", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#support-update", 
            "text": "Baylor (BrianL) - CE installation help, troubleshooting mapping and blahp issues  Clemson (BrianL) - blahp segfault, held jobs due to bad folder ownership  Georgia Tech (BrianL) - accounting records reporting start date as 1/1/1970", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-release-team", 
            "text": "March 11th Release - OSG 3.3.22    Development Freeze 2/27    Data Release - IGTF 1.80      3.3.22   Status      15  (+15)  Open    11  (+11)  In Progress    2  ( 2)  Ready for Testing    0  ( 3)  Ready for Release    28  (+16)  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-3322", 
            "text": "Testing    XRootD 4.6.0  frontier-squid 3.5.24-1.1 in Upcoming", 
            "title": "OSG 3.3.22"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#last-week", 
            "text": "GRACC Transfer Summaries  Syracuse StashCache running now.  Packaged Auth StashCache is coming from Marian.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#this-week", 
            "text": "Finish GRACC Tickets  Blahp Merge  Begin PEARC paper", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#ongoing", 
            "text": "Gratia V2: Derek will be working on this.  Jira project:  GRACC .  Project documentation located at  https://opensciencegrid.github.io/gracc .  New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.  UNL setting up authenticated StashCache as well  See Support Update section - StashCache troubleshooting at BNL, host migration to el7 caused some odd xrootd behavior, investigating", 
            "title": "Ongoing"
        }
    ]
}