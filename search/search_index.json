{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG Technology Area Welcome to the home page of the OSG Technology Team documentation area! If you are looking for site administrator documentation, please visit the OSG Documentation page . The Team Software and Release Technology Brian Lin (software manager) Brian Bockelman (manager) (50%) Carl Edquist Derek Weitzel (50%) Edgar Fajardo (50%) Edgar Fajardo (50%) Mat Selmeci Jeff Dost (50%) Suchandra Thapa (50%) Marian Zvada (25%) Tim Cartwright (5%) Tim Theisen (release manager) (50%) Contact Us osg-software@opensciencegrid.org - General discussion amongst team members ( subscribe ) Slack channel - if you can't create an account, send an e-mail to osg-software@opensciencegrid.org osg-commits@cs.wisc.edu - Broadcast of all source code repo commits Meeting Notes When: Every Monday, 11:00 a.m. (U.S. Central) Where: +1 719-284-5267, PIN #57363; Uber Conference Recent Notes 15 October 2018 8 October 2018 1 October 2018 24 September 2018 17 September 2018 10 September 2018 3 September 2018 (Canceled - Labor Day) 27 August 2018 20 August 2018 13 August 2018 6 August 2018 30 July 2018 23 July 2018 16 July 2018 9 July 2018 (Canceled - OSG User School) 2 July 2018 25 June 2018 18 June 2018 11 June 2018 4 June 2018 29 May 2018 - Memorial Day 21 May 2018 (Canceled - HTCondor Week) 14 May 2018 7 May 2018 30 April 2018 23 April 2018 16 April 2018 9 April 2018 2 April 2018 26 March 2018 (Canceled) 19 March 2018 (Canceled - OSG All Hands) 12 March 2018 5 March 2018 26 February 2018 19 February 2018 12 February 2018 5 February 2018 29 January 2018 22 January 2018 15 January 2018 (Canceled - MLK day) 8 January 2018 2 January 2018 Archived notes Meeting note archives older than January 1, 2018 and oler than February 27, 2017 can be found here Meeting note archives older than February 27, 2017 can be found here .","title":"Home"},{"location":"#osg-technology-area","text":"Welcome to the home page of the OSG Technology Team documentation area! If you are looking for site administrator documentation, please visit the OSG Documentation page .","title":"OSG Technology Area"},{"location":"#the-team","text":"Software and Release Technology Brian Lin (software manager) Brian Bockelman (manager) (50%) Carl Edquist Derek Weitzel (50%) Edgar Fajardo (50%) Edgar Fajardo (50%) Mat Selmeci Jeff Dost (50%) Suchandra Thapa (50%) Marian Zvada (25%) Tim Cartwright (5%) Tim Theisen (release manager) (50%)","title":"The Team"},{"location":"#contact-us","text":"osg-software@opensciencegrid.org - General discussion amongst team members ( subscribe ) Slack channel - if you can't create an account, send an e-mail to osg-software@opensciencegrid.org osg-commits@cs.wisc.edu - Broadcast of all source code repo commits","title":"Contact Us"},{"location":"#meeting-notes","text":"When: Every Monday, 11:00 a.m. (U.S. Central) Where: +1 719-284-5267, PIN #57363; Uber Conference","title":"Meeting Notes"},{"location":"#recent-notes","text":"15 October 2018 8 October 2018 1 October 2018 24 September 2018 17 September 2018 10 September 2018 3 September 2018 (Canceled - Labor Day) 27 August 2018 20 August 2018 13 August 2018 6 August 2018 30 July 2018 23 July 2018 16 July 2018 9 July 2018 (Canceled - OSG User School) 2 July 2018 25 June 2018 18 June 2018 11 June 2018 4 June 2018 29 May 2018 - Memorial Day 21 May 2018 (Canceled - HTCondor Week) 14 May 2018 7 May 2018 30 April 2018 23 April 2018 16 April 2018 9 April 2018 2 April 2018 26 March 2018 (Canceled) 19 March 2018 (Canceled - OSG All Hands) 12 March 2018 5 March 2018 26 February 2018 19 February 2018 12 February 2018 5 February 2018 29 January 2018 22 January 2018 15 January 2018 (Canceled - MLK day) 8 January 2018 2 January 2018","title":"Recent Notes"},{"location":"#archived-notes","text":"Meeting note archives older than January 1, 2018 and oler than February 27, 2017 can be found here Meeting note archives older than February 27, 2017 can be found here .","title":"Archived notes"},{"location":"documentation/markdown-migration/","text":"Migrating to Markdown As part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to Markdown . The following document outlines the conversion process and conventions. Choosing the git repository First you will need to choose which git repoository you will be working with: If you are converting a document from... Use this github repository... SoftwareTeam technology Release3 docs Once you've chosen the target repository for your document, move onto the next section and pick your conversion method. Automatic TWiki conversion Note If you are only archiving the documents, skip to this section . Choose one of the following methods for converting TWiki documents: Using our own docker conversion image (recommended) Directly using pandoc and mkdocs on your own machine Using docker The twiki-converter docker image can be used to preview the document tree via a mkdocs development server, archive TWiki documents, and convert documents to Markdown via pandoc . The image is available on osghost , otherwise, it is availble on dockerhub . user@host $ docker pull opensciencegrid/docker-twiki-converter Requirements To perform a document migration using docker, you will need the following tools and accounts: Fork and clone the repository that you chose in the above section A host with a running docker service sudo or membership in the docker group If you cannot install the above tools locally, they are available on osghost . Speak with Brian L for access. Preparing the git repository cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git Create a branch for the document you plan to convert: user@host $ git branch BRANCH NAME master Change to the branch you just created user@host $ git checkout BRANCH NAME Previewing the document tree When starting a twiki-converter docker container, it expects your local github repository to be mounted in /source so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image: Create a container from the image with the following command: user@host $ docker run -d -v PATH TO LOCAL GITHUB REPO :/source -p 8000 opensciencegrid/docker-twiki-converter The above command should return the container ID, which will be used in subsequent commands. Note If the docker container exits immediately, remove the -d option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode. To find the port that your development server is listening on, use the container ID (you should only need the first few chars of the ID) returned from the previous command: user@host $ docker port CONTAINER ID Access the development server in your browser via http://osghost.chtc.wisc.edu: PORT or localhost: PORT for containers run on osghost or locally, respectively. osghost has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the osghost port to a local port. Converting documents The docker image contains a convenience script, convert-twiki for saving archives and converting them to Markdown. To run the script in a running container, run the following command: user@host $ docker exec CONTAINER ID convert-twiki TWIKI URL Where is the docker container ID and is the link to the TWiki document that you want to convert, e.g. https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc, archive/SoftwareDevelopmentProcess , in your local repo and a converted copy, SoftwareDevelopmentProcess.md , placed into the root of your local github repository. If the twiki url is for a specific revision of the document, a .rNN will be included in the output filenames. Warning If the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See Troubleshooting conversion for next steps. To see the converted document in your browser: Rename, move the converted document into a folder in docs/ . Document file names should be lowercase, - delimited, and descriptive but concise, e.g. markdown-migration.md or cutting-release.md It's not important to get the name/location correct on the first try as this can be discussed in the pull request sudo chown the archived and converted documents to be owned by you Add the document to the pages: section of mkdocs.yml in title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' Refresh the document tree in your browser Once you can view the converted document in your browser, move onto the next section Troubleshooting conversion Pandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion. Copy the archive of the document into the root of your git repository Kill the process in the docker container: user@host $ docker exec CONTAINER ID pkill -9 pandoc Remove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy) Run pandoc manually: user@host $ docker exec CONTAINER ID pandoc -f twiki -t markdown_github ARCHIVE COPY MARKDOWN FILE Repeat steps 2-4 until you've narrowed down the problematic section Manually convert the offending section Conversion without Docker If you've already used the docker method , skip to the section about completing the conversion . Requirements This method requires the following: Fork and clone the repository that you chose in the above section pandoc ( 1.16) mkdocs MarkdownHighlight pygments Preparing the git repository cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git Create a branch for the document you plan to convert: user@host $ git branch BRANCH NAME master Change to the branch you just created user@host $ git checkout BRANCH NAME Archiving the TWiki document Follow the instructions for archival then continue to the next section to convert the document with pandoc. Initial conversion with Pandoc Pandoc is a tool that's useful for automated conversion of markdown languages. Once installed (alternatively, run pandoc via docker ), run the following command to convert TWiki to Markdown: $ pandoc -f twiki -t markdown_github TWIKI FILE MARKDOWN FILE Where TWIKI FILE is the path to initial document in raw TWiki and MARKDOWN FILE is the path to the resulting document in GitHub Markdown. Note If you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section. Previewing your document(s) with Mkdocs Mkdocs has a development mode that can be used to preview documents as you work on them and is available via package manager or pip . Once installed , add your document(s) to the pages section of mkdocs.yml and launch the mkdocs server with the following command from the dir containing mkdocs.yml : $ PYTHONPATH = src/ mkdocs serve Access the server at http://127.0.0.1:8000 and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two. Completing the conversion Manual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents. Visit the style guide to ensure that the document meets all style guidelines. Archiving Documents If the document is slated for archival (check if it says \"yes\" in the \"archived\" column of the spreadsheet), just download the document to the archive folder of your local git repository: user@host $ cd technology/ user@host $ curl TWIKI URL ?raw=text | iconv -f windows-1252 archive/ TWIKI TITLE For example: user@host $ cd technology user@host $ curl https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text | iconv -f windows-1252 archive/SHA2Compliance After downloading the document, continue onto the next section to walk through pull request submission. Submitting the pull request Stage the archived raw TWiki (as well as the converted Markdown document(s) and mkdocs.yml if you are converting the document): user@host $ git add mkdocs.yml archive/ TWIKI ARCHIVE PATH TO CONVERTED DOC Commit and push your changes to your GitHub repo: user@host $ git commit -m COMMIT MSG user@host $ git push origin BRANCH NAME Open your browser and navigate to your GitHub fork Submit a pull request containing with the following body: LINK TO TWIKI DOCUMENT - [ ] Enter date into Migrated column of google sheet If you are migrating a document, also add this task: - [ ] Add migration header to TWiki document If you are archiving a document, add this task: - [ ] Move TWiki document to the trash See an example pull request here . After the pull request After the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document: div style= border: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600; This document has been migrated to !GitHub ( LINK TO GITHUB DOCUMENT ). If you wish to see the old TWiki document, use the TWiki history below. Background: At the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below: * Release3: https://www.opensciencegrid.org/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]]) * !SoftwareTeam: https://www.opensciencegrid.org/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]]) /div If you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document: - [X] Enter date into Migrated column of google sheet - [X] Add migration div to TWiki document Currently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL. This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub. This advice may change in the future as we gain experience with this transition. Reviewing pull requests To review pull requests, cd into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's cut-sw-release branch of the technology repository: # Add the requester s repo as a remote if you haven t already user@host $ git remote add blin https://www.github.com/brianhlin/technology.git user@host $ git fetch --all user@host $ git checkout blin/cut-sw-release Refresh your browser and navigate to the document in the request.","title":"Migrating Documents to Markdown"},{"location":"documentation/markdown-migration/#migrating-to-markdown","text":"As part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to Markdown . The following document outlines the conversion process and conventions.","title":"Migrating to Markdown"},{"location":"documentation/markdown-migration/#choosing-the-git-repository","text":"First you will need to choose which git repoository you will be working with: If you are converting a document from... Use this github repository... SoftwareTeam technology Release3 docs Once you've chosen the target repository for your document, move onto the next section and pick your conversion method.","title":"Choosing the git repository"},{"location":"documentation/markdown-migration/#automatic-twiki-conversion","text":"Note If you are only archiving the documents, skip to this section . Choose one of the following methods for converting TWiki documents: Using our own docker conversion image (recommended) Directly using pandoc and mkdocs on your own machine","title":"Automatic TWiki conversion"},{"location":"documentation/markdown-migration/#using-docker","text":"The twiki-converter docker image can be used to preview the document tree via a mkdocs development server, archive TWiki documents, and convert documents to Markdown via pandoc . The image is available on osghost , otherwise, it is availble on dockerhub . user@host $ docker pull opensciencegrid/docker-twiki-converter","title":"Using docker"},{"location":"documentation/markdown-migration/#requirements","text":"To perform a document migration using docker, you will need the following tools and accounts: Fork and clone the repository that you chose in the above section A host with a running docker service sudo or membership in the docker group If you cannot install the above tools locally, they are available on osghost . Speak with Brian L for access.","title":"Requirements"},{"location":"documentation/markdown-migration/#preparing-the-git-repository","text":"cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git Create a branch for the document you plan to convert: user@host $ git branch BRANCH NAME master Change to the branch you just created user@host $ git checkout BRANCH NAME","title":"Preparing the git repository"},{"location":"documentation/markdown-migration/#previewing-the-document-tree","text":"When starting a twiki-converter docker container, it expects your local github repository to be mounted in /source so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image: Create a container from the image with the following command: user@host $ docker run -d -v PATH TO LOCAL GITHUB REPO :/source -p 8000 opensciencegrid/docker-twiki-converter The above command should return the container ID, which will be used in subsequent commands. Note If the docker container exits immediately, remove the -d option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode. To find the port that your development server is listening on, use the container ID (you should only need the first few chars of the ID) returned from the previous command: user@host $ docker port CONTAINER ID Access the development server in your browser via http://osghost.chtc.wisc.edu: PORT or localhost: PORT for containers run on osghost or locally, respectively. osghost has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the osghost port to a local port.","title":"Previewing the document tree"},{"location":"documentation/markdown-migration/#converting-documents","text":"The docker image contains a convenience script, convert-twiki for saving archives and converting them to Markdown. To run the script in a running container, run the following command: user@host $ docker exec CONTAINER ID convert-twiki TWIKI URL Where is the docker container ID and is the link to the TWiki document that you want to convert, e.g. https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc, archive/SoftwareDevelopmentProcess , in your local repo and a converted copy, SoftwareDevelopmentProcess.md , placed into the root of your local github repository. If the twiki url is for a specific revision of the document, a .rNN will be included in the output filenames. Warning If the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See Troubleshooting conversion for next steps. To see the converted document in your browser: Rename, move the converted document into a folder in docs/ . Document file names should be lowercase, - delimited, and descriptive but concise, e.g. markdown-migration.md or cutting-release.md It's not important to get the name/location correct on the first try as this can be discussed in the pull request sudo chown the archived and converted documents to be owned by you Add the document to the pages: section of mkdocs.yml in title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' Refresh the document tree in your browser Once you can view the converted document in your browser, move onto the next section","title":"Converting documents"},{"location":"documentation/markdown-migration/#troubleshooting-conversion","text":"Pandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion. Copy the archive of the document into the root of your git repository Kill the process in the docker container: user@host $ docker exec CONTAINER ID pkill -9 pandoc Remove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy) Run pandoc manually: user@host $ docker exec CONTAINER ID pandoc -f twiki -t markdown_github ARCHIVE COPY MARKDOWN FILE Repeat steps 2-4 until you've narrowed down the problematic section Manually convert the offending section","title":"Troubleshooting conversion"},{"location":"documentation/markdown-migration/#conversion-without-docker","text":"If you've already used the docker method , skip to the section about completing the conversion .","title":"Conversion without Docker"},{"location":"documentation/markdown-migration/#requirements_1","text":"This method requires the following: Fork and clone the repository that you chose in the above section pandoc ( 1.16) mkdocs MarkdownHighlight pygments","title":"Requirements"},{"location":"documentation/markdown-migration/#preparing-the-git-repository_1","text":"cd into your local git repository Add opensciencegrid/technology as the upstream remote repository for merging upstream changes: user@host $ git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git Create a branch for the document you plan to convert: user@host $ git branch BRANCH NAME master Change to the branch you just created user@host $ git checkout BRANCH NAME","title":"Preparing the git repository"},{"location":"documentation/markdown-migration/#archiving-the-twiki-document","text":"Follow the instructions for archival then continue to the next section to convert the document with pandoc.","title":"Archiving the TWiki document"},{"location":"documentation/markdown-migration/#initial-conversion-with-pandoc","text":"Pandoc is a tool that's useful for automated conversion of markdown languages. Once installed (alternatively, run pandoc via docker ), run the following command to convert TWiki to Markdown: $ pandoc -f twiki -t markdown_github TWIKI FILE MARKDOWN FILE Where TWIKI FILE is the path to initial document in raw TWiki and MARKDOWN FILE is the path to the resulting document in GitHub Markdown. Note If you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section.","title":"Initial conversion with Pandoc"},{"location":"documentation/markdown-migration/#previewing-your-documents-with-mkdocs","text":"Mkdocs has a development mode that can be used to preview documents as you work on them and is available via package manager or pip . Once installed , add your document(s) to the pages section of mkdocs.yml and launch the mkdocs server with the following command from the dir containing mkdocs.yml : $ PYTHONPATH = src/ mkdocs serve Access the server at http://127.0.0.1:8000 and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two.","title":"Previewing your document(s) with Mkdocs"},{"location":"documentation/markdown-migration/#completing-the-conversion","text":"Manual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents. Visit the style guide to ensure that the document meets all style guidelines.","title":"Completing the conversion"},{"location":"documentation/markdown-migration/#archiving-documents","text":"If the document is slated for archival (check if it says \"yes\" in the \"archived\" column of the spreadsheet), just download the document to the archive folder of your local git repository: user@host $ cd technology/ user@host $ curl TWIKI URL ?raw=text | iconv -f windows-1252 archive/ TWIKI TITLE For example: user@host $ cd technology user@host $ curl https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text | iconv -f windows-1252 archive/SHA2Compliance After downloading the document, continue onto the next section to walk through pull request submission.","title":"Archiving Documents"},{"location":"documentation/markdown-migration/#submitting-the-pull-request","text":"Stage the archived raw TWiki (as well as the converted Markdown document(s) and mkdocs.yml if you are converting the document): user@host $ git add mkdocs.yml archive/ TWIKI ARCHIVE PATH TO CONVERTED DOC Commit and push your changes to your GitHub repo: user@host $ git commit -m COMMIT MSG user@host $ git push origin BRANCH NAME Open your browser and navigate to your GitHub fork Submit a pull request containing with the following body: LINK TO TWIKI DOCUMENT - [ ] Enter date into Migrated column of google sheet If you are migrating a document, also add this task: - [ ] Add migration header to TWiki document If you are archiving a document, add this task: - [ ] Move TWiki document to the trash See an example pull request here .","title":"Submitting the pull request"},{"location":"documentation/markdown-migration/#after-the-pull-request","text":"After the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document: div style= border: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600; This document has been migrated to !GitHub ( LINK TO GITHUB DOCUMENT ). If you wish to see the old TWiki document, use the TWiki history below. Background: At the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below: * Release3: https://www.opensciencegrid.org/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]]) * !SoftwareTeam: https://www.opensciencegrid.org/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]]) /div If you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document: - [X] Enter date into Migrated column of google sheet - [X] Add migration div to TWiki document Currently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL. This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub. This advice may change in the future as we gain experience with this transition.","title":"After the pull request"},{"location":"documentation/markdown-migration/#reviewing-pull-requests","text":"To review pull requests, cd into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's cut-sw-release branch of the technology repository: # Add the requester s repo as a remote if you haven t already user@host $ git remote add blin https://www.github.com/brianhlin/technology.git user@host $ git fetch --all user@host $ git checkout blin/cut-sw-release Refresh your browser and navigate to the document in the request.","title":"Reviewing pull requests"},{"location":"documentation/new-doc-area/","text":"Creating a New Area This document contains instructions for creating a new OSG documentation area via GitHub Pages and deploying it automatically with Travis-CI . Before starting, make sure that you have the git and gem tools installed. Create a new repository in the opensciencegrid organization (referred to as REPO NAME in the rest of this document) Check the box marked Initialize this repository with a README Once created, add the mkdocs topic by clicking on the \"Add topics\" button Clone the repository and cd into the directory: git clone git@github.com:opensciencegrid/ REPO NAME cd REPO NAME Create a gh-pages branch in the GitHub repository: git push origin master:gh-pages Update the contents of README.md and populate the LICENSE file with a Creative Commons Attribution 4.0 license : wget https://creativecommons.org/licenses/by/4.0/legalcode.txt LICENSE Follow these instructions to add the doc-ci-scripts sub-module Create mkdocs.yml and a docs directory Create and encrypt the repository deploy key Generate the repository deploy key: ssh-keygen -t rsa -b 4096 -C help@opensciencegrid.org -f deploy-key Install the travis gem: gem install travis Encrypt the deploy key: travis encrypt-file deploy-key Add and commit your files: git add LICENSE mkdocs.yml docs deploy-key.enc git commit -m Prepare the repository for Travis-CI deployment Danger Do NOT commit the unencrypted deploy-key ! Add deploy-key.pub to your repository's list of deploy keys . Make sure to check Allow write access . Push local changes to the GitHub repository: git push origin master Your documents should be shortly available at https://www.opensciencegrid.org/ REPO NAME Creating an ITB Area This section describes creating an ITB repository for a documentation area created in the previous section Create a new repository in the opensciencegrid organization and name it REPO NAME -itb . For example, an ITB area for the docs repository has a repository name of docs-itb . The ITB repository will be referred to as ITB REPO NAME in the rest of this document. Check the box marked Initialize this repository with a README Once created, add the mkdocs topic by clicking on the \"Add topics\" button Clone the repository and cd into the directory: git clone git@github.com:opensciencegrid/ ITB REPO NAME cd ITB REPO NAME Create a gh-pages branch in the GitHub repository: git push origin master:gh-pages Update the contents of README.md In the non-ITB repository, create and encrypt the ITB repository deploy key cd into the non-ITB repository and generate the ITB deploy key cd REPO NAME ssh-keygen -t rsa -b 4096 -C help@opensciencegrid.org -f deploy-itb Install the travis gem: gem install travis Encrypt the deploy key: travis encrypt-file deploy-itb Update .travis.env with the appropriate ITB values Add and commit your files: git add .travis.env deploy-itb.enc git commit -m Add ITB deployment Danger Do NOT commit the unencrypted deploy-itb ! Add deploy-itb.pub to the ITB repository's list of deploy keys . Make sure to check Allow write access . Still in the non-ITB repository, push your local changes to the GitHub repository git push origin master Your documents should be shortly available at https://www.opensciencegrid.org/ REPO NAME","title":"Creating a New Area"},{"location":"documentation/new-doc-area/#creating-a-new-area","text":"This document contains instructions for creating a new OSG documentation area via GitHub Pages and deploying it automatically with Travis-CI . Before starting, make sure that you have the git and gem tools installed. Create a new repository in the opensciencegrid organization (referred to as REPO NAME in the rest of this document) Check the box marked Initialize this repository with a README Once created, add the mkdocs topic by clicking on the \"Add topics\" button Clone the repository and cd into the directory: git clone git@github.com:opensciencegrid/ REPO NAME cd REPO NAME Create a gh-pages branch in the GitHub repository: git push origin master:gh-pages Update the contents of README.md and populate the LICENSE file with a Creative Commons Attribution 4.0 license : wget https://creativecommons.org/licenses/by/4.0/legalcode.txt LICENSE Follow these instructions to add the doc-ci-scripts sub-module Create mkdocs.yml and a docs directory Create and encrypt the repository deploy key Generate the repository deploy key: ssh-keygen -t rsa -b 4096 -C help@opensciencegrid.org -f deploy-key Install the travis gem: gem install travis Encrypt the deploy key: travis encrypt-file deploy-key Add and commit your files: git add LICENSE mkdocs.yml docs deploy-key.enc git commit -m Prepare the repository for Travis-CI deployment Danger Do NOT commit the unencrypted deploy-key ! Add deploy-key.pub to your repository's list of deploy keys . Make sure to check Allow write access . Push local changes to the GitHub repository: git push origin master Your documents should be shortly available at https://www.opensciencegrid.org/ REPO NAME","title":"Creating a New Area"},{"location":"documentation/new-doc-area/#creating-an-itb-area","text":"This section describes creating an ITB repository for a documentation area created in the previous section Create a new repository in the opensciencegrid organization and name it REPO NAME -itb . For example, an ITB area for the docs repository has a repository name of docs-itb . The ITB repository will be referred to as ITB REPO NAME in the rest of this document. Check the box marked Initialize this repository with a README Once created, add the mkdocs topic by clicking on the \"Add topics\" button Clone the repository and cd into the directory: git clone git@github.com:opensciencegrid/ ITB REPO NAME cd ITB REPO NAME Create a gh-pages branch in the GitHub repository: git push origin master:gh-pages Update the contents of README.md In the non-ITB repository, create and encrypt the ITB repository deploy key cd into the non-ITB repository and generate the ITB deploy key cd REPO NAME ssh-keygen -t rsa -b 4096 -C help@opensciencegrid.org -f deploy-itb Install the travis gem: gem install travis Encrypt the deploy key: travis encrypt-file deploy-itb Update .travis.env with the appropriate ITB values Add and commit your files: git add .travis.env deploy-itb.enc git commit -m Add ITB deployment Danger Do NOT commit the unencrypted deploy-itb ! Add deploy-itb.pub to the ITB repository's list of deploy keys . Make sure to check Allow write access . Still in the non-ITB repository, push your local changes to the GitHub repository git push origin master Your documents should be shortly available at https://www.opensciencegrid.org/ REPO NAME","title":"Creating an ITB Area"},{"location":"documentation/reviewing-documentation/","text":"Review OSG Software Documentation To maintain quality documentation, we must regularly review our documentation for clarity and correctness. We differentiate between two types of reviews, content and editorial, which you can think of as for correctness and clarity, respectively. If you are reviewing a document as part of the release process, perform a content review. After completing a review, update the appropriate columns in the document tracking spreadsheet . Speak to Brian Lin for write access. Content Review Content reviews are for validating the correctness of technical steps and details. To perform a content review, follow the instructions of the document to a tee as if you were an OSG neophyte. Things to note and/or fix: After completing the instructions in the document: Does the document inform you how to use the product? Does the document tell you how to verify that the product is functioning? Does the product work? Incorrect or out of date instructions Steps that may be particularly conducive to software or default configuration as a solution Lack of clarity or any other confusion you may have Editorial Review Editorial reviews are for ensuring docs meet our style and layout guidelines; improving readability; and proofing spelling, grammar, and punctuation.","title":"Reviewing Documentation"},{"location":"documentation/reviewing-documentation/#review-osg-software-documentation","text":"To maintain quality documentation, we must regularly review our documentation for clarity and correctness. We differentiate between two types of reviews, content and editorial, which you can think of as for correctness and clarity, respectively. If you are reviewing a document as part of the release process, perform a content review. After completing a review, update the appropriate columns in the document tracking spreadsheet . Speak to Brian Lin for write access.","title":"Review OSG Software Documentation"},{"location":"documentation/reviewing-documentation/#content-review","text":"Content reviews are for validating the correctness of technical steps and details. To perform a content review, follow the instructions of the document to a tee as if you were an OSG neophyte. Things to note and/or fix: After completing the instructions in the document: Does the document inform you how to use the product? Does the document tell you how to verify that the product is functioning? Does the product work? Incorrect or out of date instructions Steps that may be particularly conducive to software or default configuration as a solution Lack of clarity or any other confusion you may have","title":"Content Review"},{"location":"documentation/reviewing-documentation/#editorial-review","text":"Editorial reviews are for ensuring docs meet our style and layout guidelines; improving readability; and proofing spelling, grammar, and punctuation.","title":"Editorial Review"},{"location":"documentation/style-guide/","text":"Markdown Style Guide This document contains markdown conventions that are used in OSG Software documentation. Meta Wherever possible, prose should be limited to 120 characters wide. In addition, using one line for each sentence is recommended since it makes update diffs easier to review. Headings Use the following conventions for headings: The title should be the only level 1 heading Level 1 headings should use the ==== format Level 2 headings should use the ---- format Use Title Case for level 1 and level 2 headings. Only capitalize the first word for all other headings. Other heading levels should use the appropriate number of # Go no deeper than of level 5 headings Spin-off a new document or re-organize the existing document if you find that you regularly need level 5 headings. Links Use site-relative ( /software/development-process ) instead of document-relative ( ../software/development-process.md ) links. This will allow us to easily search for links and move documents around in the future. Section links To link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple sections with the same name you can link the subsequent sections by appending _N where N is the section's ordinal number minus one, e.g. append _1 for the second section. For example, if you have three sections named \"Optional Configuration\", link them like so: [1st section](#optional-configuration) [2nd section](#optional-configuration_1) [3rd section](#optional-configuration_2) Command blocks and file snippets Command blocks and file snippets outside of lists should be wrapped in three back-ticks (```) followed by an optional code highlighting format: ```console # stuff ``` Command blocks and file snippets inside of a list should use the appropriate number of spaces before three colons followed by an optional code highlighting format: # stuff See the lists section for details on properly formatting command blocks within a list. We use the Pygments highlighting library for syntax; it knows about 100 different languages. The Pygments website contains a live renderer if you want to see how your text will come out. Please use the console language for shell sessions. Root and user prompts When specifying instructions for the command-line, indicate to users whether the commands can be run as root ( root@host # ) or as an unprivileged user ( user@host $ ). For example: root@host # useradd -m osguser root@host # su - osguser user@host $ whoami osguser It can provide helpful context to use a more specific hostname in the prompt than host . For example, if you're writing a doc for setting up a Storage Element and a command is run as root on the SE, use root@se # . Or if you're testing the SE from the client side and the command is run as a normal user on a client, use user@client $ . Highlighting user input Use descriptive, all-caps text wrapped in angle brackets to to highlight areas that users would have to insert text specific to their site. You may also use TWiki-style color highlighting. For example: root@host # condor_ce_trace -d CE HOSTNAME Lists When constructing lists, use the following guidelines: Use 1. for each item in a numbered list To make sure the contents of code blocks, file snippets, and subsequent paragraphs are indented properly, use the following formatting: For code blocks or file snippets, add an empty line after any regular text, then insert (N+1)*4 spaces at the beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code block with ::: FORMAT ; see this page for details, including possible highlighting formats. For an example of formatting a code section inside a list, see the release series document . For additional text (i.e. after a code block), insert N*4 spaces at the beginning of each line, where N is the level of the item in the list. For example: 1. Foo - Bar :::console COMMAND BLOCK text associated with Bar text associated with Foo 1. Baz FILE SNIPPET There are 12 spaces and 8 spaces in front of the command block and text associated with Bar , respectively; 4 spaces in front of the text associated with Foo ; and 8 spaces in front of the file snippet associated with Baz . The above block is rendered below: Foo Bar COMMAND BLOCK text associated with Bar text associated with Foo Baz FILE SNIPPET Notes To catch the user's attention for important items or pitfalls, we used %NOTE% TWiki macros, these can be replaced with admonition-style notes and warnings: !!! note things to note or !!! warning if a user doesn t do this thing, bad stuff will happen The above blocks are rendered below as an example. !!! note things to note and !!! warning if a user doesn't do this thing, bad stuff will happen For a full list of admonition styles, see the documentation here .","title":"Markdown Style Guide"},{"location":"documentation/style-guide/#markdown-style-guide","text":"This document contains markdown conventions that are used in OSG Software documentation.","title":"Markdown Style Guide"},{"location":"documentation/style-guide/#meta","text":"Wherever possible, prose should be limited to 120 characters wide. In addition, using one line for each sentence is recommended since it makes update diffs easier to review.","title":"Meta"},{"location":"documentation/style-guide/#headings","text":"Use the following conventions for headings: The title should be the only level 1 heading Level 1 headings should use the ==== format Level 2 headings should use the ---- format Use Title Case for level 1 and level 2 headings. Only capitalize the first word for all other headings. Other heading levels should use the appropriate number of # Go no deeper than of level 5 headings Spin-off a new document or re-organize the existing document if you find that you regularly need level 5 headings.","title":"Headings"},{"location":"documentation/style-guide/#links","text":"Use site-relative ( /software/development-process ) instead of document-relative ( ../software/development-process.md ) links. This will allow us to easily search for links and move documents around in the future.","title":"Links"},{"location":"documentation/style-guide/#section-links","text":"To link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple sections with the same name you can link the subsequent sections by appending _N where N is the section's ordinal number minus one, e.g. append _1 for the second section. For example, if you have three sections named \"Optional Configuration\", link them like so: [1st section](#optional-configuration) [2nd section](#optional-configuration_1) [3rd section](#optional-configuration_2)","title":"Section links"},{"location":"documentation/style-guide/#command-blocks-and-file-snippets","text":"Command blocks and file snippets outside of lists should be wrapped in three back-ticks (```) followed by an optional code highlighting format: ```console # stuff ``` Command blocks and file snippets inside of a list should use the appropriate number of spaces before three colons followed by an optional code highlighting format: # stuff See the lists section for details on properly formatting command blocks within a list. We use the Pygments highlighting library for syntax; it knows about 100 different languages. The Pygments website contains a live renderer if you want to see how your text will come out. Please use the console language for shell sessions.","title":"Command blocks and file snippets"},{"location":"documentation/style-guide/#root-and-user-prompts","text":"When specifying instructions for the command-line, indicate to users whether the commands can be run as root ( root@host # ) or as an unprivileged user ( user@host $ ). For example: root@host # useradd -m osguser root@host # su - osguser user@host $ whoami osguser It can provide helpful context to use a more specific hostname in the prompt than host . For example, if you're writing a doc for setting up a Storage Element and a command is run as root on the SE, use root@se # . Or if you're testing the SE from the client side and the command is run as a normal user on a client, use user@client $ .","title":"Root and user prompts"},{"location":"documentation/style-guide/#highlighting-user-input","text":"Use descriptive, all-caps text wrapped in angle brackets to to highlight areas that users would have to insert text specific to their site. You may also use TWiki-style color highlighting. For example: root@host # condor_ce_trace -d CE HOSTNAME","title":"Highlighting user input"},{"location":"documentation/style-guide/#lists","text":"When constructing lists, use the following guidelines: Use 1. for each item in a numbered list To make sure the contents of code blocks, file snippets, and subsequent paragraphs are indented properly, use the following formatting: For code blocks or file snippets, add an empty line after any regular text, then insert (N+1)*4 spaces at the beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code block with ::: FORMAT ; see this page for details, including possible highlighting formats. For an example of formatting a code section inside a list, see the release series document . For additional text (i.e. after a code block), insert N*4 spaces at the beginning of each line, where N is the level of the item in the list. For example: 1. Foo - Bar :::console COMMAND BLOCK text associated with Bar text associated with Foo 1. Baz FILE SNIPPET There are 12 spaces and 8 spaces in front of the command block and text associated with Bar , respectively; 4 spaces in front of the text associated with Foo ; and 8 spaces in front of the file snippet associated with Baz . The above block is rendered below: Foo Bar COMMAND BLOCK text associated with Bar text associated with Foo Baz FILE SNIPPET","title":"Lists"},{"location":"documentation/style-guide/#notes","text":"To catch the user's attention for important items or pitfalls, we used %NOTE% TWiki macros, these can be replaced with admonition-style notes and warnings: !!! note things to note or !!! warning if a user doesn t do this thing, bad stuff will happen The above blocks are rendered below as an example. !!! note things to note and !!! warning if a user doesn't do this thing, bad stuff will happen For a full list of admonition styles, see the documentation here .","title":"Notes"},{"location":"documentation/writing-documentation/","text":"Writing OSG Documentation Many OSG pages are written in markdown , built using MkDocs , and served via GitHub Pages . To contribute content , submit a pull request to the relevant github repository: Site administrator documentation Internal Technology Area documentation . Networking documentation Security documentation Operations documentation Production meeting notes Management pages Outreach pages User School 2017 pages User School 2018 pages This document contains instructions, recommendations, and guidelines for writing OSG content. Contributing Content To contribute minor content changes (e.g., fixing typos, changing a couple of sentences), we recommend using the GitHub web interface to submit a pull request. To contribute major content changes to one of the above OSG areas, make sure you and the machine you'll be working on meet the following requirements: Have a Github account Installations of the following tools and languages: git Python pip (usually comes by default with Python 2 = 2.7.9 or Python 3 = 3.4) Note On macOS, the OS-distributed Python does not come with pip. Run the following to install it: $ sudo easy_install pip pip will be installed in /usr/local/bin/ , so you will need /usr/local/bin in your PATH. Preparing the git repository Before making any content changes, you will need to prepare a local git clone and set up a Python virtual environment: Fork and clone the GitHub repository that you'd like to contribute to cd into the directory containing the local clone of your Github fork Run the following command to update the contents of the ci directory: $ git submodule update --init --recursive Add the upstream Github repository as a remote . For example, if you are working on the User School 2018 pages: $ git remote add upstream https://github.com/opensciencegrid/user-school-2018 Install the virtualenv package: $ pip install --user virtualenv Set up your Python virtual environment: $ virtualenv env $ env/bin/pip install -r ci/pip-requirements.txt Previewing the pages To preview the pages, start a MkDocs development server. The development server will automatically detect any content changes and make them viewable in your browser. cd into the directory containing the local clone of your GitHub fork Start a MkDocs development server to preview your changes: $ PYTHONPATH = src env/bin/mkdocs serve To preview your changes visit localhost:8000 in the browser of your choice. The server can be stopped with Ctrl-C . Making content changes To contribute content to the OSG, follow these steps to submit a pull request with your desired changes: cd into the directory containing the local clone of your Github fork Create a branch based on a branch from the upstream repository: $ git fetch --all $ git checkout -b BRANCH NAME upstream/ UPSTREAM BRANCH NAME Replace BRANCH NAME with a name of your choice and UPSTREAM BRANCH NAME with a branch name from the upstream repository. For example, instructors for the 2018 User School should use the materials branch: $ git checkout -b example_branch_name upstream/materials If you do not know which upstream branch to use, pick master . Make your changes in the docs/ directory of your local clone, following the style guide : If you are making changes to an existing page: Open mkdocs.yml and find the location of the file relative to the docs/ directory Make your changes to that file and move onto the next step If you are contributing a new page: Name the page. Page file names should be lowercase, - delimited, and concise but descriptive, e.g. markdown-migration.md or cutting-release.md Place the page in the relevant sub-folder of the docs/ directory. If you are unsure of the appropriate location, note that in the description of the pull request. Add the document to the pages: section of mkdocs.yml in title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' If you are writing site administrator documentation, following the suggested document layout If you haven't already, start a Mkdocs development server to preview your changes . Continue making changes until you are satisfied with the preview, then stage your changes in git: $ git add YOUR FILE YOUR 2nd FILE ... YOUR Nth FILE Adding each file that contains changes that you'd wish to make. If you are adding a new page, one of the files should be mkdocs.yml . Commit your changes and push them to your Github fork: $ git commit -m DESCRIPTIVE COMMIT MESSAGE $ git push origin From your Github fork, submit a pull request Deploying content to the ITB (advanced) If you are a member of the OSG software and release team, you can preview large changes to the ITB docs or ITB technology by pushing a branch that starts with an itb. prefix to the opensciencegrid/docs repo. For example: $ git remote add upstream https://github.com/opensciencegrid/docs.git $ git checkout new_docs $ git push upstream new_docs:itb.new_docs Note Since there is only one ITB docs area, simultaneous new commits to different itb.* branches will overwrite each other's changes. To re-deploy your changes, find your Travis-CI build and restart it BUT coordinate with the author of the other commits to avoid conflicts. Document Layout This section contains suggested layouts of externally-facing, site administrator documentation . The introduction is the only layout requirement for documents except for installation guides. Introductions All documents should start with an introduction that explains what the document contains, what the product does, and why someone may want to use it. In the past, document introductions were included in About this... sections due to the layout of the table of contents. Since the table of contents is included in the sidebar, introductions should go directly below the title header. The HTCondor-CE installation guide is an example that meet all of the above criteria. Installation guides In addition to the introduction above, installation documents should have the following sections: Before Starting: This section should contain information for any prepatory work that the site administrator should do or consider before proceeding with the installation ( example ). Installation: Procedural instructions that tell the user how to install the software ( example ) Validation: How does the user make sure their installation is functional? Help: Often just a link to the relevant help document as well as contact information for specific support groups, if applicable. Optionally, the following sections should be included as necessary. Overview: if the introduction becomes large and unwieldy, extract the details of what the product does into an overview section Configuration: required configuration steps ( example ) as well as a sub-section for optional configurations. For long optional configuration sections, consider creating alist of contents at the top of the sub-section ( example ). Troubleshooting: common issues that users encounter and their fixes Reference: Details about configuration and log files, unix users, certificates, networking, links to relevant upstream documentation, etc. ( example ) If any of the sections become too large, consider separating them out and linking to the new documents ( example ). Tips for Writing Procedural Instructions Title the procedure with the user goal, usually starting with a gerund; e.g.: Installing the Frobnosticator Number all steps (as opposed to using bullets) List steps in order in which they are performed Each step should begin with a single-line instruction in plain English, in command form; e.g.: Make sure that the Frobnosticator configuration file is world-writable If the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.: chmod a+x /usr/share/frobnosticator/frob.conf Put clarifying information in separate paragraphs within the step Put critical information about the whole procedure in one or more paragraphs before the numbered steps Put supplemental information about the whole procedure in one or more paragraphs after the numbered steps Avoid pronouns when writing technical articles or documentation e.g., install foo rather than install it . Avoid superfluous statements like you will want , you want , you should e.g., install foo rather than you will want to install foo .","title":"Writing Documentation"},{"location":"documentation/writing-documentation/#writing-osg-documentation","text":"Many OSG pages are written in markdown , built using MkDocs , and served via GitHub Pages . To contribute content , submit a pull request to the relevant github repository: Site administrator documentation Internal Technology Area documentation . Networking documentation Security documentation Operations documentation Production meeting notes Management pages Outreach pages User School 2017 pages User School 2018 pages This document contains instructions, recommendations, and guidelines for writing OSG content.","title":"Writing OSG Documentation"},{"location":"documentation/writing-documentation/#contributing-content","text":"To contribute minor content changes (e.g., fixing typos, changing a couple of sentences), we recommend using the GitHub web interface to submit a pull request. To contribute major content changes to one of the above OSG areas, make sure you and the machine you'll be working on meet the following requirements: Have a Github account Installations of the following tools and languages: git Python pip (usually comes by default with Python 2 = 2.7.9 or Python 3 = 3.4) Note On macOS, the OS-distributed Python does not come with pip. Run the following to install it: $ sudo easy_install pip pip will be installed in /usr/local/bin/ , so you will need /usr/local/bin in your PATH.","title":"Contributing Content"},{"location":"documentation/writing-documentation/#preparing-the-git-repository","text":"Before making any content changes, you will need to prepare a local git clone and set up a Python virtual environment: Fork and clone the GitHub repository that you'd like to contribute to cd into the directory containing the local clone of your Github fork Run the following command to update the contents of the ci directory: $ git submodule update --init --recursive Add the upstream Github repository as a remote . For example, if you are working on the User School 2018 pages: $ git remote add upstream https://github.com/opensciencegrid/user-school-2018 Install the virtualenv package: $ pip install --user virtualenv Set up your Python virtual environment: $ virtualenv env $ env/bin/pip install -r ci/pip-requirements.txt","title":"Preparing the git repository"},{"location":"documentation/writing-documentation/#previewing-the-pages","text":"To preview the pages, start a MkDocs development server. The development server will automatically detect any content changes and make them viewable in your browser. cd into the directory containing the local clone of your GitHub fork Start a MkDocs development server to preview your changes: $ PYTHONPATH = src env/bin/mkdocs serve To preview your changes visit localhost:8000 in the browser of your choice. The server can be stopped with Ctrl-C .","title":"Previewing the pages"},{"location":"documentation/writing-documentation/#making-content-changes","text":"To contribute content to the OSG, follow these steps to submit a pull request with your desired changes: cd into the directory containing the local clone of your Github fork Create a branch based on a branch from the upstream repository: $ git fetch --all $ git checkout -b BRANCH NAME upstream/ UPSTREAM BRANCH NAME Replace BRANCH NAME with a name of your choice and UPSTREAM BRANCH NAME with a branch name from the upstream repository. For example, instructors for the 2018 User School should use the materials branch: $ git checkout -b example_branch_name upstream/materials If you do not know which upstream branch to use, pick master . Make your changes in the docs/ directory of your local clone, following the style guide : If you are making changes to an existing page: Open mkdocs.yml and find the location of the file relative to the docs/ directory Make your changes to that file and move onto the next step If you are contributing a new page: Name the page. Page file names should be lowercase, - delimited, and concise but descriptive, e.g. markdown-migration.md or cutting-release.md Place the page in the relevant sub-folder of the docs/ directory. If you are unsure of the appropriate location, note that in the description of the pull request. Add the document to the pages: section of mkdocs.yml in title case , e.g. - Migrating Documents to Markdown: 'software/markdown-migration.md' If you are writing site administrator documentation, following the suggested document layout If you haven't already, start a Mkdocs development server to preview your changes . Continue making changes until you are satisfied with the preview, then stage your changes in git: $ git add YOUR FILE YOUR 2nd FILE ... YOUR Nth FILE Adding each file that contains changes that you'd wish to make. If you are adding a new page, one of the files should be mkdocs.yml . Commit your changes and push them to your Github fork: $ git commit -m DESCRIPTIVE COMMIT MESSAGE $ git push origin From your Github fork, submit a pull request","title":"Making content changes"},{"location":"documentation/writing-documentation/#deploying-content-to-the-itb-advanced","text":"If you are a member of the OSG software and release team, you can preview large changes to the ITB docs or ITB technology by pushing a branch that starts with an itb. prefix to the opensciencegrid/docs repo. For example: $ git remote add upstream https://github.com/opensciencegrid/docs.git $ git checkout new_docs $ git push upstream new_docs:itb.new_docs Note Since there is only one ITB docs area, simultaneous new commits to different itb.* branches will overwrite each other's changes. To re-deploy your changes, find your Travis-CI build and restart it BUT coordinate with the author of the other commits to avoid conflicts.","title":"Deploying content to the ITB (advanced)"},{"location":"documentation/writing-documentation/#document-layout","text":"This section contains suggested layouts of externally-facing, site administrator documentation . The introduction is the only layout requirement for documents except for installation guides.","title":"Document Layout"},{"location":"documentation/writing-documentation/#introductions","text":"All documents should start with an introduction that explains what the document contains, what the product does, and why someone may want to use it. In the past, document introductions were included in About this... sections due to the layout of the table of contents. Since the table of contents is included in the sidebar, introductions should go directly below the title header. The HTCondor-CE installation guide is an example that meet all of the above criteria.","title":"Introductions"},{"location":"documentation/writing-documentation/#installation-guides","text":"In addition to the introduction above, installation documents should have the following sections: Before Starting: This section should contain information for any prepatory work that the site administrator should do or consider before proceeding with the installation ( example ). Installation: Procedural instructions that tell the user how to install the software ( example ) Validation: How does the user make sure their installation is functional? Help: Often just a link to the relevant help document as well as contact information for specific support groups, if applicable. Optionally, the following sections should be included as necessary. Overview: if the introduction becomes large and unwieldy, extract the details of what the product does into an overview section Configuration: required configuration steps ( example ) as well as a sub-section for optional configurations. For long optional configuration sections, consider creating alist of contents at the top of the sub-section ( example ). Troubleshooting: common issues that users encounter and their fixes Reference: Details about configuration and log files, unix users, certificates, networking, links to relevant upstream documentation, etc. ( example ) If any of the sections become too large, consider separating them out and linking to the new documents ( example ).","title":"Installation guides"},{"location":"documentation/writing-documentation/#tips-for-writing-procedural-instructions","text":"Title the procedure with the user goal, usually starting with a gerund; e.g.: Installing the Frobnosticator Number all steps (as opposed to using bullets) List steps in order in which they are performed Each step should begin with a single-line instruction in plain English, in command form; e.g.: Make sure that the Frobnosticator configuration file is world-writable If the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.: chmod a+x /usr/share/frobnosticator/frob.conf Put clarifying information in separate paragraphs within the step Put critical information about the whole procedure in one or more paragraphs before the numbered steps Put supplemental information about the whole procedure in one or more paragraphs after the numbered steps Avoid pronouns when writing technical articles or documentation e.g., install foo rather than install it . Avoid superfluous statements like you will want , you want , you should e.g., install foo rather than you will want to install foo .","title":"Tips for Writing Procedural Instructions"},{"location":"infrastructure/koji-hub-setup/","text":"Notes on Koji-Hub Setup Current Koji documentation may be of use. Tags Base tags dist-el[567]-build Tag list These tags contains 3 external repos each, hosted locally under http://mirror.batlab.org/pub/linux : dist-el[567]-epel : A mirror of the EPEL 5/6/7 repositories dist-el[567]-centos*-os : A mirror of the base CentOS repositories dist-el[567]-centos*-updates* : A mirror of the CentOS Updates repositories We don't put any packages in them (except for ones required for building, like buildsys-macros and fetch-sources ), and generally don't build from them directly, but use tag inheritance. osg-el[567] Tag list These tags contains all the package names that we put into OSG; koji add-pkg adds to them. osg-build does this automatically. The tags do not actually contain any builds (i.e. packages with version-release). All the other osg-* tags inherit from these (either directly or indirectly). The purpose of this is to make promoting builds easier, since it keeps you from having to run add-pkg when you promote. kojira-fake Tag This tag (and targets building to it) were created because kojira does not automatically regenerate a repo unless it's the source of another repo. Without this, the osg-development tags (for example) wouldn't get regenerated automatically after a build. Main OSG tags osg-3.[123]-el[567]-build Tag list These are used to initialize the buildroot of most packages we make. They inherit from their respective dist-build and osg-development tags. The EL5 and EL6 tags also contain the jpackage[56]-bin external repos under http://mirror.batlab.org/pub/jpackage/ since we use those for some builds. Note that the JPackage external repos must have a better priority than the OS and EPEL external repos to avoid build problems for Java packages. osg-upcoming-el[567]-build Tag list These tags are special in that they also need to inherit from the latest mainline osg-build repo (that is, if trunk is 3.3, then osg-upcoming-el6-build should inherit from osg-3.3-el6-build ). osg-*-el[567]-development Tag list These contain the builds in the osg-minefield repos. The osg-development repos hosted by the GOC take packages from this, so osg-development is pretty much osg-minefield after a 1-hour delay. They inherit from osg-testing (and occasionally from the more specialized branches like el5-gt52-experimental, though that is now discouraged). Builds that are made using the osg-el[567] targets (default if you're using osg-build ) get their buildroots from the newest osg-build tags and put their results in the newest osg-development tags. osg-*-el[567]-testing Tag list These contain the builds in the osg-testing repos. They inherit from the respective osg-release tags. osg-*-el[567]-prerelease Tag list These are a staging are for packages that we are certain will be released in the next release. They are otherwise empty. These are used for testing and for building the tarball clients. osg-*-el[567]-release Tag list These contain the builds in the osg-release repos. They should be locked except for when moving packages from the osg-prerelease repos to the osg-release repos. They inherit from osg-el[567] . osg-3.[123]-el[567]-release-build Tag list These inherit the dist-*-build tags and the osg-*-release tags, putting a base OS along with OSG packages in a single repo, without the need for yum priorities. It is used, along with osg-*-prerelease , for building the tarball client. Note that there are no release-build repos for upcoming. osg-3.[123]-el[567]-contrib Tag list These contain the builds in the osg-contrib repos. Note that there are no osg-upcoming-contrib repos. Specialized tags These tags are generally made for long projects which may be in an unstable state and should not interfere with the main development of OSG packages. An example is a full-scale Globus update, where many packages have to be built, using each other as dependencies, and the whole system is not considered usable until all the updates are done. They should generally be removed after the work is done. el[67]-globus and el[67]-globus-build These tags were made by Matyas Selmeci for mass Globus updates. Collaborator Tags hcc-* For use by the Holland Computing Center at UNL. Build Targets A koji target pairs a build tag (which contains packages needed to build software) and a destination tag (which the software will be tagged into once it is built). osg-el[567] These build from the osg-*-el[567]-build tags for the current release series into the osg-*-el[567]-development tags and are the primary targets used for building OSG software. osg-3.[123]-el[567] These build from the osg-*-el[567]-build tags into the osg-*-el[567]-development tags and are used for building to releases other than the current one. osg-upcoming-el[567] These build from the osg-upcoming-el[567]-build tags into the osg-upcoming-el[567]-development tags and are used for building to upcoming. dist-el[567]-build These build from the dist-el*-build tag directly into the dist-el*-build tag. It is used for making builds that should be in every buildroot. kojira-fake-* These fool kojira into regenerating their build tags as repos we can yum install from. Without this, the osg-development tags (for example) wouldn't get regenerated and osg-minefield wouldn't work. hcc-el[567], panda-el6 These were made for builds made for our collaborators to build into their tags. Signing plugin The signing plugin is used to sign packages right after they are built. We give it a GPG signing key and corresponding passphrase. It is configured per build tag. The current default is to use the OSG key to sign if a configuration is not specified. This is because it's very difficult to sign packages after the fact, so it's better to erroneously sign some of them with the wrong key than to not sign them. It is therefore important that whenever a new build tag is created, a corresponding config section for the signing plugin is added, too. This comes from the package koji-plugin-sign and has configs in /etc/koji-sign-plugin and /etc/koji-hub/plugins . There is a script called fix-permissions in both directories that will make sure the plugin can read the config. Tweaks These are local config changes we needed to make to get certain features to work. Using proxy certs /etc/sysconfig/httpd needed to be changed to include the following lines: OPENSSL_ALLOW_PROXY=1 OPENSSL_ALLOW_PROXY_CERTS=1 export OPENSSL_ALLOW_PROXY export OPENSSL_ALLOW_PROXY_CERTS The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer. Procedures User cert switch This procedure is now documented on the user management page . Adding CAs for user authentication Since our Koji instance uses certs for auth, we specify which CAs we trust for signing user certs. The CA certs for user auth are concatenated together in the file /etc/pki/tls/certs/allowed-cas-for-users.crt . A comment line before the BEGIN CERTIFICATE line is used to name the file the cert comes from. We take the certs from the osg-ca-certs repository. For example, when I added the CERN CAs to the bundle, I installed osg-ca-certs onto a Fermicloud VM, copied /etc/grid-security/certificates/CERN-TCA.pem (which signed user certs) and CERN-Root.pem (which signed CERN-TCA.pem ) to koji.chtc.wisc.edu , catted them to the end of the allowed-cas-for-users.crt file, edited the file to add comments before the certs, and restarted httpd . A few tidbits of knowledge for administrators of our Koji server: Three services need to be running for koji-hub to be functioning: kojira, kojid, and the Apache web server. To restart these: service kojid restart service kojira restart service httpd restart Logfiles can be found here: /var/log/kojid.log /var/log/kojira.log /var/log/messages kojid is configured to stop starting new tasks if it has less than 8GB free. Failed build roots are kept for 4 hours, and each build root is about 1GB currently. Hence, if too many tasks fail, the kojid might stop accepting new tasks for 4 hours. Manually clean these out. Koji Permissions To add a new user to Koji for someone with a given DN, first extract the CN. For example, Alain has the DN /DC=org/DC=doegrids/OU=People/CN=Alain Roy 424511 , and the CN is just Alain Roy 424511 . The commands below use just the CN. [you@client ~]$ osg-koji add-user CN [you@client ~]$ osg-koji grant-permission build CN [you@client ~]$ osg-koji grant-permission repo CN If you want to see the set of possible permissions: [you@client ~]$ koji list-permissions Enter PEM pass phrase: admin build repo livecd maven-import win-import win-admin appliance If you want to see someone's permissions: [you@client ~]$ koji list-permissions --user Alain Roy 424511 Enter PEM pass phrase: admin If you want to see your own permissions: [you@client ~]$ koji list-permissions --mine Enter PEM pass phrase: admin To see the list of users: [you@client ~]$ koji search user * Renewing host and service certs Certs and keys are stored in /p/condor/home/certificates/... To obtain renewed certificates, use the OSG PKI commandline clients or the web interface at https://oim.opensciencegrid.org/oim/certificaterequesthost . The following cert files are necessary: hostcert.pem koji.chtc host cert ( CN=koji.chtc.wisc.edu ) hostkey.pem koji.chtc host key kojiracert.pem koji.chtc/kojira service cert ( CN=koji.chtc.wisc.edu/kojira ) kojirakey.pem koji.chtc/kojira service key kojiweb.pem Concatenation of hostcert.pem and hostkey.pem kojira.pem Concatenation of kojiracert.pem and kojirakey.pem To create kojiweb.pem and kojira.pem from their respective cert/key files, do: [root@koji ~]# ( dos2unix hostcert.pem ; echo ; dos2unix hostkey.pem ) kojiweb.pem [root@koji ~]# chmod 0600 kojiweb.pem [root@koji ~]# ( dos2unix kojiracert.pem ; echo ; dos2unix kojirakey.pem ) kojira.pem [root@koji ~]# chmod 0600 kojira.pem As root in /etc on koji.chtc.wisc.edu : Run git status . Run etckeeper commit to commit any uncommited changes (including unversioned files). Put the files onto koji.chtc.wisc.edu as follows: File Location chown chmod hostcert.pem /etc/pki/tls/certs/hostcert.pem root:root 0644 hostkey.pem /etc/pki/tls/private/hostkey.pem root:root 0600 kojiweb.pem /etc/pki/tls/private/kojiweb.pem apache:apache 0600 kojira.pem /etc/pki/tls/private/kojira.pem root:root 0600 Shut down kojira and kojid . Restart httpd . Log in via your own cert to the web interface to verify that it is working. Run osg-koji list-permissions --mine to verify command-line access is working. Run etckeeper commit to commit your changes in /etc . Start up kojira and kojid .","title":"Koji-Hub Setup"},{"location":"infrastructure/koji-hub-setup/#notes-on-koji-hub-setup","text":"Current Koji documentation may be of use.","title":"Notes on Koji-Hub Setup"},{"location":"infrastructure/koji-hub-setup/#tags","text":"","title":"Tags"},{"location":"infrastructure/koji-hub-setup/#base-tags","text":"","title":"Base tags"},{"location":"infrastructure/koji-hub-setup/#dist-el9156793-build","text":"Tag list These tags contains 3 external repos each, hosted locally under http://mirror.batlab.org/pub/linux : dist-el[567]-epel : A mirror of the EPEL 5/6/7 repositories dist-el[567]-centos*-os : A mirror of the base CentOS repositories dist-el[567]-centos*-updates* : A mirror of the CentOS Updates repositories We don't put any packages in them (except for ones required for building, like buildsys-macros and fetch-sources ), and generally don't build from them directly, but use tag inheritance.","title":"dist-el[567]-build"},{"location":"infrastructure/koji-hub-setup/#osg-el9156793","text":"Tag list These tags contains all the package names that we put into OSG; koji add-pkg adds to them. osg-build does this automatically. The tags do not actually contain any builds (i.e. packages with version-release). All the other osg-* tags inherit from these (either directly or indirectly). The purpose of this is to make promoting builds easier, since it keeps you from having to run add-pkg when you promote.","title":"osg-el[567]"},{"location":"infrastructure/koji-hub-setup/#kojira-fake","text":"Tag This tag (and targets building to it) were created because kojira does not automatically regenerate a repo unless it's the source of another repo. Without this, the osg-development tags (for example) wouldn't get regenerated automatically after a build.","title":"kojira-fake"},{"location":"infrastructure/koji-hub-setup/#main-osg-tags","text":"","title":"Main OSG tags"},{"location":"infrastructure/koji-hub-setup/#osg-39112393-el9156793-build","text":"Tag list These are used to initialize the buildroot of most packages we make. They inherit from their respective dist-build and osg-development tags. The EL5 and EL6 tags also contain the jpackage[56]-bin external repos under http://mirror.batlab.org/pub/jpackage/ since we use those for some builds. Note that the JPackage external repos must have a better priority than the OS and EPEL external repos to avoid build problems for Java packages.","title":"osg-3.[123]-el[567]-build"},{"location":"infrastructure/koji-hub-setup/#osg-upcoming-el9156793-build","text":"Tag list These tags are special in that they also need to inherit from the latest mainline osg-build repo (that is, if trunk is 3.3, then osg-upcoming-el6-build should inherit from osg-3.3-el6-build ).","title":"osg-upcoming-el[567]-build"},{"location":"infrastructure/koji-hub-setup/#osg-42-el9156793-development","text":"Tag list These contain the builds in the osg-minefield repos. The osg-development repos hosted by the GOC take packages from this, so osg-development is pretty much osg-minefield after a 1-hour delay. They inherit from osg-testing (and occasionally from the more specialized branches like el5-gt52-experimental, though that is now discouraged). Builds that are made using the osg-el[567] targets (default if you're using osg-build ) get their buildroots from the newest osg-build tags and put their results in the newest osg-development tags.","title":"osg-*-el[567]-development"},{"location":"infrastructure/koji-hub-setup/#osg-42-el9156793-testing","text":"Tag list These contain the builds in the osg-testing repos. They inherit from the respective osg-release tags.","title":"osg-*-el[567]-testing"},{"location":"infrastructure/koji-hub-setup/#osg-42-el9156793-prerelease","text":"Tag list These are a staging are for packages that we are certain will be released in the next release. They are otherwise empty. These are used for testing and for building the tarball clients.","title":"osg-*-el[567]-prerelease"},{"location":"infrastructure/koji-hub-setup/#osg-42-el9156793-release","text":"Tag list These contain the builds in the osg-release repos. They should be locked except for when moving packages from the osg-prerelease repos to the osg-release repos. They inherit from osg-el[567] .","title":"osg-*-el[567]-release"},{"location":"infrastructure/koji-hub-setup/#osg-39112393-el9156793-release-build","text":"Tag list These inherit the dist-*-build tags and the osg-*-release tags, putting a base OS along with OSG packages in a single repo, without the need for yum priorities. It is used, along with osg-*-prerelease , for building the tarball client. Note that there are no release-build repos for upcoming.","title":"osg-3.[123]-el[567]-release-build"},{"location":"infrastructure/koji-hub-setup/#osg-39112393-el9156793-contrib","text":"Tag list These contain the builds in the osg-contrib repos. Note that there are no osg-upcoming-contrib repos.","title":"osg-3.[123]-el[567]-contrib"},{"location":"infrastructure/koji-hub-setup/#specialized-tags","text":"These tags are generally made for long projects which may be in an unstable state and should not interfere with the main development of OSG packages. An example is a full-scale Globus update, where many packages have to be built, using each other as dependencies, and the whole system is not considered usable until all the updates are done. They should generally be removed after the work is done.","title":"Specialized tags"},{"location":"infrastructure/koji-hub-setup/#el916793-globus-and-el916793-globus-build","text":"These tags were made by Matyas Selmeci for mass Globus updates.","title":"el[67]-globus and el[67]-globus-build"},{"location":"infrastructure/koji-hub-setup/#collaborator-tags","text":"","title":"Collaborator Tags"},{"location":"infrastructure/koji-hub-setup/#hcc-42","text":"For use by the Holland Computing Center at UNL.","title":"hcc-*"},{"location":"infrastructure/koji-hub-setup/#build-targets","text":"A koji target pairs a build tag (which contains packages needed to build software) and a destination tag (which the software will be tagged into once it is built).","title":"Build Targets"},{"location":"infrastructure/koji-hub-setup/#osg-el9156793_1","text":"These build from the osg-*-el[567]-build tags for the current release series into the osg-*-el[567]-development tags and are the primary targets used for building OSG software.","title":"osg-el[567]"},{"location":"infrastructure/koji-hub-setup/#osg-39112393-el9156793","text":"These build from the osg-*-el[567]-build tags into the osg-*-el[567]-development tags and are used for building to releases other than the current one.","title":"osg-3.[123]-el[567]"},{"location":"infrastructure/koji-hub-setup/#osg-upcoming-el9156793","text":"These build from the osg-upcoming-el[567]-build tags into the osg-upcoming-el[567]-development tags and are used for building to upcoming.","title":"osg-upcoming-el[567]"},{"location":"infrastructure/koji-hub-setup/#dist-el9156793-build_1","text":"These build from the dist-el*-build tag directly into the dist-el*-build tag. It is used for making builds that should be in every buildroot.","title":"dist-el[567]-build"},{"location":"infrastructure/koji-hub-setup/#kojira-fake-42","text":"These fool kojira into regenerating their build tags as repos we can yum install from. Without this, the osg-development tags (for example) wouldn't get regenerated and osg-minefield wouldn't work.","title":"kojira-fake-*"},{"location":"infrastructure/koji-hub-setup/#hcc-el9156793-panda-el6","text":"These were made for builds made for our collaborators to build into their tags.","title":"hcc-el[567], panda-el6"},{"location":"infrastructure/koji-hub-setup/#signing-plugin","text":"The signing plugin is used to sign packages right after they are built. We give it a GPG signing key and corresponding passphrase. It is configured per build tag. The current default is to use the OSG key to sign if a configuration is not specified. This is because it's very difficult to sign packages after the fact, so it's better to erroneously sign some of them with the wrong key than to not sign them. It is therefore important that whenever a new build tag is created, a corresponding config section for the signing plugin is added, too. This comes from the package koji-plugin-sign and has configs in /etc/koji-sign-plugin and /etc/koji-hub/plugins . There is a script called fix-permissions in both directories that will make sure the plugin can read the config.","title":"Signing plugin"},{"location":"infrastructure/koji-hub-setup/#tweaks","text":"These are local config changes we needed to make to get certain features to work.","title":"Tweaks"},{"location":"infrastructure/koji-hub-setup/#using-proxy-certs","text":"/etc/sysconfig/httpd needed to be changed to include the following lines: OPENSSL_ALLOW_PROXY=1 OPENSSL_ALLOW_PROXY_CERTS=1 export OPENSSL_ALLOW_PROXY export OPENSSL_ALLOW_PROXY_CERTS The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.","title":"Using proxy certs"},{"location":"infrastructure/koji-hub-setup/#procedures","text":"","title":"Procedures"},{"location":"infrastructure/koji-hub-setup/#user-cert-switch","text":"This procedure is now documented on the user management page .","title":"User cert switch"},{"location":"infrastructure/koji-hub-setup/#adding-cas-for-user-authentication","text":"Since our Koji instance uses certs for auth, we specify which CAs we trust for signing user certs. The CA certs for user auth are concatenated together in the file /etc/pki/tls/certs/allowed-cas-for-users.crt . A comment line before the BEGIN CERTIFICATE line is used to name the file the cert comes from. We take the certs from the osg-ca-certs repository. For example, when I added the CERN CAs to the bundle, I installed osg-ca-certs onto a Fermicloud VM, copied /etc/grid-security/certificates/CERN-TCA.pem (which signed user certs) and CERN-Root.pem (which signed CERN-TCA.pem ) to koji.chtc.wisc.edu , catted them to the end of the allowed-cas-for-users.crt file, edited the file to add comments before the certs, and restarted httpd . A few tidbits of knowledge for administrators of our Koji server: Three services need to be running for koji-hub to be functioning: kojira, kojid, and the Apache web server. To restart these: service kojid restart service kojira restart service httpd restart Logfiles can be found here: /var/log/kojid.log /var/log/kojira.log /var/log/messages kojid is configured to stop starting new tasks if it has less than 8GB free. Failed build roots are kept for 4 hours, and each build root is about 1GB currently. Hence, if too many tasks fail, the kojid might stop accepting new tasks for 4 hours. Manually clean these out.","title":"Adding CAs for user authentication"},{"location":"infrastructure/koji-hub-setup/#koji-permissions","text":"To add a new user to Koji for someone with a given DN, first extract the CN. For example, Alain has the DN /DC=org/DC=doegrids/OU=People/CN=Alain Roy 424511 , and the CN is just Alain Roy 424511 . The commands below use just the CN. [you@client ~]$ osg-koji add-user CN [you@client ~]$ osg-koji grant-permission build CN [you@client ~]$ osg-koji grant-permission repo CN If you want to see the set of possible permissions: [you@client ~]$ koji list-permissions Enter PEM pass phrase: admin build repo livecd maven-import win-import win-admin appliance If you want to see someone's permissions: [you@client ~]$ koji list-permissions --user Alain Roy 424511 Enter PEM pass phrase: admin If you want to see your own permissions: [you@client ~]$ koji list-permissions --mine Enter PEM pass phrase: admin To see the list of users: [you@client ~]$ koji search user *","title":"Koji Permissions"},{"location":"infrastructure/koji-hub-setup/#renewing-host-and-service-certs","text":"Certs and keys are stored in /p/condor/home/certificates/... To obtain renewed certificates, use the OSG PKI commandline clients or the web interface at https://oim.opensciencegrid.org/oim/certificaterequesthost . The following cert files are necessary: hostcert.pem koji.chtc host cert ( CN=koji.chtc.wisc.edu ) hostkey.pem koji.chtc host key kojiracert.pem koji.chtc/kojira service cert ( CN=koji.chtc.wisc.edu/kojira ) kojirakey.pem koji.chtc/kojira service key kojiweb.pem Concatenation of hostcert.pem and hostkey.pem kojira.pem Concatenation of kojiracert.pem and kojirakey.pem To create kojiweb.pem and kojira.pem from their respective cert/key files, do: [root@koji ~]# ( dos2unix hostcert.pem ; echo ; dos2unix hostkey.pem ) kojiweb.pem [root@koji ~]# chmod 0600 kojiweb.pem [root@koji ~]# ( dos2unix kojiracert.pem ; echo ; dos2unix kojirakey.pem ) kojira.pem [root@koji ~]# chmod 0600 kojira.pem As root in /etc on koji.chtc.wisc.edu : Run git status . Run etckeeper commit to commit any uncommited changes (including unversioned files). Put the files onto koji.chtc.wisc.edu as follows: File Location chown chmod hostcert.pem /etc/pki/tls/certs/hostcert.pem root:root 0644 hostkey.pem /etc/pki/tls/private/hostkey.pem root:root 0600 kojiweb.pem /etc/pki/tls/private/kojiweb.pem apache:apache 0600 kojira.pem /etc/pki/tls/private/kojira.pem root:root 0600 Shut down kojira and kojid . Restart httpd . Log in via your own cert to the web interface to verify that it is working. Run osg-koji list-permissions --mine to verify command-line access is working. Run etckeeper commit to commit your changes in /etc . Start up kojira and kojid .","title":"Renewing host and service certs"},{"location":"infrastructure/koji-inf-overview/","text":"Koji Infrastructure Overview In Madison In WID koji.chtc.wisc.edu is the main Koji server. It runs: koji-hub (httpd/mod_python service) -- controls everything else koji-web (httpd/mod_python service) -- provides the web interface to koji-hub kojid (standalone daemon) -- builds packages kojira (standalone daemon) -- creates tasks to regen repos automatically rsyncd (via xinetd) puppetd (cron job) (also stores RPMs in its /mnt/koji directory) db-01.batlab.org is the database server. It runs: postgres (standalone daemon) rsyncd (via xinetd) puppetd (cron job) (others) host-3.chtc.wisc.edu is a backup server. It runs: rsync (cron job) puppetd (cron job) (others) wid-service-1.chtc.wisc.edu is a Puppet Master. It runs: puppetmaster (standalone daemon) (others) In CS (3370A) osghost.chtc.wisc.edu is a VM host. It runs: kojibuilder2.chtc.wisc.edu and kojibuilder3.chtc.wisc.edu are builder VMs. Each runs: kojid (standalone daemon) -- builds packages puppetd (cron job) In Indiana repo1.grid.iu.edu , repo2.grid.iu.edu and repo-itb.grid.iu.edu are repo hosts. Each runs: mash (cron job) -- pulls RPMs from a koji-hub (others) repo.grid.iu.edu is a DNS alias pointing to either repo1 or repo2 Lines of Communication koji-web provides a web interface to koji-hub koji-hub sends jobs to all kojid daemons and gets back the results kojira sends requests to koji-hub to create tasks koji-hub talks directly to postgres for all metadata koji-hub writes to and reads from /mnt/koji mash pulls RPMs from /mnt/koji and communicates with koji-hub to get tag information puppetd on all Madison machines pulls Puppet configuration from puppetmaster on wid-service-1 rsync on host-3.chtc.wisc.edu pulls files from rsyncd on koji.chtc.wisc.edu and db-01.batlab.org Management Managed by the GOC: repo1.grid.iu.edu repo2.grid.iu.edu repo.grid.iu.edu repo-itb.grid.iu.edu Fully managed by CHTC Infrastructure: db-01.batlab.org host-3.chtc.wisc.edu wid-service-1.chtc.wisc.edu Management split between CHTC infrastructure and OSG-Software: koji.chtc.wisc.edu kojibuilder2.chtc.wisc.edu kojibuilder3.chtc.wisc.edu osghost.chtc.wisc.edu (In general, CHTC-inf takes care of accounts, firewalls, other basic config, and OSG takes care of the Koji services)","title":"Koji Infrastructure Overview"},{"location":"infrastructure/koji-inf-overview/#koji-infrastructure-overview","text":"","title":"Koji Infrastructure Overview"},{"location":"infrastructure/koji-inf-overview/#in-madison","text":"","title":"In Madison"},{"location":"infrastructure/koji-inf-overview/#in-wid","text":"koji.chtc.wisc.edu is the main Koji server. It runs: koji-hub (httpd/mod_python service) -- controls everything else koji-web (httpd/mod_python service) -- provides the web interface to koji-hub kojid (standalone daemon) -- builds packages kojira (standalone daemon) -- creates tasks to regen repos automatically rsyncd (via xinetd) puppetd (cron job) (also stores RPMs in its /mnt/koji directory) db-01.batlab.org is the database server. It runs: postgres (standalone daemon) rsyncd (via xinetd) puppetd (cron job) (others) host-3.chtc.wisc.edu is a backup server. It runs: rsync (cron job) puppetd (cron job) (others) wid-service-1.chtc.wisc.edu is a Puppet Master. It runs: puppetmaster (standalone daemon) (others)","title":"In WID"},{"location":"infrastructure/koji-inf-overview/#in-cs-3370a","text":"osghost.chtc.wisc.edu is a VM host. It runs: kojibuilder2.chtc.wisc.edu and kojibuilder3.chtc.wisc.edu are builder VMs. Each runs: kojid (standalone daemon) -- builds packages puppetd (cron job)","title":"In CS (3370A)"},{"location":"infrastructure/koji-inf-overview/#in-indiana","text":"repo1.grid.iu.edu , repo2.grid.iu.edu and repo-itb.grid.iu.edu are repo hosts. Each runs: mash (cron job) -- pulls RPMs from a koji-hub (others) repo.grid.iu.edu is a DNS alias pointing to either repo1 or repo2","title":"In Indiana"},{"location":"infrastructure/koji-inf-overview/#lines-of-communication","text":"koji-web provides a web interface to koji-hub koji-hub sends jobs to all kojid daemons and gets back the results kojira sends requests to koji-hub to create tasks koji-hub talks directly to postgres for all metadata koji-hub writes to and reads from /mnt/koji mash pulls RPMs from /mnt/koji and communicates with koji-hub to get tag information puppetd on all Madison machines pulls Puppet configuration from puppetmaster on wid-service-1 rsync on host-3.chtc.wisc.edu pulls files from rsyncd on koji.chtc.wisc.edu and db-01.batlab.org","title":"Lines of Communication"},{"location":"infrastructure/koji-inf-overview/#management","text":"Managed by the GOC: repo1.grid.iu.edu repo2.grid.iu.edu repo.grid.iu.edu repo-itb.grid.iu.edu Fully managed by CHTC Infrastructure: db-01.batlab.org host-3.chtc.wisc.edu wid-service-1.chtc.wisc.edu Management split between CHTC infrastructure and OSG-Software: koji.chtc.wisc.edu kojibuilder2.chtc.wisc.edu kojibuilder3.chtc.wisc.edu osghost.chtc.wisc.edu (In general, CHTC-inf takes care of accounts, firewalls, other basic config, and OSG takes care of the Koji services)","title":"Management"},{"location":"infrastructure/koji-initial-install/","text":"Notes on Koji initial install Note This document was written for the original install of koji/koji-hub version 1.6 to koji-hub.batlab.org . The document is kept for historical purposes, as this setup is no longer accurate with newer versions of Koji, machine moves and renames, cert authority changes, etc. Machine setup koji is run on koji-hub.batlab.org ; right now a single machine was used for the hub (koji-hub), the web frontend (koji-web), repo generation (kojira), and el5 building (kojid). A second machine, kojibuilder1.batlab.org was used for el6 building. As instructions for setting up the machine, I'm just going to take the Fedora guide at http://fedoraproject.org/wiki/Koji/ServerHowTo , and add modifications / comments to suit our setup. Sections taken from the Fedora guide will be between dividers like this: For an overview of yum, mock, Koji (and all its subcomponents), mash, and how they all work together, see the excellent slides put together by Steve Traylen at CERN http://indico.cern.ch/event/55091 Packages to install On the server (koji-hub/koji-web) httpd mod_ssl postgresql-server mod_python ( = 3.3.1 for Kerberos authentication) On the builder (koji-builder) mock setarch (for some archs you'll require a patched version) rpm-build createrepo We used one machine for both these roles, so all of the above had to be installed. The koji packages to install are: koji-hub koji-web koji-builder koji-utils Filesystems A note on filesystem space Koji will consume copious amounts of disk space under the primary KojiDir directory (as set in the kojihub.conf file). This is /mnt/koji by default. Koji keeps all RPMs built in here so a complete history is kept. In addition, repositories are kept here. A repository is just a set of text files, but since a new one is generated for every build, the space will add up. Koji does clean up the repositories once they get old enough. For koji-hub.batlab.org , we allocated a 120G partition for /mnt/koji . However, as koji makes use of mock on the backend to actually create build roots and perform the builds in those build roots, it might come to a surprise to users that a running koji server will consume large amounts of disk space under /var/lib/mock and /var/cache/mock as well. kojid (the builder daemon) will refuse to start a build if it does not have sufficient space under /var/lib/mock . By default, this is an 8G surplus, but we lowered it to 4G for koji-hub.batlab.org . The build roots will be wiped after successful builds, but the build roots for failed builds will be kept around for 2 weeks (?); as a result, disk usage of /var/lib/mock can swing wildly. For koji-hub.batlab.org , we have allocated 35G for /var/lib/mock . /var/cache/mock doesn't use up that much space, so we didn't make a separate partition for it, it just uses what's under / . Authentication Most of the \"Koji Authentication Selection\" section can be skipped. We got our certs from DigiCert. Note that certs are sensitive to line ending issues -- you may have to run dos2unix on them before they would work. We have 3 certs, with the following subjects: /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=koji-hub.batlab.org (for koji-hub, koji-web, and kojid) /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=kojira/koji-hub.batlab.org (for kojira) /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=kojibuilder1.batlab.org (for kojid on kojibuilder1) kojid needed a different cert than kojira because the two would connect to koji-hub at the same time and log each other off. Copies of the certs exist in AFS at /p/condor/home/certificates/koji-hub.batlab.org/ and /p/condor/home/certificates/kojibuilder1.batlab.org/ Clients connecting to koji-hub need a file containing the CA certs; this is just a concatenation of the DigiCert Grid Root CA and the DigiCert Grid CA-1 certs, which can be obtained from DigiCert's website (get PEM format). We have a digicert-chain.crt in /etc/pki/tls/certs/digicert-chain.crt on koji-hub We made a kojiadmin user, but we didn't make a cert for it so we just used user/pass authentication until we set up our own accounts and then disabled kojiadmin . Using proxy certs /etc/sysconfig/httpd needed to be changed to include the following lines: OPENSSL_ALLOW_PROXY = 1 OPENSSL_ALLOW_PROXY_CERTS = 1 export OPENSSL_ALLOW_PROXY export OPENSSL_ALLOW_PROXY_CERTS The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer. Postgres Database Install postgres: [root@koji-hub]# yum install postgresql-server Init the db: [root@koji-hub]# su - postgres -c \"PGDATA=/var/lib/pgsql/data\" initdb Start the db: [root@koji-hub]# service postgresql start Make a koji account: [root@koji-hub]# useradd koji; passwd -d koji Setup PostgreSQL and populate schema: The following commands will create the koji user within PostgreSQL and will then create the koji database using the schema within the /usr/share/doc/koji*/docs/schema.sql directory root@localhost$ su - postgres postgres@localhost$ createuser koji Shall the new role be a superuser? (y/n) n Shall the new role be allowed to create databases? (y/n) n Shall the new role be allowed to create more new roles? (y/n) n postgres@localhost$ createdb -O koji koji postgres@localhost$ logout root@localhost$ su - koji koji@localhost$ psql koji koji /usr/share/doc/koji*/docs/schema.sql koji@localhost$ exit NOTE: When issuing the command to import the psql schema into the new database it is important to ensure that the directory path /usr/share/doc/koji*/docs/schema.sql remains intact and is not resolved to a specific version of koji. In test it was discovered that when the path is resolved to a specific version of koji then not all of the tables were created correctly. To authorize the koji-web and koji-hub resources, make the following additions to /var/lib/pgsql/data/pg_hba.conf (IP addresses will vary): # access for koji host koji postgres 127.0.0.1/32 trust host koji koji 127.0.0.1/32 trust host koji koji 128.104.100.41/32 trust host koji koji ::1/128 trust Next, we'll need a koji admin user to run some commands. We'll need to add it to the database manually. Once again, we used user/pass authentication for the kojiadmin user until we disabled it. All database commands should be done as the koji user: [root@koji-hub]# sudo -u koji psql koji= insert into users ( name , password , status , usertype ) values ( kojiadmin , some-throwaway-admin-password-in-plain-text , 0 , 0 ); koji= insert into user_perms ( user_id , perm_id , creator_id ) values (( select id from users where name = kojiadmin ), 1 , ( select id from users where name = kojiadmin )); Koji-Hub Koji-hub is the center of all Koji operations. It is an XML-RPC server running under mod_python in Apache. koji-hub is passive in that it only receives XML-RPC calls and relies upon the build daemons and other components to initiate communication. Koji-hub is the only component that has direct access to the database and is one of the two components that have write access to the file system. Config files we care about: /etc/httpd/conf/httpd.conf /etc/httpd/conf.d/kojihub.conf /etc/httpd/conf.d/ssl.conf /etc/koji-hub/hub.conf Install the necessary packages. [root@koji-hub]# yum install koji-hub httpd mod_ssl mod_python /etc/httpd/conf/httpd.conf: The apache web server has two places that it sets maximum requests a server will handle before the server restarts. The xmlrpc interface in kojihub is a python application, and mod_python can sometimes grow outrageously large when it doesn't reap memory often enough. As a result, it is strongly recommended that you set both instances of MaxRequestsPerChild in httpd.conf to something reasonable in order to prevent the server from becoming overloaded and crashing (at 100 the httpd processes will grow to about 75MB resident set size before respawning). IfModule prefork.c ... MaxRequestsPerChild 100 /IfModule IfModule worker.c ... MaxRequestsPerChild 100 /IfModule /etc/koji-hub/hub.conf: This file contains the configuration information for the hub. You will need to edit this configuration to point Koji Hub to the database you are using and to setup Koji Hub to utilize the authentication scheme you selected in the beginning. We made multiple changes to this config file. Notable changes: We turned off LoginCreatesUser KojiWebURL is http://koji-hub.batlab.org/koji PluginPath is /usr/lib/koji-hub-plugins Plugins = sign since there is an RPM signing plugin (package name: koji-plugin-sign ) that we use Other notes: Should not be world-readable Must be readable by the apache user though Finally, there is a [policy] section for controlling ACLs. The contents of that are described later in this document. /etc/httpd/conf.d/kojihub.conf: If using SSL auth, uncomment these lines for kojiweb to allow logins. Location /kojihub SSLOptions +StdEnvVars /Location This is actually outdated information, useful for Koji 1.4.0. Instead, we add the following: Location /kojihub/ssllogin SSLVerifyClient require SSLVerifyDepth 10 SSLOptions +StdEnvVars /Location /etc/httpd/conf.d/ssl.conf: If using SSL you will also need to add the needed SSL options for apache. These options should point to where the certificates are located on the hub. These are the config lines we use: ## The host cert for koji-hub SSLCertificateFile /etc/pki/tls/certs/digicert_hostcert.crt ## The private key for koji-hub SSLCertificateKeyFile /etc/pki/tls/private/digicert_hostkey.key ## The concatenation of the DigiCert CA certs we made above. SSLCertificateChainFile /etc/pki/tls/certs/digicert-chain.crt ## All the CA certs on the system. SSLCACertificateFile /etc/pki/tls/certs/ca-bundle.crt SSLVerifyClient require SSLVerifyDepth 10 Restart httpd after doing this. Adding a real admin user At this point, SSL should be set up on Koji Hub. Create an account for yourself (your CN) and give yourself the admin bit: [kojiadmin@koji-hub]$ koji add-user Your Name 123456 [kojiadmin@koji-hub]$ koji grant-permission admin Your Name 123456 Once you're done with all the setup , you should remove the admin bit from kojiadmin , block the user, and remove its password from the database. [you@koji-hub]$ koji revoke-permission admin kojiadmin [you@koji-hub]$ koji disable-user kojiadmin In the database: koji= update users set password = null where name = kojiadmin ; Koji filesystem KojiDir is /mnt/koji , so the following tree should be created: [root@koji-hub]# mkdir -p /mnt/koji/ { packages,repos,work,scratch } [root@koji-hub]# chown apache:apache * Accounts for kojira and kojid It is important to note that the kojira component needs repo privileges, but if you just let the account get auto created the first time you run kojira, it won't have that privilege, so you should pre-create the account and grant it the repo privilege now. kojiadmin@koji-hub$ koji add-user kojira kojiadmin@koji-hub$ koji grant-permission repo kojira For similar technical reasons, you need to add-host each build host prior to starting kojid on that host the first time and could also do that now. We turned off auto account creation, so there won't be a problem with accounts with bad perms getting created. This means they have to be created manually. We have kojid running on koji-hub, so this is the command we used to add it: [you@koji-hub]$ koji add-host koji-hub.batlab.org i386 x86_64 Koji-Web, Kojid Installing: [root@koji-hub]# yum install koji-web mod_ssl [root@koji-hub]# yum install koji-builder Note that we use a patched koji-builder, so be sure to install what's in the OSG repository. (The patch is not likely to be accepted upstream). SSL certs The following files should be in the /etc/pki/tls tree: /etc/pki/tls/private/digicert_hostkey.key the host key from DigiCert /etc/pki/tls/private/kojiweb.pem digicert_hostcert.crt catted together with digicert_hostkey.key (public key first) /etc/pki/tls/private/kojira.key kojira's private key from DigiCert /etc/pki/tls/private/kojira.pem kojira.crt catted together with kojira.key (public key first) /etc/pki/tls/cert.pem symlink to certs/ca-bundle.crt /etc/pki/tls/certs/ca-bundle.crt all certs available on the system; the DigiCert Grid certs should be in here /etc/pki/tls/certs/kojira.crt kojira's cert /etc/pki/tls/certs/digicert_hostcert.crt the host cert from DigiCert /etc/pki/tls/certs/digicert-chain.crt the DigiCert CAs catted together Adding koji-builder host to database [you@koji-hub]$ koji add-host-to-channel koji-hub.batlab.org createrepo In the database: koji= # update host set capacity = 4 where name = koji-hub.batlab.org ; Start kojid [root@koji-hub]# /sbin/service kojid start Check /var/log/kojid.log to make sure everything started up. Kojira Installing: [root@koji-hub]# yum install koji-utils Kojira also needs a user [you@koji-hub]$ koji add-user kojira [you@koji-hub]$ koji grant-permission repo kojira Start it up [root@koji-hub]# /sbin/service kojira start","title":"Koji Initial Install"},{"location":"infrastructure/koji-initial-install/#notes-on-koji-initial-install","text":"Note This document was written for the original install of koji/koji-hub version 1.6 to koji-hub.batlab.org . The document is kept for historical purposes, as this setup is no longer accurate with newer versions of Koji, machine moves and renames, cert authority changes, etc.","title":"Notes on Koji initial install"},{"location":"infrastructure/koji-initial-install/#machine-setup","text":"koji is run on koji-hub.batlab.org ; right now a single machine was used for the hub (koji-hub), the web frontend (koji-web), repo generation (kojira), and el5 building (kojid). A second machine, kojibuilder1.batlab.org was used for el6 building. As instructions for setting up the machine, I'm just going to take the Fedora guide at http://fedoraproject.org/wiki/Koji/ServerHowTo , and add modifications / comments to suit our setup. Sections taken from the Fedora guide will be between dividers like this: For an overview of yum, mock, Koji (and all its subcomponents), mash, and how they all work together, see the excellent slides put together by Steve Traylen at CERN http://indico.cern.ch/event/55091","title":"Machine setup"},{"location":"infrastructure/koji-initial-install/#packages-to-install","text":"On the server (koji-hub/koji-web) httpd mod_ssl postgresql-server mod_python ( = 3.3.1 for Kerberos authentication) On the builder (koji-builder) mock setarch (for some archs you'll require a patched version) rpm-build createrepo We used one machine for both these roles, so all of the above had to be installed. The koji packages to install are: koji-hub koji-web koji-builder koji-utils","title":"Packages to install"},{"location":"infrastructure/koji-initial-install/#filesystems","text":"","title":"Filesystems"},{"location":"infrastructure/koji-initial-install/#a-note-on-filesystem-space","text":"Koji will consume copious amounts of disk space under the primary KojiDir directory (as set in the kojihub.conf file). This is /mnt/koji by default. Koji keeps all RPMs built in here so a complete history is kept. In addition, repositories are kept here. A repository is just a set of text files, but since a new one is generated for every build, the space will add up. Koji does clean up the repositories once they get old enough. For koji-hub.batlab.org , we allocated a 120G partition for /mnt/koji . However, as koji makes use of mock on the backend to actually create build roots and perform the builds in those build roots, it might come to a surprise to users that a running koji server will consume large amounts of disk space under /var/lib/mock and /var/cache/mock as well. kojid (the builder daemon) will refuse to start a build if it does not have sufficient space under /var/lib/mock . By default, this is an 8G surplus, but we lowered it to 4G for koji-hub.batlab.org . The build roots will be wiped after successful builds, but the build roots for failed builds will be kept around for 2 weeks (?); as a result, disk usage of /var/lib/mock can swing wildly. For koji-hub.batlab.org , we have allocated 35G for /var/lib/mock . /var/cache/mock doesn't use up that much space, so we didn't make a separate partition for it, it just uses what's under / .","title":"A note on filesystem space"},{"location":"infrastructure/koji-initial-install/#authentication","text":"Most of the \"Koji Authentication Selection\" section can be skipped. We got our certs from DigiCert. Note that certs are sensitive to line ending issues -- you may have to run dos2unix on them before they would work. We have 3 certs, with the following subjects: /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=koji-hub.batlab.org (for koji-hub, koji-web, and kojid) /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=kojira/koji-hub.batlab.org (for kojira) /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=kojibuilder1.batlab.org (for kojid on kojibuilder1) kojid needed a different cert than kojira because the two would connect to koji-hub at the same time and log each other off. Copies of the certs exist in AFS at /p/condor/home/certificates/koji-hub.batlab.org/ and /p/condor/home/certificates/kojibuilder1.batlab.org/ Clients connecting to koji-hub need a file containing the CA certs; this is just a concatenation of the DigiCert Grid Root CA and the DigiCert Grid CA-1 certs, which can be obtained from DigiCert's website (get PEM format). We have a digicert-chain.crt in /etc/pki/tls/certs/digicert-chain.crt on koji-hub We made a kojiadmin user, but we didn't make a cert for it so we just used user/pass authentication until we set up our own accounts and then disabled kojiadmin .","title":"Authentication"},{"location":"infrastructure/koji-initial-install/#using-proxy-certs","text":"/etc/sysconfig/httpd needed to be changed to include the following lines: OPENSSL_ALLOW_PROXY = 1 OPENSSL_ALLOW_PROXY_CERTS = 1 export OPENSSL_ALLOW_PROXY export OPENSSL_ALLOW_PROXY_CERTS The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.","title":"Using proxy certs"},{"location":"infrastructure/koji-initial-install/#postgres-database","text":"Install postgres: [root@koji-hub]# yum install postgresql-server Init the db: [root@koji-hub]# su - postgres -c \"PGDATA=/var/lib/pgsql/data\" initdb Start the db: [root@koji-hub]# service postgresql start Make a koji account: [root@koji-hub]# useradd koji; passwd -d koji","title":"Postgres Database"},{"location":"infrastructure/koji-initial-install/#setup-postgresql-and-populate-schema","text":"The following commands will create the koji user within PostgreSQL and will then create the koji database using the schema within the /usr/share/doc/koji*/docs/schema.sql directory root@localhost$ su - postgres postgres@localhost$ createuser koji Shall the new role be a superuser? (y/n) n Shall the new role be allowed to create databases? (y/n) n Shall the new role be allowed to create more new roles? (y/n) n postgres@localhost$ createdb -O koji koji postgres@localhost$ logout root@localhost$ su - koji koji@localhost$ psql koji koji /usr/share/doc/koji*/docs/schema.sql koji@localhost$ exit NOTE: When issuing the command to import the psql schema into the new database it is important to ensure that the directory path /usr/share/doc/koji*/docs/schema.sql remains intact and is not resolved to a specific version of koji. In test it was discovered that when the path is resolved to a specific version of koji then not all of the tables were created correctly. To authorize the koji-web and koji-hub resources, make the following additions to /var/lib/pgsql/data/pg_hba.conf (IP addresses will vary): # access for koji host koji postgres 127.0.0.1/32 trust host koji koji 127.0.0.1/32 trust host koji koji 128.104.100.41/32 trust host koji koji ::1/128 trust Next, we'll need a koji admin user to run some commands. We'll need to add it to the database manually. Once again, we used user/pass authentication for the kojiadmin user until we disabled it. All database commands should be done as the koji user: [root@koji-hub]# sudo -u koji psql koji= insert into users ( name , password , status , usertype ) values ( kojiadmin , some-throwaway-admin-password-in-plain-text , 0 , 0 ); koji= insert into user_perms ( user_id , perm_id , creator_id ) values (( select id from users where name = kojiadmin ), 1 , ( select id from users where name = kojiadmin ));","title":"Setup PostgreSQL and populate schema:"},{"location":"infrastructure/koji-initial-install/#koji-hub","text":"Koji-hub is the center of all Koji operations. It is an XML-RPC server running under mod_python in Apache. koji-hub is passive in that it only receives XML-RPC calls and relies upon the build daemons and other components to initiate communication. Koji-hub is the only component that has direct access to the database and is one of the two components that have write access to the file system. Config files we care about: /etc/httpd/conf/httpd.conf /etc/httpd/conf.d/kojihub.conf /etc/httpd/conf.d/ssl.conf /etc/koji-hub/hub.conf Install the necessary packages. [root@koji-hub]# yum install koji-hub httpd mod_ssl mod_python","title":"Koji-Hub"},{"location":"infrastructure/koji-initial-install/#etchttpdconfhttpdconf","text":"The apache web server has two places that it sets maximum requests a server will handle before the server restarts. The xmlrpc interface in kojihub is a python application, and mod_python can sometimes grow outrageously large when it doesn't reap memory often enough. As a result, it is strongly recommended that you set both instances of MaxRequestsPerChild in httpd.conf to something reasonable in order to prevent the server from becoming overloaded and crashing (at 100 the httpd processes will grow to about 75MB resident set size before respawning). IfModule prefork.c ... MaxRequestsPerChild 100 /IfModule IfModule worker.c ... MaxRequestsPerChild 100 /IfModule","title":"/etc/httpd/conf/httpd.conf:"},{"location":"infrastructure/koji-initial-install/#etckoji-hubhubconf","text":"This file contains the configuration information for the hub. You will need to edit this configuration to point Koji Hub to the database you are using and to setup Koji Hub to utilize the authentication scheme you selected in the beginning. We made multiple changes to this config file. Notable changes: We turned off LoginCreatesUser KojiWebURL is http://koji-hub.batlab.org/koji PluginPath is /usr/lib/koji-hub-plugins Plugins = sign since there is an RPM signing plugin (package name: koji-plugin-sign ) that we use Other notes: Should not be world-readable Must be readable by the apache user though Finally, there is a [policy] section for controlling ACLs. The contents of that are described later in this document.","title":"/etc/koji-hub/hub.conf:"},{"location":"infrastructure/koji-initial-install/#etchttpdconfdkojihubconf","text":"If using SSL auth, uncomment these lines for kojiweb to allow logins. Location /kojihub SSLOptions +StdEnvVars /Location This is actually outdated information, useful for Koji 1.4.0. Instead, we add the following: Location /kojihub/ssllogin SSLVerifyClient require SSLVerifyDepth 10 SSLOptions +StdEnvVars /Location","title":"/etc/httpd/conf.d/kojihub.conf:"},{"location":"infrastructure/koji-initial-install/#etchttpdconfdsslconf","text":"If using SSL you will also need to add the needed SSL options for apache. These options should point to where the certificates are located on the hub. These are the config lines we use: ## The host cert for koji-hub SSLCertificateFile /etc/pki/tls/certs/digicert_hostcert.crt ## The private key for koji-hub SSLCertificateKeyFile /etc/pki/tls/private/digicert_hostkey.key ## The concatenation of the DigiCert CA certs we made above. SSLCertificateChainFile /etc/pki/tls/certs/digicert-chain.crt ## All the CA certs on the system. SSLCACertificateFile /etc/pki/tls/certs/ca-bundle.crt SSLVerifyClient require SSLVerifyDepth 10 Restart httpd after doing this.","title":"/etc/httpd/conf.d/ssl.conf:"},{"location":"infrastructure/koji-initial-install/#adding-a-real-admin-user","text":"At this point, SSL should be set up on Koji Hub. Create an account for yourself (your CN) and give yourself the admin bit: [kojiadmin@koji-hub]$ koji add-user Your Name 123456 [kojiadmin@koji-hub]$ koji grant-permission admin Your Name 123456 Once you're done with all the setup , you should remove the admin bit from kojiadmin , block the user, and remove its password from the database. [you@koji-hub]$ koji revoke-permission admin kojiadmin [you@koji-hub]$ koji disable-user kojiadmin In the database: koji= update users set password = null where name = kojiadmin ;","title":"Adding a real admin user"},{"location":"infrastructure/koji-initial-install/#koji-filesystem","text":"KojiDir is /mnt/koji , so the following tree should be created: [root@koji-hub]# mkdir -p /mnt/koji/ { packages,repos,work,scratch } [root@koji-hub]# chown apache:apache *","title":"Koji filesystem"},{"location":"infrastructure/koji-initial-install/#accounts-for-kojira-and-kojid","text":"It is important to note that the kojira component needs repo privileges, but if you just let the account get auto created the first time you run kojira, it won't have that privilege, so you should pre-create the account and grant it the repo privilege now. kojiadmin@koji-hub$ koji add-user kojira kojiadmin@koji-hub$ koji grant-permission repo kojira For similar technical reasons, you need to add-host each build host prior to starting kojid on that host the first time and could also do that now. We turned off auto account creation, so there won't be a problem with accounts with bad perms getting created. This means they have to be created manually. We have kojid running on koji-hub, so this is the command we used to add it: [you@koji-hub]$ koji add-host koji-hub.batlab.org i386 x86_64","title":"Accounts for kojira and kojid"},{"location":"infrastructure/koji-initial-install/#koji-web-kojid","text":"Installing: [root@koji-hub]# yum install koji-web mod_ssl [root@koji-hub]# yum install koji-builder Note that we use a patched koji-builder, so be sure to install what's in the OSG repository. (The patch is not likely to be accepted upstream).","title":"Koji-Web, Kojid"},{"location":"infrastructure/koji-initial-install/#ssl-certs","text":"The following files should be in the /etc/pki/tls tree: /etc/pki/tls/private/digicert_hostkey.key the host key from DigiCert /etc/pki/tls/private/kojiweb.pem digicert_hostcert.crt catted together with digicert_hostkey.key (public key first) /etc/pki/tls/private/kojira.key kojira's private key from DigiCert /etc/pki/tls/private/kojira.pem kojira.crt catted together with kojira.key (public key first) /etc/pki/tls/cert.pem symlink to certs/ca-bundle.crt /etc/pki/tls/certs/ca-bundle.crt all certs available on the system; the DigiCert Grid certs should be in here /etc/pki/tls/certs/kojira.crt kojira's cert /etc/pki/tls/certs/digicert_hostcert.crt the host cert from DigiCert /etc/pki/tls/certs/digicert-chain.crt the DigiCert CAs catted together","title":"SSL certs"},{"location":"infrastructure/koji-initial-install/#adding-koji-builder-host-to-database","text":"[you@koji-hub]$ koji add-host-to-channel koji-hub.batlab.org createrepo In the database: koji= # update host set capacity = 4 where name = koji-hub.batlab.org ;","title":"Adding koji-builder host to database"},{"location":"infrastructure/koji-initial-install/#start-kojid","text":"[root@koji-hub]# /sbin/service kojid start Check /var/log/kojid.log to make sure everything started up.","title":"Start kojid"},{"location":"infrastructure/koji-initial-install/#kojira","text":"Installing: [root@koji-hub]# yum install koji-utils Kojira also needs a user [you@koji-hub]$ koji add-user kojira [you@koji-hub]$ koji grant-permission repo kojira Start it up [root@koji-hub]# /sbin/service kojira start","title":"Kojira"},{"location":"infrastructure/koji-policy-writing/","text":"Koji permissions and policy These are some notes I wrote on Koji ACLs/policy after doing some source diving in the Koji code. The version of Koji was 1.6.0. I later found some documentation at https://docs.pagure.org/koji/defining_hub_policies/ . Go read it, that page has better examples. Default policies (defined in hub/kojixmlrpc.py) build_from_srpm = has_perm admin :: allow all :: deny build_from_repo_id = has_perm admin :: allow all :: deny channel = has req_channel :: req is_child_task :: parent all :: use default package_list = has_perm admin :: allow all :: deny vm = has_perm admin win-admin :: allow all :: deny If MissingPolicyOk is true (default), then policies that do not exist default to \"allow\". Policy syntax Policies are set in the [policy] section of /etc/koji-hub/hub.conf Each policy definition starts with policy_name = The rest of the definition is indented. The lines in the policy definition have the following format: Simple tests: test [params] [ test [params] ...] :: action-if-true test [params] [ test [params] ...] !! action-if-false Complex tests: test [params [ ...]] :: { test [params [ ...]] :: action test [params [ ...]] :: { ... } } The following generic tests are defined in koji/policy.py : true / all always true false / none always false has FIELD true if policy data contains a field called FIELD bool FIELD true if FIELD is true match FIELD PATTERN1 [PATTERN2 ...] true if FIELD matches any of the patterns (globs) compare FIELD OP NUMBER compare FIELD against a number. OP can be , , =, =, =, != the following koji-specific tests are defined in hub/kojihub.py : buildtag PATTERN1 [PATTERN2 ...] true if the build tag of a build matches a pattern fromtag PATTERN1 [PATTERN2 ...] true if the tag we're moving a package from matches a pattern has_perm PATTERN1 [PATTERN2 ...] true if user has any matching permission hastag TAG true if the build has the tag TAG imported true if the build was imported is_build_owner true if the user doing this task owns the build is_child_task true if the task is a child of some other task is_new_package true if the package being looked at is new (i.e. doesn't have an 'id' yet) method PATTERN1 [PATTERN2 ...] true if the method matches a pattern operation PATTERN1 [PATTERN2 ...] true if current operation matches any of the patterns package PATTERN1 [PATTERN2 ...] true if the package name matches any of the patterns policy POLICY true if the named policy is true skip_tag true if the skip_tag option is true source PATTERN1 [PATTERN2 ...] true if source matches patterns tag PATTERN1 [PATTERN2 ...] true if the tag name matches any of the patterns user PATTERN1 [PATTERN2 ...] true if username matches a pattern user_in_group PATTERN1 [PATTERN2 ...] true if the user is in any matching group vm_name PATTERN1 [PATTERN2 ...] true if vm name matches a pattern The actions are: allow allow the action deny deny the action req ? parent ? use default ? Default permissions These are the permissions that people can be given in koji: admin build repo livecd maven-import win-import win-admin appliance Additional permissions can be added with grant-permission --new , see below. The following permissions are checked by name in the koji command-line utility (i.e. policies are not used): admin : add-group , add-tag , add-target , clone-tag , edit-target , remove-tag , remove-target , wrapper-rpm maven-import : import-archive with the --type=maven option win-import : import-archive with the --type=win option repo : regen-repo I haven't found out where some of the other permissions are used. Adding permissions You can give someone a permission that doesn't exist by doing: osg-koji grant-permission --new PERMISSION USER Doing so creates the permission as a side effect. I'm not aware of a way to create the permission without granting it, other than with database hackery. Where policies are used and what policy data is passed on: build_from_srpm source: builder/kojid:BuildTask.handler used when source url points to an SRPM (as opposed to an scm) and the build is not a scratch build. policy data: user_id the owner of the task source the url of the source file task_id the id of the task build_tag the id of the build tag skip_tag true if we're not tagging this build ( --scratch or --skip-tag passed on the command line) target the build target (only if we have one?) tag the destination tag (only if skip_tag is false) build_from_repo_id source: builder/kojid:BuildTask.handler used when the --repo-id option is passed to koji build policy data: same as build_from_srpm package_list source: hub/kojihub.py:pkglist_add add-pkg , block-pkg , set-pkg-arches , set-pkg-owner commands policy data: action 'add', 'update', 'block' depending on what is being done force true if --force is passed on the command line package package info (the id I think?) tag the id of the tag we're trying to add the package to/package is in source: hub/kojihub.py:pkglist_remove used internally by the koji clone-tag command? policy data: same as above, except action is 'remove' source: hub/kojihub.py:pkglist_unblock unblock-pkg command policy data: same as above, except action is 'unblock' tag Note RootExports is the class containing functions exported via XMLRPC. In general, each function corresponds to a koji task. source: hub/kojihub.py:RootExports.tagBuild tagging builds policy data: build the id of the build fromtag the id of the tag we're moving the build from, if there is one operation 'tag' or 'move' tag the id of the tag source: hub/kojihub.py:RootExports.untagBuild untagging builds policy data: same as above, except operation is 'untag', and tag is None source: hub/kojihub.py:RootExports.moveAllBuilds moving all builds of a package from tag1 to tag2 policy data: same as for tagBuild , except operation is 'move'. The policy is checked once for each build being moved. source: hub/kojihub.py:HostExports.tagBuild tagging builds (\"host version\" ?) policy data: same as for tagBuild , plus user_id vm source: hub/kojihub.py:RootExports.winBuild windows builds in a vm ( win-build command) policy data: tag the destination tag vm_name the name of the vm Examples Let people with the \"build\" permission also add packages and build SRPMs package_list = has_perm admin :: allow has_perm build match action add update :: allow all :: deny build_from_srpm = has_perm admin build :: allow all :: deny Promotion policy for different teams Software team members can tag any package as testing/release. Operations team members can tag vo-clients as testing/release. Security team members can tag CA packages as testing/release. promotion = has_perm software-team :: allow has_perm operations-team package vo-client :: allow has_perm security-team package *-ca-certs* :: allow all :: deny tag = has_perm admin :: allow operation tag :: { tag *testing *release* policy promotion :: allow tag *testing *release* !! allow } operation untag :: { fromtag *testing *release* policy promotion :: allow fromtag *testing *release* !! allow } operation move :: { tag *testing *release* policy promotion :: allow fromtag *testing *release* policy promotion :: allow tag *testing *release* !! { fromtag *testing *release* !! allow } } all :: deny","title":"Koji Policy Writing"},{"location":"infrastructure/koji-policy-writing/#koji-permissions-and-policy","text":"These are some notes I wrote on Koji ACLs/policy after doing some source diving in the Koji code. The version of Koji was 1.6.0. I later found some documentation at https://docs.pagure.org/koji/defining_hub_policies/ . Go read it, that page has better examples.","title":"Koji permissions and policy"},{"location":"infrastructure/koji-policy-writing/#default-policies-defined-in-hubkojixmlrpcpy","text":"build_from_srpm = has_perm admin :: allow all :: deny build_from_repo_id = has_perm admin :: allow all :: deny channel = has req_channel :: req is_child_task :: parent all :: use default package_list = has_perm admin :: allow all :: deny vm = has_perm admin win-admin :: allow all :: deny If MissingPolicyOk is true (default), then policies that do not exist default to \"allow\".","title":"Default policies (defined in hub/kojixmlrpc.py)"},{"location":"infrastructure/koji-policy-writing/#policy-syntax","text":"Policies are set in the [policy] section of /etc/koji-hub/hub.conf Each policy definition starts with policy_name = The rest of the definition is indented. The lines in the policy definition have the following format: Simple tests: test [params] [ test [params] ...] :: action-if-true test [params] [ test [params] ...] !! action-if-false Complex tests: test [params [ ...]] :: { test [params [ ...]] :: action test [params [ ...]] :: { ... } } The following generic tests are defined in koji/policy.py : true / all always true false / none always false has FIELD true if policy data contains a field called FIELD bool FIELD true if FIELD is true match FIELD PATTERN1 [PATTERN2 ...] true if FIELD matches any of the patterns (globs) compare FIELD OP NUMBER compare FIELD against a number. OP can be , , =, =, =, != the following koji-specific tests are defined in hub/kojihub.py : buildtag PATTERN1 [PATTERN2 ...] true if the build tag of a build matches a pattern fromtag PATTERN1 [PATTERN2 ...] true if the tag we're moving a package from matches a pattern has_perm PATTERN1 [PATTERN2 ...] true if user has any matching permission hastag TAG true if the build has the tag TAG imported true if the build was imported is_build_owner true if the user doing this task owns the build is_child_task true if the task is a child of some other task is_new_package true if the package being looked at is new (i.e. doesn't have an 'id' yet) method PATTERN1 [PATTERN2 ...] true if the method matches a pattern operation PATTERN1 [PATTERN2 ...] true if current operation matches any of the patterns package PATTERN1 [PATTERN2 ...] true if the package name matches any of the patterns policy POLICY true if the named policy is true skip_tag true if the skip_tag option is true source PATTERN1 [PATTERN2 ...] true if source matches patterns tag PATTERN1 [PATTERN2 ...] true if the tag name matches any of the patterns user PATTERN1 [PATTERN2 ...] true if username matches a pattern user_in_group PATTERN1 [PATTERN2 ...] true if the user is in any matching group vm_name PATTERN1 [PATTERN2 ...] true if vm name matches a pattern The actions are: allow allow the action deny deny the action req ? parent ? use default ?","title":"Policy syntax"},{"location":"infrastructure/koji-policy-writing/#default-permissions","text":"These are the permissions that people can be given in koji: admin build repo livecd maven-import win-import win-admin appliance Additional permissions can be added with grant-permission --new , see below. The following permissions are checked by name in the koji command-line utility (i.e. policies are not used): admin : add-group , add-tag , add-target , clone-tag , edit-target , remove-tag , remove-target , wrapper-rpm maven-import : import-archive with the --type=maven option win-import : import-archive with the --type=win option repo : regen-repo I haven't found out where some of the other permissions are used.","title":"Default permissions"},{"location":"infrastructure/koji-policy-writing/#adding-permissions","text":"You can give someone a permission that doesn't exist by doing: osg-koji grant-permission --new PERMISSION USER Doing so creates the permission as a side effect. I'm not aware of a way to create the permission without granting it, other than with database hackery.","title":"Adding permissions"},{"location":"infrastructure/koji-policy-writing/#where-policies-are-used-and-what-policy-data-is-passed-on","text":"","title":"Where policies are used and what policy data is passed on:"},{"location":"infrastructure/koji-policy-writing/#build95from95srpm","text":"source: builder/kojid:BuildTask.handler used when source url points to an SRPM (as opposed to an scm) and the build is not a scratch build. policy data: user_id the owner of the task source the url of the source file task_id the id of the task build_tag the id of the build tag skip_tag true if we're not tagging this build ( --scratch or --skip-tag passed on the command line) target the build target (only if we have one?) tag the destination tag (only if skip_tag is false)","title":"build_from_srpm"},{"location":"infrastructure/koji-policy-writing/#build95from95repo95id","text":"source: builder/kojid:BuildTask.handler used when the --repo-id option is passed to koji build policy data: same as build_from_srpm","title":"build_from_repo_id"},{"location":"infrastructure/koji-policy-writing/#package95list","text":"source: hub/kojihub.py:pkglist_add add-pkg , block-pkg , set-pkg-arches , set-pkg-owner commands policy data: action 'add', 'update', 'block' depending on what is being done force true if --force is passed on the command line package package info (the id I think?) tag the id of the tag we're trying to add the package to/package is in source: hub/kojihub.py:pkglist_remove used internally by the koji clone-tag command? policy data: same as above, except action is 'remove' source: hub/kojihub.py:pkglist_unblock unblock-pkg command policy data: same as above, except action is 'unblock'","title":"package_list"},{"location":"infrastructure/koji-policy-writing/#tag","text":"Note RootExports is the class containing functions exported via XMLRPC. In general, each function corresponds to a koji task. source: hub/kojihub.py:RootExports.tagBuild tagging builds policy data: build the id of the build fromtag the id of the tag we're moving the build from, if there is one operation 'tag' or 'move' tag the id of the tag source: hub/kojihub.py:RootExports.untagBuild untagging builds policy data: same as above, except operation is 'untag', and tag is None source: hub/kojihub.py:RootExports.moveAllBuilds moving all builds of a package from tag1 to tag2 policy data: same as for tagBuild , except operation is 'move'. The policy is checked once for each build being moved. source: hub/kojihub.py:HostExports.tagBuild tagging builds (\"host version\" ?) policy data: same as for tagBuild , plus user_id","title":"tag"},{"location":"infrastructure/koji-policy-writing/#vm","text":"source: hub/kojihub.py:RootExports.winBuild windows builds in a vm ( win-build command) policy data: tag the destination tag vm_name the name of the vm","title":"vm"},{"location":"infrastructure/koji-policy-writing/#examples","text":"","title":"Examples"},{"location":"infrastructure/koji-policy-writing/#let-people-with-the-build-permission-also-add-packages-and-build-srpms","text":"package_list = has_perm admin :: allow has_perm build match action add update :: allow all :: deny build_from_srpm = has_perm admin build :: allow all :: deny","title":"Let people with the \"build\" permission also add packages and build SRPMs"},{"location":"infrastructure/koji-policy-writing/#promotion-policy-for-different-teams","text":"Software team members can tag any package as testing/release. Operations team members can tag vo-clients as testing/release. Security team members can tag CA packages as testing/release. promotion = has_perm software-team :: allow has_perm operations-team package vo-client :: allow has_perm security-team package *-ca-certs* :: allow all :: deny tag = has_perm admin :: allow operation tag :: { tag *testing *release* policy promotion :: allow tag *testing *release* !! allow } operation untag :: { fromtag *testing *release* policy promotion :: allow fromtag *testing *release* !! allow } operation move :: { tag *testing *release* policy promotion :: allow fromtag *testing *release* policy promotion :: allow tag *testing *release* !! { fromtag *testing *release* !! allow } } all :: deny","title":"Promotion policy for different teams"},{"location":"infrastructure/koji-restore-recipe/","text":"How to Restore Koji This document contains recipes on how to restore the Koji services and the database they require. It is divided into two sections: one for the database (to be done if something happens to db-01 ), and one for the server hosting the koji services (to be done if something happens to koji.chtc ). In case both the database and the hub need to be restored, the database should be restored first. Background information Backups of koji.chtc.wisc.edu and db-01.batlab.org are on host-3.chtc.wisc.edu in /export/backup/ DATE . That machine is in WID. (Same room as koji.chtc itself, which is why we have offsite backups). It's a homebrew rsync-based backup system. (Not our home -- Nate told me it was written for Midwest Tier 2.) They go back up to a week, with a monthly snapshot for a year. Setting up your environment For all of these steps, you will need a root shell on host-3.chtc.wisc.edu and have the following environment variables defined: NEWDB= FQDN OF NEW DATABASE SERVER NEWKOJI= FQDN OF NEW KOJI HOST DATE= YYYY-MM-DD DATE OF MOST RECENT GOOD BACKUP DBBACKUP=/export/backup/$DATE/db-01.batlab.org KOJIBACKUP=/export/backup/$DATE/koji.chtc.wisc.edu RSYNC= rsync --archive --hard-links --verbose Restoring the database The entire filesystem of db-01 is backed up -- this includes all of /var/lib/pgsql , including the database as-is. In theory, this means that we could just rsync all the files to a blank hard drive, boot up, and we'd have a db-01 again. However, the Postgres manual warns against restoring the database from a filesystem backup that was made while the database was live, and we do not shut down the database before backups. We might be able to restore every other part of the filesystem besides the database, which would speed up the overall restoration process, but only the fresh install was tested. The new database server is called newdb in these instructions. Restoring Services Prerequisites for newdb : an EL 6+ host with an SSH server set up and accessible (as root) from host-3.chtc.wisc.edu # # On newdb: # # Install postgres, get a blank DB up and create the user that koji # # will be using. [root@newdb]# yum install -y postgresql-server [root@newdb]# service postgresql initdb [root@newdb]# useradd -r -m koji # # Make a directory we ll put the restored files into. [root@newdb]# mkdir -p /root/dbrestore # # On host-3: [you@host-3]$ sudo $RSYNC $DBBACKUP /homefs/ $NEWDB :/root/dbrestore/home [you@host-3]$ for dir in root etc var ; do \\ sudo $RSYNC $DBBACKUP /rootfs/ $dir / \\ $NEWDB :/root/dbrestore/ $dir \\ done Continue on to the next section Restoring Database Contents Assumes you have restored the /var directory from backup into /root/dbrestore/var . Restore the postgres config files so the koji-hub daemon can log in: # # On newdb: [root@newdb]# service postgresql stop [root@newdb]# cp -a /root/dbrestore/var/lib/pgsql/data/ { *.conf,postmaster.opts } \\ /var/lib/pgsql/data/ Edit /var/lib/pgsql/data/pg_hba.conf . There are lines like: # Koji-hub IPv4: host koji koji 128.104.100.41/32 md5 Change the IP address to the public IP address of the host that will serve as the new hub. Restore the actual database: # # On newdb: [root@newdb]# chown -R postgres:postgres /var/lib/pgsql/* [root@newdb]# service postgresql start [root@newdb]# gunzip -c /root/dbrestore/var/lib/pgsql-backup/postgres-db-01.sql.gz | psql -U postgres postgres Validation Do the following tests to make sure the database is ready to use: Test that the contents got properly restored: [root@newdb]# psql -U koji koji koji koji= select * from users ; koji= select * from build order by id desc limit 10 ; Test logging in as the koji user: [root@newdb]# psql -U koji -h newdb koji (you must use the FQDN of newdb , not localhost ). Be sure you get prompted for a password, and the password from /etc/koji-hub/hub.conf works. Continue to \"Restoring Koji\" if needed, otherwise skip to \"Starting Services and Validation\" Restoring Koji Both the root filesystem of koji.chtc and /mnt/koji are backed up. The root filesystem backups are in the rootfs subdirectory of /export/backup/$DATE/koji.chtc.wisc.edu and the backups of /mnt/koji are in the kojifs subdirectory. The following instructions show how to restore the critical components of Koji onto a new machine. In the instructions, the new host will be named newkoji . Installing the OS Prerequisites for newkoji : an EL 6 host with an SSH server set up and accessible (as root) from host-3.chtc.wisc.edu (This recipe was tested for EL 6, on the same machine as newdb ). Install EPEL and OSG repos: [root@newkoji]# rpm -Uvh \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm \\ https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm [root@newkoji]# yum install -y yum-plugin-priorities Edit /etc/yum.repos.d/osg-*development.repo : Enable the development repo Add includepkg=koji* to the definition for the development repo Go through the other repo files and make sure that EPEL and OS priorities are worse than 98. This means absent or numerically greater. Especially look at cobbler-config.repo if it exists. Install the koji packages and dependencies, making sure the koji packages themselves come from osg: [root@newkoji]# yum install koji koji-builder koji-hub koji-plugin-sign \\ koji-theme-fedora koji-utils koji-web mod_ssl postgresql Mount /mnt/koji if necessary Restore the contents of the koji filesystem. On host-3 : # # At a minimum, you must restore the /mnt/koji/packages directory [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/packages/ $NEWKOJI :/mnt/koji/packages # # The other directories are optional, though it saves a lot of time to restore /mnt/koji/repos [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/repos/ $NEWKOJI :/mnt/koji/repos [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/work/ $NEWKOJI :/mnt/koji/work [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/scratch/ $NEWKOJI :/mnt/koji/scratch # # Any dirs you did not restore should be created. Fix permissions if needed. On newkoji : [root@newkoji]# chown -R apache:apache /mnt/koji/ { packages,repos,work,scratch } [root@newkoji]# chmod 0755 /mnt/koji/ { packages,repos,work,scratch } Continue on to the next section Restoring Configuration On newkoji , define the shell function dirclone , listed below: dirclone () { srcdir = $( dirname $1 ) / $( basename $1 ) destdir = $( dirname $2 ) / $( basename $2 ) mkdir -p $( dirname $2 ) rsync --archive --delete-after --acls --xattrs \\ --partial --partial-dir = .rsync-partial \\ $srcdir / $destdir } On newkoji : [root@newkoji]# mkdir -p /root/hubrestore On host-3 : [you@host-3]$ sudo $RSYNC $KOJIBACKUP /rootfs/ { root,home,etc } $NEWKOJI :/root/hubrestore/ [you@host-3]$ sudo $RSYNC $KOJIBACKUP /varfs/ $NEWKOJI :/root/hubrestore/var/ On newkoji , install some utils we will need later: [root@newkoji]# yum install -y dos2unix vim-enhanced (vim-enhanced is used for vimdiff) On newkoji : # # Restore some of the directories in /etc: [root@newkoji]# while read subtree ; do dirclone /root/hubrestore/etc/$subtree /etc/$subtree done ___END___ httpd kojid koji-hub kojira koji-sign-plugin kojiweb mock pki/koji pki/tls/certs pki/tls/private ___END___ # # Restore some of the files: [root@newkoji]# cp -a /root/hubrestore/etc/koji.conf /etc/ [root@newkoji]# cp -a /root/hubrestore/etc/sysconfig/ { httpd,kojid,kojira } /etc/sysconfig/ Restore users and home directories If newkoji is on a separate host from newdb , then just simply copy over the files: [root@newkoji]# dirclone /root/hubrestore/home /home [root@newkoji]# cp -a /root/hubrestore/etc/ { passwd,shadow,group,gshadow } /etc If newkoji is on the same host as newdb , then you will have to be more careful: # # Skip home directories for the special users [root@newkoji]# for dir in /root/hubrestore/home/* ; do bndir=$(basename $dir ) if [[ $bndir = koji $bndir = postgres ]]; then dirclone $dir /home/ $bndir fi done # # Now merge the passwd, group, shadow, and gshadow files in /etc. # # Make sure that your editor does not create backup files # # ( set nobackup in vim), and that shadow and gshadow are owned by # # root and have 0400 permissions. Ensure a 'koji' user exists Fix dirs in /var : [root@newkoji]# rm -rf /var/lib/mock/* [root@newkoji]# chown root:mock /var/lib/mock [root@newkoji]# chmod 2775 /var/lib/mock Restore /var/www/html and /var/spool/cron (TODO) /var should have been backed up, but in case it isn't, the following files need to exist in /var/www/html : A symlink mnt - /mnt A robots.txt with contents User-agent: * Disallow: / Fixing Names This section should be done if newdb or newkoji do not have the same as the previous db server and hub (i.e. db-01.batlab.org and koji.chtc.wisc.edu ). This section should be completed on newkoji . Fixing config files if newdb was renamed The only change that's needed if newdb was renamed is to /etc/koji-hub/hub.conf . Edit that file and change the DBHost line to point to the new hostname. After editing, make sure hub.conf is owned by root:apache and chmodded 0640. Installing new cert/key pairs for newkoji You will need a cert/key pair for the new hostname. Run dos2unix on all cert and key files before using them. Define the shell function makepem , listed below. makepem combines a public and private keypair to make a .pem file that the Koji services use. Usage: makepem CERTFILE KEYFILE OUTPUT_FILE makepem () { certfile = $1 keyfile = $2 outputfile = $3 ( set -e keymodulus = $( openssl rsa -noout -modulus -in $keyfile ) certmodulus = $( openssl x509 -noout -modulus -in $certfile ) if [[ $keymodulus = $certmodulus ]] ; then echo keyfile and certfile do not match ; return 1 fi if [[ -f $outputfile ]] ; then mv -f $outputfile { ,.bak } fi ( dos2unix $certfile ; echo ; dos2unix $keyfile ) $outputfile chmod 0600 $outputfile ) } Place cert and key files into the following paths: host cert /root/hostcert.pem host key /root/hostkey.pem Then use makepem to combine the certs and put them in the proper locations. [root@newkoji]# makepem /root/hostcert.pem /root/hostkey.pem \\ /etc/pki/tls/private/kojiweb.pem [root@newkoji]# chown apache:apache /etc/pki/tls/private/kojiweb.pem In addition, copy the host cert and key into the locations HTTPD expects it them. [root@newkoji]# cp -a /root/hostcert.pem /etc/pki/tls/certs/hostcert.pem [root@newkoji]# cp -a /root/hostkey.pem /etc/pki/tls/private/hostkey.pem Fixing hostname in config files Use sed to replace the hostname in the following config files in /etc: /etc/kojira/kojira.conf /etc/koji.conf /etc/koji-hub/hub.conf /etc/httpd/conf.d/kojiweb.conf /etc/httpd/conf/httpd.conf /etc/kojid/kojid.conf You will need to fix /etc/kojid/kojid.conf on all builder machines as well (e.g. kojibuilder2.chtc.wisc.edu ). Fixing hostname in database You will need to find and fix entries that contain the hostname in the following tables: host (should be 1 entry) users (should be 2 entries, one for the host, and one for the kojira user) Fixing hostname elsewhere These steps are only necessary if you cannot get a DNS Canonical Name (CN) record such that koji.chtc.wisc.edu resolves to newkoji . Update the repo definitions in the osg-release package Update the mash script(s) at the GOC Mail the software team and users that anyone using the minefield repos will need to update osg-release Fix all the build machines to point to the new name Fix the following files in osg-build and make a new release data/osg-koji-home.conf data/osg-koji-site.conf osgbuild/constants.py osgbuild/kojiinter.py Mail people that they will need to update osg-build and rerun osg-koji setup Starting Services and Validation Now you will start up Koji services and verify that they function. Prerequisite: previous restore steps have been completed and postgresql is running on the database host. All steps will be run on newkoji . Start the main koji daemon: [root@newkoji]# service httpd start Use ps to verify that it came up Connect to the web interface in your browser. Make sure you can use https and you can log in. As yourself, run the koji command-line tool and make a few queries (e.g. list-tags) Start the koji build daemon: [root@newkoji]# service kojid start Use ps to verify that it came up If you did not restore the /mnt/koji/repos directory, you will now need to regenerate the build repos. Use koji list-tags to get a list of tags and run koji regen-repo on all of the ones with -build in the name. This will take several hours. You will also need to regen the -development repos so that minefield works again. Keep an eye on the tasks in the web interface to make sure they are getting farmed out to the right hosts. Try a scratch build Start kojira : [root@newkoji]# service kojira start Use ps to verify that it came up Wait half a minute and use ps to verify that kojid is still up; the two processes can kick each other off if they are both using the same certificate Bump a package if needed and try a real, non-scratch build Make sure that kojira is regenerating the repos If you have updated osg-release and/or osg-build , rebuild those packages now.","title":"Koji Restore Recipe"},{"location":"infrastructure/koji-restore-recipe/#how-to-restore-koji","text":"This document contains recipes on how to restore the Koji services and the database they require. It is divided into two sections: one for the database (to be done if something happens to db-01 ), and one for the server hosting the koji services (to be done if something happens to koji.chtc ). In case both the database and the hub need to be restored, the database should be restored first.","title":"How to Restore Koji"},{"location":"infrastructure/koji-restore-recipe/#background-information","text":"Backups of koji.chtc.wisc.edu and db-01.batlab.org are on host-3.chtc.wisc.edu in /export/backup/ DATE . That machine is in WID. (Same room as koji.chtc itself, which is why we have offsite backups). It's a homebrew rsync-based backup system. (Not our home -- Nate told me it was written for Midwest Tier 2.) They go back up to a week, with a monthly snapshot for a year.","title":"Background information"},{"location":"infrastructure/koji-restore-recipe/#setting-up-your-environment","text":"For all of these steps, you will need a root shell on host-3.chtc.wisc.edu and have the following environment variables defined: NEWDB= FQDN OF NEW DATABASE SERVER NEWKOJI= FQDN OF NEW KOJI HOST DATE= YYYY-MM-DD DATE OF MOST RECENT GOOD BACKUP DBBACKUP=/export/backup/$DATE/db-01.batlab.org KOJIBACKUP=/export/backup/$DATE/koji.chtc.wisc.edu RSYNC= rsync --archive --hard-links --verbose","title":"Setting up your environment"},{"location":"infrastructure/koji-restore-recipe/#restoring-the-database","text":"The entire filesystem of db-01 is backed up -- this includes all of /var/lib/pgsql , including the database as-is. In theory, this means that we could just rsync all the files to a blank hard drive, boot up, and we'd have a db-01 again. However, the Postgres manual warns against restoring the database from a filesystem backup that was made while the database was live, and we do not shut down the database before backups. We might be able to restore every other part of the filesystem besides the database, which would speed up the overall restoration process, but only the fresh install was tested. The new database server is called newdb in these instructions.","title":"Restoring the database"},{"location":"infrastructure/koji-restore-recipe/#restoring-services","text":"Prerequisites for newdb : an EL 6+ host with an SSH server set up and accessible (as root) from host-3.chtc.wisc.edu # # On newdb: # # Install postgres, get a blank DB up and create the user that koji # # will be using. [root@newdb]# yum install -y postgresql-server [root@newdb]# service postgresql initdb [root@newdb]# useradd -r -m koji # # Make a directory we ll put the restored files into. [root@newdb]# mkdir -p /root/dbrestore # # On host-3: [you@host-3]$ sudo $RSYNC $DBBACKUP /homefs/ $NEWDB :/root/dbrestore/home [you@host-3]$ for dir in root etc var ; do \\ sudo $RSYNC $DBBACKUP /rootfs/ $dir / \\ $NEWDB :/root/dbrestore/ $dir \\ done Continue on to the next section","title":"Restoring Services"},{"location":"infrastructure/koji-restore-recipe/#restoring-database-contents","text":"Assumes you have restored the /var directory from backup into /root/dbrestore/var . Restore the postgres config files so the koji-hub daemon can log in: # # On newdb: [root@newdb]# service postgresql stop [root@newdb]# cp -a /root/dbrestore/var/lib/pgsql/data/ { *.conf,postmaster.opts } \\ /var/lib/pgsql/data/ Edit /var/lib/pgsql/data/pg_hba.conf . There are lines like: # Koji-hub IPv4: host koji koji 128.104.100.41/32 md5 Change the IP address to the public IP address of the host that will serve as the new hub. Restore the actual database: # # On newdb: [root@newdb]# chown -R postgres:postgres /var/lib/pgsql/* [root@newdb]# service postgresql start [root@newdb]# gunzip -c /root/dbrestore/var/lib/pgsql-backup/postgres-db-01.sql.gz | psql -U postgres postgres","title":"Restoring Database Contents"},{"location":"infrastructure/koji-restore-recipe/#validation","text":"Do the following tests to make sure the database is ready to use: Test that the contents got properly restored: [root@newdb]# psql -U koji koji koji koji= select * from users ; koji= select * from build order by id desc limit 10 ; Test logging in as the koji user: [root@newdb]# psql -U koji -h newdb koji (you must use the FQDN of newdb , not localhost ). Be sure you get prompted for a password, and the password from /etc/koji-hub/hub.conf works. Continue to \"Restoring Koji\" if needed, otherwise skip to \"Starting Services and Validation\"","title":"Validation"},{"location":"infrastructure/koji-restore-recipe/#restoring-koji","text":"Both the root filesystem of koji.chtc and /mnt/koji are backed up. The root filesystem backups are in the rootfs subdirectory of /export/backup/$DATE/koji.chtc.wisc.edu and the backups of /mnt/koji are in the kojifs subdirectory. The following instructions show how to restore the critical components of Koji onto a new machine. In the instructions, the new host will be named newkoji .","title":"Restoring Koji"},{"location":"infrastructure/koji-restore-recipe/#installing-the-os","text":"Prerequisites for newkoji : an EL 6 host with an SSH server set up and accessible (as root) from host-3.chtc.wisc.edu (This recipe was tested for EL 6, on the same machine as newdb ). Install EPEL and OSG repos: [root@newkoji]# rpm -Uvh \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm \\ https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm [root@newkoji]# yum install -y yum-plugin-priorities Edit /etc/yum.repos.d/osg-*development.repo : Enable the development repo Add includepkg=koji* to the definition for the development repo Go through the other repo files and make sure that EPEL and OS priorities are worse than 98. This means absent or numerically greater. Especially look at cobbler-config.repo if it exists. Install the koji packages and dependencies, making sure the koji packages themselves come from osg: [root@newkoji]# yum install koji koji-builder koji-hub koji-plugin-sign \\ koji-theme-fedora koji-utils koji-web mod_ssl postgresql Mount /mnt/koji if necessary Restore the contents of the koji filesystem. On host-3 : # # At a minimum, you must restore the /mnt/koji/packages directory [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/packages/ $NEWKOJI :/mnt/koji/packages # # The other directories are optional, though it saves a lot of time to restore /mnt/koji/repos [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/repos/ $NEWKOJI :/mnt/koji/repos [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/work/ $NEWKOJI :/mnt/koji/work [you@host-3]$ sudo $RSYNC $KOJIBACKUP /kojifs/scratch/ $NEWKOJI :/mnt/koji/scratch # # Any dirs you did not restore should be created. Fix permissions if needed. On newkoji : [root@newkoji]# chown -R apache:apache /mnt/koji/ { packages,repos,work,scratch } [root@newkoji]# chmod 0755 /mnt/koji/ { packages,repos,work,scratch } Continue on to the next section","title":"Installing the OS"},{"location":"infrastructure/koji-restore-recipe/#restoring-configuration","text":"On newkoji , define the shell function dirclone , listed below: dirclone () { srcdir = $( dirname $1 ) / $( basename $1 ) destdir = $( dirname $2 ) / $( basename $2 ) mkdir -p $( dirname $2 ) rsync --archive --delete-after --acls --xattrs \\ --partial --partial-dir = .rsync-partial \\ $srcdir / $destdir } On newkoji : [root@newkoji]# mkdir -p /root/hubrestore On host-3 : [you@host-3]$ sudo $RSYNC $KOJIBACKUP /rootfs/ { root,home,etc } $NEWKOJI :/root/hubrestore/ [you@host-3]$ sudo $RSYNC $KOJIBACKUP /varfs/ $NEWKOJI :/root/hubrestore/var/ On newkoji , install some utils we will need later: [root@newkoji]# yum install -y dos2unix vim-enhanced (vim-enhanced is used for vimdiff) On newkoji : # # Restore some of the directories in /etc: [root@newkoji]# while read subtree ; do dirclone /root/hubrestore/etc/$subtree /etc/$subtree done ___END___ httpd kojid koji-hub kojira koji-sign-plugin kojiweb mock pki/koji pki/tls/certs pki/tls/private ___END___ # # Restore some of the files: [root@newkoji]# cp -a /root/hubrestore/etc/koji.conf /etc/ [root@newkoji]# cp -a /root/hubrestore/etc/sysconfig/ { httpd,kojid,kojira } /etc/sysconfig/ Restore users and home directories If newkoji is on a separate host from newdb , then just simply copy over the files: [root@newkoji]# dirclone /root/hubrestore/home /home [root@newkoji]# cp -a /root/hubrestore/etc/ { passwd,shadow,group,gshadow } /etc If newkoji is on the same host as newdb , then you will have to be more careful: # # Skip home directories for the special users [root@newkoji]# for dir in /root/hubrestore/home/* ; do bndir=$(basename $dir ) if [[ $bndir = koji $bndir = postgres ]]; then dirclone $dir /home/ $bndir fi done # # Now merge the passwd, group, shadow, and gshadow files in /etc. # # Make sure that your editor does not create backup files # # ( set nobackup in vim), and that shadow and gshadow are owned by # # root and have 0400 permissions. Ensure a 'koji' user exists Fix dirs in /var : [root@newkoji]# rm -rf /var/lib/mock/* [root@newkoji]# chown root:mock /var/lib/mock [root@newkoji]# chmod 2775 /var/lib/mock Restore /var/www/html and /var/spool/cron (TODO) /var should have been backed up, but in case it isn't, the following files need to exist in /var/www/html : A symlink mnt - /mnt A robots.txt with contents User-agent: * Disallow: /","title":"Restoring Configuration"},{"location":"infrastructure/koji-restore-recipe/#fixing-names","text":"This section should be done if newdb or newkoji do not have the same as the previous db server and hub (i.e. db-01.batlab.org and koji.chtc.wisc.edu ). This section should be completed on newkoji .","title":"Fixing Names"},{"location":"infrastructure/koji-restore-recipe/#fixing-config-files-if-newdb-was-renamed","text":"The only change that's needed if newdb was renamed is to /etc/koji-hub/hub.conf . Edit that file and change the DBHost line to point to the new hostname. After editing, make sure hub.conf is owned by root:apache and chmodded 0640.","title":"Fixing config files if newdb was renamed"},{"location":"infrastructure/koji-restore-recipe/#installing-new-certkey-pairs-for-newkoji","text":"You will need a cert/key pair for the new hostname. Run dos2unix on all cert and key files before using them. Define the shell function makepem , listed below. makepem combines a public and private keypair to make a .pem file that the Koji services use. Usage: makepem CERTFILE KEYFILE OUTPUT_FILE makepem () { certfile = $1 keyfile = $2 outputfile = $3 ( set -e keymodulus = $( openssl rsa -noout -modulus -in $keyfile ) certmodulus = $( openssl x509 -noout -modulus -in $certfile ) if [[ $keymodulus = $certmodulus ]] ; then echo keyfile and certfile do not match ; return 1 fi if [[ -f $outputfile ]] ; then mv -f $outputfile { ,.bak } fi ( dos2unix $certfile ; echo ; dos2unix $keyfile ) $outputfile chmod 0600 $outputfile ) } Place cert and key files into the following paths: host cert /root/hostcert.pem host key /root/hostkey.pem Then use makepem to combine the certs and put them in the proper locations. [root@newkoji]# makepem /root/hostcert.pem /root/hostkey.pem \\ /etc/pki/tls/private/kojiweb.pem [root@newkoji]# chown apache:apache /etc/pki/tls/private/kojiweb.pem In addition, copy the host cert and key into the locations HTTPD expects it them. [root@newkoji]# cp -a /root/hostcert.pem /etc/pki/tls/certs/hostcert.pem [root@newkoji]# cp -a /root/hostkey.pem /etc/pki/tls/private/hostkey.pem","title":"Installing new cert/key pairs for newkoji"},{"location":"infrastructure/koji-restore-recipe/#fixing-hostname-in-config-files","text":"Use sed to replace the hostname in the following config files in /etc: /etc/kojira/kojira.conf /etc/koji.conf /etc/koji-hub/hub.conf /etc/httpd/conf.d/kojiweb.conf /etc/httpd/conf/httpd.conf /etc/kojid/kojid.conf You will need to fix /etc/kojid/kojid.conf on all builder machines as well (e.g. kojibuilder2.chtc.wisc.edu ).","title":"Fixing hostname in config files"},{"location":"infrastructure/koji-restore-recipe/#fixing-hostname-in-database","text":"You will need to find and fix entries that contain the hostname in the following tables: host (should be 1 entry) users (should be 2 entries, one for the host, and one for the kojira user)","title":"Fixing hostname in database"},{"location":"infrastructure/koji-restore-recipe/#fixing-hostname-elsewhere","text":"These steps are only necessary if you cannot get a DNS Canonical Name (CN) record such that koji.chtc.wisc.edu resolves to newkoji . Update the repo definitions in the osg-release package Update the mash script(s) at the GOC Mail the software team and users that anyone using the minefield repos will need to update osg-release Fix all the build machines to point to the new name Fix the following files in osg-build and make a new release data/osg-koji-home.conf data/osg-koji-site.conf osgbuild/constants.py osgbuild/kojiinter.py Mail people that they will need to update osg-build and rerun osg-koji setup","title":"Fixing hostname elsewhere"},{"location":"infrastructure/koji-restore-recipe/#starting-services-and-validation","text":"Now you will start up Koji services and verify that they function. Prerequisite: previous restore steps have been completed and postgresql is running on the database host. All steps will be run on newkoji . Start the main koji daemon: [root@newkoji]# service httpd start Use ps to verify that it came up Connect to the web interface in your browser. Make sure you can use https and you can log in. As yourself, run the koji command-line tool and make a few queries (e.g. list-tags) Start the koji build daemon: [root@newkoji]# service kojid start Use ps to verify that it came up If you did not restore the /mnt/koji/repos directory, you will now need to regenerate the build repos. Use koji list-tags to get a list of tags and run koji regen-repo on all of the ones with -build in the name. This will take several hours. You will also need to regen the -development repos so that minefield works again. Keep an eye on the tasks in the web interface to make sure they are getting farmed out to the right hosts. Try a scratch build Start kojira : [root@newkoji]# service kojira start Use ps to verify that it came up Wait half a minute and use ps to verify that kojid is still up; the two processes can kick each other off if they are both using the same certificate Bump a package if needed and try a real, non-scratch build Make sure that kojira is regenerating the repos If you have updated osg-release and/or osg-build , rebuild those packages now.","title":"Starting Services and Validation"},{"location":"infrastructure/koji-user-management/","text":"Koji User Management All of these operations require Koji admin privileges. You can see what privileges you have by running osg-koji list-permissions --mine . If you don't see admin , you do not have Koji admin privileges. As of March 2018, the people with admin privileges are: Mat Selmeci Carl Edquist Brian Lin Tim Cartwright Tim Theisen Adding a User Adding a user requires two pieces of information about the user: the DN of the cert they're using their purpose for using Koji, e.g. what package(s) they are going to build, what repos they are going to build into, or whether they should have promotion abilities for certain repos Koji only allows certs signed by one of the following CAs: ESnet Root CA 1 CERN Trusted Certification Authority CERN Grid Certification Authority CILogon Basic CA 1 (note: Fermi certs are signed by this) CILogon OSG CA 1 This is controlled by the /etc/pki/tls/CAs/allowed-cas-for-users.crt file, which contains the full cert chains for these CAs. Note The file also contains the cert chains for the following two CAs, which are used for build hosts and automated users. koji.chtc.wisc.edu InCommon RSA Server CA You can usually guess which CA signed the user's cert from their DN. If unsure, ask them for the output of: $ openssl x509 -in CERTFILE -noout -issuer If the CA is not one of the above ones, you will have to add the CA. Get permission to do this from the Software Manager, then follow the procedure on the koji-hub setup page . The user's Koji username will be the first Common Name (CN) of their certificate subject. For example: $ openssl x509 -in CERTFILE -noout -subject subject= /DC=org/DC=cilogon/C=US/O=Fermi National Accelerator Laboratory/OU=People/CN=Matyas Selmeci/CN=UID:matyas will result in the Koji username \"Matyas Selmeci\". To add the user, run $ osg-koji add-user USERNAME This will create the user but will give them no permissions. Warning It is effectively impossible to delete a user. If you get the name wrong, follow the instructions below for renaming a user below. To give them permissions, use the command $ osg-koji grant-permission PERMISSION USERNAME Generally, the permissions you should give are (more explanation below): most people should be given build and repo HCC people should also be given hcc-build S R people should also be given software-team Security people should also be given security-team Ops people should also be given operations-team other software devs should also be given software-contributor few people should be given admin To revoke permissions, use the command $ osg-koji revoke-permission PERMISSION USERNAME Permissions details To list the available permissions, run $ osg-koji list-permissions The important permissions are: repo : the ability to run osg-koji regen-repo . Should be given to anyone that builds or tests packages. build : the ability to build (but not tag!) any package. Should be given to anyone that builds packages, including automated users. This perm is necessary but not sufficient for them to build into the development repos; it is enough to let them do scratch builds. software-contributor : the ability to build into any of the development repos and promote into the osg-testing repos (but not any other repos). Should be given to people that build specific software but do not belong to the OSG S R team or HCC. operations-team : the ability to build into development and promote into testing the vo-client package. Should be given to people in OSG Operations. security-team : the ability to build into development and promote into testing the *-ca-certs* packages. Should be given to people in OSG Security. hcc-build : the ability to build and promote into any of the hcc tags. Should be given to people in Nebraska's Holland Computing Center. software-team , release-team : the ability to build any package into any development tag, and promote to any tag. Current policy does not distinguish between these two permissions. Should be given to people on the S R team. admin : the ability to do anything (short of direct database or config manipulation). Should be given sparingly. The grant-permission command can also be used to create new permissions; doing so is beyond the scope of this document. For further permission details, see the policy writing doc and /etc/koji-hub/hub.conf on koji.chtc.wisc.edu. Handling User Cert Changes / Renaming a User Koji users are identified by the first Common Name (CN) of their X.509 certificate. If this changes for a user (e.g. they get a cert from a new CA), someone with root access on koji.chtc.wisc.edu will need to SSH there and do the following: $ sudo /root/psql-kojidb koji= BEGIN ; koji= SELECT * FROM users WHERE name = OLDNAME ; -- make sure you find one and exactly one person koji= UPDATE users SET name = NEWNAME WHERE name = OLDNAME ; -- 1 row should have been updated koji= SELECT * FROM users ; -- sanity check; if everything looks ok, commit koji= COMMIT ; koji= \\q If you mess up, do ROLLBACK; BEGIN; and try again. Inform the user: they can no longer use their old cert if they aren't using a proxy for Koji auth, they must rerun osg-koji setup to fix their client.crt files they must import their new cert into their browsers they must clear their browser cache and cookies for koji.chtc.wisc.edu before using the web interface (or else they'll get a Python stack trace when they try to connect) Disabling and Enabling a User Users cannot be deleted, but they can be disabled. We also put (disabled) in their username to let us easily distinguish between enabled and disabled users. To disable a user, use the command: $ osg-koji disable-user USERNAME Follow the rename procedure above to change their name from USERNAME to USERNAME (disabled) . To enable a user, follow the rename procedure above to change their name from USERNAME (disabled) to USERNAME . Then, use the command: $ osg-koji enable-user USERNAME","title":"Koji User Management"},{"location":"infrastructure/koji-user-management/#koji-user-management","text":"All of these operations require Koji admin privileges. You can see what privileges you have by running osg-koji list-permissions --mine . If you don't see admin , you do not have Koji admin privileges. As of March 2018, the people with admin privileges are: Mat Selmeci Carl Edquist Brian Lin Tim Cartwright Tim Theisen","title":"Koji User Management"},{"location":"infrastructure/koji-user-management/#adding-a-user","text":"Adding a user requires two pieces of information about the user: the DN of the cert they're using their purpose for using Koji, e.g. what package(s) they are going to build, what repos they are going to build into, or whether they should have promotion abilities for certain repos Koji only allows certs signed by one of the following CAs: ESnet Root CA 1 CERN Trusted Certification Authority CERN Grid Certification Authority CILogon Basic CA 1 (note: Fermi certs are signed by this) CILogon OSG CA 1 This is controlled by the /etc/pki/tls/CAs/allowed-cas-for-users.crt file, which contains the full cert chains for these CAs. Note The file also contains the cert chains for the following two CAs, which are used for build hosts and automated users. koji.chtc.wisc.edu InCommon RSA Server CA You can usually guess which CA signed the user's cert from their DN. If unsure, ask them for the output of: $ openssl x509 -in CERTFILE -noout -issuer If the CA is not one of the above ones, you will have to add the CA. Get permission to do this from the Software Manager, then follow the procedure on the koji-hub setup page . The user's Koji username will be the first Common Name (CN) of their certificate subject. For example: $ openssl x509 -in CERTFILE -noout -subject subject= /DC=org/DC=cilogon/C=US/O=Fermi National Accelerator Laboratory/OU=People/CN=Matyas Selmeci/CN=UID:matyas will result in the Koji username \"Matyas Selmeci\". To add the user, run $ osg-koji add-user USERNAME This will create the user but will give them no permissions. Warning It is effectively impossible to delete a user. If you get the name wrong, follow the instructions below for renaming a user below. To give them permissions, use the command $ osg-koji grant-permission PERMISSION USERNAME Generally, the permissions you should give are (more explanation below): most people should be given build and repo HCC people should also be given hcc-build S R people should also be given software-team Security people should also be given security-team Ops people should also be given operations-team other software devs should also be given software-contributor few people should be given admin To revoke permissions, use the command $ osg-koji revoke-permission PERMISSION USERNAME","title":"Adding a User"},{"location":"infrastructure/koji-user-management/#permissions-details","text":"To list the available permissions, run $ osg-koji list-permissions The important permissions are: repo : the ability to run osg-koji regen-repo . Should be given to anyone that builds or tests packages. build : the ability to build (but not tag!) any package. Should be given to anyone that builds packages, including automated users. This perm is necessary but not sufficient for them to build into the development repos; it is enough to let them do scratch builds. software-contributor : the ability to build into any of the development repos and promote into the osg-testing repos (but not any other repos). Should be given to people that build specific software but do not belong to the OSG S R team or HCC. operations-team : the ability to build into development and promote into testing the vo-client package. Should be given to people in OSG Operations. security-team : the ability to build into development and promote into testing the *-ca-certs* packages. Should be given to people in OSG Security. hcc-build : the ability to build and promote into any of the hcc tags. Should be given to people in Nebraska's Holland Computing Center. software-team , release-team : the ability to build any package into any development tag, and promote to any tag. Current policy does not distinguish between these two permissions. Should be given to people on the S R team. admin : the ability to do anything (short of direct database or config manipulation). Should be given sparingly. The grant-permission command can also be used to create new permissions; doing so is beyond the scope of this document. For further permission details, see the policy writing doc and /etc/koji-hub/hub.conf on koji.chtc.wisc.edu.","title":"Permissions details"},{"location":"infrastructure/koji-user-management/#handling-user-cert-changes-renaming-a-user","text":"Koji users are identified by the first Common Name (CN) of their X.509 certificate. If this changes for a user (e.g. they get a cert from a new CA), someone with root access on koji.chtc.wisc.edu will need to SSH there and do the following: $ sudo /root/psql-kojidb koji= BEGIN ; koji= SELECT * FROM users WHERE name = OLDNAME ; -- make sure you find one and exactly one person koji= UPDATE users SET name = NEWNAME WHERE name = OLDNAME ; -- 1 row should have been updated koji= SELECT * FROM users ; -- sanity check; if everything looks ok, commit koji= COMMIT ; koji= \\q If you mess up, do ROLLBACK; BEGIN; and try again. Inform the user: they can no longer use their old cert if they aren't using a proxy for Koji auth, they must rerun osg-koji setup to fix their client.crt files they must import their new cert into their browsers they must clear their browser cache and cookies for koji.chtc.wisc.edu before using the web interface (or else they'll get a Python stack trace when they try to connect)","title":"Handling User Cert Changes / Renaming a User"},{"location":"infrastructure/koji-user-management/#disabling-and-enabling-a-user","text":"Users cannot be deleted, but they can be disabled. We also put (disabled) in their username to let us easily distinguish between enabled and disabled users. To disable a user, use the command: $ osg-koji disable-user USERNAME Follow the rename procedure above to change their name from USERNAME to USERNAME (disabled) . To enable a user, follow the rename procedure above to change their name from USERNAME (disabled) to USERNAME . Then, use the command: $ osg-koji enable-user USERNAME","title":"Disabling and Enabling a User"},{"location":"infrastructure/madison-itb/","text":"pre em { color: red; font-weight: normal; font-style: normal; } .old { color: red; } .off { color: blue; } Software/Release Team ITB Site Design Madison ITB Machines All physical hosts are located in 3370A in the VDT rack. Host Purpose OS CPU Model CPUs RAM Storage Notes itb-data1 worker node SL 6.9 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as HDFS data node itb-data2 worker node SL 6.9 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as HDFS data node itb-data3 worker node SL 7.4 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as HDFS data node itb-data4 worker node SL 6.9 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as XRootD data node itb-data5 worker node SL 6.9 Xeon E3-1220 3.10GHz 2 / 4 8 GB 750 GB \u00d7 2 (RAID?) planned as XRootD data node itb-data6 worker node SL 7.4 Xeon E3-1220 3.10GHz 2 / 4 8 GB ??? planned as XRootD data node itb-host-1 KVM host SL 7.4 Xeon E5-2450 2.10GHz 16 / 32 64 GB 1 TB \u00d7 4 (HW RAID 5) \u00b7 itb-ce1 HTCondor-CE SL 6.9 VM 4 6 GB 192 GB \u00b7 itb-ce2 HTCondor-CE SL 6.9 VM 4 6 GB 192 GB \u00b7 itb-cm HTCondor CM SL 7.4 VM 4 6 GB 192 GB \u00b7 itb-glidein GlideinWMS VO frontend? SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-gums-rsv GUMS, RSV SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-hdfs-name1 \u2014 (so far) SL ? VM 4 6 GB 192 GB \u00b7 itb-hdfs-name2 \u2014 (so far) SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-se-hdfs \u2014 (so far) SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-se-xrootd \u2014 (so far) SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-submit HTCondor submit SL 6.9 VM 4 6 GB 192 GB \u00b7 itb-xrootd \u2014 (so far) SL ? VM 4 6 GB 192 GB itb-host-2 worker node SL 6.9 Xeon E5-2450 2.10GHz 16 / 32 64 GB 352 GB in $(EXECUTE) itb-host-3 worker node SL 7.4 Xeon E5-2450 2.10GHz 16 / 32 64 GB 352 GB in $(EXECUTE) (Data last updated 2017-10-13 by Tim C. Red indicates a host that has yet to be rebuilt; Blue is rebuilt but currently off.) ITB Goals Goals for the Madison ITB site are now maintained in a Google document . Configuration Basic host configuration is handled by Ansible and a local Git repository of playbooks. Git Repository The authoritative Git repository for Madison ITB configuration is gitolite@git.chtc.wisc.edu:osgitb . Clone the repository and push validated changes back to it. Ansible The osghost machine has Ansible 2.3.1.0 installed via RPM. Use other hosts and versions at your own risk. Common Ansible commands Note: For critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person All commands below are meant to be run from the top directory of your osgitb Git repo (e.g. on osghost , not on the target machine) To run Ansible for the first time for a new machine (using the root password for the target machine when prompted): ansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN ansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN The HOST-PATTERN can be a glob-like pattern or a regular expression that matches host names in the inventory file; see Ansible documentation for details. After initial successful runs of both playbooks, subsequent runs should replace the -u root -k part with -bK to use your own login and sudo on the target machine . For example: ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN ansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ] Omit the -l option to apply configuration to all hosts. If you want to run only a single role from a playbook, use the -t option with the corresponding tag name. For example, to run the iptables tag/role: ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN -t iptables If you have your own playbook to manage personal configuration, run it as follows: ansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ] Adding host and other certificates (This is in very rough form, but the key bits are here.) Ask Mat to get new certificates \u2014 be sure to think about http , rsv , and other service certificates Wait for Mat to tell you that the new certificates are in /p/condor/home/certificates scp -p the certificate(s) ( *cert.pem* and *key.pem ) to your home directory on osghost or whatever machine you use for Ansible Note that if you are renewing a certificate, only the *cert.pem will be updated and the *key.pem will remain the same. Find the corresponding certificate location(s) in the Ansible roles/certs/files directory cp -p the certificate files over the top of the existing Ansible ones (or create new, equivalent paths) Run ansible-vault encrypt FILE(S) to encrypt the files \u2014 get the Ansible vault password from Tim C. if you need it Note that only the *key.pem files need to be encrypted, as the *cert.pem files are meant to be public. If the (unencrypted) *key.pem file is not getting updated, there is no need to re-encrypt a new copy. Verify permissions, contents (you can cat the encrypted files), etc. Apply the files with something like ansible-playbook secure.yml -i inventory -bK -f 20 -t certs --ask-vault-pass Commit changes (now or after applying) Push changes to origin ( gitolite@git.chtc.wisc.edu:osgitb ) Doing yum updates Check to see if updates are needed and, if so, what would be updated: ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a yum check-update You can name a single HOST or an inventory GROUP (such as the handy current group); with a group, you can further restrict the hosts with a -l option. Note: yum check-update exits with status code 100 when it succeeds in identifying packages to update; therefore Ansible shows such results as failures. Review the package lists to be updated and decide whether to proceed with all updates or limited ones Do updates: ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a yum --assumeyes update [ -l LIMITS ] If needed (and if unsure, ask a sysadmin), reboot machines: ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a /sbin/shutdown -r +1 [ -l LIMITS ] Updating HTCondor from Upcoming Something like this: ansible condordev -i inventory -bK -f 10 -m command -a yum --enablerepo=osg-upcoming --assumeyes update condor Monitoring HTCondor-CE View Once we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available here through HTCondor-CE View. Tracking payload jobs via Kibana At this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via Kibana . To view all payload jobs that have run on our ITB site in the past week, use this query . Making a New Virtual Machine on itb-host-1 For this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins. If you do not have an account, request one from the CHTC system administrators. If this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure Pick a MAC address, starting with 00:16:3e: followed by three random octets (e.g., 00:16:3e:f7:29:ee ) Email with a request for a new OSG ITB VM, including the chosen MAC address Wait to receive the associated IP address for the new host Create or edit the Cobbler system object for the host Access https://cobbler-widmir.chtc.wisc.edu/cobbler_web In the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link If desired, filter (at the bottom) by \u201cname\u201d on something like itb-*.chtc.wisc.edu For a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d For a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry In the first \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific_6_8_osg_vm\u201d or \u201cScientific_7_2_osg_vm\u201d In the second \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox In the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine In the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host Click the \u201cSave\u201d button In the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link Log in to itb-host-1 and become root Create the libvirt definition file Create a new XML file named after the desired hostname (e.g., itb-ce2.xml ) and copy in the template below Replace {{ HOSTNAME }} with the fully qualified hostname of the new virtual host Replace {{ MAC_ADDRESS }} with the MAC address of the new virtual host (from above) If desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed Save the XML file Create a new, empty disk image for the virtual host, in its correct location (as specified in the XML file): truncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd chown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd Load the new host definition into libvirt: virsh define XML-FILE Install the new machine Start the virtual machine: virsh start HOSTNAME At this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS, then reboot the now-installed machine. The whole process typically takes 15\u201320 minutes. You may be able to ssh into the machine during the install process, but there is no need to monitor or interfere. Once the machine is available (which you can only guess at), ssh in and verify that the machine basically works Immediately run Ansible on the machine, first with the secure.yml playbook, then the site.yml one (see above) Log in to the machine and look around to make sure it seems OK When things look good, tell virsh to start the virtual machine when the host itself starts: virsh autostart HOSTNAME Libvirt VM Template domain type= kvm name {{ HOSTNAME }} /name memory unit= GiB 6 /memory vcpu 4 /vcpu os type hvm /type boot dev= network / boot dev= hd / bios useserial= yes rebootTimeout= 0 / /os features acpi/ apic/ pae/ /features devices emulator /usr/libexec/qemu-kvm /emulator disk type= file device= disk source file= /var/lib/libvirt/images/{{ HOSTNAME }}.dd / target dev= vda bus= virtio / /disk interface type= bridge mac address= {{ MAC_ADDRESS }} / source bridge= br0 / model type= virtio / /interface serial type= pty target port= 0 / /serial console type= pty target type= serial port= 0 / /console graphics type= vnc autoport= yes listen= 127.0.0.1 / /devices /domain","title":"Madison ITB"},{"location":"infrastructure/madison-itb/#softwarerelease-team-itb-site-design","text":"","title":"Software/Release Team ITB Site Design"},{"location":"infrastructure/madison-itb/#madison-itb-machines","text":"All physical hosts are located in 3370A in the VDT rack. Host Purpose OS CPU Model CPUs RAM Storage Notes itb-data1 worker node SL 6.9 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as HDFS data node itb-data2 worker node SL 6.9 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as HDFS data node itb-data3 worker node SL 7.4 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as HDFS data node itb-data4 worker node SL 6.9 Celeron G530 2.4Ghz 2 / 2 8 GB 750 GB \u00d7 2 (RAID?) planned as XRootD data node itb-data5 worker node SL 6.9 Xeon E3-1220 3.10GHz 2 / 4 8 GB 750 GB \u00d7 2 (RAID?) planned as XRootD data node itb-data6 worker node SL 7.4 Xeon E3-1220 3.10GHz 2 / 4 8 GB ??? planned as XRootD data node itb-host-1 KVM host SL 7.4 Xeon E5-2450 2.10GHz 16 / 32 64 GB 1 TB \u00d7 4 (HW RAID 5) \u00b7 itb-ce1 HTCondor-CE SL 6.9 VM 4 6 GB 192 GB \u00b7 itb-ce2 HTCondor-CE SL 6.9 VM 4 6 GB 192 GB \u00b7 itb-cm HTCondor CM SL 7.4 VM 4 6 GB 192 GB \u00b7 itb-glidein GlideinWMS VO frontend? SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-gums-rsv GUMS, RSV SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-hdfs-name1 \u2014 (so far) SL ? VM 4 6 GB 192 GB \u00b7 itb-hdfs-name2 \u2014 (so far) SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-se-hdfs \u2014 (so far) SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-se-xrootd \u2014 (so far) SL 6.3 VM 3 6 GB 50 GB \u00b7 itb-submit HTCondor submit SL 6.9 VM 4 6 GB 192 GB \u00b7 itb-xrootd \u2014 (so far) SL ? VM 4 6 GB 192 GB itb-host-2 worker node SL 6.9 Xeon E5-2450 2.10GHz 16 / 32 64 GB 352 GB in $(EXECUTE) itb-host-3 worker node SL 7.4 Xeon E5-2450 2.10GHz 16 / 32 64 GB 352 GB in $(EXECUTE) (Data last updated 2017-10-13 by Tim C. Red indicates a host that has yet to be rebuilt; Blue is rebuilt but currently off.)","title":"Madison ITB Machines"},{"location":"infrastructure/madison-itb/#itb-goals","text":"Goals for the Madison ITB site are now maintained in a Google document .","title":"ITB Goals"},{"location":"infrastructure/madison-itb/#configuration","text":"Basic host configuration is handled by Ansible and a local Git repository of playbooks.","title":"Configuration"},{"location":"infrastructure/madison-itb/#git-repository","text":"The authoritative Git repository for Madison ITB configuration is gitolite@git.chtc.wisc.edu:osgitb . Clone the repository and push validated changes back to it.","title":"Git Repository"},{"location":"infrastructure/madison-itb/#ansible","text":"The osghost machine has Ansible 2.3.1.0 installed via RPM. Use other hosts and versions at your own risk.","title":"Ansible"},{"location":"infrastructure/madison-itb/#common-ansible-commands","text":"Note: For critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person All commands below are meant to be run from the top directory of your osgitb Git repo (e.g. on osghost , not on the target machine) To run Ansible for the first time for a new machine (using the root password for the target machine when prompted): ansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN ansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN The HOST-PATTERN can be a glob-like pattern or a regular expression that matches host names in the inventory file; see Ansible documentation for details. After initial successful runs of both playbooks, subsequent runs should replace the -u root -k part with -bK to use your own login and sudo on the target machine . For example: ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN ansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ] Omit the -l option to apply configuration to all hosts. If you want to run only a single role from a playbook, use the -t option with the corresponding tag name. For example, to run the iptables tag/role: ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN -t iptables If you have your own playbook to manage personal configuration, run it as follows: ansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ]","title":"Common Ansible commands"},{"location":"infrastructure/madison-itb/#adding-host-and-other-certificates","text":"(This is in very rough form, but the key bits are here.) Ask Mat to get new certificates \u2014 be sure to think about http , rsv , and other service certificates Wait for Mat to tell you that the new certificates are in /p/condor/home/certificates scp -p the certificate(s) ( *cert.pem* and *key.pem ) to your home directory on osghost or whatever machine you use for Ansible Note that if you are renewing a certificate, only the *cert.pem will be updated and the *key.pem will remain the same. Find the corresponding certificate location(s) in the Ansible roles/certs/files directory cp -p the certificate files over the top of the existing Ansible ones (or create new, equivalent paths) Run ansible-vault encrypt FILE(S) to encrypt the files \u2014 get the Ansible vault password from Tim C. if you need it Note that only the *key.pem files need to be encrypted, as the *cert.pem files are meant to be public. If the (unencrypted) *key.pem file is not getting updated, there is no need to re-encrypt a new copy. Verify permissions, contents (you can cat the encrypted files), etc. Apply the files with something like ansible-playbook secure.yml -i inventory -bK -f 20 -t certs --ask-vault-pass Commit changes (now or after applying) Push changes to origin ( gitolite@git.chtc.wisc.edu:osgitb )","title":"Adding host and other certificates"},{"location":"infrastructure/madison-itb/#doing-yum-updates","text":"Check to see if updates are needed and, if so, what would be updated: ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a yum check-update You can name a single HOST or an inventory GROUP (such as the handy current group); with a group, you can further restrict the hosts with a -l option. Note: yum check-update exits with status code 100 when it succeeds in identifying packages to update; therefore Ansible shows such results as failures. Review the package lists to be updated and decide whether to proceed with all updates or limited ones Do updates: ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a yum --assumeyes update [ -l LIMITS ] If needed (and if unsure, ask a sysadmin), reboot machines: ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a /sbin/shutdown -r +1 [ -l LIMITS ]","title":"Doing yum updates"},{"location":"infrastructure/madison-itb/#updating-htcondor-from-upcoming","text":"Something like this: ansible condordev -i inventory -bK -f 10 -m command -a yum --enablerepo=osg-upcoming --assumeyes update condor","title":"Updating HTCondor from Upcoming"},{"location":"infrastructure/madison-itb/#monitoring","text":"","title":"Monitoring"},{"location":"infrastructure/madison-itb/#htcondor-ce-view","text":"Once we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available here through HTCondor-CE View.","title":"HTCondor-CE View"},{"location":"infrastructure/madison-itb/#tracking-payload-jobs-via-kibana","text":"At this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via Kibana . To view all payload jobs that have run on our ITB site in the past week, use this query .","title":"Tracking payload jobs via Kibana"},{"location":"infrastructure/madison-itb/#making-a-new-virtual-machine-on-itb-host-1","text":"For this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins. If you do not have an account, request one from the CHTC system administrators. If this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure Pick a MAC address, starting with 00:16:3e: followed by three random octets (e.g., 00:16:3e:f7:29:ee ) Email with a request for a new OSG ITB VM, including the chosen MAC address Wait to receive the associated IP address for the new host Create or edit the Cobbler system object for the host Access https://cobbler-widmir.chtc.wisc.edu/cobbler_web In the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link If desired, filter (at the bottom) by \u201cname\u201d on something like itb-*.chtc.wisc.edu For a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d For a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry In the first \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific_6_8_osg_vm\u201d or \u201cScientific_7_2_osg_vm\u201d In the second \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox In the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine In the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host Click the \u201cSave\u201d button In the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link Log in to itb-host-1 and become root Create the libvirt definition file Create a new XML file named after the desired hostname (e.g., itb-ce2.xml ) and copy in the template below Replace {{ HOSTNAME }} with the fully qualified hostname of the new virtual host Replace {{ MAC_ADDRESS }} with the MAC address of the new virtual host (from above) If desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed Save the XML file Create a new, empty disk image for the virtual host, in its correct location (as specified in the XML file): truncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd chown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd Load the new host definition into libvirt: virsh define XML-FILE Install the new machine Start the virtual machine: virsh start HOSTNAME At this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS, then reboot the now-installed machine. The whole process typically takes 15\u201320 minutes. You may be able to ssh into the machine during the install process, but there is no need to monitor or interfere. Once the machine is available (which you can only guess at), ssh in and verify that the machine basically works Immediately run Ansible on the machine, first with the secure.yml playbook, then the site.yml one (see above) Log in to the machine and look around to make sure it seems OK When things look good, tell virsh to start the virtual machine when the host itself starts: virsh autostart HOSTNAME","title":"Making a New Virtual Machine on itb-host-1"},{"location":"infrastructure/madison-itb/#libvirt-vm-template","text":"domain type= kvm name {{ HOSTNAME }} /name memory unit= GiB 6 /memory vcpu 4 /vcpu os type hvm /type boot dev= network / boot dev= hd / bios useserial= yes rebootTimeout= 0 / /os features acpi/ apic/ pae/ /features devices emulator /usr/libexec/qemu-kvm /emulator disk type= file device= disk source file= /var/lib/libvirt/images/{{ HOSTNAME }}.dd / target dev= vda bus= virtio / /disk interface type= bridge mac address= {{ MAC_ADDRESS }} / source bridge= br0 / model type= virtio / /interface serial type= pty target port= 0 / /serial console type= pty target type= serial port= 0 / /console graphics type= vnc autoport= yes listen= 127.0.0.1 / /devices /domain","title":"Libvirt VM Template"},{"location":"meetings/TechAreaTemplate/","text":"OSG Technology Area Meeting, 17 July 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Jeff, Marian, Marty, Mat, Suchandra, TimC, TimT, Xin Announcements Triage Duty This week: Next week: ( ) open tickets JIRA # of tickets State 141 -17 Open 33 +9 In Progress 4 +2 Ready for Testing 0 -12 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Discussions Support Update OSG Release Team 3.3.27 Both 3.4.2 Total Status 1 +0 4 +0 2 +0 7 +0 Open 0 +0 10 +0 4 +0 14 +0 In Progress 2 +0 1 +0 1 +0 4 +0 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +0 15 +0 7 +0 25 +0 Total Discussions OSG Investigations Team Last Week This Week Ongoing","title":"OSG Technology Area Meeting, 17 July 2017"},{"location":"meetings/TechAreaTemplate/#osg-technology-area-meeting-17-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Jeff, Marian, Marty, Mat, Suchandra, TimC, TimT, Xin","title":"OSG Technology Area Meeting, 17 July 2017"},{"location":"meetings/TechAreaTemplate/#announcements","text":"","title":"Announcements"},{"location":"meetings/TechAreaTemplate/#triage-duty","text":"This week: Next week: ( ) open tickets","title":"Triage Duty"},{"location":"meetings/TechAreaTemplate/#jira","text":"# of tickets State 141 -17 Open 33 +9 In Progress 4 +2 Ready for Testing 0 -12 Ready for Release","title":"JIRA"},{"location":"meetings/TechAreaTemplate/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/TechAreaTemplate/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/TechAreaTemplate/#discussions","text":"","title":"Discussions"},{"location":"meetings/TechAreaTemplate/#support-update","text":"","title":"Support Update"},{"location":"meetings/TechAreaTemplate/#osg-release-team","text":"3.3.27 Both 3.4.2 Total Status 1 +0 4 +0 2 +0 7 +0 Open 0 +0 10 +0 4 +0 14 +0 In Progress 2 +0 1 +0 1 +0 4 +0 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +0 15 +0 7 +0 25 +0 Total","title":"OSG Release Team"},{"location":"meetings/TechAreaTemplate/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/TechAreaTemplate/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/TechAreaTemplate/#last-week","text":"","title":"Last Week"},{"location":"meetings/TechAreaTemplate/#this-week","text":"","title":"This Week"},{"location":"meetings/TechAreaTemplate/#ongoing","text":"","title":"Ongoing"},{"location":"meetings/2017/TechArea20170227/","text":"OSG Technology Area Meeting, 27 February 2017 Coordinates: Conference: 857-216-4999, PIN: 32390; https://www.uberconference.com/osgcat Attending: Announcements No meeting next week, OSG All Hands Triage Duty This week: Mat Next week: Suchandra 6 (0) open tickets JIRA # of tickets State 141 ( 17) Open 33 (+9) In Progress 4 (+2) Ready for Testing 0 ( 12) Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.22 2017-02-27 2017-03-06 2017-03-14 3.3.23 2017-03-27 2017-04-03 2017-04-11 3.3.24 2017-04-25 2017-05-01 2017-05-09 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Discussions Software freeze today Assignee Tickets Not RFT BrianL 7 Mat 7 Carl 4 Edgar 2 Derek 2 Marian 1 Support Update Baylor (BrianL) - CE installation help, troubleshooting mapping and blahp issues Clemson (BrianL) - blahp segfault, held jobs due to bad folder ownership Georgia Tech (BrianL) - accounting records reporting start date as 1/1/1970 OSG Release Team March 11th Release - OSG 3.3.22 Development Freeze 2/27 Data Release - IGTF 1.80 3.3.22 Status 15 (+15) Open 11 (+11) In Progress 2 ( 2) Ready for Testing 0 ( 3) Ready for Release 28 (+16) Total OSG 3.3.22 Testing XRootD 4.6.0 frontier-squid 3.5.24-1.1 in Upcoming Discussions None this week OSG Investigations Team Last Week GRACC Transfer Summaries Syracuse StashCache running now. Packaged Auth StashCache is coming from Marian. This Week Finish GRACC Tickets Blahp Merge Begin PEARC paper Ongoing Gratia V2: Derek will be working on this. Jira project: GRACC . Project documentation located at https://opensciencegrid.github.io/gracc . New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed. UNL setting up authenticated StashCache as well See Support Update section - StashCache troubleshooting at BNL, host migration to el7 caused some odd xrootd behavior, investigating","title":"OSG Technology Area Meeting, 27 February 2017"},{"location":"meetings/2017/TechArea20170227/#osg-technology-area-meeting-27-february-2017","text":"Coordinates: Conference: 857-216-4999, PIN: 32390; https://www.uberconference.com/osgcat Attending:","title":"OSG Technology Area Meeting, 27 February 2017"},{"location":"meetings/2017/TechArea20170227/#announcements","text":"No meeting next week, OSG All Hands","title":"Announcements"},{"location":"meetings/2017/TechArea20170227/#triage-duty","text":"This week: Mat Next week: Suchandra 6 (0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170227/#jira","text":"# of tickets State 141 ( 17) Open 33 (+9) In Progress 4 (+2) Ready for Testing 0 ( 12) Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170227/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.22 2017-02-27 2017-03-06 2017-03-14 3.3.23 2017-03-27 2017-04-03 2017-04-11 3.3.24 2017-04-25 2017-05-01 2017-05-09 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170227/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170227/#discussions","text":"Software freeze today Assignee Tickets Not RFT BrianL 7 Mat 7 Carl 4 Edgar 2 Derek 2 Marian 1","title":"Discussions"},{"location":"meetings/2017/TechArea20170227/#support-update","text":"Baylor (BrianL) - CE installation help, troubleshooting mapping and blahp issues Clemson (BrianL) - blahp segfault, held jobs due to bad folder ownership Georgia Tech (BrianL) - accounting records reporting start date as 1/1/1970","title":"Support Update"},{"location":"meetings/2017/TechArea20170227/#osg-release-team","text":"March 11th Release - OSG 3.3.22 Development Freeze 2/27 Data Release - IGTF 1.80 3.3.22 Status 15 (+15) Open 11 (+11) In Progress 2 ( 2) Ready for Testing 0 ( 3) Ready for Release 28 (+16) Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170227/#osg-3322","text":"Testing XRootD 4.6.0 frontier-squid 3.5.24-1.1 in Upcoming","title":"OSG 3.3.22"},{"location":"meetings/2017/TechArea20170227/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170227/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170227/#last-week","text":"GRACC Transfer Summaries Syracuse StashCache running now. Packaged Auth StashCache is coming from Marian.","title":"Last Week"},{"location":"meetings/2017/TechArea20170227/#this-week","text":"Finish GRACC Tickets Blahp Merge Begin PEARC paper","title":"This Week"},{"location":"meetings/2017/TechArea20170227/#ongoing","text":"Gratia V2: Derek will be working on this. Jira project: GRACC . Project documentation located at https://opensciencegrid.github.io/gracc . New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed. UNL setting up authenticated StashCache as well See Support Update section - StashCache troubleshooting at BNL, host migration to el7 caused some odd xrootd behavior, investigating","title":"Ongoing"},{"location":"meetings/2017/TechArea20170417/","text":"OSG Technology Area Meeting, 17 April 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Announcements Triage Duty This week: Suchandra Next week: Mat 5 (+1) open tickets JIRA # of tickets State 166 +14 Open 17 +1 In Progress 0 0 Ready for Testing 15 15 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Dev freeze next week but limited testing time due to HTCondor week and Suchandra's vacation XRootD 4.6.1 release candidate testing at UNL Discussions Marian will speak with Mat about proper XRootD release candidate versioning name BrianL will look into system dashboard solution for missing project summary page For testing gratia-probe/GRACC interaction, there is a GRACC testing interface that doesn't store the records but provides the proper responses to the probes UPITT (Marian) - XRootD assistance (https://ticket.grid.iu.edu/32605) Support Update BNL (BrianL/Derek) - Worked with Xin to investigate missing CE ScheddAd from central collector caused by this bug OSG Release Team 3.3.24 Status 17 +17 Open 6 +6 In Progress 0 0 Ready for Testing 0 0 Ready for Release 23 +23 Total OSG 3.3.24 Ready for Testing None yet Ready for Release None yet Discussions If XRootD 4.6.1 isn't ready by the package freeze, we will revisit the stability of the release candidates OSG Investigations Team Last Week GRACC operations transition XRootD bugs in caching and HTTPS connections. Seems to be fixed. Blahp merge work continues. Now on to getting binaries in the correct areas. This Week More GRACC Operations transition More BLAHP merge move *.osgstorage.org CVMFS repos to new host Ongoing Gratia V2: Derek will be working on this. Jira project: GRACC . Project documentation located at https://opensciencegrid.github.io/gracc . New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.","title":"OSG Technology Area Meeting, 17 April 2017"},{"location":"meetings/2017/TechArea20170417/#osg-technology-area-meeting-17-april-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending:","title":"OSG Technology Area Meeting, 17 April 2017"},{"location":"meetings/2017/TechArea20170417/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170417/#triage-duty","text":"This week: Suchandra Next week: Mat 5 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170417/#jira","text":"# of tickets State 166 +14 Open 17 +1 In Progress 0 0 Ready for Testing 15 15 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170417/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170417/#osg-software-team","text":"Dev freeze next week but limited testing time due to HTCondor week and Suchandra's vacation XRootD 4.6.1 release candidate testing at UNL","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170417/#discussions","text":"Marian will speak with Mat about proper XRootD release candidate versioning name BrianL will look into system dashboard solution for missing project summary page For testing gratia-probe/GRACC interaction, there is a GRACC testing interface that doesn't store the records but provides the proper responses to the probes UPITT (Marian) - XRootD assistance (https://ticket.grid.iu.edu/32605)","title":"Discussions"},{"location":"meetings/2017/TechArea20170417/#support-update","text":"BNL (BrianL/Derek) - Worked with Xin to investigate missing CE ScheddAd from central collector caused by this bug","title":"Support Update"},{"location":"meetings/2017/TechArea20170417/#osg-release-team","text":"3.3.24 Status 17 +17 Open 6 +6 In Progress 0 0 Ready for Testing 0 0 Ready for Release 23 +23 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170417/#osg-3324","text":"Ready for Testing None yet Ready for Release None yet","title":"OSG 3.3.24"},{"location":"meetings/2017/TechArea20170417/#discussions_1","text":"If XRootD 4.6.1 isn't ready by the package freeze, we will revisit the stability of the release candidates","title":"Discussions"},{"location":"meetings/2017/TechArea20170417/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170417/#last-week","text":"GRACC operations transition XRootD bugs in caching and HTTPS connections. Seems to be fixed. Blahp merge work continues. Now on to getting binaries in the correct areas.","title":"Last Week"},{"location":"meetings/2017/TechArea20170417/#this-week","text":"More GRACC Operations transition More BLAHP merge move *.osgstorage.org CVMFS repos to new host","title":"This Week"},{"location":"meetings/2017/TechArea20170417/#ongoing","text":"Gratia V2: Derek will be working on this. Jira project: GRACC . Project documentation located at https://opensciencegrid.github.io/gracc . New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.","title":"Ongoing"},{"location":"meetings/2017/TechArea20170424/","text":"OSG Technology Area Meeting, 24 April 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Jeff, Marian, Mat, TimC, TimT Announcements BrianL on vacation Wednesday through Friday Suchandra on vacation until 5/5 Condor week starts next Tuesday, 5/2 Triage Duty This week: Mat Next week: TimT 4 ( 1) open tickets JIRA # of tickets State 171 +5 Open 23 +6 In Progress 3 +3 Ready for Testing 0 0 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Dev freeze today, tickets not RFT by owner: Developer # Mat 12 Marian 2 BrianL 1 Carl 1 Discussions osg-build does not yet support building packages for 3.4. Expected support available in May. Support Update UMich (BrianL/Jeff/Marian) - Bob Ball's rebuilt CE is experiencing security handshake timeouts. Also had an issue where pilots were running even though his site was in downtime OSG Release Team 3.3.24 Status 8 -9 Open 8 +2 In Progress 3 +3 Ready for Testing 0 0 Ready for Release 19 -4 Total OSG 3.3.24 Tim Theisen is handling the May 9th release Development freeze today VO Package ?? Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week next week Ready for Testing CVMFS X.509 Helper 1.0 gsissh in tarballs Upcoming: GlideinWMS 3.3.2 (Needs factory testing) Ready for Release None yet Discussions None this week OSG Investigations Team Last Week GRACC operations transition. Lots of fires Cloud provider is adding more storage, and reconfiguring storage. Causes IO timeouts for VMs Blahp merge work continues. Now on to getting binaries in the correct areas. Stash CVMFS repo had to be built from scratch due to bug in CVMFS's auto-catalog (only affects stratum 0's) This Week More GRACC Operations transition More BLAHP merge StashCP work to work with multiple origins. Ongoing Gratia V2: Derek will be working on this. Jira project: GRACC . Project documentation located at https://opensciencegrid.github.io/gracc . New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.","title":"OSG Technology Area Meeting, 24 April 2017"},{"location":"meetings/2017/TechArea20170424/#osg-technology-area-meeting-24-april-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Jeff, Marian, Mat, TimC, TimT","title":"OSG Technology Area Meeting, 24 April 2017"},{"location":"meetings/2017/TechArea20170424/#announcements","text":"BrianL on vacation Wednesday through Friday Suchandra on vacation until 5/5 Condor week starts next Tuesday, 5/2","title":"Announcements"},{"location":"meetings/2017/TechArea20170424/#triage-duty","text":"This week: Mat Next week: TimT 4 ( 1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170424/#jira","text":"# of tickets State 171 +5 Open 23 +6 In Progress 3 +3 Ready for Testing 0 0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170424/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170424/#osg-software-team","text":"Dev freeze today, tickets not RFT by owner: Developer # Mat 12 Marian 2 BrianL 1 Carl 1","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170424/#discussions","text":"osg-build does not yet support building packages for 3.4. Expected support available in May.","title":"Discussions"},{"location":"meetings/2017/TechArea20170424/#support-update","text":"UMich (BrianL/Jeff/Marian) - Bob Ball's rebuilt CE is experiencing security handshake timeouts. Also had an issue where pilots were running even though his site was in downtime","title":"Support Update"},{"location":"meetings/2017/TechArea20170424/#osg-release-team","text":"3.3.24 Status 8 -9 Open 8 +2 In Progress 3 +3 Ready for Testing 0 0 Ready for Release 19 -4 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170424/#osg-3324","text":"Tim Theisen is handling the May 9th release Development freeze today VO Package ?? Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week next week Ready for Testing CVMFS X.509 Helper 1.0 gsissh in tarballs Upcoming: GlideinWMS 3.3.2 (Needs factory testing) Ready for Release None yet","title":"OSG 3.3.24"},{"location":"meetings/2017/TechArea20170424/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170424/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170424/#last-week","text":"GRACC operations transition. Lots of fires Cloud provider is adding more storage, and reconfiguring storage. Causes IO timeouts for VMs Blahp merge work continues. Now on to getting binaries in the correct areas. Stash CVMFS repo had to be built from scratch due to bug in CVMFS's auto-catalog (only affects stratum 0's)","title":"Last Week"},{"location":"meetings/2017/TechArea20170424/#this-week","text":"More GRACC Operations transition More BLAHP merge StashCP work to work with multiple origins.","title":"This Week"},{"location":"meetings/2017/TechArea20170424/#ongoing","text":"Gratia V2: Derek will be working on this. Jira project: GRACC . Project documentation located at https://opensciencegrid.github.io/gracc . New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.","title":"Ongoing"},{"location":"meetings/2017/TechArea20170501/","text":"OSG Technology Area Meeting, 1 May 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, TimT Announcements HTCondor Week Tuesday - Friday Suchandra on vacation until 5/5 Triage Duty This week: TimT Next week: BrianL 10 (+6) open tickets JIRA # of tickets State 167 4 Open 18 5 In Progress 18 +15 Ready for Testing 1 +1 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Package freeze today Utilizing JIRA ticket type and dropping unused types: Access Change Fault IT Help Ongoing Purchase Request Story Sub-task-ongoing Technical task Discussions XRootD 4.6.1 release candidate has CMS/OSG approval (Marian) but no new version cut yet. Punting to the next release. VOMS Admin Server also punted to next release. BrianL will start removing above unused JIRA ticket types next week and writing a summary for remaining ticket types Support Update UMich (BrianL) - Bob Ball's security handshakes were due to a local network issue UW-Madison (Carl) - Working on discrepancies with APEL reporting OSG Release Team Tim Theisen is handling the May 9th release Package freeze today VO Package v73 Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week this week 3.3.24 Status 2 -6 Open 3 -5 In Progress 14 +11 Ready for Testing 1 +1 Ready for Release 20 +1 Total OSG 3.3.24 Ready for Testing osg-configure 1.7.0 Edit lcmaps.db to use the VOMS plugin Add template lcmaps.db files drop unused BDII data Don't error out if user-vo-map missing, issue warning with suggestion Fix HTCondor Gratia probe to not call .eval() if not present CVMFS X.509 helper - fix for running inside a container gsissh in tarballs HTCondor 8.6.2 in Upcoming osg-build 1.9.0 Split osg-build into subpackages add supprt for git repos in .source files (for HCC) osg-build notes default options add support for 3.4 Ready for Release Upcoming: GlideinWMS 3.3.2 Discussions OSG Investigations Team Top priority is the APEL reports from GRACC. They need to debugged this week! GRACC-19 Last Week Lots of GRACC work, still taking some time BLAHP work, lots of little changes here and there. Stashcp to handle multiple origins StashCache documentation for admins of caches origins This Week More GRACC transition. Finish up BLAHP initial pull request Ongoing GRACC Project","title":"OSG Technology Area Meeting,  1 May 2017"},{"location":"meetings/2017/TechArea20170501/#osg-technology-area-meeting-1-may-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, TimT","title":"OSG Technology Area Meeting,  1 May 2017"},{"location":"meetings/2017/TechArea20170501/#announcements","text":"HTCondor Week Tuesday - Friday Suchandra on vacation until 5/5","title":"Announcements"},{"location":"meetings/2017/TechArea20170501/#triage-duty","text":"This week: TimT Next week: BrianL 10 (+6) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170501/#jira","text":"# of tickets State 167 4 Open 18 5 In Progress 18 +15 Ready for Testing 1 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170501/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170501/#osg-software-team","text":"Package freeze today Utilizing JIRA ticket type and dropping unused types: Access Change Fault IT Help Ongoing Purchase Request Story Sub-task-ongoing Technical task","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170501/#discussions","text":"XRootD 4.6.1 release candidate has CMS/OSG approval (Marian) but no new version cut yet. Punting to the next release. VOMS Admin Server also punted to next release. BrianL will start removing above unused JIRA ticket types next week and writing a summary for remaining ticket types","title":"Discussions"},{"location":"meetings/2017/TechArea20170501/#support-update","text":"UMich (BrianL) - Bob Ball's security handshakes were due to a local network issue UW-Madison (Carl) - Working on discrepancies with APEL reporting","title":"Support Update"},{"location":"meetings/2017/TechArea20170501/#osg-release-team","text":"Tim Theisen is handling the May 9th release Package freeze today VO Package v73 Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week this week 3.3.24 Status 2 -6 Open 3 -5 In Progress 14 +11 Ready for Testing 1 +1 Ready for Release 20 +1 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170501/#osg-3324","text":"Ready for Testing osg-configure 1.7.0 Edit lcmaps.db to use the VOMS plugin Add template lcmaps.db files drop unused BDII data Don't error out if user-vo-map missing, issue warning with suggestion Fix HTCondor Gratia probe to not call .eval() if not present CVMFS X.509 helper - fix for running inside a container gsissh in tarballs HTCondor 8.6.2 in Upcoming osg-build 1.9.0 Split osg-build into subpackages add supprt for git repos in .source files (for HCC) osg-build notes default options add support for 3.4 Ready for Release Upcoming: GlideinWMS 3.3.2","title":"OSG 3.3.24"},{"location":"meetings/2017/TechArea20170501/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170501/#osg-investigations-team","text":"Top priority is the APEL reports from GRACC. They need to debugged this week! GRACC-19","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170501/#last-week","text":"Lots of GRACC work, still taking some time BLAHP work, lots of little changes here and there. Stashcp to handle multiple origins StashCache documentation for admins of caches origins","title":"Last Week"},{"location":"meetings/2017/TechArea20170501/#this-week","text":"More GRACC transition. Finish up BLAHP initial pull request","title":"This Week"},{"location":"meetings/2017/TechArea20170501/#ongoing","text":"GRACC Project","title":"Ongoing"},{"location":"meetings/2017/TechArea20170508/","text":"OSG Technology Area Meeting, 8 May 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: BrianL Next week: Carl 7 ( 3) open tickets JIRA # of tickets State 161 3 Open 20 +2 In Progress 4 14 Ready for Testing 16 +15 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Dropped unused JIRA ticket types; still need to summarize remaining types OSG 3.4 preparation in full force TWiki - GH doc transition needs to begin Discussions None this week Support Update CHTC (BrianL/Jeff) - Asssisted Moate with bringing their frontend back up; turned out to just be a dead httpd service UW-Madison (Carl) - Working on discrepancies with APEL reporting OSG Release Team Tim Theisen is handling the May 9th release Release tomorrow VO Package v73 - release this week (Wednesday or Thursday) 3.3.24 Status 0 -2 Open 0 -3 In Progress 0 -14 Ready for Testing 17 +16 Ready for Release 17 -3 Total OSG 3.3.24 Ready for Release osg-configure 1.7.0 Edit lcmaps.db to use the VOMS plugin Add template lcmaps.db files Make all attributes relating to the defunct BDII service optional Don't error out if user-vo-map missing, issue warning with suggestion CVMFS X.509 helper - fix for running inside a container gsissh in tarballs Fix HTCondor Gratia probe to not call .eval() if not present osg-build 1.9.0 Split osg-build into subpackages add supprt for git repos in .source files (for HCC) osg-build notes default options add support for 3.4 Upcoming: HTCondor 8.6.2 Upcoming: GlideinWMS 3.3.2 Discussions None this week OSG Investigations Team Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server. Last Week Debug some GRACC issues with new changes to OIM VO Field of Science BLAHP work, lots of little changes here and there. StashCache documentation for admins of caches origins https://opensciencegrid.github.io/StashCache/ Gather investigation publications for Tim et. al. This Week Hopefully limited GRACC transition. Lots of StashCache authenticated packaging. Ongoing GRACC Project StashCache Project","title":"OSG Technology Area Meeting,  8 May 2017"},{"location":"meetings/2017/TechArea20170508/#osg-technology-area-meeting-8-may-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting,  8 May 2017"},{"location":"meetings/2017/TechArea20170508/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170508/#triage-duty","text":"This week: BrianL Next week: Carl 7 ( 3) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170508/#jira","text":"# of tickets State 161 3 Open 20 +2 In Progress 4 14 Ready for Testing 16 +15 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170508/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.24 2017-04-25 2017-05-01 2017-05-09 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170508/#osg-software-team","text":"Dropped unused JIRA ticket types; still need to summarize remaining types OSG 3.4 preparation in full force TWiki - GH doc transition needs to begin","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170508/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170508/#support-update","text":"CHTC (BrianL/Jeff) - Asssisted Moate with bringing their frontend back up; turned out to just be a dead httpd service UW-Madison (Carl) - Working on discrepancies with APEL reporting","title":"Support Update"},{"location":"meetings/2017/TechArea20170508/#osg-release-team","text":"Tim Theisen is handling the May 9th release Release tomorrow VO Package v73 - release this week (Wednesday or Thursday) 3.3.24 Status 0 -2 Open 0 -3 In Progress 0 -14 Ready for Testing 17 +16 Ready for Release 17 -3 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170508/#osg-3324","text":"Ready for Release osg-configure 1.7.0 Edit lcmaps.db to use the VOMS plugin Add template lcmaps.db files Make all attributes relating to the defunct BDII service optional Don't error out if user-vo-map missing, issue warning with suggestion CVMFS X.509 helper - fix for running inside a container gsissh in tarballs Fix HTCondor Gratia probe to not call .eval() if not present osg-build 1.9.0 Split osg-build into subpackages add supprt for git repos in .source files (for HCC) osg-build notes default options add support for 3.4 Upcoming: HTCondor 8.6.2 Upcoming: GlideinWMS 3.3.2","title":"OSG 3.3.24"},{"location":"meetings/2017/TechArea20170508/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170508/#osg-investigations-team","text":"Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170508/#last-week","text":"Debug some GRACC issues with new changes to OIM VO Field of Science BLAHP work, lots of little changes here and there. StashCache documentation for admins of caches origins https://opensciencegrid.github.io/StashCache/ Gather investigation publications for Tim et. al.","title":"Last Week"},{"location":"meetings/2017/TechArea20170508/#this-week","text":"Hopefully limited GRACC transition. Lots of StashCache authenticated packaging.","title":"This Week"},{"location":"meetings/2017/TechArea20170508/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2017/TechArea20170515/","text":"OSG Technology Area Meeting, 15 May 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: Carl Next week: Derek 6 ( 1) open tickets JIRA # of tickets State 163 +2 Open 26 +6 In Progress 4 0 Ready for Testing 0 16 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team OSG 3.4 targeted for June, will revisit progress at Software Freeze Discussions 3.4.0 testing strategy: mostly automated testing with strategic manual testing, particularly with Globus-dependent software Edgar training new UCSD staff on OSG basics this week Support Update OU (Derek) - slurm gratia-probe not reporting for a few days. Then suddenly re-appeared. Going to take some slurm magic to figure it out. But it's working now. OSG Release Team Suchandra Thapa is handling the June 13th release Development Release in two weeks VO Package v73 - release tomorrow 3.3.25 Both 3.4.0 Total Status 20 20 1 1 11 11 32 32 Open 7 7 0 0 6 6 13 13 In Progress 1 1 0 0 0 0 1 1 Ready for Testing 0 0 0 0 0 0 0 0 Ready for Release 1 1 0 0 0 0 1 1 Closed 29 29 1 1 17 17 47 47 Total OSG 3.3.25 Ready for Testing Upcoming: HTCondor 8.6.3 Discussions None this week OSG Investigations Team Week 2 of StashCache focus. Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server. Last Week Setup GRACC-ITB instance - Ongoing Better monitoring for CVMFS StashCache StashCache documentation for admins of caches origins https://opensciencegrid.github.io/StashCache/ This Week Improve docs even more through feedback from sites. Lots of StashCache authenticated packaging. Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 15 May 2017"},{"location":"meetings/2017/TechArea20170515/#osg-technology-area-meeting-15-may-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 15 May 2017"},{"location":"meetings/2017/TechArea20170515/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170515/#triage-duty","text":"This week: Carl Next week: Derek 6 ( 1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170515/#jira","text":"# of tickets State 163 +2 Open 26 +6 In Progress 4 0 Ready for Testing 0 16 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170515/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170515/#osg-software-team","text":"OSG 3.4 targeted for June, will revisit progress at Software Freeze","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170515/#discussions","text":"3.4.0 testing strategy: mostly automated testing with strategic manual testing, particularly with Globus-dependent software Edgar training new UCSD staff on OSG basics this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170515/#support-update","text":"OU (Derek) - slurm gratia-probe not reporting for a few days. Then suddenly re-appeared. Going to take some slurm magic to figure it out. But it's working now.","title":"Support Update"},{"location":"meetings/2017/TechArea20170515/#osg-release-team","text":"Suchandra Thapa is handling the June 13th release Development Release in two weeks VO Package v73 - release tomorrow 3.3.25 Both 3.4.0 Total Status 20 20 1 1 11 11 32 32 Open 7 7 0 0 6 6 13 13 In Progress 1 1 0 0 0 0 1 1 Ready for Testing 0 0 0 0 0 0 0 0 Ready for Release 1 1 0 0 0 0 1 1 Closed 29 29 1 1 17 17 47 47 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170515/#osg-3325","text":"Ready for Testing Upcoming: HTCondor 8.6.3","title":"OSG 3.3.25"},{"location":"meetings/2017/TechArea20170515/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170515/#osg-investigations-team","text":"Week 2 of StashCache focus. Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170515/#last-week","text":"Setup GRACC-ITB instance - Ongoing Better monitoring for CVMFS StashCache StashCache documentation for admins of caches origins https://opensciencegrid.github.io/StashCache/","title":"Last Week"},{"location":"meetings/2017/TechArea20170515/#this-week","text":"Improve docs even more through feedback from sites. Lots of StashCache authenticated packaging.","title":"This Week"},{"location":"meetings/2017/TechArea20170515/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170522/","text":"OSG Technology Area Meeting, 22 May 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Memorial day next Monday, meeting on Tuesday instead. Triage Duty This week: Derek Next week: Edgar 6 (0) open tickets JIRA # of tickets State 156 7 Open 32 +6 In Progress 4 0 Ready for Testing 0 0 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team OSG 3.4.0 targeted for June Discussions Singularity may be updated in EPEL, Derek will test it this week and if successful, this may be able to be dropped from 3.4. Support Update FIT (BrianL) - Assisting with GRAM - HTCondor-CE transition MIT (BrianL) - Investigating multiple GridFTP processes spawning on the real server when starting keepalived on the load balancing node OSG Release Team Suchandra Thapa is handling the June 13th release Development Freeze next Tuesday 3.3.25 Both 3.4.0 Total Status 2 18 16 15 7 4 25 7 Open 1 6 9 9 10 4 20 7 In Progress 1 0 3 3 0 0 4 3 Ready for Testing 0 0 0 0 0 0 0 0 Ready for Release 0 1 1 1 0 0 1 0 Closed 4 25 29 28 17 0 50 3 Total Ready for Testing OSG 3.3.25 Drop timeout_close.patch in globus-xio Both osg-update-vos: clean yum cache before downloading vo-client Update to rsv-perfsonar 1.3.1+ OSG 3.4.0 Upcoming Update to HTCondor 8.6.3+ in Upcoming Discussions None this week OSG Investigations Team Week 3 of StashCache focus. Effort decreasing... Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server. Last Week Setup GRACC-ITB instance - Ongoing Better GRACC Alerting Better StashCache Cache Alerting This Week Continue to improve StashCache alerting Help debug HTTP stalls on XrootD Improve StashCache docs even more through feedback from sites. (hopefully we get some) GRACC improvements to some memory leaky daemons Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 22 May 2017"},{"location":"meetings/2017/TechArea20170522/#osg-technology-area-meeting-22-may-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 22 May 2017"},{"location":"meetings/2017/TechArea20170522/#announcements","text":"Memorial day next Monday, meeting on Tuesday instead.","title":"Announcements"},{"location":"meetings/2017/TechArea20170522/#triage-duty","text":"This week: Derek Next week: Edgar 6 (0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170522/#jira","text":"# of tickets State 156 7 Open 32 +6 In Progress 4 0 Ready for Testing 0 0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170522/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170522/#osg-software-team","text":"OSG 3.4.0 targeted for June","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170522/#discussions","text":"Singularity may be updated in EPEL, Derek will test it this week and if successful, this may be able to be dropped from 3.4.","title":"Discussions"},{"location":"meetings/2017/TechArea20170522/#support-update","text":"FIT (BrianL) - Assisting with GRAM - HTCondor-CE transition MIT (BrianL) - Investigating multiple GridFTP processes spawning on the real server when starting keepalived on the load balancing node","title":"Support Update"},{"location":"meetings/2017/TechArea20170522/#osg-release-team","text":"Suchandra Thapa is handling the June 13th release Development Freeze next Tuesday 3.3.25 Both 3.4.0 Total Status 2 18 16 15 7 4 25 7 Open 1 6 9 9 10 4 20 7 In Progress 1 0 3 3 0 0 4 3 Ready for Testing 0 0 0 0 0 0 0 0 Ready for Release 0 1 1 1 0 0 1 0 Closed 4 25 29 28 17 0 50 3 Total Ready for Testing OSG 3.3.25 Drop timeout_close.patch in globus-xio Both osg-update-vos: clean yum cache before downloading vo-client Update to rsv-perfsonar 1.3.1+ OSG 3.4.0 Upcoming Update to HTCondor 8.6.3+ in Upcoming","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170522/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170522/#osg-investigations-team","text":"Week 3 of StashCache focus. Effort decreasing... Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170522/#last-week","text":"Setup GRACC-ITB instance - Ongoing Better GRACC Alerting Better StashCache Cache Alerting","title":"Last Week"},{"location":"meetings/2017/TechArea20170522/#this-week","text":"Continue to improve StashCache alerting Help debug HTTP stalls on XrootD Improve StashCache docs even more through feedback from sites. (hopefully we get some) GRACC improvements to some memory leaky daemons","title":"This Week"},{"location":"meetings/2017/TechArea20170522/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170530/","text":"OSG Technology Area Meeting, 30 May 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT, Vaibhav Announcements Triage Duty This week: Edgar Next week: Mat 9 (+3) open tickets JIRA # of tickets State 160 +4 Open 37 +5 In Progress 6 +2 Ready for Testing 1 +1 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.3.25 / 3.4.0 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 / 3.4.1 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.3.27 / 3.4.2 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Developer Tickets not RFT Mat 17 Brian 10 Marian 2 Edgar 2 Carl 1 Singularity build in EPEL testing looks good, we should be able to drop it from 3.4.0 Bob Ball starting GUMS - LCMAPS VOMS transition this week, Horst will start the edg-mkgridmap - LCMAPS VOMS transition next week Globus Toolkit support ends Jan 2018 Discussions Support Update Hosted CE GSISSH meeting today; Suchandra, BrianL, and possibly Jaime Frey will attend Suchandra had questions about running multiple CE services on a single host to limit IPv4 addresses. Will try IPv6 instead and coordinate with Edgar for Factory integration. Support Update MIT (BrianL) - Investigated open UDP ports for GridFTP: turned out to be an old version of GridFTP UFL (BrianL) - Assisted Bockjoo with Slurm timeout issues Purdue (Derek) - Assist in configuration of OSG_WN_TMP for their cluster. ALICE (Derek) - Alice VO had issues with submitting usage to GRACC. So far, seems like it's the probe. Fermilab (Suchandra) - disabling SSLv2/SSLv3 for BeStMan, BrianB to speak to them about transitioning them to another storage solution OSG Release Team Tim Theisen is handling the June 13th release Development Freeze today Data Release Coming: IGTF Update, VO Package?? 3.3.25 Both 3.4.0 Total Status 0 2 4 12 1 6 5 20 Open 3 +2 8 1 16 +6 27 +7 In Progress 1 +0 3 +0 2 +2 6 +2 Ready for Testing 0 +0 1 +1 0 +0 1 +1 Ready for Release 0 +0 0 1 0 +0 0 1 Closed 4 +0 16 13 19 +2 39 11 Total Ready for Testing OSG 3.3.25 Drop timeout_close.patch in globus-xio Both osg-update-vos: clean yum cache before downloading vo-client Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts OSG 3.4.0 Drop conflicts from cvmfs-config-osg Drop bestman2 and globus*run RSV metrics Upcoming Update to HTCondor 8.6.3+ in Upcoming (labeled for both releases but in Upcoming) Ready for Release OSG 3.3.25 Both Update to rsv-perfsonar 1.3.1+ OSG 3.4.0 Upcoming Discussions None this week OSG Investigations Team Focused StashCache effort is over. Last Week Setup GRACC-ITB instance - Ongoing Better GRACC Alerting - Now alert on all the things! Better StashCache Cache Alerting - HTTP Accesses GRACC daemons no longer leak memory like it's their job Python tar file objects keep the metadata for every object added to the tar file, be sure to clear it! CVMFS syncing with Stash now works, increased time out. Packaging of GRACC Agent daemons in Docker This Week Complete packaging of GRACC agents in docker. Help debug HTTP stalls on XrootD Improve StashCache docs even more through feedback from sites. (hopefully we get some) Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 30 May 2017"},{"location":"meetings/2017/TechArea20170530/#osg-technology-area-meeting-30-may-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT, Vaibhav","title":"OSG Technology Area Meeting, 30 May 2017"},{"location":"meetings/2017/TechArea20170530/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170530/#triage-duty","text":"This week: Edgar Next week: Mat 9 (+3) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170530/#jira","text":"# of tickets State 160 +4 Open 37 +5 In Progress 6 +2 Ready for Testing 1 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170530/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.3.25 / 3.4.0 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.3.26 / 3.4.1 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.3.27 / 3.4.2 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170530/#osg-software-team","text":"Developer Tickets not RFT Mat 17 Brian 10 Marian 2 Edgar 2 Carl 1 Singularity build in EPEL testing looks good, we should be able to drop it from 3.4.0 Bob Ball starting GUMS - LCMAPS VOMS transition this week, Horst will start the edg-mkgridmap - LCMAPS VOMS transition next week Globus Toolkit support ends Jan 2018","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170530/#discussions","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170530/#support-update","text":"Hosted CE GSISSH meeting today; Suchandra, BrianL, and possibly Jaime Frey will attend Suchandra had questions about running multiple CE services on a single host to limit IPv4 addresses. Will try IPv6 instead and coordinate with Edgar for Factory integration.","title":"Support Update"},{"location":"meetings/2017/TechArea20170530/#support-update_1","text":"MIT (BrianL) - Investigated open UDP ports for GridFTP: turned out to be an old version of GridFTP UFL (BrianL) - Assisted Bockjoo with Slurm timeout issues Purdue (Derek) - Assist in configuration of OSG_WN_TMP for their cluster. ALICE (Derek) - Alice VO had issues with submitting usage to GRACC. So far, seems like it's the probe. Fermilab (Suchandra) - disabling SSLv2/SSLv3 for BeStMan, BrianB to speak to them about transitioning them to another storage solution","title":"Support Update"},{"location":"meetings/2017/TechArea20170530/#osg-release-team","text":"Tim Theisen is handling the June 13th release Development Freeze today Data Release Coming: IGTF Update, VO Package?? 3.3.25 Both 3.4.0 Total Status 0 2 4 12 1 6 5 20 Open 3 +2 8 1 16 +6 27 +7 In Progress 1 +0 3 +0 2 +2 6 +2 Ready for Testing 0 +0 1 +1 0 +0 1 +1 Ready for Release 0 +0 0 1 0 +0 0 1 Closed 4 +0 16 13 19 +2 39 11 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170530/#ready-for-testing","text":"OSG 3.3.25 Drop timeout_close.patch in globus-xio Both osg-update-vos: clean yum cache before downloading vo-client Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts OSG 3.4.0 Drop conflicts from cvmfs-config-osg Drop bestman2 and globus*run RSV metrics Upcoming Update to HTCondor 8.6.3+ in Upcoming (labeled for both releases but in Upcoming)","title":"Ready for Testing"},{"location":"meetings/2017/TechArea20170530/#ready-for-release","text":"OSG 3.3.25 Both Update to rsv-perfsonar 1.3.1+ OSG 3.4.0 Upcoming","title":"Ready for Release"},{"location":"meetings/2017/TechArea20170530/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170530/#osg-investigations-team","text":"Focused StashCache effort is over.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170530/#last-week","text":"Setup GRACC-ITB instance - Ongoing Better GRACC Alerting - Now alert on all the things! Better StashCache Cache Alerting - HTTP Accesses GRACC daemons no longer leak memory like it's their job Python tar file objects keep the metadata for every object added to the tar file, be sure to clear it! CVMFS syncing with Stash now works, increased time out. Packaging of GRACC Agent daemons in Docker","title":"Last Week"},{"location":"meetings/2017/TechArea20170530/#this-week","text":"Complete packaging of GRACC agents in docker. Help debug HTTP stalls on XrootD Improve StashCache docs even more through feedback from sites. (hopefully we get some)","title":"This Week"},{"location":"meetings/2017/TechArea20170530/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170605/","text":"OSG Technology Area Meeting, 5 June 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT, Vaibhav Announcements Triage Duty This week: Mat Next week: Suchandra 8 ( 1) open tickets JIRA # of tickets State 165 +5 Open 14 23 In Progress 33 +27 Ready for Testing 1 +0 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.4.0 / 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Singularity build in EPEL stable so we can exclude it from OSG 3.4.0 Bob Ball completed GUMS - LCMAPS VOMS transition on one host last week, Horst will start the edg-mkgridmap - LCMAPS VOMS transition this week GridFTP/XRootD docs need updating for us with the LCMAPS VOMS plugin, any ideas on how to track down TWiki section usage? RHEL VMU tests working for the being even though our RHEL subscription ended. Moate working on a RHEL7 VM exec host using his developer license. Hadoop + Ganglia and GridFTP + umask questions sitting in osg-software list Discussions OSG 3.4 singularity policy needs to be clear and set Docs need to specify that the LCMAPS VOMS plugin is the preferred authentication method Kyle should be able to help search for TWiki %STARTSECTION% usage Mat will follow up with AGLT2 about a bug with the testing version of osg-configure that affects CEs BrianL will find other edg-mkgridmap sites and coordinate with Suchandra to transition hosted CEs to LCMAPS VOMS plugin BrianL will find owners for software mailing list issues TimC and BrianL to discuss the future of the many OSG Software mailing lists Support Update Purdue used OSG_WN_TMP configuration in osg-configure to set it to condor's scratch directory (execute directory?) (Derek) Utah renewed certificate, and is now working (Derek). APEL update from ALICE usage (Derek). OSG Release Team Tim Theisen is handling the June 13th release Package Freeze today Data Release Coming: IGTF Update, VO Package?? 3.3.25 Both 3.4.0 Total Status 0 +0 2 -2 0 -1 2 -3 Open 0 -3 2 -6 1 -15 3 -24 In Progress 4 +3 10 +7 19 +17 33 +27 Ready for Testing 0 +0 1 +0 0 +0 1 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 4 +0 15 -1 20 +1 39 +0 Total Ready for Testing OSG 3.3.25 Drop timeout_close.patch in globus-xio Release voms-admin-server-2.7.0-1.22+ Release osg-configure 1.8.1 Enable JSP implementation for tomcat webapps Both osg-update-vos: clean yum cache before downloading vo-client Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts Add ability to request whole node jobs osg-configure: reject empty allowed_vos in subclusters unnecessary check for OSG_APP and OSG_DATA in osg-configure xrootd-lcmaps-1.3.2-2 build fails for EL6 Update to XRootD to 4.6.1 Release StashCache metapackage 0.7+ osg-configure: Get default allowed_vos with lcmaps voms plugin Add OSG VOMS mapfile to osg-ce OSG 3.4.0 Drop conflicts from cvmfs-config-osg Update to HTCondor 8.6.3+ in OSG 3.4 Release osg-ce-3.4-1+ Drop conflicts from globus-gridftp-osg-extensions Remove requirements for packages dropped in 3.4 in osg-tested-internal osg-configure: Drop glexec support for 3.4 Release osg-configure 2.0.0 Prepare lcmaps for 3.4 Drop conflicts from HTCondor-CE packaging Drop bestman2 and globus*run RSV metrics osg-configure: Drop managedfork and network config from 2.0.0 Remove gridftp from the CE metapackages osg-configure: Drop osg-cleanup options from 10-misc.ini osg-configure: Deprecate GUMS support Drop client tools from osg-ce metapackages osg-configure: Disable GRAM configuration (2.0.0) osg-configure: Drop 'rsv is not installed' warning Drop glexec and java from osg-wn-client osg-configure: Remove \"configure-osg\" alias Upcoming Nothing Ready for Release OSG 3.3.25 Both Update to rsv-perfsonar 1.3.1+ OSG 3.4.0 Upcoming Discussions None this week OSG Investigations Team Last Week Setup GRACC-ITB instance - Ongoing Better GRACC Alerting Better StashCache Cache Alerting Docker'ification of GRACC Agents Fix unknown projectnames This Week Continue to improve StashCache alerting Continue to dockerify GRACC agents and services. Improve StashCache docs even more through feedback from sites. (hopefully we get some) Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting,  5 June 2017"},{"location":"meetings/2017/TechArea20170605/#osg-technology-area-meeting-5-june-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT, Vaibhav","title":"OSG Technology Area Meeting,  5 June 2017"},{"location":"meetings/2017/TechArea20170605/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170605/#triage-duty","text":"This week: Mat Next week: Suchandra 8 ( 1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170605/#jira","text":"# of tickets State 165 +5 Open 14 23 In Progress 33 +27 Ready for Testing 1 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170605/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.4.0 / 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170605/#osg-software-team","text":"Singularity build in EPEL stable so we can exclude it from OSG 3.4.0 Bob Ball completed GUMS - LCMAPS VOMS transition on one host last week, Horst will start the edg-mkgridmap - LCMAPS VOMS transition this week GridFTP/XRootD docs need updating for us with the LCMAPS VOMS plugin, any ideas on how to track down TWiki section usage? RHEL VMU tests working for the being even though our RHEL subscription ended. Moate working on a RHEL7 VM exec host using his developer license. Hadoop + Ganglia and GridFTP + umask questions sitting in osg-software list","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170605/#discussions","text":"OSG 3.4 singularity policy needs to be clear and set Docs need to specify that the LCMAPS VOMS plugin is the preferred authentication method Kyle should be able to help search for TWiki %STARTSECTION% usage Mat will follow up with AGLT2 about a bug with the testing version of osg-configure that affects CEs BrianL will find other edg-mkgridmap sites and coordinate with Suchandra to transition hosted CEs to LCMAPS VOMS plugin BrianL will find owners for software mailing list issues TimC and BrianL to discuss the future of the many OSG Software mailing lists","title":"Discussions"},{"location":"meetings/2017/TechArea20170605/#support-update","text":"Purdue used OSG_WN_TMP configuration in osg-configure to set it to condor's scratch directory (execute directory?) (Derek) Utah renewed certificate, and is now working (Derek). APEL update from ALICE usage (Derek).","title":"Support Update"},{"location":"meetings/2017/TechArea20170605/#osg-release-team","text":"Tim Theisen is handling the June 13th release Package Freeze today Data Release Coming: IGTF Update, VO Package?? 3.3.25 Both 3.4.0 Total Status 0 +0 2 -2 0 -1 2 -3 Open 0 -3 2 -6 1 -15 3 -24 In Progress 4 +3 10 +7 19 +17 33 +27 Ready for Testing 0 +0 1 +0 0 +0 1 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 4 +0 15 -1 20 +1 39 +0 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170605/#ready-for-testing","text":"OSG 3.3.25 Drop timeout_close.patch in globus-xio Release voms-admin-server-2.7.0-1.22+ Release osg-configure 1.8.1 Enable JSP implementation for tomcat webapps Both osg-update-vos: clean yum cache before downloading vo-client Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts Add ability to request whole node jobs osg-configure: reject empty allowed_vos in subclusters unnecessary check for OSG_APP and OSG_DATA in osg-configure xrootd-lcmaps-1.3.2-2 build fails for EL6 Update to XRootD to 4.6.1 Release StashCache metapackage 0.7+ osg-configure: Get default allowed_vos with lcmaps voms plugin Add OSG VOMS mapfile to osg-ce OSG 3.4.0 Drop conflicts from cvmfs-config-osg Update to HTCondor 8.6.3+ in OSG 3.4 Release osg-ce-3.4-1+ Drop conflicts from globus-gridftp-osg-extensions Remove requirements for packages dropped in 3.4 in osg-tested-internal osg-configure: Drop glexec support for 3.4 Release osg-configure 2.0.0 Prepare lcmaps for 3.4 Drop conflicts from HTCondor-CE packaging Drop bestman2 and globus*run RSV metrics osg-configure: Drop managedfork and network config from 2.0.0 Remove gridftp from the CE metapackages osg-configure: Drop osg-cleanup options from 10-misc.ini osg-configure: Deprecate GUMS support Drop client tools from osg-ce metapackages osg-configure: Disable GRAM configuration (2.0.0) osg-configure: Drop 'rsv is not installed' warning Drop glexec and java from osg-wn-client osg-configure: Remove \"configure-osg\" alias Upcoming Nothing","title":"Ready for Testing"},{"location":"meetings/2017/TechArea20170605/#ready-for-release","text":"OSG 3.3.25 Both Update to rsv-perfsonar 1.3.1+ OSG 3.4.0 Upcoming","title":"Ready for Release"},{"location":"meetings/2017/TechArea20170605/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170605/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170605/#last-week","text":"Setup GRACC-ITB instance - Ongoing Better GRACC Alerting Better StashCache Cache Alerting Docker'ification of GRACC Agents Fix unknown projectnames","title":"Last Week"},{"location":"meetings/2017/TechArea20170605/#this-week","text":"Continue to improve StashCache alerting Continue to dockerify GRACC agents and services. Improve StashCache docs even more through feedback from sites. (hopefully we get some)","title":"This Week"},{"location":"meetings/2017/TechArea20170605/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170612/","text":"OSG Technology Area Meeting, 12 June 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT, Vaibhav Announcements Vaibhav at UW Madison this week Mat out this week Triage Duty This week: Suchandra Next week: TimT 9 (+) open tickets JIRA # of tickets State 157 8 Open 18 +4 In Progress 14 29 Ready for Testing 42 +1 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.4.0 / 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team LCMAPS VOMS plugin bug : all FQANs are considered instead of the first one. Blocker for June release. Pre-release tests failed due to install/update failures required osg-koji regen-repo osg-3.4-el{6,7}-prerelease Need to start transitioning hosted CEs to LCMAPS VOMS plugin this week, after the release RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription Horst completed the edg-mkgridmap - LCMAPS VOMS transition successfully last week xrootd-cmstfc (contrib) fails to build for EL7, need help with CMake to place libs in /usr/lib64 rather than /usr/lib/ Discussions None this week Support Update FIT (BrianL) - Investing CE that doesn't appear to be running TAMU (BrianL) - SAM tests failing intermittently due to transient issues submitting to the CE UFL (BrianL) - Bockjoo found a blahp bug that resulted in incorrect multicore job requests OSG Release Team Tim Theisen is handling the June 13th release Software Release tomorrow Data Release Coming: IGTF Update, VO Package 3.3.25 Both 3.4.0 Total Status 0 +0 0 -2 0 +0 0 -2 Open 0 +0 1 -1 0 -1 1 -2 In Progress 0 -4 0 -10 0 -19 0 -33 Ready for Testing 4 +4 15 +14 22 +22 41 +40 Ready for Release 0 +0 1 +1 0 +0 1 +1 Closed 4 +0 17 +2 22 +2 43 +4 Total Late breaking update lcmaps-plugins-voms maps all FQANs Ready for Release OSG 3.3.25 Drop timeout_close.patch in globus-xio Release voms-admin-server-2.7.0-1.22+ Release osg-configure 1.8.1 Enable JSP implementation for tomcat webapps Both osg-update-vos: clean yum cache before downloading vo-client Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts Add ability to request whole node jobs osg-configure: reject empty allowed_vos in subclusters unnecessary check for OSG_APP and OSG_DATA in osg-configure Release xrootd-lcmaps-1.3.2-2 + Update to XRootD to 4.6.1 Release StashCache metapackage 0.7+ osg-configure: Get default allowed_vos with lcmaps voms plugin Add OSG VOMS mapfile to osg-ce lcmaps-plugins-voms maps all FQANs Document configuration of lcmaps-voms-plugin Release Glideinwms v3.2.19+ Release osg-build 1.10.0 osg-build: drop vdt-build osg-build: drop ~/.osg-build.ini Add vo-client-lcmaps-voms dependency to osg-gridftp OSG 3.4.0 Drop conflicts from cvmfs-config-osg Update to HTCondor 8.6.3+ in OSG 3.4 Release osg-ce-3.4-1+ Drop conflicts from globus-gridftp-osg-extensions Remove requirements for packages dropped in 3.4 in osg-tested-internal osg-configure: Drop glexec support for 3.4 Release osg-configure 2.0.0 Prepare lcmaps for 3.4 Drop conflicts from HTCondor-CE packaging Drop bestman2 and globus*run RSV metrics osg-configure: Drop managedfork and network config from 2.0.0 Remove gridftp from the CE metapackages osg-configure: Drop osg-cleanup options from 10-misc.ini osg-configure: Deprecate GUMS support Drop client tools from osg-ce metapackages osg-configure: Disable GRAM configuration (2.0.0) osg-configure: Drop 'rsv is not installed' warning Drop glexec and java from osg-wn-client osg-configure: Remove \"configure-osg\" alias Drop edg-mkgridmap from OSG 3.4 Drop bestman2 from OSG 3.4 Drop GUMS from 3.4 Discussions None this week OSG Investigations Team Lots of vacation from Investigations team this week. Not much to update. Last Week Setup GRACC-ITB instance - Ongoing Better StashCache Cache Alerting Docker'ification of GRACC Agents This Week Continue to dockerify GRACC agents and services. Next on the list is gracc-summary. Improve StashCache docs even more through feedback from sites. (hopefully we get some) Write StashCache article for user support team. Some BLAHP work. Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 12 June 2017"},{"location":"meetings/2017/TechArea20170612/#osg-technology-area-meeting-12-june-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT, Vaibhav","title":"OSG Technology Area Meeting, 12 June 2017"},{"location":"meetings/2017/TechArea20170612/#announcements","text":"Vaibhav at UW Madison this week Mat out this week","title":"Announcements"},{"location":"meetings/2017/TechArea20170612/#triage-duty","text":"This week: Suchandra Next week: TimT 9 (+) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170612/#jira","text":"# of tickets State 157 8 Open 18 +4 In Progress 14 29 Ready for Testing 42 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170612/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.4.0 / 3.3.25 2017-05-30 2017-06-05 2017-06-13 5 week cycle 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170612/#osg-software-team","text":"LCMAPS VOMS plugin bug : all FQANs are considered instead of the first one. Blocker for June release. Pre-release tests failed due to install/update failures required osg-koji regen-repo osg-3.4-el{6,7}-prerelease Need to start transitioning hosted CEs to LCMAPS VOMS plugin this week, after the release RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription Horst completed the edg-mkgridmap - LCMAPS VOMS transition successfully last week xrootd-cmstfc (contrib) fails to build for EL7, need help with CMake to place libs in /usr/lib64 rather than /usr/lib/","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170612/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170612/#support-update","text":"FIT (BrianL) - Investing CE that doesn't appear to be running TAMU (BrianL) - SAM tests failing intermittently due to transient issues submitting to the CE UFL (BrianL) - Bockjoo found a blahp bug that resulted in incorrect multicore job requests","title":"Support Update"},{"location":"meetings/2017/TechArea20170612/#osg-release-team","text":"Tim Theisen is handling the June 13th release Software Release tomorrow Data Release Coming: IGTF Update, VO Package 3.3.25 Both 3.4.0 Total Status 0 +0 0 -2 0 +0 0 -2 Open 0 +0 1 -1 0 -1 1 -2 In Progress 0 -4 0 -10 0 -19 0 -33 Ready for Testing 4 +4 15 +14 22 +22 41 +40 Ready for Release 0 +0 1 +1 0 +0 1 +1 Closed 4 +0 17 +2 22 +2 43 +4 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170612/#late-breaking-update","text":"lcmaps-plugins-voms maps all FQANs","title":"Late breaking update"},{"location":"meetings/2017/TechArea20170612/#ready-for-release","text":"OSG 3.3.25 Drop timeout_close.patch in globus-xio Release voms-admin-server-2.7.0-1.22+ Release osg-configure 1.8.1 Enable JSP implementation for tomcat webapps Both osg-update-vos: clean yum cache before downloading vo-client Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts Add ability to request whole node jobs osg-configure: reject empty allowed_vos in subclusters unnecessary check for OSG_APP and OSG_DATA in osg-configure Release xrootd-lcmaps-1.3.2-2 + Update to XRootD to 4.6.1 Release StashCache metapackage 0.7+ osg-configure: Get default allowed_vos with lcmaps voms plugin Add OSG VOMS mapfile to osg-ce lcmaps-plugins-voms maps all FQANs Document configuration of lcmaps-voms-plugin Release Glideinwms v3.2.19+ Release osg-build 1.10.0 osg-build: drop vdt-build osg-build: drop ~/.osg-build.ini Add vo-client-lcmaps-voms dependency to osg-gridftp OSG 3.4.0 Drop conflicts from cvmfs-config-osg Update to HTCondor 8.6.3+ in OSG 3.4 Release osg-ce-3.4-1+ Drop conflicts from globus-gridftp-osg-extensions Remove requirements for packages dropped in 3.4 in osg-tested-internal osg-configure: Drop glexec support for 3.4 Release osg-configure 2.0.0 Prepare lcmaps for 3.4 Drop conflicts from HTCondor-CE packaging Drop bestman2 and globus*run RSV metrics osg-configure: Drop managedfork and network config from 2.0.0 Remove gridftp from the CE metapackages osg-configure: Drop osg-cleanup options from 10-misc.ini osg-configure: Deprecate GUMS support Drop client tools from osg-ce metapackages osg-configure: Disable GRAM configuration (2.0.0) osg-configure: Drop 'rsv is not installed' warning Drop glexec and java from osg-wn-client osg-configure: Remove \"configure-osg\" alias Drop edg-mkgridmap from OSG 3.4 Drop bestman2 from OSG 3.4 Drop GUMS from 3.4","title":"Ready for Release"},{"location":"meetings/2017/TechArea20170612/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170612/#osg-investigations-team","text":"Lots of vacation from Investigations team this week. Not much to update.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170612/#last-week","text":"Setup GRACC-ITB instance - Ongoing Better StashCache Cache Alerting Docker'ification of GRACC Agents","title":"Last Week"},{"location":"meetings/2017/TechArea20170612/#this-week","text":"Continue to dockerify GRACC agents and services. Next on the list is gracc-summary. Improve StashCache docs even more through feedback from sites. (hopefully we get some) Write StashCache article for user support team. Some BLAHP work.","title":"This Week"},{"location":"meetings/2017/TechArea20170612/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170619/","text":"OSG Technology Area Meeting, 19 June 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Announcements Triage Duty This week: TimT Next week: BrianL 8 ( 1) open tickets JIRA # of tickets State 156 8 Open 13 5 In Progress 3 1 Ready for Testing 1 41 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Dev freeze next Monday Started transitioning hosted CEs to LCMAPS VOMS plugin last week RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription xrootd-cmstfc (contrib) fails to build for EL7, need help with CMake to place libs in /usr/lib64 rather than /usr/lib/ Discussions None this week Support Update CTSC (BrianL) - Provided rough size estimate of OSG CE packaging FIT (BrianL) - Assisting transition from GRAM to HTCondor-CE TAMU (BrianL) - SAM tests are still getting held with \"Job not found\" Syracuse (Derek) - Implemented GPU submission on their HTCondor-CE. Fermilab is creating an entry on the ITB GlideinWMS factories to utilize and test this new GPU capability. OSG will follow suite once singularity configuration is developed. UCSD (Derek) - Gave some assistance to job route configuration for GPUs. Nebraska (Derek) - Assisted in debugging missing hours in WLCG report. Turns out that removed jobs were not being accounted correctly. Also, Multi-CPU jobs and whole node jobs where not being accounted. OSG Release Team Tim Theisen is handling the July 11th release Data Release Coming: IGTF Update, VO Package 3.3.26 Both 3.4.1 Total Status 1 +1 12 +12 5 +5 18 +18 Open 1 +1 2 +2 1 +1 4 +4 In Progress 0 +0 3 +3 0 +0 3 +3 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 2 +2 17 +0 6 +6 25 +25 Total Ready for Testing Fix for CVMFS client failing to mount when very large groups exist Added ability to include arbitrary ClassAd attributes in Gratia records Added osg-configure-misc dependency to osg-gridftp Discussions None this week OSG Investigations Team Lots of vacation from Investigations team this week. Not much to update. Last Week GRACC debugging of Underreported sites Debug Nova CVMFS repo, will need to redo repo with chunking. Blog post on StashCache: https://djw8605.github.io/2017/06/14/stashcache/ and on Planet OSG http://blogs.grid.iu.edu/ This Week Reprocess underreported sites and upload new APEL report to WLCG Docker'ification of GRACC Agents Packaging of CVMFS-Sync and configurations. Investigate backups of GRACC peripheral services Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 19 June 2017"},{"location":"meetings/2017/TechArea20170619/#osg-technology-area-meeting-19-june-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending:","title":"OSG Technology Area Meeting, 19 June 2017"},{"location":"meetings/2017/TechArea20170619/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170619/#triage-duty","text":"This week: TimT Next week: BrianL 8 ( 1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170619/#jira","text":"# of tickets State 156 8 Open 13 5 In Progress 3 1 Ready for Testing 1 41 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170619/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170619/#osg-software-team","text":"Dev freeze next Monday Started transitioning hosted CEs to LCMAPS VOMS plugin last week RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription xrootd-cmstfc (contrib) fails to build for EL7, need help with CMake to place libs in /usr/lib64 rather than /usr/lib/","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170619/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170619/#support-update","text":"CTSC (BrianL) - Provided rough size estimate of OSG CE packaging FIT (BrianL) - Assisting transition from GRAM to HTCondor-CE TAMU (BrianL) - SAM tests are still getting held with \"Job not found\" Syracuse (Derek) - Implemented GPU submission on their HTCondor-CE. Fermilab is creating an entry on the ITB GlideinWMS factories to utilize and test this new GPU capability. OSG will follow suite once singularity configuration is developed. UCSD (Derek) - Gave some assistance to job route configuration for GPUs. Nebraska (Derek) - Assisted in debugging missing hours in WLCG report. Turns out that removed jobs were not being accounted correctly. Also, Multi-CPU jobs and whole node jobs where not being accounted.","title":"Support Update"},{"location":"meetings/2017/TechArea20170619/#osg-release-team","text":"Tim Theisen is handling the July 11th release Data Release Coming: IGTF Update, VO Package 3.3.26 Both 3.4.1 Total Status 1 +1 12 +12 5 +5 18 +18 Open 1 +1 2 +2 1 +1 4 +4 In Progress 0 +0 3 +3 0 +0 3 +3 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 2 +2 17 +0 6 +6 25 +25 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170619/#ready-for-testing","text":"Fix for CVMFS client failing to mount when very large groups exist Added ability to include arbitrary ClassAd attributes in Gratia records Added osg-configure-misc dependency to osg-gridftp","title":"Ready for Testing"},{"location":"meetings/2017/TechArea20170619/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170619/#osg-investigations-team","text":"Lots of vacation from Investigations team this week. Not much to update.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170619/#last-week","text":"GRACC debugging of Underreported sites Debug Nova CVMFS repo, will need to redo repo with chunking. Blog post on StashCache: https://djw8605.github.io/2017/06/14/stashcache/ and on Planet OSG http://blogs.grid.iu.edu/","title":"Last Week"},{"location":"meetings/2017/TechArea20170619/#this-week","text":"Reprocess underreported sites and upload new APEL report to WLCG Docker'ification of GRACC Agents Packaging of CVMFS-Sync and configurations. Investigate backups of GRACC peripheral services","title":"This Week"},{"location":"meetings/2017/TechArea20170619/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170626/","text":"OSG Technology Area Meeting, 26 June 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, Suchandra, TimC, Vaibhav Announcements TimT, Edgar out until next week Derek will be 50% OSG starting July 1, down from 75% Triage Duty This week: BrianL Next week: Carl 8 (+0) open tickets JIRA # of tickets State 149 7 Open 18 +5 In Progress 6 +3 Ready for Testing 1 +0 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Dev freeze today: Owner # open tickets BrianL 3 Carl 2 Mat 2 Suchandra: How's the LCMAPS VOMS transition going? HTCondor-CE whole node accounting + memory request issues VMU tests: Erin investigating network issues with new RHEL VMs using the dev subscription. Dakota was brought up to speed on image generation/troubleshooting. ITB progress: New pool configs sprayed out for separated CM and an additional CE for testing pre-release Internal doc migration started and lives here (formerly https://github.com/brianhlin/technology/tree/internal_migration) xrootd-cmstfc (contrib) fails to build for EL7, need help with CMake to place libs in /usr/lib64 rather than /usr/lib/ Discussions RSV JIRA ticket incoming to fix querying condor_q output Incoming Gratia probe changes for whole node accounting issues by Derek, Carl to review Hosted CE LCMAPS VOMS transition complete Support Update FIT (BrianL) - Jobs held, passed on troubleshooting doc TAMU (BrianL) - SAM tests are better with updates to their scheduler config. Jobs submitted to their backend condor are being held with \"job not found\" UNL (Derek) - Reprocessed Accounting records for whole node jobs. Pull request for HTCondor-CE (https://github.com/opensciencegrid/htcondor-ce/pull/151) and Gratia-Probes (https://github.com/opensciencegrid/gratia-probe/pull/18) Syracuse UCSD (Derek) - GPU nodes are starting up. Small amount of support last week, but this week I expect production use to start, so possibly some user support. OSG Release Team Tim Theisen is handling the July 11th release 3.3.26 Both 3.4.1 Total Status 0 1 1 0 5 1 17 Open 0 1 6 +4 0 1 6 +2 In Progress 1 +1 6 +3 3 +3 10 +7 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 1 1 13 4 3 3 17 8 Total Ready for Testing Condor 8.6.4 and 8.7.2 in 3.4 and upcoming, respectively. Including af ix for HTCondor-CE-Bosco without certs blahp fix for multicore requests for SLURM batch systems New package, gridftp-dsi-posix , to replace xrootd-dsi Fix for GridFTP startup to use the correct plugin configuration Fix for CVMFS client failing to mount when very large groups exist Added ability to include arbitrary ClassAd attributes in Gratia records Added osg-configure-misc dependency to osg-gridftp Fix for HDFS NameNode infinite loop Discussions None this week OSG Investigations Team Lots of vacation from Investigations team this week. Not much to update. Last Week Reprocess underreported sites and upload new APEL report to WLCG Debug Nova CVMFS repo, will need to redo repo with chunking. Packaging of CVMFS-Sync and configurations. https://github.com/bbockelm/cvmfs-sync/pull/1 Investigate backups of GRACC peripheral services This Week Will need to redo nova repo with chunking. Docker'ification of GRACC Agents Start backups of grafana configurations (dashboards and datasources) Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 26 June 2017"},{"location":"meetings/2017/TechArea20170626/#osg-technology-area-meeting-26-june-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, Suchandra, TimC, Vaibhav","title":"OSG Technology Area Meeting, 26 June 2017"},{"location":"meetings/2017/TechArea20170626/#announcements","text":"TimT, Edgar out until next week Derek will be 50% OSG starting July 1, down from 75%","title":"Announcements"},{"location":"meetings/2017/TechArea20170626/#triage-duty","text":"This week: BrianL Next week: Carl 8 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170626/#jira","text":"# of tickets State 149 7 Open 18 +5 In Progress 6 +3 Ready for Testing 1 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170626/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170626/#osg-software-team","text":"Dev freeze today: Owner # open tickets BrianL 3 Carl 2 Mat 2 Suchandra: How's the LCMAPS VOMS transition going? HTCondor-CE whole node accounting + memory request issues VMU tests: Erin investigating network issues with new RHEL VMs using the dev subscription. Dakota was brought up to speed on image generation/troubleshooting. ITB progress: New pool configs sprayed out for separated CM and an additional CE for testing pre-release Internal doc migration started and lives here (formerly https://github.com/brianhlin/technology/tree/internal_migration) xrootd-cmstfc (contrib) fails to build for EL7, need help with CMake to place libs in /usr/lib64 rather than /usr/lib/","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170626/#discussions","text":"RSV JIRA ticket incoming to fix querying condor_q output Incoming Gratia probe changes for whole node accounting issues by Derek, Carl to review Hosted CE LCMAPS VOMS transition complete","title":"Discussions"},{"location":"meetings/2017/TechArea20170626/#support-update","text":"FIT (BrianL) - Jobs held, passed on troubleshooting doc TAMU (BrianL) - SAM tests are better with updates to their scheduler config. Jobs submitted to their backend condor are being held with \"job not found\" UNL (Derek) - Reprocessed Accounting records for whole node jobs. Pull request for HTCondor-CE (https://github.com/opensciencegrid/htcondor-ce/pull/151) and Gratia-Probes (https://github.com/opensciencegrid/gratia-probe/pull/18) Syracuse UCSD (Derek) - GPU nodes are starting up. Small amount of support last week, but this week I expect production use to start, so possibly some user support.","title":"Support Update"},{"location":"meetings/2017/TechArea20170626/#osg-release-team","text":"Tim Theisen is handling the July 11th release 3.3.26 Both 3.4.1 Total Status 0 1 1 0 5 1 17 Open 0 1 6 +4 0 1 6 +2 In Progress 1 +1 6 +3 3 +3 10 +7 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 1 1 13 4 3 3 17 8 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170626/#ready-for-testing","text":"Condor 8.6.4 and 8.7.2 in 3.4 and upcoming, respectively. Including af ix for HTCondor-CE-Bosco without certs blahp fix for multicore requests for SLURM batch systems New package, gridftp-dsi-posix , to replace xrootd-dsi Fix for GridFTP startup to use the correct plugin configuration Fix for CVMFS client failing to mount when very large groups exist Added ability to include arbitrary ClassAd attributes in Gratia records Added osg-configure-misc dependency to osg-gridftp Fix for HDFS NameNode infinite loop","title":"Ready for Testing"},{"location":"meetings/2017/TechArea20170626/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170626/#osg-investigations-team","text":"Lots of vacation from Investigations team this week. Not much to update.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170626/#last-week","text":"Reprocess underreported sites and upload new APEL report to WLCG Debug Nova CVMFS repo, will need to redo repo with chunking. Packaging of CVMFS-Sync and configurations. https://github.com/bbockelm/cvmfs-sync/pull/1 Investigate backups of GRACC peripheral services","title":"Last Week"},{"location":"meetings/2017/TechArea20170626/#this-week","text":"Will need to redo nova repo with chunking. Docker'ification of GRACC Agents Start backups of grafana configurations (dashboards and datasources)","title":"This Week"},{"location":"meetings/2017/TechArea20170626/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170703/","text":"OSG Technology Area Meeting, 3 July 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, Carl, Derek, Mat, Marian, Suchandra, Vaibhav Announcements None Triage Duty This week: Carl Next week: Derek 6 (-2) open tickets JIRA # of tickets State 149 (+0) Open 11 (-7) In Progress 23 (+17) Ready for Testing 2 (+1) Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Package freeze today All tickets ready for testing, one ready for release Support Update Utah (Derek) - working with Utah, still debugging. Proxy is trying to be read by the pilot and the proxy doesn't exist in the expected directory. Only happening on one cluster IIT, Clemson (BrianL) - Stacktraces when authenticating some proxies OSG Release Team 3.3.26 Both 3.4.1 Total Status 0 +0 0 1 0 0 1 Open 0 +0 0 +0 0 +0 0 6 In Progress 2 +1 18 +12 3 +0 23 +13 Ready for Testing 0 +0 1 +0 0 +0 1 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 2 +1 19 +6 3 +0 24 +7 Total Short testing week due to holiday OSG Investigations Team Last Week OSG-Connect wanted some project renames, done, but waiting on verification. This Week GRACC host cert expired, but that only affects backups so everything is still online WCLG wants some info on missing accounting, but they are slow on finalizing their own accounting. But it looks like it is now finished. Derek will be at PEARC next week and will be presenting Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 3 July 2017"},{"location":"meetings/2017/TechArea20170703/#osg-technology-area-meeting-3-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, Carl, Derek, Mat, Marian, Suchandra, Vaibhav","title":"OSG Technology Area Meeting, 3 July 2017"},{"location":"meetings/2017/TechArea20170703/#announcements","text":"None","title":"Announcements"},{"location":"meetings/2017/TechArea20170703/#triage-duty","text":"This week: Carl Next week: Derek 6 (-2) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170703/#jira","text":"# of tickets State 149 (+0) Open 11 (-7) In Progress 23 (+17) Ready for Testing 2 (+1) Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170703/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170703/#osg-software-team","text":"Package freeze today All tickets ready for testing, one ready for release","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170703/#support-update","text":"Utah (Derek) - working with Utah, still debugging. Proxy is trying to be read by the pilot and the proxy doesn't exist in the expected directory. Only happening on one cluster IIT, Clemson (BrianL) - Stacktraces when authenticating some proxies","title":"Support Update"},{"location":"meetings/2017/TechArea20170703/#osg-release-team","text":"3.3.26 Both 3.4.1 Total Status 0 +0 0 1 0 0 1 Open 0 +0 0 +0 0 +0 0 6 In Progress 2 +1 18 +12 3 +0 23 +13 Ready for Testing 0 +0 1 +0 0 +0 1 +0 Ready for Release 0 +0 0 +0 0 +0 0 +0 Closed 2 +1 19 +6 3 +0 24 +7 Total Short testing week due to holiday","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170703/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170703/#last-week","text":"OSG-Connect wanted some project renames, done, but waiting on verification.","title":"Last Week"},{"location":"meetings/2017/TechArea20170703/#this-week","text":"GRACC host cert expired, but that only affects backups so everything is still online WCLG wants some info on missing accounting, but they are slow on finalizing their own accounting. But it looks like it is now finished. Derek will be at PEARC next week and will be presenting","title":"This Week"},{"location":"meetings/2017/TechArea20170703/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170710/","text":"OSG Technology Area Meeting, 10 July 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Edgar, Marian, Mat, TimT, Vaibhav Announcements OSG User School next week Triage Duty This week: Derek Next week: Edgar 5 ( 1) open tickets JIRA # of tickets State 156 +7 Open 11 +0 In Progress 2 21 Ready for Testing 25 +23 Ready for Release Release Schedule Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Two weeks until August development freeze Major LCMAPS VOMS plugin issues plaguing multiple sites, causing extensive downtime Discussions The root cause of the LCMAPS VOMS segfaults appear to be due to memory issues in the proxy verification module. BrianB hopes to have a fix by the end of the day. Support Update Clemson, IIT, UConn (BrianL) - LCMAPS VOMS plugin issue above OSG Release Team Tim Theisen is handling the July 11th release Release tomorrow 3.3.26 Both 3.4.1 Total Status 0 +0 0 +0 0 +0 0 +0 Open 0 +0 0 +0 0 +0 0 +0 In Progress 0 -2 18 -18 0 -3 0 -23 Ready for Testing 2 +2 19 +18 4 +4 25 +24 Ready for Release 2 +0 19 +0 4 +1 25 +1 Total Discussions Operations will be adopting a more flexible release schedule, at least for changes transparent to the user Cert update incoming; Edgar will help test it while Suchandra is at PEARC. In the future, it may make more sense to test cert packages in an automated fashion \\minus; perhaps with rpmdiff . OSG Investigations Team This Week Derek at PEARC this week and the OSG User School next week Contributing to CVMFS and looking at he LIGO use case again Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 10 July 2017"},{"location":"meetings/2017/TechArea20170710/#osg-technology-area-meeting-10-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Edgar, Marian, Mat, TimT, Vaibhav","title":"OSG Technology Area Meeting, 10 July 2017"},{"location":"meetings/2017/TechArea20170710/#announcements","text":"OSG User School next week","title":"Announcements"},{"location":"meetings/2017/TechArea20170710/#triage-duty","text":"This week: Derek Next week: Edgar 5 ( 1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170710/#jira","text":"# of tickets State 156 +7 Open 11 +0 In Progress 2 21 Ready for Testing 25 +23 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170710/#release-schedule","text":"Version Development Freeze Package Freeze Release Notes 3.4.1 / 3.3.26 2017-06-26 2017-07-03 2017-07-11 Independence Day 3.4.2 / 3.3.27 2017-07-24 2017-07-31 2017-08-08 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170710/#osg-software-team","text":"Two weeks until August development freeze Major LCMAPS VOMS plugin issues plaguing multiple sites, causing extensive downtime","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170710/#discussions","text":"The root cause of the LCMAPS VOMS segfaults appear to be due to memory issues in the proxy verification module. BrianB hopes to have a fix by the end of the day.","title":"Discussions"},{"location":"meetings/2017/TechArea20170710/#support-update","text":"Clemson, IIT, UConn (BrianL) - LCMAPS VOMS plugin issue above","title":"Support Update"},{"location":"meetings/2017/TechArea20170710/#osg-release-team","text":"Tim Theisen is handling the July 11th release Release tomorrow 3.3.26 Both 3.4.1 Total Status 0 +0 0 +0 0 +0 0 +0 Open 0 +0 0 +0 0 +0 0 +0 In Progress 0 -2 18 -18 0 -3 0 -23 Ready for Testing 2 +2 19 +18 4 +4 25 +24 Ready for Release 2 +0 19 +0 4 +1 25 +1 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170710/#discussions_1","text":"Operations will be adopting a more flexible release schedule, at least for changes transparent to the user Cert update incoming; Edgar will help test it while Suchandra is at PEARC. In the future, it may make more sense to test cert packages in an automated fashion \\minus; perhaps with rpmdiff .","title":"Discussions"},{"location":"meetings/2017/TechArea20170710/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170710/#this-week","text":"Derek at PEARC this week and the OSG User School next week Contributing to CVMFS and looking at he LIGO use case again","title":"This Week"},{"location":"meetings/2017/TechArea20170710/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170717/","text":"OSG Technology Area Meeting, 17 July 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, Edgar, Marian, Mat, Suchandra, Tim T Announcements OSG User School this week Triage Duty This week: Edgar Next week: Mat 11 (+6) open tickets JIRA # of tickets State 155 1 Open 23 +12 In Progress 4 2 Ready for Testing 0 25 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Discussions New GFAL tools issue causing problems for XENON1T, on CVMFS. -- need to check if it's a 3.4 issue only New Condor 8.6.5 release this week to fix IPv6 bugs, but the new release breaks Dave Dykstra's Python modules so we're debugging that Support Update none OSG Release Team Suchandra Thapa is handling the August 8th release Development Freeze next week 3.3.27 Both 3.4.2 Total Status 1 +0 4 +0 2 +0 7 +0 Open 0 +0 10 +0 4 +0 14 +0 In Progress 2 +0 1 +0 1 +0 4 +0 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +0 15 +0 7 +0 25 +0 Total Ready for Testing Both Update gsi-openssh-server 3.4.2 Merge osg-ce packages Both Update to HTCondor 8.4.12 in OSG 3.3 JGlobus incorrectly refuses proxies with key usage Discussions none OSG Investigations Team Last Week Derek at PEARC This Week Derek at OSG school Stash origin maintenance at UChicago over the weekend so StashCache inaccessible over the weekend GRACC accounting issue with Florida revealed that Florida wasn't correctly running Gratia probes Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 17 July 2017"},{"location":"meetings/2017/TechArea20170717/#osg-technology-area-meeting-17-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, Edgar, Marian, Mat, Suchandra, Tim T","title":"OSG Technology Area Meeting, 17 July 2017"},{"location":"meetings/2017/TechArea20170717/#announcements","text":"OSG User School this week","title":"Announcements"},{"location":"meetings/2017/TechArea20170717/#triage-duty","text":"This week: Edgar Next week: Mat 11 (+6) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170717/#jira","text":"# of tickets State 155 1 Open 23 +12 In Progress 4 2 Ready for Testing 0 25 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170717/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170717/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170717/#discussions","text":"New GFAL tools issue causing problems for XENON1T, on CVMFS. -- need to check if it's a 3.4 issue only New Condor 8.6.5 release this week to fix IPv6 bugs, but the new release breaks Dave Dykstra's Python modules so we're debugging that","title":"Discussions"},{"location":"meetings/2017/TechArea20170717/#support-update","text":"none","title":"Support Update"},{"location":"meetings/2017/TechArea20170717/#osg-release-team","text":"Suchandra Thapa is handling the August 8th release Development Freeze next week 3.3.27 Both 3.4.2 Total Status 1 +0 4 +0 2 +0 7 +0 Open 0 +0 10 +0 4 +0 14 +0 In Progress 2 +0 1 +0 1 +0 4 +0 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +0 15 +0 7 +0 25 +0 Total Ready for Testing Both Update gsi-openssh-server 3.4.2 Merge osg-ce packages Both Update to HTCondor 8.4.12 in OSG 3.3 JGlobus incorrectly refuses proxies with key usage","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170717/#discussions_1","text":"none","title":"Discussions"},{"location":"meetings/2017/TechArea20170717/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170717/#last-week","text":"Derek at PEARC","title":"Last Week"},{"location":"meetings/2017/TechArea20170717/#this-week","text":"Derek at OSG school Stash origin maintenance at UChicago over the weekend so StashCache inaccessible over the weekend GRACC accounting issue with Florida revealed that Florida wasn't correctly running Gratia probes","title":"This Week"},{"location":"meetings/2017/TechArea20170717/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170724/","text":"OSG Technology Area Meeting, 24 July 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Announcements Vaibhav no longer with UCSD OSG Annual Planning Retreat Tue - Wed Triage Duty This week: Mat Next week: Suchandra 12 (+1) open tickets JIRA # of tickets State 161 +6 Open 15 -8 In Progress 17 +13 Ready for Testing 1 +1 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Open tickets: Developer # non-RFT Brian L 8 Dave D 2 Mat S 1 Need volunteers for building HTCondor 8.6.5 and developer test Singularity 2.3 VMU tests are unavailable due to Gluster maintenance. Expected to be functional by EOB Tuesday In the meantime, use Travis-CI Mysterious gsi-openssh-server failures in the nightlies on EL7, 3.3. testing Discussions Support Update FIT (BrianL): CE can't contact backend schedd/pool UConn (BrianL): CE auth issues likely an internal OSG issue at this point UWash (BrianL): jobs held due to \"non-existent route or job route limit\" Florida (Derek): Issues with slurm probe not reporting records. Caught in WLCG report. (ongoing) Wisconsin (Derek): Routes for RHEL6 and RHEL7 nodes (ongoing) OSG Release Team Suchandra Thapa is handling the August 8th release Dev freeze today 3.3.27 Both 3.4.2 Total Status 0 +0 1 +1 2 +2 3 +3 Open 1 +1 5 +5 2 +2 8 +8 In Progress 3 +3 9 +9 5 +5 17 +17 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 4 +4 15 +15 9 +9 28 +28 Total Discussions OSG Investigations Team Last Week Not much going on with GRACC, which is good for a production service. Derek at OSG User School OSG User School used StashCP with great success! No issues found with the infrastructure or tools with any of the 60 students using it. This Week GRACC Backup reports CVMFS-Sync RPM across finish line. Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 24 July 2017"},{"location":"meetings/2017/TechArea20170724/#osg-technology-area-meeting-24-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending:","title":"OSG Technology Area Meeting, 24 July 2017"},{"location":"meetings/2017/TechArea20170724/#announcements","text":"Vaibhav no longer with UCSD OSG Annual Planning Retreat Tue - Wed","title":"Announcements"},{"location":"meetings/2017/TechArea20170724/#triage-duty","text":"This week: Mat Next week: Suchandra 12 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170724/#jira","text":"# of tickets State 161 +6 Open 15 -8 In Progress 17 +13 Ready for Testing 1 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170724/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170724/#osg-software-team","text":"Open tickets: Developer # non-RFT Brian L 8 Dave D 2 Mat S 1 Need volunteers for building HTCondor 8.6.5 and developer test Singularity 2.3 VMU tests are unavailable due to Gluster maintenance. Expected to be functional by EOB Tuesday In the meantime, use Travis-CI Mysterious gsi-openssh-server failures in the nightlies on EL7, 3.3. testing","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170724/#discussions","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170724/#support-update","text":"FIT (BrianL): CE can't contact backend schedd/pool UConn (BrianL): CE auth issues likely an internal OSG issue at this point UWash (BrianL): jobs held due to \"non-existent route or job route limit\" Florida (Derek): Issues with slurm probe not reporting records. Caught in WLCG report. (ongoing) Wisconsin (Derek): Routes for RHEL6 and RHEL7 nodes (ongoing)","title":"Support Update"},{"location":"meetings/2017/TechArea20170724/#osg-release-team","text":"Suchandra Thapa is handling the August 8th release Dev freeze today 3.3.27 Both 3.4.2 Total Status 0 +0 1 +1 2 +2 3 +3 Open 1 +1 5 +5 2 +2 8 +8 In Progress 3 +3 9 +9 5 +5 17 +17 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 4 +4 15 +15 9 +9 28 +28 Total","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170724/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170724/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170724/#last-week","text":"Not much going on with GRACC, which is good for a production service. Derek at OSG User School OSG User School used StashCP with great success! No issues found with the infrastructure or tools with any of the 60 students using it.","title":"Last Week"},{"location":"meetings/2017/TechArea20170724/#this-week","text":"GRACC Backup reports CVMFS-Sync RPM across finish line.","title":"This Week"},{"location":"meetings/2017/TechArea20170724/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170731/","text":"OSG Technology Area Meeting, 31 July 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements Triage Duty This week: Suchandra Next week: TimT 12 (+0) open tickets JIRA # of tickets State 158 -3 Open 14 -1 In Progress 25 +8 Ready for Testing 1 +0 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 OSG Software Team htcondor collector python plugin broken in 8.6.4; fixed in 8.6.5 (SOFTWARE-2816) which will be released tomorrow morning Discussions Alejandro at CERN wants a new fix for JGlobus to add the SSL option DontInsertEmptyFragments to fix a Bestman issue; Mat will take a look Support Update FIT (BrianL): CE can't contact backend schedd/pool UConn (BrianL): CE auth issues likely an internal OSG issue at this point UWash (BrianL): jobs held due to \"non-existent route or job route limit\" OSG Release Team Suchandra Thapa is handling the August 8th release Package freeze today Need testing help 3.3.27 Both 3.4.2 Total Status 0 +0 1 +0 0 -2 1 -2 Open 0 -1 0 -5 2 +0 2 -6 In Progress 4 +1 13 +4 8 +3 25 +8 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 4 +0 14 -1 10 +1 28 +0 Total Both Fix selinux issues with GSI OpenSSH in EL7 nightly tests Update gsi-openssh-server Add gsi-openssh packages to osg-tested-internal osg-configure does not warn/error in -v condor-cron: disable gsi authz Release condor-cron 1.1.3 condor-cron: add a way for users to override condor_ids osg-configure: Configure GUMS before running gums-host-cron osg-configure: Fix logging in ensure_valid_user_vo_file Add blahp configuration to differentiate PBS flavors CEView VO tab throws 500 error on inital installation Condor-CE: Do not hold running jobs with expired proxy HTCondor-CE: only warn about configuration if osg-configure is present osg-configure: Make exception usage consistent 3.4.2 Fix selinux issues with GSI OpenSSH in EL7 nightly tests Update gsi-openssh-server Add gsi-openssh packages to osg-tested-internal osg-configure does not warn/error in -v condor-cron: disable gsi authz Release condor-cron 1.1.3 condor-cron: add a way for users to override condor_ids osg-configure: Configure GUMS before running gums-host-cron osg-configure: Fix logging in ensure_valid_user_vo_file Add blahp configuration to differentiate PBS flavors CEView VO tab throws 500 error on inital installation Condor-CE: Do not hold running jobs with expired proxy HTCondor-CE: only warn about configuration if osg-configure is present osg-configure: Make exception usage consistent 3.2.27 JGlobus incorrectly refuses proxies with key usage Release osg-configure 1.9.1 (OSG 3.3) Update to HTCondor 8.4.12 in OSG 3.3 Release htcondor-ce-2.2.2-1+ Discussions Will need software team's help testing From the planning retreat: Release team might go to a \"community testing\" model where we will ask the stakeholders to test changes Maybe might go to a \"rolling release\" model instead of point releases; may be not until 3.5; Tim T to write proposal OSG Investigations Team Last Week GRACC-ITB moving forward Some GRACC support of a slurm_meter issue, still debugging. This Week GRACC Backup reports CVMFS-Sync RPM across finish line. GRACC backup of Dashboards Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting, 31 July 2017"},{"location":"meetings/2017/TechArea20170731/#osg-technology-area-meeting-31-july-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 31 July 2017"},{"location":"meetings/2017/TechArea20170731/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170731/#triage-duty","text":"This week: Suchandra Next week: TimT 12 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170731/#jira","text":"# of tickets State 158 -3 Open 14 -1 In Progress 25 +8 Ready for Testing 1 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170731/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170731/#osg-software-team","text":"htcondor collector python plugin broken in 8.6.4; fixed in 8.6.5 (SOFTWARE-2816) which will be released tomorrow morning","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170731/#discussions","text":"Alejandro at CERN wants a new fix for JGlobus to add the SSL option DontInsertEmptyFragments to fix a Bestman issue; Mat will take a look","title":"Discussions"},{"location":"meetings/2017/TechArea20170731/#support-update","text":"FIT (BrianL): CE can't contact backend schedd/pool UConn (BrianL): CE auth issues likely an internal OSG issue at this point UWash (BrianL): jobs held due to \"non-existent route or job route limit\"","title":"Support Update"},{"location":"meetings/2017/TechArea20170731/#osg-release-team","text":"Suchandra Thapa is handling the August 8th release Package freeze today Need testing help 3.3.27 Both 3.4.2 Total Status 0 +0 1 +0 0 -2 1 -2 Open 0 -1 0 -5 2 +0 2 -6 In Progress 4 +1 13 +4 8 +3 25 +8 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 4 +0 14 -1 10 +1 28 +0 Total Both Fix selinux issues with GSI OpenSSH in EL7 nightly tests Update gsi-openssh-server Add gsi-openssh packages to osg-tested-internal osg-configure does not warn/error in -v condor-cron: disable gsi authz Release condor-cron 1.1.3 condor-cron: add a way for users to override condor_ids osg-configure: Configure GUMS before running gums-host-cron osg-configure: Fix logging in ensure_valid_user_vo_file Add blahp configuration to differentiate PBS flavors CEView VO tab throws 500 error on inital installation Condor-CE: Do not hold running jobs with expired proxy HTCondor-CE: only warn about configuration if osg-configure is present osg-configure: Make exception usage consistent 3.4.2 Fix selinux issues with GSI OpenSSH in EL7 nightly tests Update gsi-openssh-server Add gsi-openssh packages to osg-tested-internal osg-configure does not warn/error in -v condor-cron: disable gsi authz Release condor-cron 1.1.3 condor-cron: add a way for users to override condor_ids osg-configure: Configure GUMS before running gums-host-cron osg-configure: Fix logging in ensure_valid_user_vo_file Add blahp configuration to differentiate PBS flavors CEView VO tab throws 500 error on inital installation Condor-CE: Do not hold running jobs with expired proxy HTCondor-CE: only warn about configuration if osg-configure is present osg-configure: Make exception usage consistent 3.2.27 JGlobus incorrectly refuses proxies with key usage Release osg-configure 1.9.1 (OSG 3.3) Update to HTCondor 8.4.12 in OSG 3.3 Release htcondor-ce-2.2.2-1+","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170731/#discussions_1","text":"Will need software team's help testing From the planning retreat: Release team might go to a \"community testing\" model where we will ask the stakeholders to test changes Maybe might go to a \"rolling release\" model instead of point releases; may be not until 3.5; Tim T to write proposal","title":"Discussions"},{"location":"meetings/2017/TechArea20170731/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170731/#last-week","text":"GRACC-ITB moving forward Some GRACC support of a slurm_meter issue, still debugging.","title":"Last Week"},{"location":"meetings/2017/TechArea20170731/#this-week","text":"GRACC Backup reports CVMFS-Sync RPM across finish line. GRACC backup of Dashboards","title":"This Week"},{"location":"meetings/2017/TechArea20170731/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170807/","text":"OSG Technology Area Meeting, 7 August 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Announcements Triage Duty This week: TimT Next week: Mat 12 (+0) open tickets JIRA # of tickets State 164 +6 Open 14 +0 In Progress 2 -23 Ready for Testing 28 +27 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team GUMS, Gratia, and GSI OpenSSH failures in the 3.3 nightlies New SELinux policies for HTCondor require rebuilds in 3.3, 3.4, and upcoming List Globus package dependencies to prepare for EOL Software team members track effot percentage numbers in shared google spreadsheet Documentation transition goal: migrate ~2 documents per week. Details incoming. Discussions TWiki doc contents should be completely replaced with header after migration Internal docs should be migrated first Support Update FIT (BrianL): CE can't contact backend schedd/pool osg-connect (BrianL) - Submit wrapper issue with new -nobatch option Vanderbilt (Derek) - HTCondor-CE issues OSG Release Team Suchandra Thapa is handling the August 8th release Release tomorrow Data Release this week (IGTF 1.85) 3.3.27 Both 3.4.2 Total Status 0 +0 1 +0 0 +0 1 +0 Open 0 +0 0 +0 0 -2 0 -2 In Progress 0 -4 0 -13 0 -8 0 -25 Ready for Testing 4 +4 13 +13 10 +10 27 +27 Ready for Release 4 +0 14 +0 10 +0 28 +0 Total Both Red Hat 7.4 update breaks HTCondor Condor-CE: Do not hold running jobs with expired proxy condor-cron: add a way for users to override condor_ids osg-configure: Fix logging in ensure_valid_user_vo_file osg-configure does not warn/error in -v CEView VO tab throws 500 error on inital installation HTCondor-CE: only warn about configuration if osg-configure is present osg-configure: Configure GUMS before running gums-host-cron Add blahp configuration to differentiate PBS flavors condor-cron: disable gsi authz Release condor-cron 1.1.3 Add gsi-openssh packages to osg-tested-internal Fix selinux issues with GSI OpenSSH in EL7 nightly tests osg-configure: Make exception usage consistent 3.4.2 htcondor collector python plugin has undefined symbols Add pilot payload auditing Update to HTCondor 8.6.5 in OSG 3.4 Merge osg-ce packages osg-configure: Remove unused test configs Release htcondor-ce-3.0.0-1+ Add osg-gridftp back to osg-tested-internal Release osg-configure 2.1.1 (OSG 3.4) Release osg-tested-internal-3.4-3+ Upcoming: Patch HTCondor 8.7.2 to work with Python Collector plugins 3.3.27 JGlobus incorrectly refuses proxies with key usage Update to HTCondor 8.4.12 in OSG 3.3 Release htcondor-ce-2.2.2-1+ Release osg-configure 1.9.1 (OSG 3.3) Discussions Edgar was a big help to the release effort, thanks! OU will help with testing the late-breaking condor packaging changes OSG Investigations Team Last Week GRACC-ITB moving forward Some GRACC support of a `slurm meter ` issue, still debugging. CVMFS-Sync RPM finished line. Indexing Glidein Logs in GRACC's ES. This Week GRACC Backup reports GRACC backup of Dashboards Ongoing GRACC Project StashCache Project (New URL!)","title":"OSG Technology Area Meeting,  7 August 2017"},{"location":"meetings/2017/TechArea20170807/#osg-technology-area-meeting-7-august-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending:","title":"OSG Technology Area Meeting,  7 August 2017"},{"location":"meetings/2017/TechArea20170807/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170807/#triage-duty","text":"This week: TimT Next week: Mat 12 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170807/#jira","text":"# of tickets State 164 +6 Open 14 +0 In Progress 2 -23 Ready for Testing 28 +27 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170807/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes August 3.4.2, 3.3.27 2017-07-24 2017-07-31 2017-08-08 September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170807/#osg-software-team","text":"GUMS, Gratia, and GSI OpenSSH failures in the 3.3 nightlies New SELinux policies for HTCondor require rebuilds in 3.3, 3.4, and upcoming List Globus package dependencies to prepare for EOL Software team members track effot percentage numbers in shared google spreadsheet Documentation transition goal: migrate ~2 documents per week. Details incoming.","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170807/#discussions","text":"TWiki doc contents should be completely replaced with header after migration Internal docs should be migrated first","title":"Discussions"},{"location":"meetings/2017/TechArea20170807/#support-update","text":"FIT (BrianL): CE can't contact backend schedd/pool osg-connect (BrianL) - Submit wrapper issue with new -nobatch option Vanderbilt (Derek) - HTCondor-CE issues","title":"Support Update"},{"location":"meetings/2017/TechArea20170807/#osg-release-team","text":"Suchandra Thapa is handling the August 8th release Release tomorrow Data Release this week (IGTF 1.85) 3.3.27 Both 3.4.2 Total Status 0 +0 1 +0 0 +0 1 +0 Open 0 +0 0 +0 0 -2 0 -2 In Progress 0 -4 0 -13 0 -8 0 -25 Ready for Testing 4 +4 13 +13 10 +10 27 +27 Ready for Release 4 +0 14 +0 10 +0 28 +0 Total Both Red Hat 7.4 update breaks HTCondor Condor-CE: Do not hold running jobs with expired proxy condor-cron: add a way for users to override condor_ids osg-configure: Fix logging in ensure_valid_user_vo_file osg-configure does not warn/error in -v CEView VO tab throws 500 error on inital installation HTCondor-CE: only warn about configuration if osg-configure is present osg-configure: Configure GUMS before running gums-host-cron Add blahp configuration to differentiate PBS flavors condor-cron: disable gsi authz Release condor-cron 1.1.3 Add gsi-openssh packages to osg-tested-internal Fix selinux issues with GSI OpenSSH in EL7 nightly tests osg-configure: Make exception usage consistent 3.4.2 htcondor collector python plugin has undefined symbols Add pilot payload auditing Update to HTCondor 8.6.5 in OSG 3.4 Merge osg-ce packages osg-configure: Remove unused test configs Release htcondor-ce-3.0.0-1+ Add osg-gridftp back to osg-tested-internal Release osg-configure 2.1.1 (OSG 3.4) Release osg-tested-internal-3.4-3+ Upcoming: Patch HTCondor 8.7.2 to work with Python Collector plugins 3.3.27 JGlobus incorrectly refuses proxies with key usage Update to HTCondor 8.4.12 in OSG 3.3 Release htcondor-ce-2.2.2-1+ Release osg-configure 1.9.1 (OSG 3.3)","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170807/#discussions_1","text":"Edgar was a big help to the release effort, thanks! OU will help with testing the late-breaking condor packaging changes","title":"Discussions"},{"location":"meetings/2017/TechArea20170807/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170807/#last-week","text":"GRACC-ITB moving forward Some GRACC support of a `slurm meter ` issue, still debugging. CVMFS-Sync RPM finished line. Indexing Glidein Logs in GRACC's ES.","title":"Last Week"},{"location":"meetings/2017/TechArea20170807/#this-week","text":"GRACC Backup reports GRACC backup of Dashboards","title":"This Week"},{"location":"meetings/2017/TechArea20170807/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170814/","text":"OSG Technology Area Meeting, 14 August 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements No meeting next week due to outages Triage Duty This week: Mat Next week: Carl 12 (+0) open tickets JIRA # of tickets State 160 -4 Open 18 +4 In Progress 2 +0 Ready for Testing 0 -28 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Novemeber 3.4.5, 3.3.29 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team 13 documents migrated last week, 3 more awaiting review GUMS and Gratia failures in the RHEL7 3.3 nightlies due to new SELinux policy Software team members track effot percentage numbers in shared google spreadsheet Discussions None this week Support Update FIT (BrianL): CE can't contact backend schedd/pool OSG Release Team Suchandra Thapa is handling the September 12th release Developement Freeze in 2 weeks. Data release today (IGTF 1.85) 3.3.28 Both 3.4.3 Total Status 5 +5 11 +11 1 +1 17 +17 Open 2 +2 6 +6 1 +1 9 +9 In Progress 0 +0 1 +1 0 +0 1 +1 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 7 +7 18 +18 2 +2 27 +27 Total Both Update gsi-openssh-server 3.4.3 Nothing yet 3.3.28 Nothing yet Discussions None this week OSG Investigations Team Last Week GRACC-ITB moving forward Some GRACC support of a `slurm meter ` issue, still debugging Indexing Glidein Logs in GRACC's ES Started backup reports First step of creating ES snapshots for backing up GRACC This Week Finish GRACC Backup reports GRACC backup of Dashboards Initiate backups of ES snapshots Start indexing GOC service status in GRACC ES. Ongoing GRACC Project StashCache Project (New URL!) Discussions Once format of Glidein Logs in GRACC ES is finalized, light documentation will be written for the benefit of internal teams Derek offered to give a short presentation on ES queries","title":"OSG Technology Area Meeting, 14 August 2017"},{"location":"meetings/2017/TechArea20170814/#osg-technology-area-meeting-14-august-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 14 August 2017"},{"location":"meetings/2017/TechArea20170814/#announcements","text":"No meeting next week due to outages","title":"Announcements"},{"location":"meetings/2017/TechArea20170814/#triage-duty","text":"This week: Mat Next week: Carl 12 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170814/#jira","text":"# of tickets State 160 -4 Open 18 +4 In Progress 2 +0 Ready for Testing 0 -28 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170814/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 Novemeber 3.4.5, 3.3.29 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170814/#osg-software-team","text":"13 documents migrated last week, 3 more awaiting review GUMS and Gratia failures in the RHEL7 3.3 nightlies due to new SELinux policy Software team members track effot percentage numbers in shared google spreadsheet","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170814/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170814/#support-update","text":"FIT (BrianL): CE can't contact backend schedd/pool","title":"Support Update"},{"location":"meetings/2017/TechArea20170814/#osg-release-team","text":"Suchandra Thapa is handling the September 12th release Developement Freeze in 2 weeks. Data release today (IGTF 1.85) 3.3.28 Both 3.4.3 Total Status 5 +5 11 +11 1 +1 17 +17 Open 2 +2 6 +6 1 +1 9 +9 In Progress 0 +0 1 +1 0 +0 1 +1 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 7 +7 18 +18 2 +2 27 +27 Total Both Update gsi-openssh-server 3.4.3 Nothing yet 3.3.28 Nothing yet","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170814/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170814/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170814/#last-week","text":"GRACC-ITB moving forward Some GRACC support of a `slurm meter ` issue, still debugging Indexing Glidein Logs in GRACC's ES Started backup reports First step of creating ES snapshots for backing up GRACC","title":"Last Week"},{"location":"meetings/2017/TechArea20170814/#this-week","text":"Finish GRACC Backup reports GRACC backup of Dashboards Initiate backups of ES snapshots Start indexing GOC service status in GRACC ES.","title":"This Week"},{"location":"meetings/2017/TechArea20170814/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170814/#discussions_2","text":"Once format of Glidein Logs in GRACC ES is finalized, light documentation will be written for the benefit of internal teams Derek offered to give a short presentation on ES queries","title":"Discussions"},{"location":"meetings/2017/TechArea20170828/","text":"OSG Technology Area Meeting, 28 August 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Suchandra, TimC Announcements Triage Duty This week: Edgar Next week: Derek 9 (-3) open tickets JIRA # of tickets State 154 -3 Open 20 -1 In Progress 14 +12 Ready for Testing 0 +0 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team 3.4.3/3.3.28 Tickets not marked RFT Owner # tickets Mat 10 Brian 7 Carl 5 Marian 1 Documentation 6 documents migrated and meeting-related pages archived last week: https://github.com/opensciencegrid/technology/pulse https://github.com/opensciencegrid/docs/pulse If you're short on time, pick shorter documents to migrate or archive the ones marked as such in the spreadsheet Derek has two students that are beginning to familiarize themselves with the migration process We have enough experience with the process that we can start focusing on high-priority Release3 documents Discussions None this week Support Update Clemson (BrianL, Marian): expired WN CRLs and possibly an HTCondor IPv6 bug JINR (BrianL): CE jobs held (\"Error parsing classad or job not found\") due to missing pbs_pro in /etc/blah.config Issues with BLAHP at some of the BOSCO-CE's. Mats and Derek are continueing to investigate. Some issue with a sub-directory of the Bosco sandbox not being populated with the X509 certificate. OSG Release Team Suchandra Thapa is handling the September 12th release Development Freeze today! TimT may call upon software team members for testing assistance 3.3.28 Both 3.4.3 Total Status 2 -3 9 -2 0 -1 11 -6 Open 3 +1 8 +2 1 +0 12 +3 In Progress 5 +5 6 +5 3 +3 14 +13 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 10 +3 22 +4 4 +2 37 +10 Total Both StashCache 0.8 ( SOFTWARE-2873 ) osg-ca-scripts 1.1.7 ( SOFTWARE-2834 ) xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 ) 3.4.3 Singularity 2.3 ( SOFTWARE-2755 ) CVMFS 2.4.1 ( SOFTWARE-2858 ) osg-configure 2.2.0 ( SOFTWARE-2864 ) 3.3.28 osg-configure 1.10.0 ( SOFTWARE-2865 ) xrootd-hdfs 1.9.2 ( SOFTWARE-2853 ) Discussions None this week OSG Investigations Team Last Week GRACC-ITB moving forward Some GRACC support of a slurm_meter issue, still debugging. Indexing Glidein Logs in GRACC's ES - Ongoing Backup Reports are now running every Monday (or, Sunday night, because GRACC node is in UTC time) First step of creating ES snapshots for backup up GRACC - Ongoing. This Week GRACC backup of Dashboards Initiate backups of ES snapshots Start indexing GOC server status in GRACC ES Work no Glidein logs in ES Fix naming issues in GRACC related to explosion of \"Fake\" sites in records. Ongoing GRACC Project StashCache Project (New URL!) Discussions XRootD developers have moved to a release model where they seek explicit sign-off from stakeholders","title":"OSG Technology Area Meeting, 28 August 2017"},{"location":"meetings/2017/TechArea20170828/#osg-technology-area-meeting-28-august-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Suchandra, TimC","title":"OSG Technology Area Meeting, 28 August 2017"},{"location":"meetings/2017/TechArea20170828/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170828/#triage-duty","text":"This week: Edgar Next week: Derek 9 (-3) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170828/#jira","text":"# of tickets State 154 -3 Open 20 -1 In Progress 14 +12 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170828/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170828/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170828/#3433328","text":"Tickets not marked RFT Owner # tickets Mat 10 Brian 7 Carl 5 Marian 1","title":"3.4.3/3.3.28"},{"location":"meetings/2017/TechArea20170828/#documentation","text":"6 documents migrated and meeting-related pages archived last week: https://github.com/opensciencegrid/technology/pulse https://github.com/opensciencegrid/docs/pulse If you're short on time, pick shorter documents to migrate or archive the ones marked as such in the spreadsheet Derek has two students that are beginning to familiarize themselves with the migration process We have enough experience with the process that we can start focusing on high-priority Release3 documents","title":"Documentation"},{"location":"meetings/2017/TechArea20170828/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170828/#support-update","text":"Clemson (BrianL, Marian): expired WN CRLs and possibly an HTCondor IPv6 bug JINR (BrianL): CE jobs held (\"Error parsing classad or job not found\") due to missing pbs_pro in /etc/blah.config Issues with BLAHP at some of the BOSCO-CE's. Mats and Derek are continueing to investigate. Some issue with a sub-directory of the Bosco sandbox not being populated with the X509 certificate.","title":"Support Update"},{"location":"meetings/2017/TechArea20170828/#osg-release-team","text":"Suchandra Thapa is handling the September 12th release Development Freeze today! TimT may call upon software team members for testing assistance 3.3.28 Both 3.4.3 Total Status 2 -3 9 -2 0 -1 11 -6 Open 3 +1 8 +2 1 +0 12 +3 In Progress 5 +5 6 +5 3 +3 14 +13 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 10 +3 22 +4 4 +2 37 +10 Total Both StashCache 0.8 ( SOFTWARE-2873 ) osg-ca-scripts 1.1.7 ( SOFTWARE-2834 ) xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 ) 3.4.3 Singularity 2.3 ( SOFTWARE-2755 ) CVMFS 2.4.1 ( SOFTWARE-2858 ) osg-configure 2.2.0 ( SOFTWARE-2864 ) 3.3.28 osg-configure 1.10.0 ( SOFTWARE-2865 ) xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170828/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170828/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170828/#last-week","text":"GRACC-ITB moving forward Some GRACC support of a slurm_meter issue, still debugging. Indexing Glidein Logs in GRACC's ES - Ongoing Backup Reports are now running every Monday (or, Sunday night, because GRACC node is in UTC time) First step of creating ES snapshots for backup up GRACC - Ongoing.","title":"Last Week"},{"location":"meetings/2017/TechArea20170828/#this-week","text":"GRACC backup of Dashboards Initiate backups of ES snapshots Start indexing GOC server status in GRACC ES Work no Glidein logs in ES Fix naming issues in GRACC related to explosion of \"Fake\" sites in records.","title":"This Week"},{"location":"meetings/2017/TechArea20170828/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170828/#discussions_2","text":"XRootD developers have moved to a release model where they seek explicit sign-off from stakeholders","title":"Discussions"},{"location":"meetings/2017/TechArea20170905/","text":"OSG Technology Area Meeting, 5 September 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Edgar, Derek, Tim C, Tim T, Carl, Suchandra Announcements Triage Duty This week: Derek Next week: Brian Lin 9 (+0) open tickets JIRA # of tickets State 152 -4 Open 19 -1 In Progress 21 +7 Ready for Testing 1 +1 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team 3.4.3/3.3.28 Tickets not marked RFT Owner # tickets Mat 7 Brian 1 Carl 3 Tim 1 Documentation Discussions New 8.7 Condor build might have to be punted to October. We might still release a new 8.6 this month -- have to look at OSG's demands to see if it's worth releasing. Support Update Some issues with Florida -- Derek to investigate. OSG Release Team Suchandra Thapa is handling the September 12th release Package Freeze today! TimT may call upon software team members for testing assistance 3.3.28 Both 3.4.3 Total Status 1 -1 2 -7 0 +0 3 -8 Open 1 -2 6 -2 2 +1 9 -3 In Progress 7 +2 10 +4 4 +1 21 +7 Ready for Testing 0 +0 1 +1 0 +0 1 +1 Ready for Release 9 -1 19 -4 6 +2 34 -3 Total Both StashCache 0.8 ( SOFTWARE-2873 ) osg-ca-scripts 1.1.7 ( SOFTWARE-2834 ) xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 ) 3.4.3 Singularity 2.3 ( SOFTWARE-2755 ) CVMFS 2.4.1 ( SOFTWARE-2858 ) osg-configure 2.2.0 ( SOFTWARE-2864 ) 3.3.28 osg-configure 1.10.0 ( SOFTWARE-2865 ) xrootd-hdfs 1.9.2 ( SOFTWARE-2853 ) Discussions Will need extra help for testing. Nebraska will help test the Globus update for 3.4, but need a 3.3 site to test the 3.3 updates -- should contact osg-sites. Suchandra may test gridftp-hdfs but Matyas still needs to debug promote. OSG Investigations Team Last Week GRACC backup of Dashboards Initiate backups of ES snapshots Start indexing GOC server status in GRACC ES Work no Glidein logs in ES Fix naming issues in GRACC related to explosion of \"Fake\" sites in records. This Week Some corruption in GRACC Testing GRACC update Stashcache XRootD update going smoothly Some minor issues at Syracuse Ongoing GRACC Project StashCache Project (New URL!) Discussions","title":"OSG Technology Area Meeting, 5 September 2017"},{"location":"meetings/2017/TechArea20170905/#osg-technology-area-meeting-5-september-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Edgar, Derek, Tim C, Tim T, Carl, Suchandra","title":"OSG Technology Area Meeting, 5 September 2017"},{"location":"meetings/2017/TechArea20170905/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170905/#triage-duty","text":"This week: Derek Next week: Brian Lin 9 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170905/#jira","text":"# of tickets State 152 -4 Open 19 -1 In Progress 21 +7 Ready for Testing 1 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170905/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170905/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170905/#3433328","text":"Tickets not marked RFT Owner # tickets Mat 7 Brian 1 Carl 3 Tim 1","title":"3.4.3/3.3.28"},{"location":"meetings/2017/TechArea20170905/#documentation","text":"","title":"Documentation"},{"location":"meetings/2017/TechArea20170905/#discussions","text":"New 8.7 Condor build might have to be punted to October. We might still release a new 8.6 this month -- have to look at OSG's demands to see if it's worth releasing.","title":"Discussions"},{"location":"meetings/2017/TechArea20170905/#support-update","text":"Some issues with Florida -- Derek to investigate.","title":"Support Update"},{"location":"meetings/2017/TechArea20170905/#osg-release-team","text":"Suchandra Thapa is handling the September 12th release Package Freeze today! TimT may call upon software team members for testing assistance 3.3.28 Both 3.4.3 Total Status 1 -1 2 -7 0 +0 3 -8 Open 1 -2 6 -2 2 +1 9 -3 In Progress 7 +2 10 +4 4 +1 21 +7 Ready for Testing 0 +0 1 +1 0 +0 1 +1 Ready for Release 9 -1 19 -4 6 +2 34 -3 Total Both StashCache 0.8 ( SOFTWARE-2873 ) osg-ca-scripts 1.1.7 ( SOFTWARE-2834 ) xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 ) 3.4.3 Singularity 2.3 ( SOFTWARE-2755 ) CVMFS 2.4.1 ( SOFTWARE-2858 ) osg-configure 2.2.0 ( SOFTWARE-2864 ) 3.3.28 osg-configure 1.10.0 ( SOFTWARE-2865 ) xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170905/#discussions_1","text":"Will need extra help for testing. Nebraska will help test the Globus update for 3.4, but need a 3.3 site to test the 3.3 updates -- should contact osg-sites. Suchandra may test gridftp-hdfs but Matyas still needs to debug promote.","title":"Discussions"},{"location":"meetings/2017/TechArea20170905/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170905/#last-week","text":"GRACC backup of Dashboards Initiate backups of ES snapshots Start indexing GOC server status in GRACC ES Work no Glidein logs in ES Fix naming issues in GRACC related to explosion of \"Fake\" sites in records.","title":"Last Week"},{"location":"meetings/2017/TechArea20170905/#this-week","text":"Some corruption in GRACC Testing GRACC update Stashcache XRootD update going smoothly Some minor issues at Syracuse","title":"This Week"},{"location":"meetings/2017/TechArea20170905/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170905/#discussions_2","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170911/","text":"OSG Technology Area Meeting, 11 September 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT Announcements WiFi issues at UW-Madison today so availability may be spotty Triage Duty This week: BrianL Next week: Suchandra 10 (+1) open tickets JIRA # of tickets State 153 -1 Open 17 -2 In Progress 6 -15 Ready for Testing 20 +19 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team 3.4.3/3.3.28 Tickets not marked RFT Owner # tickets Mat 10 Brian 7 Carl 5 Marian 1 Documentation Carl has written a doc migration/archival wrapper for Software There were concerns about pandoc hanging so it's not yet foolproof as an area mass migration tool Carl will work on robustness so that other areas can benefit Derek to speak to his two students this week about their progress with migrating docs ITB HTCondor 8.6.6 pre-release is installed on an ITB CE and some worker nodes BrianL will contact factory ops to add Madison ITB entries to the ITB factory Discussions None this week Support Update Baylor (BrianL): Fixed issues with edg-mkgridmap - LCMAPS VOMS transition due to an old version of HTCondor-CE. Docs to be updated. Clemson (BrianL): Blahp using too much CPU and segfaulting - need to talk to Jaime OSG Release Team Tim Theisen is handling the September 12th release Release tomorrow??? 3.3.28 Both 3.4.3 Total Status 1 +0 0 -2 0 +0 1 -2 Open 0 -1 0 -6 0 -2 0 -9 In Progress 3 -4 3 -7 0 -2 6 -15 Ready for Testing 5 +5 11 +10 4 +4 20 +19 Ready for Release 9 +0 14 -5 4 -2 27 -7 Total Tickets needing attention Both Release StashCache metapackage 0.8+ ( SOFTWARE-2873 ) Migrate transfer limit code from HDFS to generic plugin ( SOFTWARE-2512 ) Package and release xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 ) 3.4.3 3.3.28 LCMAPS VOMS plugin and xrootd-lcmaps ( SOFTWARE-2848 ) Update to xrootd-hdfs 1.9.2 ( SOFTWARE-2853 ) Migrate GridFTP-HDFS from Globus-Toolkit back to OSG ( SOFTWARE-2856 ) Discussions OSG Investigations Team Last Week Some GRACC support of a slurm_meter issue, still debugging. Indexing Glidein Logs in GRACC's ES - Ongoing. We now have worker node hostnames! First step of creating ES snapshots for backup up GRACC - Ongoing. Fixed explosion of \"Fake\" sites in records. Also fixed incorrect user VOs This Week Initiate backups of ES snapshots GRACC-ITB work Start indexing GOC server status in GRACC ES Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 11 September 2017"},{"location":"meetings/2017/TechArea20170911/#osg-technology-area-meeting-11-september-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT","title":"OSG Technology Area Meeting, 11 September 2017"},{"location":"meetings/2017/TechArea20170911/#announcements","text":"WiFi issues at UW-Madison today so availability may be spotty","title":"Announcements"},{"location":"meetings/2017/TechArea20170911/#triage-duty","text":"This week: BrianL Next week: Suchandra 10 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170911/#jira","text":"# of tickets State 153 -1 Open 17 -2 In Progress 6 -15 Ready for Testing 20 +19 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170911/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes September 3.4.3, 3.3.28 2017-08-28 2017-09-05 2017-09-12 5 week cycle October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170911/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170911/#3433328","text":"Tickets not marked RFT Owner # tickets Mat 10 Brian 7 Carl 5 Marian 1","title":"3.4.3/3.3.28"},{"location":"meetings/2017/TechArea20170911/#documentation","text":"Carl has written a doc migration/archival wrapper for Software There were concerns about pandoc hanging so it's not yet foolproof as an area mass migration tool Carl will work on robustness so that other areas can benefit Derek to speak to his two students this week about their progress with migrating docs","title":"Documentation"},{"location":"meetings/2017/TechArea20170911/#itb","text":"HTCondor 8.6.6 pre-release is installed on an ITB CE and some worker nodes BrianL will contact factory ops to add Madison ITB entries to the ITB factory","title":"ITB"},{"location":"meetings/2017/TechArea20170911/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170911/#support-update","text":"Baylor (BrianL): Fixed issues with edg-mkgridmap - LCMAPS VOMS transition due to an old version of HTCondor-CE. Docs to be updated. Clemson (BrianL): Blahp using too much CPU and segfaulting - need to talk to Jaime","title":"Support Update"},{"location":"meetings/2017/TechArea20170911/#osg-release-team","text":"Tim Theisen is handling the September 12th release Release tomorrow??? 3.3.28 Both 3.4.3 Total Status 1 +0 0 -2 0 +0 1 -2 Open 0 -1 0 -6 0 -2 0 -9 In Progress 3 -4 3 -7 0 -2 6 -15 Ready for Testing 5 +5 11 +10 4 +4 20 +19 Ready for Release 9 +0 14 -5 4 -2 27 -7 Total Tickets needing attention Both Release StashCache metapackage 0.8+ ( SOFTWARE-2873 ) Migrate transfer limit code from HDFS to generic plugin ( SOFTWARE-2512 ) Package and release xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 ) 3.4.3 3.3.28 LCMAPS VOMS plugin and xrootd-lcmaps ( SOFTWARE-2848 ) Update to xrootd-hdfs 1.9.2 ( SOFTWARE-2853 ) Migrate GridFTP-HDFS from Globus-Toolkit back to OSG ( SOFTWARE-2856 )","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170911/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170911/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170911/#last-week","text":"Some GRACC support of a slurm_meter issue, still debugging. Indexing Glidein Logs in GRACC's ES - Ongoing. We now have worker node hostnames! First step of creating ES snapshots for backup up GRACC - Ongoing. Fixed explosion of \"Fake\" sites in records. Also fixed incorrect user VOs","title":"Last Week"},{"location":"meetings/2017/TechArea20170911/#this-week","text":"Initiate backups of ES snapshots GRACC-ITB work Start indexing GOC server status in GRACC ES","title":"This Week"},{"location":"meetings/2017/TechArea20170911/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170911/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170918/","text":"OSG Technology Area Meeting, 18 September 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Announcements Triage Duty This week: Suchandra Next week: TimT 10 (+0) open tickets JIRA # of tickets State 162 +9 Open 19 +2 In Progress 1 -5 Ready for Testing 0 -20 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team osghost downtime starts tomorrow at 2pm Central Documentation https://github.com/opensciencegrid/docs/pulse#new-issues 7 docs fully migrated (many more awaiting review!), 100 docs archived Release3: 17 high-priority and ~30 low-priority docs remaining SoftwareTeam: ~56 docs remaining Carl's mass migration and archival wrapper tools are robust enough to be used by other areas Discussions None this week Support Update Clemson (BrianL): Blahp segfaults appeared to be due to overloaded NFS server CTSC (BrianL): Attended meeting to discuss HTCondor issues encountered LBNL (BrianL): Strange Gridmanager errors showing up intermittently OSG Release Team 3.3.29 Both 3.4.4 Total Status 3 +3 12 +12 3 +3 18 +18 Open 0 +0 8 +8 2 +2 10 +10 In Progress 0 +0 1 +1 0 +0 1 +1 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +3 21 +21 5 +5 29 +29 Total Both Update globus-gridftp-server-control to 5.2 3.4.4 3.3.29 Discussions OSG Investigations Team Last Week First step of creating ES snapshots for backup up GRACC - Ongoing. Fixed explosion of \"Fake\" sites in records. Also fixed incorrect user VOs. Changing indexes to relefect the changes. This Week Initiate backups of ES snapshots GRACC-ITB work Start indexing GOC server status in GRACC ES Stash Writeback hack-a-thon on Tuesday Wednesday (Derek, Brian, Lincoln, Marian) Ongoing GRACC Project StashCache Project (New URL!) Discussions Starting to setup time for Kibana walkthrough for Glidein Logs.","title":"OSG Technology Area Meeting, 18 September 2017"},{"location":"meetings/2017/TechArea20170918/#osg-technology-area-meeting-18-september-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending:","title":"OSG Technology Area Meeting, 18 September 2017"},{"location":"meetings/2017/TechArea20170918/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20170918/#triage-duty","text":"This week: Suchandra Next week: TimT 10 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170918/#jira","text":"# of tickets State 162 +9 Open 19 +2 In Progress 1 -5 Ready for Testing 0 -20 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170918/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170918/#osg-software-team","text":"osghost downtime starts tomorrow at 2pm Central","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170918/#documentation","text":"https://github.com/opensciencegrid/docs/pulse#new-issues 7 docs fully migrated (many more awaiting review!), 100 docs archived Release3: 17 high-priority and ~30 low-priority docs remaining SoftwareTeam: ~56 docs remaining Carl's mass migration and archival wrapper tools are robust enough to be used by other areas","title":"Documentation"},{"location":"meetings/2017/TechArea20170918/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170918/#support-update","text":"Clemson (BrianL): Blahp segfaults appeared to be due to overloaded NFS server CTSC (BrianL): Attended meeting to discuss HTCondor issues encountered LBNL (BrianL): Strange Gridmanager errors showing up intermittently","title":"Support Update"},{"location":"meetings/2017/TechArea20170918/#osg-release-team","text":"3.3.29 Both 3.4.4 Total Status 3 +3 12 +12 3 +3 18 +18 Open 0 +0 8 +8 2 +2 10 +10 In Progress 0 +0 1 +1 0 +0 1 +1 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +3 21 +21 5 +5 29 +29 Total Both Update globus-gridftp-server-control to 5.2 3.4.4 3.3.29","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170918/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20170918/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170918/#last-week","text":"First step of creating ES snapshots for backup up GRACC - Ongoing. Fixed explosion of \"Fake\" sites in records. Also fixed incorrect user VOs. Changing indexes to relefect the changes.","title":"Last Week"},{"location":"meetings/2017/TechArea20170918/#this-week","text":"Initiate backups of ES snapshots GRACC-ITB work Start indexing GOC server status in GRACC ES Stash Writeback hack-a-thon on Tuesday Wednesday (Derek, Brian, Lincoln, Marian)","title":"This Week"},{"location":"meetings/2017/TechArea20170918/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170918/#discussions_2","text":"Starting to setup time for Kibana walkthrough for Glidein Logs.","title":"Discussions"},{"location":"meetings/2017/TechArea20170925/","text":"OSG Technology Area Meeting, 25 September 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Tim C, Brian L, Suchandra, Marian, Edgar, Mat, Derek, Carl Announcements OSG All Hands 2018 at University of Utah Mar 19 - 22, 2018 Triage Duty This week: TimT Next week: BrianL 11 (+1) open tickets JIRA # of tickets State 155 -7 Open 20 +1 In Progress 8 +7 Ready for Testing 0 0 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Open JIRA tickets Owner # tickets not RFT Mat 12 Carl 4 Edgar 2 BrianL 1 Software and release teams merged. Effort #s updated here It sounds like HDFS 3 isn't expected until at least April 2018 Ship the newest version of HDFS 2 in OSG 3.4 Aim for November, but don't know how much work it will take yet Documentation https://github.com/opensciencegrid/docs/pulse#new-issues https://github.com/opensciencegrid/technology/pulse#new-issues Only 3 docs fully migrated, ~7 docs awaiting review Release3: ~45 docs remaining SoftwareTeam: ~50 docs remaining Discussions Software/Release merger finalized -- see front page of tech area docs Software folks should expect to do more testing; release folks might have to help out with software updates Support Update LBNL (BrianL): Strange Gridmanager errors showing up intermittently; asked them to update to condor 8.6 OSG Release Team 3.3.29 Both 3.4.4 Total Status 0 -3 2 -10 0 -3 2 -16 Open 1 +1 6 -2 0 -2 7 -3 In Progress 2 +2 5 +4 4 +4 11 +10 Ready for Testing 0 +0 1 +1 1 +1 2 +2 Ready for Release 3 +0 14 -7 5 +0 22 -7 Total Both Update globus-gridftp-server-control to 5.2 Don't use mirrors for goc repos osg-ca-scripts: require wget osg-configure: Detect when fetch-crl missing osg-configure: don't use condor_config_val -expand 3.4.4 Update to HTCondor 8.6.6 in OSG 3.4 Update to HTCondor 8.7.3 in Upcoming Update to singularity-2.3.2+ Add singularity to osg-tested-internal osg-configure: Release 2.2.1 3.3.29 Release voms-admin-server-2.7.0-1.23+ osg-configure: Release 1.10.1 Discussions None this week OSG Investigations Team Last Week Enabling writeback of Stash Extra backups made of GRACC records APEL report issues related to firewall; issues were fixed and a check_mk report was added This Week Still working on cleaning up GRACC data Finish writeback of Stash Better monitoring of APEL uploads Ongoing GRACC Project StashCache Project (New URL!) Discussions Starting to setup time for Kibana walkthrough for Glidein Logs","title":"OSG Technology Area Meeting, 25 September 2017"},{"location":"meetings/2017/TechArea20170925/#osg-technology-area-meeting-25-september-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Tim C, Brian L, Suchandra, Marian, Edgar, Mat, Derek, Carl","title":"OSG Technology Area Meeting, 25 September 2017"},{"location":"meetings/2017/TechArea20170925/#announcements","text":"OSG All Hands 2018 at University of Utah Mar 19 - 22, 2018","title":"Announcements"},{"location":"meetings/2017/TechArea20170925/#triage-duty","text":"This week: TimT Next week: BrianL 11 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20170925/#jira","text":"# of tickets State 155 -7 Open 20 +1 In Progress 8 +7 Ready for Testing 0 0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20170925/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20170925/#osg-software-team","text":"Open JIRA tickets Owner # tickets not RFT Mat 12 Carl 4 Edgar 2 BrianL 1 Software and release teams merged. Effort #s updated here It sounds like HDFS 3 isn't expected until at least April 2018 Ship the newest version of HDFS 2 in OSG 3.4 Aim for November, but don't know how much work it will take yet","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20170925/#documentation","text":"https://github.com/opensciencegrid/docs/pulse#new-issues https://github.com/opensciencegrid/technology/pulse#new-issues Only 3 docs fully migrated, ~7 docs awaiting review Release3: ~45 docs remaining SoftwareTeam: ~50 docs remaining","title":"Documentation"},{"location":"meetings/2017/TechArea20170925/#discussions","text":"Software/Release merger finalized -- see front page of tech area docs Software folks should expect to do more testing; release folks might have to help out with software updates","title":"Discussions"},{"location":"meetings/2017/TechArea20170925/#support-update","text":"LBNL (BrianL): Strange Gridmanager errors showing up intermittently; asked them to update to condor 8.6","title":"Support Update"},{"location":"meetings/2017/TechArea20170925/#osg-release-team","text":"3.3.29 Both 3.4.4 Total Status 0 -3 2 -10 0 -3 2 -16 Open 1 +1 6 -2 0 -2 7 -3 In Progress 2 +2 5 +4 4 +4 11 +10 Ready for Testing 0 +0 1 +1 1 +1 2 +2 Ready for Release 3 +0 14 -7 5 +0 22 -7 Total Both Update globus-gridftp-server-control to 5.2 Don't use mirrors for goc repos osg-ca-scripts: require wget osg-configure: Detect when fetch-crl missing osg-configure: don't use condor_config_val -expand 3.4.4 Update to HTCondor 8.6.6 in OSG 3.4 Update to HTCondor 8.7.3 in Upcoming Update to singularity-2.3.2+ Add singularity to osg-tested-internal osg-configure: Release 2.2.1 3.3.29 Release voms-admin-server-2.7.0-1.23+ osg-configure: Release 1.10.1","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20170925/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20170925/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20170925/#last-week","text":"Enabling writeback of Stash Extra backups made of GRACC records APEL report issues related to firewall; issues were fixed and a check_mk report was added","title":"Last Week"},{"location":"meetings/2017/TechArea20170925/#this-week","text":"Still working on cleaning up GRACC data Finish writeback of Stash Better monitoring of APEL uploads","title":"This Week"},{"location":"meetings/2017/TechArea20170925/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20170925/#discussions_2","text":"Starting to setup time for Kibana walkthrough for Glidein Logs","title":"Discussions"},{"location":"meetings/2017/TechArea20171002/","text":"OSG Technology Area Meeting, 2 October 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Mat, Carl, Edgar, Suchandra, Derek, Marian, BrianB Announcements Triage Duty This week: Carl Next week: BrianL 10 (-1) open tickets JIRA # of tickets State 159 +4 Open 23 +3 In Progress 12 +4 Ready for Testing 2 +2 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Open JIRA tickets Owner # tickets not RFT BrianL 5 Mat 4 Edgar 2 Carl 0 Software and release teams merged. Effort #s updated here Documentation https://github.com/opensciencegrid/docs/pulse#new-issues https://github.com/opensciencegrid/technology/pulse#new-issues Hold off on these for now Discussions Support Update Still debugging XRootD issues at Florida; possibly found leaks? OSG Release Team 3.3.29 Both 3.4.4 Total Status 0 -3 2 -10 0 -3 2 -16 Open 1 +1 6 -2 0 -2 7 -3 In Progress 2 +2 5 +4 4 +4 11 +10 Ready for Testing 0 +0 1 +1 1 +1 2 +2 Ready for Release 3 +0 14 -7 5 +0 22 -7 Total Both Update globus-gridftp-server-control to 5.2 Don't use mirrors for goc repos osg-ca-scripts: require wget osg-configure: Detect when fetch-crl missing osg-configure: don't use condor_config_val -expand 3.4.4 Update to HTCondor 8.6.6 in OSG 3.4 Update to HTCondor 8.7.3 in Upcoming Update to singularity-2.3.2+ Add singularity to osg-tested-internal osg-configure: Release 2.2.1 3.3.29 Release voms-admin-server-2.7.0-1.23+ osg-configure: Release 1.10.1 Discussions OSG Investigations Team Last Week Worked on cleaning up GRACC data This Week Finalizing GRACC changes Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix Ongoing GRACC Project StashCache Project (New URL!) Discussions","title":"OSG Technology Area Meeting, 2 October 2017"},{"location":"meetings/2017/TechArea20171002/#osg-technology-area-meeting-2-october-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Mat, Carl, Edgar, Suchandra, Derek, Marian, BrianB","title":"OSG Technology Area Meeting, 2 October 2017"},{"location":"meetings/2017/TechArea20171002/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20171002/#triage-duty","text":"This week: Carl Next week: BrianL 10 (-1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171002/#jira","text":"# of tickets State 159 +4 Open 23 +3 In Progress 12 +4 Ready for Testing 2 +2 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171002/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171002/#osg-software-team","text":"Open JIRA tickets Owner # tickets not RFT BrianL 5 Mat 4 Edgar 2 Carl 0 Software and release teams merged. Effort #s updated here","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171002/#documentation","text":"https://github.com/opensciencegrid/docs/pulse#new-issues https://github.com/opensciencegrid/technology/pulse#new-issues Hold off on these for now","title":"Documentation"},{"location":"meetings/2017/TechArea20171002/#discussions","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20171002/#support-update","text":"Still debugging XRootD issues at Florida; possibly found leaks?","title":"Support Update"},{"location":"meetings/2017/TechArea20171002/#osg-release-team","text":"3.3.29 Both 3.4.4 Total Status 0 -3 2 -10 0 -3 2 -16 Open 1 +1 6 -2 0 -2 7 -3 In Progress 2 +2 5 +4 4 +4 11 +10 Ready for Testing 0 +0 1 +1 1 +1 2 +2 Ready for Release 3 +0 14 -7 5 +0 22 -7 Total Both Update globus-gridftp-server-control to 5.2 Don't use mirrors for goc repos osg-ca-scripts: require wget osg-configure: Detect when fetch-crl missing osg-configure: don't use condor_config_val -expand 3.4.4 Update to HTCondor 8.6.6 in OSG 3.4 Update to HTCondor 8.7.3 in Upcoming Update to singularity-2.3.2+ Add singularity to osg-tested-internal osg-configure: Release 2.2.1 3.3.29 Release voms-admin-server-2.7.0-1.23+ osg-configure: Release 1.10.1","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171002/#discussions_1","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20171002/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171002/#last-week","text":"Worked on cleaning up GRACC data","title":"Last Week"},{"location":"meetings/2017/TechArea20171002/#this-week","text":"Finalizing GRACC changes Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix","title":"This Week"},{"location":"meetings/2017/TechArea20171002/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171002/#discussions_2","text":"","title":"Discussions"},{"location":"meetings/2017/TechArea20171009/","text":"OSG Technology Area Meeting, 9 October 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: BrianL Next week: Derek 12 (+1) open tickets JIRA # of tickets State 166 +7 Open 25 +2 In Progress 1 -11 Ready for Testing 17 +15 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Documentation transition on temporary hold as we improve the migration process Discussions None this week Support Update None last week OSG Release Team Tim Theisen is handling the October Release VO Package v75 needs testing by CMS IGTF Released today 3.3.29 Both 3.4.4 Total Status 0 +0 0 -2 0 +0 0 -2 Open 0 -1 0 -6 0 +0 0 -7 In Progress 0 -2 0 +5 0 -4 0 -11 Ready for Testing 2 +2 10 +9 5 +4 17 +15 Ready for Release 2 -1 10 -4 5 +0 17 -5 Total Both Update globus-gridftp-server-control to 5.2 Don't use mirrors for goc repos osg-ca-scripts: require wget osg-configure: Detect when fetch-crl missing osg-configure: don't use condor_config_val -expand gsi-openssh-server update for LIGO 3.4.4 Update to HTCondor 8.6.6 in OSG 3.4 Update to HTCondor 8.7.3 in Upcoming Update to singularity-2.3.2+ Add singularity to osg-tested-internal osg-configure: Release 2.2.1 3.3.29 Release voms-admin-server-2.7.0-1.23+ osg-configure: Release 1.10.1 Discussions Derek and Marian will get a tester from CMS for the vo-client package OSG Investigations Team Last Week Worked on cleaning up GRACC data Comparing cleaned GRACC Data More work on writable stashcache, still waiting on bugfix This Week Finalizing GRACC changes Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix OSG - CVMFS focus day to improve the *.osgstorage and singularity.opensciencegrid.org repo stability. Ongoing GRACC Project StashCache Project Discussions None this week","title":"OSG Technology Area Meeting,  9 October 2017"},{"location":"meetings/2017/TechArea20171009/#osg-technology-area-meeting-9-october-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting,  9 October 2017"},{"location":"meetings/2017/TechArea20171009/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20171009/#triage-duty","text":"This week: BrianL Next week: Derek 12 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171009/#jira","text":"# of tickets State 166 +7 Open 25 +2 In Progress 1 -11 Ready for Testing 17 +15 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171009/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes October 3.4.4, 3.3.29 2017-09-25 2017-10-02 2017-10-10 November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171009/#osg-software-team","text":"Documentation transition on temporary hold as we improve the migration process","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171009/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171009/#support-update","text":"None last week","title":"Support Update"},{"location":"meetings/2017/TechArea20171009/#osg-release-team","text":"Tim Theisen is handling the October Release VO Package v75 needs testing by CMS IGTF Released today 3.3.29 Both 3.4.4 Total Status 0 +0 0 -2 0 +0 0 -2 Open 0 -1 0 -6 0 +0 0 -7 In Progress 0 -2 0 +5 0 -4 0 -11 Ready for Testing 2 +2 10 +9 5 +4 17 +15 Ready for Release 2 -1 10 -4 5 +0 17 -5 Total Both Update globus-gridftp-server-control to 5.2 Don't use mirrors for goc repos osg-ca-scripts: require wget osg-configure: Detect when fetch-crl missing osg-configure: don't use condor_config_val -expand gsi-openssh-server update for LIGO 3.4.4 Update to HTCondor 8.6.6 in OSG 3.4 Update to HTCondor 8.7.3 in Upcoming Update to singularity-2.3.2+ Add singularity to osg-tested-internal osg-configure: Release 2.2.1 3.3.29 Release voms-admin-server-2.7.0-1.23+ osg-configure: Release 1.10.1","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171009/#discussions_1","text":"Derek and Marian will get a tester from CMS for the vo-client package","title":"Discussions"},{"location":"meetings/2017/TechArea20171009/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171009/#last-week","text":"Worked on cleaning up GRACC data Comparing cleaned GRACC Data More work on writable stashcache, still waiting on bugfix","title":"Last Week"},{"location":"meetings/2017/TechArea20171009/#this-week","text":"Finalizing GRACC changes Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix OSG - CVMFS focus day to improve the *.osgstorage and singularity.opensciencegrid.org repo stability.","title":"This Week"},{"location":"meetings/2017/TechArea20171009/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2017/TechArea20171009/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171016/","text":"OSG Technology Area Meeting, 16 October 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Mat, Suchandra, TimC, TimT Announcements LIGO announced neutron star collision detection! Triage Duty This week: Derek Next week: Edgar 9 (-3) open tickets JIRA # of tickets State 160 -6 Open 26 +1 In Progress 4 +4 Ready for Testing 0 -17 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Documentation migration remaining items SoftwareTeam migrations Release3 and SoftwareTeam archivals TWiki migration headers and archive doc deletions Globus EOL: Actively discussing repo destination with EGI and WLCG November release development freeze - 11/30 Discussions Per-page migration headers are preferred but a per-web migration would be easier We need to search and replace twiki references in software Support Update Colorado (BrianL) - BeStMan not accepting RFC-compliant proxies JINR (BrianL) - blahp failures due to some issue with ASCII chars LBNL (BrianL) - issues with the blahp may have been due to misconfiguration Taiwan (BrianL) - idle jobs on CE, likely a basic configuration issue GLOW (Derek, Edgar) - GLOW isn't running on many OSG sites we would expect. Edgar is leading the investigation. CMS / NERSC (Derek) - Add userspace StashCache for CMS data caching. Ongoing. OSG Release Team Tim Theisen will handle the next release if the new procedure is used 3.3.30 Both 3.4.5 Total Status 1 +1 9 +9 0 +0 10 +10 Open 1 +1 12 +12 2 +2 15 +15 In Progress 0 +0 4 +4 0 +0 4 +4 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +2 25 +25 2 +2 29 +29 Total Both Update osg-system profiler 3.4.5 3.3.30 Discussions CA Certificate update handled in 2 days OSG Investigations Team Last Week Worked on cleaning up GRACC data. Major update of GRACC-APEL reports to reflect non-zero CpuDuration in WLCG reports Worked on getting data from GRACC for press release (requested by BrianB/FKW) More work on writable stashcache. Installed CMS data cache at NERSC in userspace. Will cache CMS data destined at NERSC. Lots of CVMFS changes on OSG hosted repos (except oasis). This Week Finalizing GRACC changes Draft GRACC SLA proposal Writable StashCache. Enabling access for initial testers. Improve CMS cache at NERSC, productionize Ongoing GRACC Project StashCache Project Discussions None this week","title":"OSG Technology Area Meeting, 16 October 2017"},{"location":"meetings/2017/TechArea20171016/#osg-technology-area-meeting-16-october-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 16 October 2017"},{"location":"meetings/2017/TechArea20171016/#announcements","text":"LIGO announced neutron star collision detection!","title":"Announcements"},{"location":"meetings/2017/TechArea20171016/#triage-duty","text":"This week: Derek Next week: Edgar 9 (-3) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171016/#jira","text":"# of tickets State 160 -6 Open 26 +1 In Progress 4 +4 Ready for Testing 0 -17 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171016/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171016/#osg-software-team","text":"Documentation migration remaining items SoftwareTeam migrations Release3 and SoftwareTeam archivals TWiki migration headers and archive doc deletions Globus EOL: Actively discussing repo destination with EGI and WLCG November release development freeze - 11/30","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171016/#discussions","text":"Per-page migration headers are preferred but a per-web migration would be easier We need to search and replace twiki references in software","title":"Discussions"},{"location":"meetings/2017/TechArea20171016/#support-update","text":"Colorado (BrianL) - BeStMan not accepting RFC-compliant proxies JINR (BrianL) - blahp failures due to some issue with ASCII chars LBNL (BrianL) - issues with the blahp may have been due to misconfiguration Taiwan (BrianL) - idle jobs on CE, likely a basic configuration issue GLOW (Derek, Edgar) - GLOW isn't running on many OSG sites we would expect. Edgar is leading the investigation. CMS / NERSC (Derek) - Add userspace StashCache for CMS data caching. Ongoing.","title":"Support Update"},{"location":"meetings/2017/TechArea20171016/#osg-release-team","text":"Tim Theisen will handle the next release if the new procedure is used 3.3.30 Both 3.4.5 Total Status 1 +1 9 +9 0 +0 10 +10 Open 1 +1 12 +12 2 +2 15 +15 In Progress 0 +0 4 +4 0 +0 4 +4 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +2 25 +25 2 +2 29 +29 Total Both Update osg-system profiler 3.4.5 3.3.30","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171016/#discussions_1","text":"CA Certificate update handled in 2 days","title":"Discussions"},{"location":"meetings/2017/TechArea20171016/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171016/#last-week","text":"Worked on cleaning up GRACC data. Major update of GRACC-APEL reports to reflect non-zero CpuDuration in WLCG reports Worked on getting data from GRACC for press release (requested by BrianB/FKW) More work on writable stashcache. Installed CMS data cache at NERSC in userspace. Will cache CMS data destined at NERSC. Lots of CVMFS changes on OSG hosted repos (except oasis).","title":"Last Week"},{"location":"meetings/2017/TechArea20171016/#this-week","text":"Finalizing GRACC changes Draft GRACC SLA proposal Writable StashCache. Enabling access for initial testers. Improve CMS cache at NERSC, productionize","title":"This Week"},{"location":"meetings/2017/TechArea20171016/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2017/TechArea20171016/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171023/","text":"OSG Technology Area Meeting, 23 October 2017 Coordinates: Conference: +1 669 900 6833 or +1 408 638 0968 or +1 646 876 9923, PIN: 735 282 244; https://nebraskaextension.zoom.us/j/735282244 Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Derek will be giving a short talk about querying pilot logs in Elasticsearch Triage Duty This week: Edgar Next week: Mat 9 (+0) open tickets JIRA # of tickets State 154 -6 Open 26 +0 In Progress 10 +6 Ready for Testing 0 +0 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Documentation migration complete! November release development freeze - 10/30 Discussions None this week Support Update CTSC (BrianL) - Worked locally with student to set up test CE for testing the blahp JINR (BrianL) - blahp failures due to issue with ASCII chars, finally got a stacktrace LBNL (BrianL) - issues with the blahp may have been due to misconfiguration Taiwan (BrianL) - idle jobs on CE, likely a basic configuration issue UCSD (BrianL) - helped Terrence troubleshoot condor not starting after issues with a full disk (resolved by restarting and waiting) GLOW (Edgar Derek) - Helping figure out why GLOW is not running as much as OSG. (mostly edgar) CMS (Derek) - Installation of StashCache at NERSC for CMS data. OSG Release Team Suchandra Thapa will handle the November 14th release 3.3.30 Both 3.4.5 Total Status 1 +0 10 +1 1 +1 12 +2 Open 1 +0 10 -2 2 +0 13 -2 In Progress 1 +1 7 +3 2 +2 10 +6 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +1 27 +2 5 +3 35 +6 Total Both Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 Discussions Flexible release model being announced. November release follows the existing process Miron wants to have documentation updates as part of the release process Will be working on a mechanism to accomplish this TimC suggested looking at the overall documentation updates Engage the end users of the documentation Respond quickly to tickets filed by the end users OSG Investigations Team Last Week More work on writable stashcache. Began working with user support for testing. Installed CMS data cache at NERSC in userspace. Will cache CMS data destined at NERSC. Review papers for OSG Bioinformatic workshop. XRootD v4.7.1-rc2 packaging for StashCache testing This Week Finalizing GRACC changes GRACC-reporting tweaks fully automated ES backups Draft GRACC SLA proposal Writable StashCache. Enabling access for initial testers. finalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska Ongoing GRACC Project StashCache Project Discussions During the production call, David from CMS has mentioned a potential issue with GRACC hours for some CMS sites","title":"OSG Technology Area Meeting, 23 October 2017"},{"location":"meetings/2017/TechArea20171023/#osg-technology-area-meeting-23-october-2017","text":"Coordinates: Conference: +1 669 900 6833 or +1 408 638 0968 or +1 646 876 9923, PIN: 735 282 244; https://nebraskaextension.zoom.us/j/735282244 Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 23 October 2017"},{"location":"meetings/2017/TechArea20171023/#announcements","text":"Derek will be giving a short talk about querying pilot logs in Elasticsearch","title":"Announcements"},{"location":"meetings/2017/TechArea20171023/#triage-duty","text":"This week: Edgar Next week: Mat 9 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171023/#jira","text":"# of tickets State 154 -6 Open 26 +0 In Progress 10 +6 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171023/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171023/#osg-software-team","text":"Documentation migration complete! November release development freeze - 10/30","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171023/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171023/#support-update","text":"CTSC (BrianL) - Worked locally with student to set up test CE for testing the blahp JINR (BrianL) - blahp failures due to issue with ASCII chars, finally got a stacktrace LBNL (BrianL) - issues with the blahp may have been due to misconfiguration Taiwan (BrianL) - idle jobs on CE, likely a basic configuration issue UCSD (BrianL) - helped Terrence troubleshoot condor not starting after issues with a full disk (resolved by restarting and waiting) GLOW (Edgar Derek) - Helping figure out why GLOW is not running as much as OSG. (mostly edgar) CMS (Derek) - Installation of StashCache at NERSC for CMS data.","title":"Support Update"},{"location":"meetings/2017/TechArea20171023/#osg-release-team","text":"Suchandra Thapa will handle the November 14th release 3.3.30 Both 3.4.5 Total Status 1 +0 10 +1 1 +1 12 +2 Open 1 +0 10 -2 2 +0 13 -2 In Progress 1 +1 7 +3 2 +2 10 +6 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 3 +1 27 +2 5 +3 35 +6 Total Both Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171023/#discussions_1","text":"Flexible release model being announced. November release follows the existing process Miron wants to have documentation updates as part of the release process Will be working on a mechanism to accomplish this TimC suggested looking at the overall documentation updates Engage the end users of the documentation Respond quickly to tickets filed by the end users","title":"Discussions"},{"location":"meetings/2017/TechArea20171023/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171023/#last-week","text":"More work on writable stashcache. Began working with user support for testing. Installed CMS data cache at NERSC in userspace. Will cache CMS data destined at NERSC. Review papers for OSG Bioinformatic workshop. XRootD v4.7.1-rc2 packaging for StashCache testing","title":"Last Week"},{"location":"meetings/2017/TechArea20171023/#this-week","text":"Finalizing GRACC changes GRACC-reporting tweaks fully automated ES backups Draft GRACC SLA proposal Writable StashCache. Enabling access for initial testers. finalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska","title":"This Week"},{"location":"meetings/2017/TechArea20171023/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2017/TechArea20171023/#discussions_2","text":"During the production call, David from CMS has mentioned a potential issue with GRACC hours for some CMS sites","title":"Discussions"},{"location":"meetings/2017/TechArea20171030/","text":"OSG Technology Area Meeting, 30 October 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements BrianL out Thu-Mon (11/2-11/6) Triage Duty This week: Mat Next week: Suchandra 6 (-3) open tickets JIRA # of tickets State 151 -3 Open 37 +11 In Progress 11 +1 Ready for Testing 0 +0 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release. OSG Software Team Number of 3.4.5/3.3.30 still open or in progress Owner # not RFT BrianL 13 Mat 6 Edgar 6 Carl 4 Marian 1 DaveD 1 Flexible releases mean no more dev/testing deadlines. To make sure your assigned tickets are progressing, please use the stale Software ticket filter Discussions None this week Support Update CTSC (BrianL) - Assisted Sanjay with writing submit files that he could use to submit to his test CE JINR (BrianL) - fixed blahp failures (use StringIO.StringIO vs io.StringIO in Python2) OSG Release Team Suchandra Thapa will handle the November Release 3.3.30 Both 3.4.5 Total Status 1 +0 5 -5 1 +0 7 -5 Open 1 +0 20 +10 3 +1 24 +11 In Progress 2 +1 8 +1 2 +0 12 +2 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 4 +1 33 +6 6 +1 43 +8 Total Both Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client Fix EPSV response for IPv4-mapped IPv6 addresses 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 GridFTP-HDFS 1.1 Discussions None this week OSG Investigations Team Last Week Installed CMS data cache at NERSC in userspace. Will cache CMS data destined at NERSC. XRootD v4.7.1-rc2 packaging for StashCache testing Enabled Starter and StartD logs for GRACC. Caused issues at factories, but now figured out. This Week Finalizing GRACC changes GRACC-reporting tweaks fully automated ES backups Draft GRACC SLA proposal Writable StashCache. Enabling access for initial testers. Finalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska GRACC focus day on Friday. Puppetizing OPS across various services check_mk improvements Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 30 October 2017"},{"location":"meetings/2017/TechArea20171030/#osg-technology-area-meeting-30-october-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 30 October 2017"},{"location":"meetings/2017/TechArea20171030/#announcements","text":"BrianL out Thu-Mon (11/2-11/6)","title":"Announcements"},{"location":"meetings/2017/TechArea20171030/#triage-duty","text":"This week: Mat Next week: Suchandra 6 (-3) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171030/#jira","text":"# of tickets State 151 -3 Open 37 +11 In Progress 11 +1 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171030/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle December 3.4.6, 3.3.31 2017-11-27 2017-12-04 2017-12-12 Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171030/#osg-software-team","text":"Number of 3.4.5/3.3.30 still open or in progress Owner # not RFT BrianL 13 Mat 6 Edgar 6 Carl 4 Marian 1 DaveD 1 Flexible releases mean no more dev/testing deadlines. To make sure your assigned tickets are progressing, please use the stale Software ticket filter","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171030/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171030/#support-update","text":"CTSC (BrianL) - Assisted Sanjay with writing submit files that he could use to submit to his test CE JINR (BrianL) - fixed blahp failures (use StringIO.StringIO vs io.StringIO in Python2)","title":"Support Update"},{"location":"meetings/2017/TechArea20171030/#osg-release-team","text":"Suchandra Thapa will handle the November Release 3.3.30 Both 3.4.5 Total Status 1 +0 5 -5 1 +0 7 -5 Open 1 +0 20 +10 3 +1 24 +11 In Progress 2 +1 8 +1 2 +0 12 +2 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 4 +1 33 +6 6 +1 43 +8 Total Both Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client Fix EPSV response for IPv4-mapped IPv6 addresses 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 GridFTP-HDFS 1.1","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171030/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171030/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171030/#last-week","text":"Installed CMS data cache at NERSC in userspace. Will cache CMS data destined at NERSC. XRootD v4.7.1-rc2 packaging for StashCache testing Enabled Starter and StartD logs for GRACC. Caused issues at factories, but now figured out.","title":"Last Week"},{"location":"meetings/2017/TechArea20171030/#this-week","text":"Finalizing GRACC changes GRACC-reporting tweaks fully automated ES backups Draft GRACC SLA proposal Writable StashCache. Enabling access for initial testers. Finalize XRootD v4.7.1-rc2 packaging and deploy at Nebraska GRACC focus day on Friday. Puppetizing OPS across various services check_mk improvements","title":"This Week"},{"location":"meetings/2017/TechArea20171030/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171030/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171106/","text":"OSG Technology Area Meeting, 11 November 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Brian B, Carl, Derek, Edgar, Marian, Mat, Suchandra, Tim C, Tim T Announcements BrianL out Thu-Mon (11/2-11/6) Triage Duty This week: Suchandra Next week: TimT 6 (+0) open tickets JIRA # of tickets State 162 +11 Open 17 -20 In Progress 40 +29 Ready for Testing 3 +3 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle 3.4.5 and 3.3.30 will be the last releases on the regular schedule. Subsequent releases will use the flexible release schedule . OSG Software Team Discussions XRootd 4.7.1 has a bug that affects StashCache servers, but is not user-facing Support Update UCSD Tier 2 (Edgar) - GPU jobs from IceCube wre causing GPU nodes to lock up and crash. Might be hardware issue since we haven't seen anything like that at Nebraska. OSG Release Team Suchandra Thapa will handle the November Release 3.3.30 Both 3.4.5 Total Status 0 -1 0 -5 0 -1 9 -7 Open 0 -1 0 -20 0 -3 0 -24 In Progress 4 +2 31 +23 3 +1 38 +26 Ready for Testing 1 +1 1 +1 2 +2 4 +4 Ready for Release 4 +1 32 -1 5 -1 42 -1 Total Both Default to authenticated request and retrieval of certificates Drop PKI tool quota verification CVMFS 2.4.2 XRootD 4.7.1 Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client Fix EPSV response for IPv4-mapped IPv6 addresses gratia probes updates Bug fixes for BLAHP: Slurm memory parsing, Unicode decode error RSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported Maintenance on osg-test 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 GridFTP-HDFS 1.1 GUMS: software.grid.iu.edu - repo.grid.iu.edu Discussions Need testing help OSG Investigations Team Last Week GRACC changes Changes to dashboard based on comments from e.g. LIGO and Minerva Overview of changes https://djw8605.github.io/2017/11/06/cleaning-up-gracc/ StashCache changes This Week More GRACC changes Help NERSC with StashCache Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 11 November 2017"},{"location":"meetings/2017/TechArea20171106/#osg-technology-area-meeting-11-november-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Brian B, Carl, Derek, Edgar, Marian, Mat, Suchandra, Tim C, Tim T","title":"OSG Technology Area Meeting, 11 November 2017"},{"location":"meetings/2017/TechArea20171106/#announcements","text":"BrianL out Thu-Mon (11/2-11/6)","title":"Announcements"},{"location":"meetings/2017/TechArea20171106/#triage-duty","text":"This week: Suchandra Next week: TimT 6 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171106/#jira","text":"# of tickets State 162 +11 Open 17 -20 In Progress 40 +29 Ready for Testing 3 +3 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171106/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle 3.4.5 and 3.3.30 will be the last releases on the regular schedule. Subsequent releases will use the flexible release schedule .","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171106/#osg-software-team","text":"","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171106/#discussions","text":"XRootd 4.7.1 has a bug that affects StashCache servers, but is not user-facing","title":"Discussions"},{"location":"meetings/2017/TechArea20171106/#support-update","text":"UCSD Tier 2 (Edgar) - GPU jobs from IceCube wre causing GPU nodes to lock up and crash. Might be hardware issue since we haven't seen anything like that at Nebraska.","title":"Support Update"},{"location":"meetings/2017/TechArea20171106/#osg-release-team","text":"Suchandra Thapa will handle the November Release 3.3.30 Both 3.4.5 Total Status 0 -1 0 -5 0 -1 9 -7 Open 0 -1 0 -20 0 -3 0 -24 In Progress 4 +2 31 +23 3 +1 38 +26 Ready for Testing 1 +1 1 +1 2 +2 4 +4 Ready for Release 4 +1 32 -1 5 -1 42 -1 Total Both Default to authenticated request and retrieval of certificates Drop PKI tool quota verification CVMFS 2.4.2 XRootD 4.7.1 Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client Fix EPSV response for IPv4-mapped IPv6 addresses gratia probes updates Bug fixes for BLAHP: Slurm memory parsing, Unicode decode error RSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported Maintenance on osg-test 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 GridFTP-HDFS 1.1 GUMS: software.grid.iu.edu - repo.grid.iu.edu","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171106/#discussions_1","text":"Need testing help","title":"Discussions"},{"location":"meetings/2017/TechArea20171106/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171106/#last-week","text":"GRACC changes Changes to dashboard based on comments from e.g. LIGO and Minerva Overview of changes https://djw8605.github.io/2017/11/06/cleaning-up-gracc/ StashCache changes","title":"Last Week"},{"location":"meetings/2017/TechArea20171106/#this-week","text":"More GRACC changes Help NERSC with StashCache","title":"This Week"},{"location":"meetings/2017/TechArea20171106/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171106/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171113/","text":"OSG Technology Area Meeting, 13 November 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Announcements Brian B and Derek at conferences this week Triage Duty This week: TimT Next week: BrianL 6 (+0) open tickets JIRA # of tickets State 180 +18 Open 23 +6 In Progress 1 -39 Ready for Testing 43 +40 Ready for Release Release Schedule Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle 3.4.5 and 3.3.30 will be the last releases on the regular schedule. Subsequent releases will use the flexible release schedule . OSG Software Team JIRA ticket 3000 created! Documentation updates ITB documentation: https://opensciencegrid.github.io/docs-itb/ (push a branch to opensciencegrid/docs that starts with \"itb.\") Monthly (?) documentation focus afternoons (incoming doodle poll). Globus Toolkit replacement = Grid Community Toolkit ( https://gridcf.org/ ) HDFS 2++: Initial Hadoop 2.6 (from Cloudera) built but gridftp-hdfs and xrootd-hdfs [[http://vdt.cs.wisc.edu/tests/20171110-1421/results.html][tests are failing]] Discussions None this week Support Update CTSC (BrianL): assisted Sanjay (UW Madison student) with certificate chain and CE questions Taiwan (BrianL): issues with condor daemon security because their host DN didn't match anything in the condor_mapfile OSG Release Team Suchandra Thapa will handle the November Release 3.3.30 Both 3.4.5 Total Status 0 +0 0 +0 0 +0 0 +0 Open 0 +0 0 +0 0 +0 0 +0 In Progress 0 -4 0 -31 0 -3 0 -38 Ready for Testing 5 +4 33 +32 5 +3 43 +39 Ready for Release 5 +0 33 +1 5 +0 43 +1 Total Both Default to authenticated request and retrieval of certificates Drop PKI tool quota verification CVMFS 2.4.2 XRootD 4.7.1 Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client Fix EPSV response for IPv4-mapped IPv6 addresses gratia probes updates Bug fixes for BLAHP: Slurm memory parsing, Unicode decode error RSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported Maintenance on osg-test 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 GridFTP-HDFS 1.1 GUMS: software.grid.iu.edu - repo.grid.iu.edu Discussions None this week OSG Investigations Team Technology area out at conferences Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 13 November 2017"},{"location":"meetings/2017/TechArea20171113/#osg-technology-area-meeting-13-november-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending:","title":"OSG Technology Area Meeting, 13 November 2017"},{"location":"meetings/2017/TechArea20171113/#announcements","text":"Brian B and Derek at conferences this week","title":"Announcements"},{"location":"meetings/2017/TechArea20171113/#triage-duty","text":"This week: TimT Next week: BrianL 6 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171113/#jira","text":"# of tickets State 180 +18 Open 23 +6 In Progress 1 -39 Ready for Testing 43 +40 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171113/#release-schedule","text":"Name Version Development Freeze Package Freeze Release Notes November 3.4.5, 3.3.30 2017-10-30 2017-11-06 2017-11-14 5 week cycle 3.4.5 and 3.3.30 will be the last releases on the regular schedule. Subsequent releases will use the flexible release schedule .","title":"Release Schedule"},{"location":"meetings/2017/TechArea20171113/#osg-software-team","text":"JIRA ticket 3000 created! Documentation updates ITB documentation: https://opensciencegrid.github.io/docs-itb/ (push a branch to opensciencegrid/docs that starts with \"itb.\") Monthly (?) documentation focus afternoons (incoming doodle poll). Globus Toolkit replacement = Grid Community Toolkit ( https://gridcf.org/ ) HDFS 2++: Initial Hadoop 2.6 (from Cloudera) built but gridftp-hdfs and xrootd-hdfs [[http://vdt.cs.wisc.edu/tests/20171110-1421/results.html][tests are failing]]","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171113/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171113/#support-update","text":"CTSC (BrianL): assisted Sanjay (UW Madison student) with certificate chain and CE questions Taiwan (BrianL): issues with condor daemon security because their host DN didn't match anything in the condor_mapfile","title":"Support Update"},{"location":"meetings/2017/TechArea20171113/#osg-release-team","text":"Suchandra Thapa will handle the November Release 3.3.30 Both 3.4.5 Total Status 0 +0 0 +0 0 +0 0 +0 Open 0 +0 0 +0 0 +0 0 +0 In Progress 0 -4 0 -31 0 -3 0 -38 Ready for Testing 5 +4 33 +32 5 +3 43 +39 Ready for Release 5 +0 33 +1 5 +0 43 +1 Total Both Default to authenticated request and retrieval of certificates Drop PKI tool quota verification CVMFS 2.4.2 XRootD 4.7.1 Update osg-system profiler globus-gridftp-server-control 6.0 lcmaps-plugins-voms manpage and docs w/ all-fqans/first-fqun options Drop SOFTWARE-1853 patch from globus-ftp-client Fix EPSV response for IPv4-mapped IPv6 addresses gratia probes updates Bug fixes for BLAHP: Slurm memory parsing, Unicode decode error RSV: CRL freshness fix; drop atla.xrootd probe; dummy out osg.version, vo-supported Maintenance on osg-test 3.4.5 osg-configure 2.2.2 tweak comments in 10-misc.ini 3.3.30 osg-configure 1.10.2 GridFTP-HDFS 1.1 GUMS: software.grid.iu.edu - repo.grid.iu.edu","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171113/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171113/#osg-investigations-team","text":"Technology area out at conferences","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171113/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171113/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171120/","text":"OSG Technology Area Meeting, 20 November 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimC, TimT Announcements Thanksgiving on Thursday Triage Duty This week: BrianL Next week: Carl 5 (-1) open tickets JIRA # of tickets State 161 -19 Open 26 +3 In Progress 5 +4 Ready for Testing 2 -41 Ready for Release OSG Software Team Documentation focus afternoon 12/14 Don't forget to update your stale software tickets Discussions None this week Support Update CTSC (BrianL): Force GSI authentication for locally submitted jobs Taiwan (BrianL): Assisted with job router misconfiguration OSG Release Team Data Release today: VO Package v76 3.3 Both 3.4 Total Status 0 +0 20 +20 5 +5 25 +25 Open 1 +1 9 +9 4 +4 14 +14 In Progress 1 +1 1 +1 1 +1 3 +3 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +2 30 +30 10 +0 42 +42 Total Both Include cvmfs upstream bug fix for losing singularity bind mounts 3.4.6 Upgrade to singularity 2.4+ (On hold for January release) 3.3.31 Release GridFTP-HDFS 1.1.1 Discussions None this week OSG Investigations Team GRACC Updates. Better data! StashCache writes implemented in production on OSG-Connect, more or less Logging for CVMFS-Singularity-Sync now going into GRACC (need to implement removal of old logs) OSG BIBM workshop was a success! http://sbbi-panda.unl.edu/bibm2017/program.html Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 20 November 2017"},{"location":"meetings/2017/TechArea20171120/#osg-technology-area-meeting-20-november-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 20 November 2017"},{"location":"meetings/2017/TechArea20171120/#announcements","text":"Thanksgiving on Thursday","title":"Announcements"},{"location":"meetings/2017/TechArea20171120/#triage-duty","text":"This week: BrianL Next week: Carl 5 (-1) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171120/#jira","text":"# of tickets State 161 -19 Open 26 +3 In Progress 5 +4 Ready for Testing 2 -41 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171120/#osg-software-team","text":"Documentation focus afternoon 12/14 Don't forget to update your stale software tickets","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171120/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171120/#support-update","text":"CTSC (BrianL): Force GSI authentication for locally submitted jobs Taiwan (BrianL): Assisted with job router misconfiguration","title":"Support Update"},{"location":"meetings/2017/TechArea20171120/#osg-release-team","text":"Data Release today: VO Package v76 3.3 Both 3.4 Total Status 0 +0 20 +20 5 +5 25 +25 Open 1 +1 9 +9 4 +4 14 +14 In Progress 1 +1 1 +1 1 +1 3 +3 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +2 30 +30 10 +0 42 +42 Total Both Include cvmfs upstream bug fix for losing singularity bind mounts 3.4.6 Upgrade to singularity 2.4+ (On hold for January release) 3.3.31 Release GridFTP-HDFS 1.1.1","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171120/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171120/#osg-investigations-team","text":"GRACC Updates. Better data! StashCache writes implemented in production on OSG-Connect, more or less Logging for CVMFS-Singularity-Sync now going into GRACC (need to implement removal of old logs) OSG BIBM workshop was a success! http://sbbi-panda.unl.edu/bibm2017/program.html","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171120/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171120/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171127/","text":"OSG Technology Area Meeting, 27 November 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: Carl Next week: Suchandra 5 (+0) open tickets JIRA # of tickets State 156 -5 Open 29 +3 In Progress 6 +1 Ready for Testing 0 -2 Ready for Release OSG Software Team XRootD 4.8.0 release candidate available. Mainly bug fixes, can go directly into the release repo. Don't forget to update your stale software tickets Discussions None this week Support Update IceCube (Derek): GPU code wasn't working due to GLOW defaulting to running in Singularity containers NERSC (Derek): Continued work to set up non-root XRootD caching for CMS workflow Purdue (BrianL): condor_ce_trace fails for IPv6-enabled hosts Taiwan (BrianL): Continued assistance with job router misconfiguration Syracuse (Edgar): Glideins couldn't find Singularity in their expected path OSG Release Team Data Release this week (IGTF 1.88) w/ SHA2 checksum (may take a little longer) 3.3 Both 3.4 Total Status 0 +0 15 -5 7 +2 22 -3 Open 1 +0 12 +3 5 +1 18 +4 In Progress 1 +0 5 +4 0 -1 6 +3 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +0 32 +2 12 +2 46 +4 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users 3.4.6 Nothing yet 3.3.31 Release GridFTP-HDFS 1.1.1 Discussions None this week OSG Investigations Team GRACC is now using a hosted RabbitMQ service. Better alarms. PerfSonar data now flowing through hosted RabbitMQ. Bug fixes in perfsonar data collection Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 27 November 2017"},{"location":"meetings/2017/TechArea20171127/#osg-technology-area-meeting-27-november-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 27 November 2017"},{"location":"meetings/2017/TechArea20171127/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20171127/#triage-duty","text":"This week: Carl Next week: Suchandra 5 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171127/#jira","text":"# of tickets State 156 -5 Open 29 +3 In Progress 6 +1 Ready for Testing 0 -2 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171127/#osg-software-team","text":"XRootD 4.8.0 release candidate available. Mainly bug fixes, can go directly into the release repo. Don't forget to update your stale software tickets","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171127/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171127/#support-update","text":"IceCube (Derek): GPU code wasn't working due to GLOW defaulting to running in Singularity containers NERSC (Derek): Continued work to set up non-root XRootD caching for CMS workflow Purdue (BrianL): condor_ce_trace fails for IPv6-enabled hosts Taiwan (BrianL): Continued assistance with job router misconfiguration Syracuse (Edgar): Glideins couldn't find Singularity in their expected path","title":"Support Update"},{"location":"meetings/2017/TechArea20171127/#osg-release-team","text":"Data Release this week (IGTF 1.88) w/ SHA2 checksum (may take a little longer) 3.3 Both 3.4 Total Status 0 +0 15 -5 7 +2 22 -3 Open 1 +0 12 +3 5 +1 18 +4 In Progress 1 +0 5 +4 0 -1 6 +3 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +0 32 +2 12 +2 46 +4 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users 3.4.6 Nothing yet 3.3.31 Release GridFTP-HDFS 1.1.1","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171127/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171127/#osg-investigations-team","text":"GRACC is now using a hosted RabbitMQ service. Better alarms. PerfSonar data now flowing through hosted RabbitMQ. Bug fixes in perfsonar data collection","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171127/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171127/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171204/","text":"OSG Technology Area Meeting, 4 December 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: Suchandra Next week: Mat 7 (+2) open tickets JIRA # of tickets State 154 -2 Open 25 -4 In Progress 12 +6 Ready for Testing 0 +0 Ready for Release OSG Software Team Spoke with XRootD team about static analysis (they run coverity). They were open to making static analysis part of their release process. Doc Focus Afternoon Doc priority list JIRA tickets incoming You may have noticed shuffling ticket ownership for doc tickets, I'm just balancing plates for the upcoming afternoon Don't forget to update your stale software tickets Assignee # of stale tickets Mat 7 BrianL 3 Carl 3 TimT 1 Discussions Suggestions to make to the XRootD team: Perform coverity diffs to easily spot new issues; dynamic analysis We will go back to labeling specific releases instead of release series to better track work in the next release Support Update GRACC (Carl, Derek, Edgar): site gratia probe timeouts due to RSV perfsonar clogging up the hosted RabbitMQ (problem seen at NET2, GOC-35513 ) NERSC (Derek, Marian): Continued work to set up non-root XRootD caching for CMS workflow, deploying service certificate to manage data access Syracuse (Edgar): path issue turned out to be a CE configuration issue ( SOFTWARE-3035 ) Texas A M (Carl): PBS probe reprocesses empty logs, causing slowdowns OSG Release Team VO Package Release expected this week (VO Package v77) Release estimated to go out week of December 18th Better aligns with XRootD 4.8.0 release 3.3.31 Both 3.4.6 Total Status 1 +1 7 -8 5 -2 13 -9 Open 0 -1 11 -1 8 +3 19 +1 In Progress 1 +0 10 +5 0 +0 11 +5 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +0 28 -4 12 +1 43 -3 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client OSG PKI Tools Default to using HTTPS Updated help information Leave old keys in place if new keys cannot be fetched osg-gridftp: add osg-configure-gratia Minor bug fix to BLAHP Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users (internal) 3.4.6 Nothing yet 3.3.31 GridFTP-HDFS: fix potential crash related to CVMFS checksums Discussions We will go back to labeling for the specific release to better track work OSG Investigations Team GRACC is now using a hosted RabbitMQ service. Better alarms. RC1 release of XRootD v4.8.0 in osg-development (StashCache at Nebrasking under testing) working on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana) Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting,  4 December 2017"},{"location":"meetings/2017/TechArea20171204/#osg-technology-area-meeting-4-december-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting,  4 December 2017"},{"location":"meetings/2017/TechArea20171204/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20171204/#triage-duty","text":"This week: Suchandra Next week: Mat 7 (+2) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171204/#jira","text":"# of tickets State 154 -2 Open 25 -4 In Progress 12 +6 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171204/#osg-software-team","text":"Spoke with XRootD team about static analysis (they run coverity). They were open to making static analysis part of their release process. Doc Focus Afternoon Doc priority list JIRA tickets incoming You may have noticed shuffling ticket ownership for doc tickets, I'm just balancing plates for the upcoming afternoon Don't forget to update your stale software tickets Assignee # of stale tickets Mat 7 BrianL 3 Carl 3 TimT 1","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171204/#discussions","text":"Suggestions to make to the XRootD team: Perform coverity diffs to easily spot new issues; dynamic analysis We will go back to labeling specific releases instead of release series to better track work in the next release","title":"Discussions"},{"location":"meetings/2017/TechArea20171204/#support-update","text":"GRACC (Carl, Derek, Edgar): site gratia probe timeouts due to RSV perfsonar clogging up the hosted RabbitMQ (problem seen at NET2, GOC-35513 ) NERSC (Derek, Marian): Continued work to set up non-root XRootD caching for CMS workflow, deploying service certificate to manage data access Syracuse (Edgar): path issue turned out to be a CE configuration issue ( SOFTWARE-3035 ) Texas A M (Carl): PBS probe reprocesses empty logs, causing slowdowns","title":"Support Update"},{"location":"meetings/2017/TechArea20171204/#osg-release-team","text":"VO Package Release expected this week (VO Package v77) Release estimated to go out week of December 18th Better aligns with XRootD 4.8.0 release 3.3.31 Both 3.4.6 Total Status 1 +1 7 -8 5 -2 13 -9 Open 0 -1 11 -1 8 +3 19 +1 In Progress 1 +0 10 +5 0 +0 11 +5 Ready for Testing 0 +0 0 +0 0 +0 0 +0 Ready for Release 2 +0 28 -4 12 +1 43 -3 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client OSG PKI Tools Default to using HTTPS Updated help information Leave old keys in place if new keys cannot be fetched osg-gridftp: add osg-configure-gratia Minor bug fix to BLAHP Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users (internal) 3.4.6 Nothing yet 3.3.31 GridFTP-HDFS: fix potential crash related to CVMFS checksums","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171204/#discussions_1","text":"We will go back to labeling for the specific release to better track work","title":"Discussions"},{"location":"meetings/2017/TechArea20171204/#osg-investigations-team","text":"GRACC is now using a hosted RabbitMQ service. Better alarms. RC1 release of XRootD v4.8.0 in osg-development (StashCache at Nebrasking under testing) working on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana)","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171204/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171204/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171211/","text":"OSG Technology Area Meeting, 11 December 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: Mat Next week: Edgar 7 (+0) open tickets JIRA # of tickets State 148 -6 Open 34 +9 In Progress 13 +1 Ready for Testing 3 +3 Ready for Release OSG Software Team All tickets for the December release must be RFR by the end of the week Doc Focus Afternoon (Thursday 12/14 1PM Central) Work list Please review tickets assigned to you in the sprint before Thursday Tackle items assigned to you in the sprint If you finish items assigned to you in the sprint, pick up items from the backlog (organized in priority order) and move them into the sprint. Use the #osg-software slack channel for discussions Discussions TimC gave us a summary of Rucio from last Friday's blueprint meeting Support Update GRACC (Suchandra): some leftover GRACC tickets from last week's slowdown Vanderbilt (Edgar): added GPU factory entries NERSC (Derek): Debugging XRootD for StashCache OSG Release Team VO Package Release expected this week (VO Package v77) Release estimated to go out week of December 18th Better aligns with XRootD 4.8.0 release 3.3.31 Both 3.4.6 Total Status 0 -1 3 -4 4 -1 7 -6 Open 2 +2 12 +1 9 +1 23 +4 In Progress 1 +0 10 +0 0 +0 11 +0 Ready for Testing 0 +0 3 +3 0 +0 3 +3 Ready for Release 3 +1 28 -4 12 +1 44 +1 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client OSG PKI Tools Default to using HTTPS Updated help information Leave old keys in place if new keys cannot be fetched Unhandled exception when network unreachable Update to help@opensciencegrid.org osg-gridftp: add osg-configure-gratia Minor bug fix to BLAHP Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users (internal) 3.4.6 Nothing yet 3.3.31 GridFTP-HDFS: fix potential crash related to CVMFS checksums Discussions Proposal coming this week for making doc review part of the release process OSG Investigations Team GRACC caught up from the backlog. Perfsonar data now flowing smoothly through the message broker. NERSC using CMS XRootD Cache. Look for changes in the gratia probe for GPU detection. working on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana) Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 11 December 2017"},{"location":"meetings/2017/TechArea20171211/#osg-technology-area-meeting-11-december-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 11 December 2017"},{"location":"meetings/2017/TechArea20171211/#announcements","text":"","title":"Announcements"},{"location":"meetings/2017/TechArea20171211/#triage-duty","text":"This week: Mat Next week: Edgar 7 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171211/#jira","text":"# of tickets State 148 -6 Open 34 +9 In Progress 13 +1 Ready for Testing 3 +3 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171211/#osg-software-team","text":"All tickets for the December release must be RFR by the end of the week Doc Focus Afternoon (Thursday 12/14 1PM Central) Work list Please review tickets assigned to you in the sprint before Thursday Tackle items assigned to you in the sprint If you finish items assigned to you in the sprint, pick up items from the backlog (organized in priority order) and move them into the sprint. Use the #osg-software slack channel for discussions","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171211/#discussions","text":"TimC gave us a summary of Rucio from last Friday's blueprint meeting","title":"Discussions"},{"location":"meetings/2017/TechArea20171211/#support-update","text":"GRACC (Suchandra): some leftover GRACC tickets from last week's slowdown Vanderbilt (Edgar): added GPU factory entries NERSC (Derek): Debugging XRootD for StashCache","title":"Support Update"},{"location":"meetings/2017/TechArea20171211/#osg-release-team","text":"VO Package Release expected this week (VO Package v77) Release estimated to go out week of December 18th Better aligns with XRootD 4.8.0 release 3.3.31 Both 3.4.6 Total Status 0 -1 3 -4 4 -1 7 -6 Open 2 +2 12 +1 9 +1 23 +4 In Progress 1 +0 10 +0 0 +0 11 +0 Ready for Testing 0 +0 3 +3 0 +0 3 +3 Ready for Release 3 +1 28 -4 12 +1 44 +1 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client OSG PKI Tools Default to using HTTPS Updated help information Leave old keys in place if new keys cannot be fetched Unhandled exception when network unreachable Update to help@opensciencegrid.org osg-gridftp: add osg-configure-gratia Minor bug fix to BLAHP Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users (internal) 3.4.6 Nothing yet 3.3.31 GridFTP-HDFS: fix potential crash related to CVMFS checksums","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171211/#discussions_1","text":"Proposal coming this week for making doc review part of the release process","title":"Discussions"},{"location":"meetings/2017/TechArea20171211/#osg-investigations-team","text":"GRACC caught up from the backlog. Perfsonar data now flowing smoothly through the message broker. NERSC using CMS XRootD Cache. Look for changes in the gratia probe for GPU detection. working on new GRACC FE to replace current one (lot of updates during migration to new ES version for services like prometheus and grafana)","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171211/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171211/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2017/TechArea20171218/","text":"OSG Technology Area Meeting, 18 December 2017 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, BrianL, Derek, Edgar, Marian, Mat, TimC, TimT Announcements See the OSG Software google calendar for outages Next meeting 2018-01-02 Triage Duty This week: Edgar Next week: TimT 7 (+0) open tickets JIRA # of tickets State 152 +4 Open 30 -4 In Progress 27 +14 Ready for Testing 16 +13 Ready for Release OSG Software Team Documentation Preview of the documentation area using the material theme: https://opensciencegrid.github.io/docs-itb/ Future focus frenzies will give more flexibility to team members outside of Madison Discussions CVMFS 2.4.2 is showing issues, reported by Lincoln Bryant and Ken Herner; may wish to pull from release, but talk to Dave Dykstra first Support Update Okalahoma (BrianL): osg-user-cert-renew is broken due different formatting of CILogon vs DigiCert serial numbers in the OIM DB fix won't make the December release NERSC (Derek): delaying caching work until after the New Year Derek, BrianL: on ticket for UConn (Richard Jones) -- might need HTCondor Team help (TJ or Greg) Marian (Florida): XRootD errors turned out to be sporadic OSG Release Team VO Package Release expected this week (VO Package v77) Release estimated to go out week of December 18th Better aligns with XRootD 4.8.0 release 3.3.31 Both 3.4.6 Total Status 0 -1 3 -4 4 -1 7 -6 Open 2 +2 12 +1 9 +1 23 +4 In Progress 1 +0 10 +0 0 +0 11 +0 Ready for Testing 0 +0 3 +3 0 +0 3 +3 Ready for Release 3 +1 28 -4 12 +1 44 +1 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client OSG PKI Tools Default to using HTTPS Updated help information Leave old keys in place if new keys cannot be fetched Unhandled exception when network unreachable Update to help@opensciencegrid.org osg-gridftp: add osg-configure-gratia Minor bug fix to BLAHP Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users (internal) 3.4.6 Nothing yet 3.3.31 GridFTP-HDFS: fix potential crash related to CVMFS checksums Discussions Proposal coming this week for making doc review part of the release process OSG Investigations Team Adding GPU support and transfer logs to GRACC. GRACC nodes getting updated; frontend node getting moved. This should require zero downtime. GRACC caught up from the backlog. Perfsonar data now flowing smoothly through the message broker. NERSC using CMS XRootD Cache. Look for changes in the gratia probe for GPU detection. Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"OSG Technology Area Meeting, 18 December 2017"},{"location":"meetings/2017/TechArea20171218/#osg-technology-area-meeting-18-december-2017","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, BrianL, Derek, Edgar, Marian, Mat, TimC, TimT","title":"OSG Technology Area Meeting, 18 December 2017"},{"location":"meetings/2017/TechArea20171218/#announcements","text":"See the OSG Software google calendar for outages Next meeting 2018-01-02","title":"Announcements"},{"location":"meetings/2017/TechArea20171218/#triage-duty","text":"This week: Edgar Next week: TimT 7 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2017/TechArea20171218/#jira","text":"# of tickets State 152 +4 Open 30 -4 In Progress 27 +14 Ready for Testing 16 +13 Ready for Release","title":"JIRA"},{"location":"meetings/2017/TechArea20171218/#osg-software-team","text":"Documentation Preview of the documentation area using the material theme: https://opensciencegrid.github.io/docs-itb/ Future focus frenzies will give more flexibility to team members outside of Madison","title":"OSG Software Team"},{"location":"meetings/2017/TechArea20171218/#discussions","text":"CVMFS 2.4.2 is showing issues, reported by Lincoln Bryant and Ken Herner; may wish to pull from release, but talk to Dave Dykstra first","title":"Discussions"},{"location":"meetings/2017/TechArea20171218/#support-update","text":"Okalahoma (BrianL): osg-user-cert-renew is broken due different formatting of CILogon vs DigiCert serial numbers in the OIM DB fix won't make the December release NERSC (Derek): delaying caching work until after the New Year Derek, BrianL: on ticket for UConn (Richard Jones) -- might need HTCondor Team help (TJ or Greg) Marian (Florida): XRootD errors turned out to be sporadic","title":"Support Update"},{"location":"meetings/2017/TechArea20171218/#osg-release-team","text":"VO Package Release expected this week (VO Package v77) Release estimated to go out week of December 18th Better aligns with XRootD 4.8.0 release 3.3.31 Both 3.4.6 Total Status 0 -1 3 -4 4 -1 7 -6 Open 2 +2 12 +1 9 +1 23 +4 In Progress 1 +0 10 +0 0 +0 11 +0 Ready for Testing 0 +0 3 +3 0 +0 3 +3 Ready for Release 3 +1 28 -4 12 +1 44 +1 Total Both Release Glideinwms v3.2.20+ Drop globus-gram-client-tools requirement from glideinwms Include the gfal2 http plugins as part of the osg-wn-client OSG PKI Tools Default to using HTTPS Updated help information Leave old keys in place if new keys cannot be fetched Unhandled exception when network unreachable Update to help@opensciencegrid.org osg-gridftp: add osg-configure-gratia Minor bug fix to BLAHP Include cvmfs upstream bug fix for losing singularity bind mounts Allow tarballs to be updated by different users (internal) 3.4.6 Nothing yet 3.3.31 GridFTP-HDFS: fix potential crash related to CVMFS checksums","title":"OSG Release Team"},{"location":"meetings/2017/TechArea20171218/#discussions_1","text":"Proposal coming this week for making doc review part of the release process","title":"Discussions"},{"location":"meetings/2017/TechArea20171218/#osg-investigations-team","text":"Adding GPU support and transfer logs to GRACC. GRACC nodes getting updated; frontend node getting moved. This should require zero downtime. GRACC caught up from the backlog. Perfsonar data now flowing smoothly through the message broker. NERSC using CMS XRootD Cache. Look for changes in the gratia probe for GPU detection.","title":"OSG Investigations Team"},{"location":"meetings/2017/TechArea20171218/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2017/TechArea20171218/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180102/","text":"OSG Technology Area Meeting, 2 January 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Brian L, Marian, Mat, Suchandra, Tim C, Tim T Announcements Please look at triage schedule and see if it works for you. Triage Duty This week: BrianL Next week: Carl (?) 7 (+0) open tickets JIRA # of tickets State 153 +1 Open 37 +7 In Progress 6 -21 Ready for Testing 0 -16 Ready for Release OSG Software Team Address PR comments from last month's doc focus Expect an e-mail this week about the January doc focus frenzy Discussions GCT build system stuff waiting for PR review XRootD: /var/run/xrootd not created in reboot Support Update None this week OSG Release Team 3.4.7 Status 13 +13 Open 22 +22 In Progress 3 +3 Ready for Testing 0 +0 Ready for Release 38 +38 Total 3.4.7 osg-ca-generator 1.3.0 backups should be created if using --force Generate LSC file Discussions None this week OSG Investigations Team GRACC is now collecting HTCondor transfer records. Graphs incoming. GRACC is now running on a new frontend (web facing node). Check for broken things. Edgar wrote documentation on how to send transfer records: https://opensciencegrid.github.io/docs/other/schedd-filebeats/ Ongoing GRACC Project StashCache Project Discussions None this week","title":"January 2, 2018"},{"location":"meetings/2018/TechArea20180102/#osg-technology-area-meeting-2-january-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Brian L, Marian, Mat, Suchandra, Tim C, Tim T","title":"OSG Technology Area Meeting,  2 January 2018"},{"location":"meetings/2018/TechArea20180102/#announcements","text":"Please look at triage schedule and see if it works for you.","title":"Announcements"},{"location":"meetings/2018/TechArea20180102/#triage-duty","text":"This week: BrianL Next week: Carl (?) 7 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180102/#jira","text":"# of tickets State 153 +1 Open 37 +7 In Progress 6 -21 Ready for Testing 0 -16 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180102/#osg-software-team","text":"Address PR comments from last month's doc focus Expect an e-mail this week about the January doc focus frenzy","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180102/#discussions","text":"GCT build system stuff waiting for PR review XRootD: /var/run/xrootd not created in reboot","title":"Discussions"},{"location":"meetings/2018/TechArea20180102/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2018/TechArea20180102/#osg-release-team","text":"3.4.7 Status 13 +13 Open 22 +22 In Progress 3 +3 Ready for Testing 0 +0 Ready for Release 38 +38 Total 3.4.7 osg-ca-generator 1.3.0 backups should be created if using --force Generate LSC file","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180102/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180102/#osg-investigations-team","text":"GRACC is now collecting HTCondor transfer records. Graphs incoming. GRACC is now running on a new frontend (web facing node). Check for broken things. Edgar wrote documentation on how to send transfer records: https://opensciencegrid.github.io/docs/other/schedd-filebeats/","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180102/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180102/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180108/","text":"OSG Technology Area Meeting, 8 January 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimC, TimT Announcements Triage Duty This week: Edgar Next week: TimT 8 (+1) open tickets JIRA # of tickets State 150 -3 Open 36 +0 In Progress 8 +2 Ready for Testing 0 +0 Ready for Release OSG Software Team GlideinWMS release candidate expected today TWiki expected to be turned down in the next few days Address PR comments from last month's doc focus The Madison team is deciding on a day for the next doc focus. Other team members are welcome to join the chosen date+time or set aside their own 4 hour focus time. Thoughts/feedback on the material theme used for the technology area vs the readthedocs theme used for the docs area? Discussions The material theme content width is too narrow. Request a configuration knob or pointers on how to adjust it. Additionally, TimC would like configuration to easily disable previous/next page links in the footer Support Update AGLT2 (BrianL): Bad UK eScience CA update may be the cause of a user's gfal-copy failures to AGLT2's SRM Oklahoma (Marian): /var/run/xrootd doesn't get recreated on EL7 upon reboot. Passing this information onto the xrootd developers. Utah (Suchandra, Derek): Adding lonepeak cluster to the OSG via hosted CE OSG Release Team 3.4.7 Status 15 +2 Open 24 +2 In Progress 5 +2 Ready for Testing 0 +0 Ready for Release 44 +6 Total 3.4.7 frontier-squid 3.5.27-2.1 osg-ca-generator 1.3.0 backups should be created if using --force Generate LSC file Remove Slurm from osg-contrib Discussions None this week OSG Investigations Team GRACC is now collecting HTCondor transfer records. All the Graphs (URL likely to change) GRACC is now running on a new frontend (web facing node). Check for broken things. New GRACC HTCondor xfer nodes coming online New PS service coming to GRACC StashCache monitoring / accounting in inadaquate. We always knew this, but now we are getting requests for reports. Need to add features to XRootD for better monitoring. Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"January 8, 2018"},{"location":"meetings/2018/TechArea20180108/#osg-technology-area-meeting-8-january-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting,  8 January 2018"},{"location":"meetings/2018/TechArea20180108/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180108/#triage-duty","text":"This week: Edgar Next week: TimT 8 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180108/#jira","text":"# of tickets State 150 -3 Open 36 +0 In Progress 8 +2 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180108/#osg-software-team","text":"GlideinWMS release candidate expected today TWiki expected to be turned down in the next few days Address PR comments from last month's doc focus The Madison team is deciding on a day for the next doc focus. Other team members are welcome to join the chosen date+time or set aside their own 4 hour focus time. Thoughts/feedback on the material theme used for the technology area vs the readthedocs theme used for the docs area?","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180108/#discussions","text":"The material theme content width is too narrow. Request a configuration knob or pointers on how to adjust it. Additionally, TimC would like configuration to easily disable previous/next page links in the footer","title":"Discussions"},{"location":"meetings/2018/TechArea20180108/#support-update","text":"AGLT2 (BrianL): Bad UK eScience CA update may be the cause of a user's gfal-copy failures to AGLT2's SRM Oklahoma (Marian): /var/run/xrootd doesn't get recreated on EL7 upon reboot. Passing this information onto the xrootd developers. Utah (Suchandra, Derek): Adding lonepeak cluster to the OSG via hosted CE","title":"Support Update"},{"location":"meetings/2018/TechArea20180108/#osg-release-team","text":"3.4.7 Status 15 +2 Open 24 +2 In Progress 5 +2 Ready for Testing 0 +0 Ready for Release 44 +6 Total 3.4.7 frontier-squid 3.5.27-2.1 osg-ca-generator 1.3.0 backups should be created if using --force Generate LSC file Remove Slurm from osg-contrib","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180108/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180108/#osg-investigations-team","text":"GRACC is now collecting HTCondor transfer records. All the Graphs (URL likely to change) GRACC is now running on a new frontend (web facing node). Check for broken things. New GRACC HTCondor xfer nodes coming online New PS service coming to GRACC StashCache monitoring / accounting in inadaquate. We always knew this, but now we are getting requests for reports. Need to add features to XRootD for better monitoring.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180108/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2018/TechArea20180108/#discussions_2","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180122/","text":"OSG Technology Area Meeting, 22 January 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, TimC, TimT Announcements Suchandra will only be available in the afternoons until Feb 5 Triage Duty This week: Mat Next week: Suchandra 10 (+2) open tickets JIRA # of tickets State 134 -8 Open 33 -5 In Progress 18 +7 Ready for Testing 4 +3 Ready for Release OSG Software Team Tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication Instructions need to be improved for sites still on 3.3 (https://ticket.grid.iu.edu/35830) gWMS site tester results GCT: Seeing some external contributions and we're ramping up on the first release. Globus references and docs need to be fixed. OSG 3.4.7: HTCondor issues with 8.6.9 and 8.7.6, will await the next release Status on RSV? It's nearly ready, there's one ticket that needs to review before being built Doc Focus 2 is scheduled for 1pm CST on Feb 1 Discussion gWMS site tester timing out because it competes with actual job. Edgar to run a slightly different probe using Condor-G (or condor ping ?) We need to communicate with the rest of the GCF that we will be taking point on documentation Derek and Edgar will attend the doc focus afternoon (Edgar will start and end ~1hr earlier) Support Update display.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate OK (Marian) - working on XrootD PID file placement. OSG Release Team 3.4.7 Status 7 -8 Open 16 -8 In Progress 14 +9 Ready for Testing 4 +4 Ready for Release 41 -3 Total Data Release IGTF 1.89 SHA256 3.4.7 HDFS 2.6 and supporting packages Singularity 2.4.2: Major upgrade from 2.3.2 frontier-squid 3.5.27-2.1 Pegasus 4.8.1 osg-pki-tools: fix to osg-user-cert-renew PerfSonar tools meta-package owamp nuttcp bwctl Remove dependencies on osg-version osg-ca-generator 1.3.0 backups should be created if using force Generate LSC file Remove Slurm from osg-contrib Discussions Expect TimT's proposal for making documentation review part of the release process this week OSG Investigations Team New XRootD Release Candidate available. Will build in OSG 3 HTCondor XFer nodes: OSG-Connect XD-Login Wisc (upcoming) CSIU New PS service coming to GRACC Ongoing GRACC Project StashCache Project (New URL!) Discussions None this week","title":"January 22, 2018"},{"location":"meetings/2018/TechArea20180122/#osg-technology-area-meeting-22-january-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, TimC, TimT","title":"OSG Technology Area Meeting, 22 January 2018"},{"location":"meetings/2018/TechArea20180122/#announcements","text":"Suchandra will only be available in the afternoons until Feb 5","title":"Announcements"},{"location":"meetings/2018/TechArea20180122/#triage-duty","text":"This week: Mat Next week: Suchandra 10 (+2) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180122/#jira","text":"# of tickets State 134 -8 Open 33 -5 In Progress 18 +7 Ready for Testing 4 +3 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180122/#osg-software-team","text":"Tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication Instructions need to be improved for sites still on 3.3 (https://ticket.grid.iu.edu/35830) gWMS site tester results GCT: Seeing some external contributions and we're ramping up on the first release. Globus references and docs need to be fixed. OSG 3.4.7: HTCondor issues with 8.6.9 and 8.7.6, will await the next release Status on RSV? It's nearly ready, there's one ticket that needs to review before being built Doc Focus 2 is scheduled for 1pm CST on Feb 1","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180122/#discussion","text":"gWMS site tester timing out because it competes with actual job. Edgar to run a slightly different probe using Condor-G (or condor ping ?) We need to communicate with the rest of the GCF that we will be taking point on documentation Derek and Edgar will attend the doc focus afternoon (Edgar will start and end ~1hr earlier)","title":"Discussion"},{"location":"meetings/2018/TechArea20180122/#support-update","text":"display.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate OK (Marian) - working on XrootD PID file placement.","title":"Support Update"},{"location":"meetings/2018/TechArea20180122/#osg-release-team","text":"3.4.7 Status 7 -8 Open 16 -8 In Progress 14 +9 Ready for Testing 4 +4 Ready for Release 41 -3 Total Data Release IGTF 1.89 SHA256 3.4.7 HDFS 2.6 and supporting packages Singularity 2.4.2: Major upgrade from 2.3.2 frontier-squid 3.5.27-2.1 Pegasus 4.8.1 osg-pki-tools: fix to osg-user-cert-renew PerfSonar tools meta-package owamp nuttcp bwctl Remove dependencies on osg-version osg-ca-generator 1.3.0 backups should be created if using force Generate LSC file Remove Slurm from osg-contrib","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180122/#discussions","text":"Expect TimT's proposal for making documentation review part of the release process this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180122/#osg-investigations-team","text":"New XRootD Release Candidate available. Will build in OSG 3 HTCondor XFer nodes: OSG-Connect XD-Login Wisc (upcoming) CSIU New PS service coming to GRACC","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180122/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2018/TechArea20180122/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180129/","text":"OSG Technology Area Meeting, 29 January 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT Announcements Suchandra will only be available in the afternoons until Feb 5 TimT will be gone starting next Monday, returning Feb 19 Triage Duty This week: Suchandra Next week: Carl 23 (+13) open tickets JIRA # of tickets State 123 -11 Open 23 -10 In Progress 24 +6 Ready for Testing 5 +1 Ready for Release OSG Software Team 15 tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication There are already instructions for edg-mkgridmap to transition. The timeline we'd like to see is that edg-mkgridmap sites transition by the end of March. GUMS sites are trickier: If they attach their GUMS config, MAKE SURE THEIR DB USERNAME/PASS ARE REDACTED Find out what GUMS clients they may have: CE, gridftp, xrootd, dcache, bestman. The first three all support LCMAPS VOMS auth, dcache and bestman2 do not. They cannot turn off their GUMS until they transition off of dcache/bestman2. Let Carl know of any attached GUMS configs and the output of `rpm -q osg-gums-config`. GCT: Frank Scheiner registered Gitbook/ReadTheDocs orgs but hasn't worked on the documentation other than that. Let's get started on making the transition from Globus to GCF/GCT. I can help setup GitHub docs. OSG 3.4.7 HDFS tested at Nebraska, looks good over the weekend Can we sneak HTCondor into this release? GOC tickets should be closed in favor of JIRA tickets if the user agrees to it ( GOC vs JIRA ): https://ticket.grid.iu.edu/34406 https://ticket.grid.iu.edu/35448 https://ticket.grid.iu.edu/35735 Doc Focus 2 is scheduled for 1pm CST on Feb 1 Discussion Edgar would like notes on how to determine what auth clients are using for help with the LCMAPS VOMS transition Support Update display.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate Nebraska/Purdue (Derek) - Issues with CPU usage being reported to the WLCG. OSG Release Team 3.4.7 Status 0 -7 Open 0 -16 In Progress 25 +11 Ready for Testing 5 +1 Ready for Release 30 -11 Total 3.4.7 HDFS 2.6 and supporting packages xrootd-hdfs gridftp-hdfs HTCondor 8.6.9 HTCondor 8.7.6 Singularity 2.4.2: Major upgrade from 2.3.2 frontier-squid 3.5.27-2.1 Pegasus 4.8.1 osg-pki-tools: fix to osg-user-cert-renew PerfSonar tools meta-package owamp nuttcp bwctl Remove dependencies on osg-version osg-ca-generator 1.3.0 backups should be created if using force Generate LSC file Remove Slurm from osg-contrib Discussions Expect TimT's proposal for making documentation review part of the release process this week OSG Investigations Team CMS WLCG cpu utilization debugging OSG Docker images are getting a bit big, working with user support team to audit their contents Doc Focus this week! New PS service coming to GRACC HTCondor xfer stats Ongoing GRACC Project StashCache Project (New URL!) Discussions Derek will follow up with Scott T to update HTCondor to a version that supports xfer stats","title":"January 29, 2018"},{"location":"meetings/2018/TechArea20180129/#osg-technology-area-meeting-29-january-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT","title":"OSG Technology Area Meeting, 29 January 2018"},{"location":"meetings/2018/TechArea20180129/#announcements","text":"Suchandra will only be available in the afternoons until Feb 5 TimT will be gone starting next Monday, returning Feb 19","title":"Announcements"},{"location":"meetings/2018/TechArea20180129/#triage-duty","text":"This week: Suchandra Next week: Carl 23 (+13) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180129/#jira","text":"# of tickets State 123 -11 Open 23 -10 In Progress 24 +6 Ready for Testing 5 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180129/#osg-software-team","text":"15 tickets will be opened with edg-mkgridmap/GUMS sites to transition to LCMAPS VOMS authentication There are already instructions for edg-mkgridmap to transition. The timeline we'd like to see is that edg-mkgridmap sites transition by the end of March. GUMS sites are trickier: If they attach their GUMS config, MAKE SURE THEIR DB USERNAME/PASS ARE REDACTED Find out what GUMS clients they may have: CE, gridftp, xrootd, dcache, bestman. The first three all support LCMAPS VOMS auth, dcache and bestman2 do not. They cannot turn off their GUMS until they transition off of dcache/bestman2. Let Carl know of any attached GUMS configs and the output of `rpm -q osg-gums-config`. GCT: Frank Scheiner registered Gitbook/ReadTheDocs orgs but hasn't worked on the documentation other than that. Let's get started on making the transition from Globus to GCF/GCT. I can help setup GitHub docs. OSG 3.4.7 HDFS tested at Nebraska, looks good over the weekend Can we sneak HTCondor into this release? GOC tickets should be closed in favor of JIRA tickets if the user agrees to it ( GOC vs JIRA ): https://ticket.grid.iu.edu/34406 https://ticket.grid.iu.edu/35448 https://ticket.grid.iu.edu/35735 Doc Focus 2 is scheduled for 1pm CST on Feb 1","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180129/#discussion","text":"Edgar would like notes on how to determine what auth clients are using for help with the LCMAPS VOMS transition","title":"Discussion"},{"location":"meetings/2018/TechArea20180129/#support-update","text":"display.opensciencegrid.org (Carl) - errors are showing because of empty data from GRACC; trying to replicate Nebraska/Purdue (Derek) - Issues with CPU usage being reported to the WLCG.","title":"Support Update"},{"location":"meetings/2018/TechArea20180129/#osg-release-team","text":"3.4.7 Status 0 -7 Open 0 -16 In Progress 25 +11 Ready for Testing 5 +1 Ready for Release 30 -11 Total 3.4.7 HDFS 2.6 and supporting packages xrootd-hdfs gridftp-hdfs HTCondor 8.6.9 HTCondor 8.7.6 Singularity 2.4.2: Major upgrade from 2.3.2 frontier-squid 3.5.27-2.1 Pegasus 4.8.1 osg-pki-tools: fix to osg-user-cert-renew PerfSonar tools meta-package owamp nuttcp bwctl Remove dependencies on osg-version osg-ca-generator 1.3.0 backups should be created if using force Generate LSC file Remove Slurm from osg-contrib","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180129/#discussions","text":"Expect TimT's proposal for making documentation review part of the release process this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180129/#osg-investigations-team","text":"CMS WLCG cpu utilization debugging OSG Docker images are getting a bit big, working with user support team to audit their contents Doc Focus this week! New PS service coming to GRACC HTCondor xfer stats","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180129/#ongoing","text":"GRACC Project StashCache Project (New URL!)","title":"Ongoing"},{"location":"meetings/2018/TechArea20180129/#discussions_1","text":"Derek will follow up with Scott T to update HTCondor to a version that supports xfer stats","title":"Discussions"},{"location":"meetings/2018/TechArea20180205/","text":"OSG Technology Area Meeting, 5 February 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, TimC Announcements OSG All Hands registration is open TimT out of office, returning Feb 19 Triage Duty This week: Carl Next week: BrianL (?) 28 (+5) open tickets JIRA # of tickets State 114 -9 Open 34 +11 In Progress 3 -21 Ready for Testing 0 -5 Ready for Release OSG Software Team Triage/LCMAPS VOMS tickets Make sure to update the \"Next Action Deadline\" after you or GOC staff have updated the ticket After a site transitions verify that pilots are still authenticating with factory ops Doc Focus Move tickets that haven't been started out of the sprint ( https://jira.opensciencegrid.org/secure/RapidBoard.jspa?rapidView=7 projectKey=SOFTWARE ) Update the doc tracking spreadsheet ( https://docs.google.com/spreadsheets/d/1b3_9WqjUVlszu_tM23ehaOPQVAKb5OSRvmtuLx0u8Go/edit?usp=sharing ) after you've reviewed a doc and changes have been merged in Doc Focus Frenzy III: 2018-03-01, 1PM Central Discussion None this week Support Update display.opensciencegrid.org (Carl) - able to reproduce the problem and has a fix FIU (BrianL) - completed LCMAPS VOMS transition Nebraska (Derek) - low CPU utilization fixed by downgrade to HTCondor 8.6.x OSC (Matyas) - Gave Trey an RSV patch to test SBGrid (Edgar) - assisting with a 3.2 - 3.4 upgrade. Looking to transition off of their VOMS Admin server OSG Release Team 3.4.8/3.3.32 Status 0 +0 Open 0 +0 In Progress 1 +1 Ready for Testing 0 +0 Ready for Release 1 +1 Total 3.4.8/3.3.32 GlideinWMS frontend bug fix Discussions None this week OSG Investigations Team CMS WLCG cpu utilization debugging Site issue with Nebraska. Fixed with update to HTCondor. Condor Ticket #6426 Not going to worry about current records. Consulted with WLCG accounting team. OSG Docker images are getting a bit big, working with user support team to audit their contents Multiple Cuda versions were installed in single image. Doc Focus this week! Success! New PS service coming to GRACC Currently running. Still need to redirect to GRACC's ES. HTCondor xfer stats Update with domain metrics. Ligo is going to run a StashCache origin server. Another round of data integrity on GRACC First focus was \"Unknown\" records in site records. Opened Ticket https://ticket.grid.iu.edu/35943 FZU updated stashcache cache. Ongoing GRACC Project StashCache Project Discussions BrianL will follow up with Scott T to update HTCondor to a version that supports xfer stats","title":"February 5, 2018"},{"location":"meetings/2018/TechArea20180205/#osg-technology-area-meeting-5-february-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, TimC","title":"OSG Technology Area Meeting,  5 February 2018"},{"location":"meetings/2018/TechArea20180205/#announcements","text":"OSG All Hands registration is open TimT out of office, returning Feb 19","title":"Announcements"},{"location":"meetings/2018/TechArea20180205/#triage-duty","text":"This week: Carl Next week: BrianL (?) 28 (+5) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180205/#jira","text":"# of tickets State 114 -9 Open 34 +11 In Progress 3 -21 Ready for Testing 0 -5 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180205/#osg-software-team","text":"Triage/LCMAPS VOMS tickets Make sure to update the \"Next Action Deadline\" after you or GOC staff have updated the ticket After a site transitions verify that pilots are still authenticating with factory ops Doc Focus Move tickets that haven't been started out of the sprint ( https://jira.opensciencegrid.org/secure/RapidBoard.jspa?rapidView=7 projectKey=SOFTWARE ) Update the doc tracking spreadsheet ( https://docs.google.com/spreadsheets/d/1b3_9WqjUVlszu_tM23ehaOPQVAKb5OSRvmtuLx0u8Go/edit?usp=sharing ) after you've reviewed a doc and changes have been merged in Doc Focus Frenzy III: 2018-03-01, 1PM Central","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180205/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180205/#support-update","text":"display.opensciencegrid.org (Carl) - able to reproduce the problem and has a fix FIU (BrianL) - completed LCMAPS VOMS transition Nebraska (Derek) - low CPU utilization fixed by downgrade to HTCondor 8.6.x OSC (Matyas) - Gave Trey an RSV patch to test SBGrid (Edgar) - assisting with a 3.2 - 3.4 upgrade. Looking to transition off of their VOMS Admin server","title":"Support Update"},{"location":"meetings/2018/TechArea20180205/#osg-release-team","text":"3.4.8/3.3.32 Status 0 +0 Open 0 +0 In Progress 1 +1 Ready for Testing 0 +0 Ready for Release 1 +1 Total 3.4.8/3.3.32 GlideinWMS frontend bug fix","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180205/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180205/#osg-investigations-team","text":"CMS WLCG cpu utilization debugging Site issue with Nebraska. Fixed with update to HTCondor. Condor Ticket #6426 Not going to worry about current records. Consulted with WLCG accounting team. OSG Docker images are getting a bit big, working with user support team to audit their contents Multiple Cuda versions were installed in single image. Doc Focus this week! Success! New PS service coming to GRACC Currently running. Still need to redirect to GRACC's ES. HTCondor xfer stats Update with domain metrics. Ligo is going to run a StashCache origin server. Another round of data integrity on GRACC First focus was \"Unknown\" records in site records. Opened Ticket https://ticket.grid.iu.edu/35943 FZU updated stashcache cache.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180205/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180205/#discussions_1","text":"BrianL will follow up with Scott T to update HTCondor to a version that supports xfer stats","title":"Discussions"},{"location":"meetings/2018/TechArea20180212/","text":"OSG Technology Area Meeting, 12 February 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra Announcements Register for OSG All Hands! TimT out of office, returning Feb 19 Triage Duty This week: Edgar Next week: Carl 28 (+0) open tickets JIRA # of tickets State 111 -3 Open 30 -4 In Progress 12 +9 Ready for Testing 0 +0 Ready for Release OSG Software Team LCMAPS VOMS tickets GUMS helper script and usage instructions dCache: can't use LCMAPS but grid-vomapfile has a similar format to the default voms-mapfile (FNAL using their own auth method) XRootD instructions currently a WIP (ATLAS sites use xrootd-voms-plugin a.k.a vomsxrd) Doc Focus Frenzy III: 2018-03-01, 1PM Central Discussion None this week Support Update ATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure display.opensciencegrid.org (Carl) - new build deployed on display-itb OU (Matyas) - LCMAPS VOMS + XRootD: Horst gave some suggestions for the LCMAPS VOMS docs OSG Release Team 3.4.9/3.3.33 Status 24 +24 Open 12 +12 In Progress 11 +11 Ready for Testing 0 +0 Ready for Release 47 +47 Total 3.4.9 Ready for Testing Frontier Squid 3.4.27-3.1 RSV 3.17.0-1 osg-test 2.1.0-1 Discussions None this week OSG Investigations Team New PS service coming to GRACC Currently running. Still need to redirect to GRACC's ES. HTCondor xfer stats Update with domain metrics. Another round of data integrity on GRACC First focus was \"Unknown\" records in site records. Opened Ticket https://ticket.grid.iu.edu/35943 Ongoing GRACC Project StashCache Project Discussions None this week","title":"February 12, 2018"},{"location":"meetings/2018/TechArea20180212/#osg-technology-area-meeting-12-february-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra","title":"OSG Technology Area Meeting, 12 February 2018"},{"location":"meetings/2018/TechArea20180212/#announcements","text":"Register for OSG All Hands! TimT out of office, returning Feb 19","title":"Announcements"},{"location":"meetings/2018/TechArea20180212/#triage-duty","text":"This week: Edgar Next week: Carl 28 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180212/#jira","text":"# of tickets State 111 -3 Open 30 -4 In Progress 12 +9 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180212/#osg-software-team","text":"LCMAPS VOMS tickets GUMS helper script and usage instructions dCache: can't use LCMAPS but grid-vomapfile has a similar format to the default voms-mapfile (FNAL using their own auth method) XRootD instructions currently a WIP (ATLAS sites use xrootd-voms-plugin a.k.a vomsxrd) Doc Focus Frenzy III: 2018-03-01, 1PM Central","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180212/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180212/#support-update","text":"ATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure display.opensciencegrid.org (Carl) - new build deployed on display-itb OU (Matyas) - LCMAPS VOMS + XRootD: Horst gave some suggestions for the LCMAPS VOMS docs","title":"Support Update"},{"location":"meetings/2018/TechArea20180212/#osg-release-team","text":"3.4.9/3.3.33 Status 24 +24 Open 12 +12 In Progress 11 +11 Ready for Testing 0 +0 Ready for Release 47 +47 Total 3.4.9 Ready for Testing Frontier Squid 3.4.27-3.1 RSV 3.17.0-1 osg-test 2.1.0-1","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180212/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180212/#osg-investigations-team","text":"New PS service coming to GRACC Currently running. Still need to redirect to GRACC's ES. HTCondor xfer stats Update with domain metrics. Another round of data integrity on GRACC First focus was \"Unknown\" records in site records. Opened Ticket https://ticket.grid.iu.edu/35943","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180212/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180212/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180219/","text":"OSG Technology Area Meeting, 19 February 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Mat, Suchandra, TimT Announcements Register for OSG All Hands ! BrianL at Fermilab Wednesday and UChicago Thursday Triage Duty This week: Carl Next week: BrianL 39 (+11) open tickets JIRA # of tickets State 106 -5 Open 34 +4 In Progress 16 +4 Ready for Testing 0 +0 Ready for Release OSG Software Team OSG 3.4.9 and 3.3.33: GlideinWMS and XRootD nearly ready; possible release as early as next week LCMAPS VOMS Anyone know anything about KIT (GermanGrid)? Support instructions Doc Focus Frenzy III: 2018-03-01, 1PM Central Discussion Derek will investigate KIT contact information Support Update ATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure Nebraska (Derek) - Carl is taking on a lot of the LCMAPS VOMS transition support. Derek will forward support instructions and Brian will assist if necessary Purdue (Derek) - Purdue is having issues with APEL reporting of CPU Duration. If I had to guess, I would blame PBS, but maybe log format changed and our very outdated PBS probe failed. MWT2 (Suchandra) - transitioning off of LCMAPS VOMS, will talk to Bob Ball about dCache configuration OSG Release Team 3.4.9/3.3.33 Status 21 -3 Open 18 +6 In Progress 12 +1 Ready for Testing 2 +2 Ready for Release 53 +6 Total 3.4.9 Ready for Testing RSV 3.17.0-1 osg-test 2.1.0-1 Ready for Release Frontier Squid 3.4.27-3.1 Discussions None this week OSG Investigations Team New PS service coming to GRACC Currently running. Still need to redirect to GRACC's ES. HTCondor xfer stats Another round of data integrity on GRACC First focus was \"Unknown\" records in site records. Opened Ticket https://ticket.grid.iu.edu/35943 This week Improved StashCache monitoring coming, using xrootd fstream monitoring! Ongoing GRACC Project StashCache Project Discussions None this week","title":"February 19, 2018"},{"location":"meetings/2018/TechArea20180219/#osg-technology-area-meeting-19-february-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 19 February 2018"},{"location":"meetings/2018/TechArea20180219/#announcements","text":"Register for OSG All Hands ! BrianL at Fermilab Wednesday and UChicago Thursday","title":"Announcements"},{"location":"meetings/2018/TechArea20180219/#triage-duty","text":"This week: Carl Next week: BrianL 39 (+11) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180219/#jira","text":"# of tickets State 106 -5 Open 34 +4 In Progress 16 +4 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180219/#osg-software-team","text":"OSG 3.4.9 and 3.3.33: GlideinWMS and XRootD nearly ready; possible release as early as next week LCMAPS VOMS Anyone know anything about KIT (GermanGrid)? Support instructions Doc Focus Frenzy III: 2018-03-01, 1PM Central","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180219/#discussion","text":"Derek will investigate KIT contact information","title":"Discussion"},{"location":"meetings/2018/TechArea20180219/#support-update","text":"ATLAS (BrianL) - HTCondor-CE transitions (BU, OU, UTA). BU/Harvard say that their Slurm pool drains after a Slurm controller failure Nebraska (Derek) - Carl is taking on a lot of the LCMAPS VOMS transition support. Derek will forward support instructions and Brian will assist if necessary Purdue (Derek) - Purdue is having issues with APEL reporting of CPU Duration. If I had to guess, I would blame PBS, but maybe log format changed and our very outdated PBS probe failed. MWT2 (Suchandra) - transitioning off of LCMAPS VOMS, will talk to Bob Ball about dCache configuration","title":"Support Update"},{"location":"meetings/2018/TechArea20180219/#osg-release-team","text":"3.4.9/3.3.33 Status 21 -3 Open 18 +6 In Progress 12 +1 Ready for Testing 2 +2 Ready for Release 53 +6 Total 3.4.9 Ready for Testing RSV 3.17.0-1 osg-test 2.1.0-1 Ready for Release Frontier Squid 3.4.27-3.1","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180219/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180219/#osg-investigations-team","text":"New PS service coming to GRACC Currently running. Still need to redirect to GRACC's ES. HTCondor xfer stats Another round of data integrity on GRACC First focus was \"Unknown\" records in site records. Opened Ticket https://ticket.grid.iu.edu/35943","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180219/#this-week","text":"Improved StashCache monitoring coming, using xrootd fstream monitoring!","title":"This week"},{"location":"meetings/2018/TechArea20180219/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180219/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180226/","text":"OSG Technology Area Meeting, 26 February 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT Announcements Register for OSG All Hands and book your hotel! Triage Duty This week: BrianL Next week: Mat 11 (+0) open tickets 27 (+0) open LCMAPS VOMS tickets JIRA # of tickets State 119 +13 Open 35 +1 In Progress 10 -6 Ready for Testing 6 +6 Ready for Release OSG Software Team VMU tests going held due to HTCondor bugs + new CHTC memory policy XRootD tmpfiles fix RFT in OSG 3.3 (developer tests still required for 3.4) Doc Focus Frenzy III: 2018-03-01, 1PM Central Discussion Dropping software support from the LCMAPS VOMS tickets. Separate out the LCMAPS VOMS tickets in the weekly meetings Support Update condor_ssh_to_job (Edgar) - Having back and forth with ToddT to get it working in the OSG UConn (BrianL) - jobs going over memory and ignoring PREEMPT_IF memory expression due to the bulk of the usage being in shared memory. CGROUP_MEMORY_LIMIT_POLICY = hard should enforce RSS + shared mem OSG Release Team 3.4.9/3.3.33 Status 20 -1 Open 17 -1 In Progress 9 -3 Ready for Testing 6 +4 Ready for Release 52 -1 Total 3.4.9 Ready for Testing RSV 3.17.0-1 Ready for Release Frontier Squid 3.4.27-3.1 osg-test 2.1.0-1 Discussions None this week OSG Investigations Team Last Week Started better monitoring for StashCache, per directory monitoring. Still much to do to make it production. https://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh (it broke over the weekend. see not production) (ongoing) Fix GRACC bug with projects. https://ticket.opensciencegrid.org/35943 (ongoing) Help push belle2 usage to EGI / APEL. https://ticket.opensciencegrid.org/35943 (Suchandra) Poked Utah again, still do not want OSG due to meltdown bug (not fully fixed?) Ongoing GRACC Project StashCache Project Discussions None this week","title":"February 26, 2018"},{"location":"meetings/2018/TechArea20180226/#osg-technology-area-meeting-26-february-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 26 February 2018"},{"location":"meetings/2018/TechArea20180226/#announcements","text":"Register for OSG All Hands and book your hotel!","title":"Announcements"},{"location":"meetings/2018/TechArea20180226/#triage-duty","text":"This week: BrianL Next week: Mat 11 (+0) open tickets 27 (+0) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180226/#jira","text":"# of tickets State 119 +13 Open 35 +1 In Progress 10 -6 Ready for Testing 6 +6 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180226/#osg-software-team","text":"VMU tests going held due to HTCondor bugs + new CHTC memory policy XRootD tmpfiles fix RFT in OSG 3.3 (developer tests still required for 3.4) Doc Focus Frenzy III: 2018-03-01, 1PM Central","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180226/#discussion","text":"Dropping software support from the LCMAPS VOMS tickets. Separate out the LCMAPS VOMS tickets in the weekly meetings","title":"Discussion"},{"location":"meetings/2018/TechArea20180226/#support-update","text":"condor_ssh_to_job (Edgar) - Having back and forth with ToddT to get it working in the OSG UConn (BrianL) - jobs going over memory and ignoring PREEMPT_IF memory expression due to the bulk of the usage being in shared memory. CGROUP_MEMORY_LIMIT_POLICY = hard should enforce RSS + shared mem","title":"Support Update"},{"location":"meetings/2018/TechArea20180226/#osg-release-team","text":"3.4.9/3.3.33 Status 20 -1 Open 17 -1 In Progress 9 -3 Ready for Testing 6 +4 Ready for Release 52 -1 Total 3.4.9 Ready for Testing RSV 3.17.0-1 Ready for Release Frontier Squid 3.4.27-3.1 osg-test 2.1.0-1","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180226/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180226/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180226/#last-week","text":"Started better monitoring for StashCache, per directory monitoring. Still much to do to make it production. https://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh (it broke over the weekend. see not production) (ongoing) Fix GRACC bug with projects. https://ticket.opensciencegrid.org/35943 (ongoing) Help push belle2 usage to EGI / APEL. https://ticket.opensciencegrid.org/35943 (Suchandra) Poked Utah again, still do not want OSG due to meltdown bug (not fully fixed?)","title":"Last Week"},{"location":"meetings/2018/TechArea20180226/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180226/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180305/","text":"OSG Technology Area Meeting, 5 March 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimT Announcements Register for OSG All Hands ! Triage Duty This week: Mat Next week: Suchandra 9 (-2) open tickets 27 (+0) open LCMAPS VOMS tickets JIRA # of tickets State 117 -2 Open 34 -1 In Progress 12 +2 Ready for Testing 6 +0 Ready for Release OSG Software Team Enthusiastic LCMAPS VOMS transition support: if users have questions, let's aim for 24 hr response times OSG 3.4.9/3.3.33: osg-se-hadoop progress in upcoming? Doc focus wrap-up: Replace GUMS references in install-xrootd with lcmaps-voms auth (BrianL to address review comments) Review the hadoop overview document (BrianL to review) Review content of HTCondor-CE installation doc (Carl to review) Update current documentation to reflect current release practice (TimT to address review comments) Discussion For the LCMAPS VOMS transitions, also aim to ping users at least twice a week Suchandra to finish osg-se-hadoop changes today Utah Hosted-CEs up and running but some troubleshooting items remain Support Update FNAL (BrianL): condor_ce_reconfig causes CollectorLog to be output to ToolLog (fixed in 8.6.10) MWT2 (Suchandra): Finishing the LCMAPS VOMS transition (including dCache) OSC (Mat): waiting for a response from Ops concerning the potential remaining Gratia collector USCD (Edgar): Issues with XRootD 4.8.0 and the developers assure us it is fixed in 4.8.1 UTA (BrianL): In some cases, routed jobs can't be removed without condor_ce_rm -forcex ( https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=6586,0 , fixed in 8.6.10) OSG Release Team 3.4.9/3.3.33 Status 1 -19 Open 3 -14 In Progress 11 +2 Ready for Testing 6 +0 Ready for Release 21 -31 Total 3.4.9 Ready for Testing XRootD 4.8.1 XRootD 4.8.0-2 in 3.3 GlideinWMS 3.2.21-2 auto proxy renewal RSV 3.17.0-1 osg-release 3.4-3 Ready for Release Frontier Squid 3.4.27-3.1 osg-test 2.1.0-1 Discussions Assigning XRootD 4.8.1 to Terrence Martin at UCSD since it may fix an issue they're having. TimT will find testers for the XRootD tmpfiles fix and the RSV GRACC collector query. OSG Investigations Team Last Week Started better monitoring for StashCache, per directory monitoring. Still much to do to make it production. https://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh (it broke over the weekend. see not production) Need to debug the GEO-Location of StashCache CVMFS. Some numbers don't add up. Pefsonar data import is ongoing. CVMFS Stratum 1 backup at Nebraska is growing very fast, need to allocate more storage... working on it. (ongoing) Fix GRACC bug with projects. https://ticket.opensciencegrid.org/35943 (ongoing) Help push belle2 usage to EGI / APEL. https://ticket.opensciencegrid.org/35943 Ongoing GRACC Project StashCache Project Discussions None this week","title":"March 5, 2018"},{"location":"meetings/2018/TechArea20180305/#osg-technology-area-meeting-5-march-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting,  5 March 2018"},{"location":"meetings/2018/TechArea20180305/#announcements","text":"Register for OSG All Hands !","title":"Announcements"},{"location":"meetings/2018/TechArea20180305/#triage-duty","text":"This week: Mat Next week: Suchandra 9 (-2) open tickets 27 (+0) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180305/#jira","text":"# of tickets State 117 -2 Open 34 -1 In Progress 12 +2 Ready for Testing 6 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180305/#osg-software-team","text":"Enthusiastic LCMAPS VOMS transition support: if users have questions, let's aim for 24 hr response times OSG 3.4.9/3.3.33: osg-se-hadoop progress in upcoming? Doc focus wrap-up: Replace GUMS references in install-xrootd with lcmaps-voms auth (BrianL to address review comments) Review the hadoop overview document (BrianL to review) Review content of HTCondor-CE installation doc (Carl to review) Update current documentation to reflect current release practice (TimT to address review comments)","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180305/#discussion","text":"For the LCMAPS VOMS transitions, also aim to ping users at least twice a week Suchandra to finish osg-se-hadoop changes today Utah Hosted-CEs up and running but some troubleshooting items remain","title":"Discussion"},{"location":"meetings/2018/TechArea20180305/#support-update","text":"FNAL (BrianL): condor_ce_reconfig causes CollectorLog to be output to ToolLog (fixed in 8.6.10) MWT2 (Suchandra): Finishing the LCMAPS VOMS transition (including dCache) OSC (Mat): waiting for a response from Ops concerning the potential remaining Gratia collector USCD (Edgar): Issues with XRootD 4.8.0 and the developers assure us it is fixed in 4.8.1 UTA (BrianL): In some cases, routed jobs can't be removed without condor_ce_rm -forcex ( https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=6586,0 , fixed in 8.6.10)","title":"Support Update"},{"location":"meetings/2018/TechArea20180305/#osg-release-team","text":"3.4.9/3.3.33 Status 1 -19 Open 3 -14 In Progress 11 +2 Ready for Testing 6 +0 Ready for Release 21 -31 Total 3.4.9 Ready for Testing XRootD 4.8.1 XRootD 4.8.0-2 in 3.3 GlideinWMS 3.2.21-2 auto proxy renewal RSV 3.17.0-1 osg-release 3.4-3 Ready for Release Frontier Squid 3.4.27-3.1 osg-test 2.1.0-1","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180305/#discussions","text":"Assigning XRootD 4.8.1 to Terrence Martin at UCSD since it may fix an issue they're having. TimT will find testers for the XRootD tmpfiles fix and the RSV GRACC collector query.","title":"Discussions"},{"location":"meetings/2018/TechArea20180305/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180305/#last-week","text":"Started better monitoring for StashCache, per directory monitoring. Still much to do to make it production. https://gracc.opensciencegrid.org/kibana/app/kibana#/dashboard/AWG5ztK58IKqxDdAglqh (it broke over the weekend. see not production) Need to debug the GEO-Location of StashCache CVMFS. Some numbers don't add up. Pefsonar data import is ongoing. CVMFS Stratum 1 backup at Nebraska is growing very fast, need to allocate more storage... working on it. (ongoing) Fix GRACC bug with projects. https://ticket.opensciencegrid.org/35943 (ongoing) Help push belle2 usage to EGI / APEL. https://ticket.opensciencegrid.org/35943","title":"Last Week"},{"location":"meetings/2018/TechArea20180305/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180305/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180312/","text":"OSG Technology Area Meeting, 12 March 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Mat, Suchandra, TimC, TimT Announcements OSG All Hands next week! Next two meetings canceled, resuming on 2018-04-02 Marian on vacation this week Triage Duty This week: Suchandra Next week: TimT 11 (+2) open tickets 36 (+9) open LCMAPS VOMS tickets JIRA # of tickets State 114 -3 Open 41 +7 In Progress 3 -9 Ready for Testing 0 -5 Ready for Release OSG Software Team OSG All Hands Software training session on Thursday morning Split up between CMS/ATLAS/FIFE on Monday Enthusiastic LCMAPS VOMS transition support: let's aim for 24 hr response times and pinging users at least twice a week Doc focus wrap-up: Review the hadoop overview document (BrianL to review) Update current documentation to reflect current release practice (TimT to address review comments) Discussion None this week Support Update None this week OSG Release Team 3.4.10 Status 27 +27 Open 21 +21 In Progress 1 +1 Ready for Testing 0 +0 Ready for Release 49 +49 Total 3.4.10 Ready for Testing xrootd-hdfs 2.0.1 Ready for Release Nothing yet Discussions Review of the flexible release model will happen some time after the All Hands Meeting OSG Investigations Team No updates: investigations team unavailable Ongoing GRACC Project StashCache Project Discussions None this week","title":"March 12, 2018"},{"location":"meetings/2018/TechArea20180312/#osg-technology-area-meeting-12-march-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 12 March 2018"},{"location":"meetings/2018/TechArea20180312/#announcements","text":"OSG All Hands next week! Next two meetings canceled, resuming on 2018-04-02 Marian on vacation this week","title":"Announcements"},{"location":"meetings/2018/TechArea20180312/#triage-duty","text":"This week: Suchandra Next week: TimT 11 (+2) open tickets 36 (+9) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180312/#jira","text":"# of tickets State 114 -3 Open 41 +7 In Progress 3 -9 Ready for Testing 0 -5 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180312/#osg-software-team","text":"OSG All Hands Software training session on Thursday morning Split up between CMS/ATLAS/FIFE on Monday Enthusiastic LCMAPS VOMS transition support: let's aim for 24 hr response times and pinging users at least twice a week Doc focus wrap-up: Review the hadoop overview document (BrianL to review) Update current documentation to reflect current release practice (TimT to address review comments)","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180312/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180312/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2018/TechArea20180312/#osg-release-team","text":"3.4.10 Status 27 +27 Open 21 +21 In Progress 1 +1 Ready for Testing 0 +0 Ready for Release 49 +49 Total 3.4.10 Ready for Testing xrootd-hdfs 2.0.1 Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180312/#discussions","text":"Review of the flexible release model will happen some time after the All Hands Meeting","title":"Discussions"},{"location":"meetings/2018/TechArea20180312/#osg-investigations-team","text":"No updates: investigations team unavailable","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180312/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180312/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180402/","text":"OSG Technology Area Meeting, 2 April 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT Announcements Triage Duty This week: Carl Next week: BrianL 11 (+0) open tickets 27 (-9) open LCMAPS VOMS tickets JIRA # of tickets State 107 -1 Open 44 +3 In Progress 15 +0 Ready for Testing 1 +0 Ready for Release OSG Software Team New vo-client package incoming XRootD and GlideinWMS still don't have release builds Doc focus: Next doc focus this month Review the hadoop overview document (Suchandra to address review comments) Update current documentation to reflect current release practice (TimT to address review comments) LCMAPS VOMS transition: ~2/3 of sites have completed or effectively completed the transition Discussion Data release and possibly a software release yet this week Support Update Wicsonsin (Marian) - HTTPS transfer to XRootD causes it to crash MWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete Vanderbilt (BrianL) - Routed gridmanager jobs still remain. New versions of HTCondor (available in testing) may fix this OSG Release Team Data Release this week IGTF 1.90 VO Package v78 Updated ATLAS VO default mappings vo-client-lcmaps-voms requiers vo-client 3.3.34 Both 3.4.10 Total Status 0 +0 0 +0 14 -13 14 -13 Open 0 +0 2 +2 22 +1 24 +3 In Progress 0 +0 1 +1 13 +12 14 +13 Ready for Testing 0 +0 0 +0 1 +1 1 +1 Ready for Release 0 +0 3 +3 50 +1 53 +4 Total Ready for Testing HTCondor 8.6.10 Upcoming: HTCondor 8.7.7 BLAHP Save debugging dir when blahp job submission fails Verify input existance xrootd-hdfs 2.0.1 Package maintenance fold osg-gridftp-hdfs into osg-gridftp osg-se-hadoop osg-build - internal tool maintenance Release cigetcert in OSG (from fermi) Ready for Release Nothing yet Discussions Review of the flexible release model will happen some time after the All Hands Meeting OSG Investigations Team lot of code updates (xrootd) to StashCache for better monitoring, testing xrootd RC3 candidate as well as newer build in HCC repo with latest code changes work with perfSonar team sending data from ps-collectors also to GRACC Elasticsearch Ongoing GRACC Project StashCache Project Discussions None this week","title":"April 2, 2018"},{"location":"meetings/2018/TechArea20180402/#osg-technology-area-meeting-2-april-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting,  2 April 2018"},{"location":"meetings/2018/TechArea20180402/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180402/#triage-duty","text":"This week: Carl Next week: BrianL 11 (+0) open tickets 27 (-9) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180402/#jira","text":"# of tickets State 107 -1 Open 44 +3 In Progress 15 +0 Ready for Testing 1 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180402/#osg-software-team","text":"New vo-client package incoming XRootD and GlideinWMS still don't have release builds Doc focus: Next doc focus this month Review the hadoop overview document (Suchandra to address review comments) Update current documentation to reflect current release practice (TimT to address review comments) LCMAPS VOMS transition: ~2/3 of sites have completed or effectively completed the transition","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180402/#discussion","text":"Data release and possibly a software release yet this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180402/#support-update","text":"Wicsonsin (Marian) - HTTPS transfer to XRootD causes it to crash MWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete Vanderbilt (BrianL) - Routed gridmanager jobs still remain. New versions of HTCondor (available in testing) may fix this","title":"Support Update"},{"location":"meetings/2018/TechArea20180402/#osg-release-team","text":"Data Release this week IGTF 1.90 VO Package v78 Updated ATLAS VO default mappings vo-client-lcmaps-voms requiers vo-client 3.3.34 Both 3.4.10 Total Status 0 +0 0 +0 14 -13 14 -13 Open 0 +0 2 +2 22 +1 24 +3 In Progress 0 +0 1 +1 13 +12 14 +13 Ready for Testing 0 +0 0 +0 1 +1 1 +1 Ready for Release 0 +0 3 +3 50 +1 53 +4 Total Ready for Testing HTCondor 8.6.10 Upcoming: HTCondor 8.7.7 BLAHP Save debugging dir when blahp job submission fails Verify input existance xrootd-hdfs 2.0.1 Package maintenance fold osg-gridftp-hdfs into osg-gridftp osg-se-hadoop osg-build - internal tool maintenance Release cigetcert in OSG (from fermi) Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180402/#discussions","text":"Review of the flexible release model will happen some time after the All Hands Meeting","title":"Discussions"},{"location":"meetings/2018/TechArea20180402/#osg-investigations-team","text":"lot of code updates (xrootd) to StashCache for better monitoring, testing xrootd RC3 candidate as well as newer build in HCC repo with latest code changes work with perfSonar team sending data from ps-collectors also to GRACC Elasticsearch","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180402/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180402/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180409/","text":"OSG Technology Area Meeting, 9 April 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Marian, Mat, Suchandra, TimC, TimT Announcements Triage Duty This week: BrianL Next week: Mat 10 (+0) open tickets 21 (-6) open LCMAPS VOMS tickets JIRA # of tickets State 107 +0 Open 45 +1 In Progress 9 -6 Ready for Testing 10 +9 Ready for Release OSG Software Team Services being retired or transitioned at IU VOMS install failures on EL6? http://vdt.cs.wisc.edu/tests/20180408-0423/results.html Singularity 2.4.6 built, needs testing No word on GlideinWMS/XRootD updates Next doc focus 2018-04-19 1-5pm Central Discussion None this week Support Update Wicsonsin (Marian) - they're happy with the testing versions of xrootd-hdfs and xrootd-lcmaps MWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete OSG Release Team 3.3.34 Both 3.4.10 Total Status 0 +0 0 +0 13 -1 13 -1 Open 0 +0 1 -1 24 +2 25 +1 In Progress 0 +0 1 +0 7 -6 8 -6 Ready for Testing 0 +0 1 +1 9 +8 10 +9 Ready for Release 0 +0 3 +0 53 +3 56 +3 Total Ready for Testing BLAHP Save debugging dir when blahp job submission fails Verify input existance xrootd-hdfs 2.0.1 Update Internet2 packages Package maintenance fold osg-gridftp-hdfs into osg-gridftp osg-se-hadoop Add cigetcert to AFS tarball Ready for Release HTCondor 8.6.10 Upcoming: HTCondor 8.7.7 Release cigetcert in OSG (from fermi) osg-build - internal tool maintenance Discussions None this week OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: 1. Have not started 1. In development - Could be provisioning hardware, adapting software, installing software... 1. Ready for testing - In a state where it can be tested by the consumers 1. Deployed Service Status OIM In Development OASIS In Development CVMFS In Development Software Repo Ready for testing OSG Display Ready for testing StashCache Redirector Ready for testing OSG Collector Ready for testing Ongoing GRACC Project StashCache Project Discussions None this week","title":"April 9, 2018"},{"location":"meetings/2018/TechArea20180409/#osg-technology-area-meeting-9-april-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Marian, Mat, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting,  9 April 2018"},{"location":"meetings/2018/TechArea20180409/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180409/#triage-duty","text":"This week: BrianL Next week: Mat 10 (+0) open tickets 21 (-6) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180409/#jira","text":"# of tickets State 107 +0 Open 45 +1 In Progress 9 -6 Ready for Testing 10 +9 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180409/#osg-software-team","text":"Services being retired or transitioned at IU VOMS install failures on EL6? http://vdt.cs.wisc.edu/tests/20180408-0423/results.html Singularity 2.4.6 built, needs testing No word on GlideinWMS/XRootD updates Next doc focus 2018-04-19 1-5pm Central","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180409/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180409/#support-update","text":"Wicsonsin (Marian) - they're happy with the testing versions of xrootd-hdfs and xrootd-lcmaps MWT2 (Suchandra) - still need to transition dCache to turn off GUMS, transition effectively complete","title":"Support Update"},{"location":"meetings/2018/TechArea20180409/#osg-release-team","text":"3.3.34 Both 3.4.10 Total Status 0 +0 0 +0 13 -1 13 -1 Open 0 +0 1 -1 24 +2 25 +1 In Progress 0 +0 1 +0 7 -6 8 -6 Ready for Testing 0 +0 1 +1 9 +8 10 +9 Ready for Release 0 +0 3 +0 53 +3 56 +3 Total Ready for Testing BLAHP Save debugging dir when blahp job submission fails Verify input existance xrootd-hdfs 2.0.1 Update Internet2 packages Package maintenance fold osg-gridftp-hdfs into osg-gridftp osg-se-hadoop Add cigetcert to AFS tarball Ready for Release HTCondor 8.6.10 Upcoming: HTCondor 8.7.7 Release cigetcert in OSG (from fermi) osg-build - internal tool maintenance","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180409/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180409/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: 1. Have not started 1. In development - Could be provisioning hardware, adapting software, installing software... 1. Ready for testing - In a state where it can be tested by the consumers 1. Deployed Service Status OIM In Development OASIS In Development CVMFS In Development Software Repo Ready for testing OSG Display Ready for testing StashCache Redirector Ready for testing OSG Collector Ready for testing","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180409/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180409/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180416/","text":"OSG Technology Area Meeting, 16 April 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements Triage Duty This week: Mat Next week: Suchandra 8 (-2) open tickets 17 (-4) open LCMAPS VOMS tickets JIRA # of tickets State 113 +6 Open 44 -1 In Progress 3 -6 Ready for Testing 27 +17 Ready for Release OSG Software Team IU transition Transition tasks are the highest priority unless stated otherwise We missed quite a few *.grid.iu.edu addresses Site technical transition document Site office hours (Central Time): Monday 4-5pm, Tuesday 1-3pm, Thursday 10-11am Status update on GCT release New Singularity breaks glideinWMS but is fixed in the newest glideinWMS version so they must be released together Next doc focus 2018-04-19 1-5pm Central Discussion Operations support staff will become unavailable starting this Thursday (Action item: Suchandra) Freshdesk accounts are needed for all members of the Technology Area (Action item: BrianL) Send instructions for GGUS account registration (Action item: BrianL) Transition active Footprints tickets Support Update Purdue (Carl): needs to speak to Derek about their APEL reporting issues Caltech (Marian): Helped LIGO set up their new StashCahce origin LCMAPS VOMS transition (Suchandra): MWT2 and UCR have effectively completed their transitions OSG Release Team 3.3.34 Both 3.4.10 Total Status 0 +0 0 +0 1 -12 1 -12 Open 1 +1 1 +0 9 -15 11 -14 In Progress 0 +0 0 -1 2 -5 2 -6 Ready for Testing 0 +0 3 +2 24 +15 27 +17 Ready for Release 1 +1 4 +1 36 -17 41 -15 Total Ready for Testing Singularity 2.4.6 (waiting on 2.4.7) Adding cigetcert to tarball (waiting on tarball build) Ready for Release HTCondor 8.6.10 xrootd-lcmaps fix crashes on EL6 with HTTPS requests xrootd-hdfs improved write support Upcoming: HTCondor 8.7.7 Release cigetcert in OSG (from fermi) BLAHP Save debugging dir when blahp job submission fails Verify input existance xrootd-hdfs 2.0.1 Fix SELinux configuration for frontier-squid Update Internet2 packages HTCondor CE - Accept InCommon certificates in default mapfile osg-configure - varies small fixes Package maintenance fold osg-gridftp-hdfs into osg-gridftp osg-se-hadoop osg-build - internal tool maintenance Discussions (Action item: TimT) Assign gWMS testing to Edgar. Edgar will try to test it against new/old versions of Singularity (Action item: Derek) S R team members need access to the new repo host. Each S R member will need to send their SSH public keys to Derek for access. (Action item: Carl) Some work still needs to be done in osg-test to fix the EL7 3.3 failures OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Ready for testing CVMFS Ready for testing Software Repo Ready for testing OSG Display Ready for testing StashCache Redirector Ready for testing OSG Collector Ready for testing Changes: - OASIS: Development - Testing. Installation was completed and starting to move things over. - CVMFS: Development - Testing. Keys where exchanged, so now we can sign repos. Ongoing GRACC Project StashCache Project Discussions None this week","title":"April 16, 2018"},{"location":"meetings/2018/TechArea20180416/#osg-technology-area-meeting-16-april-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 16 April 2018"},{"location":"meetings/2018/TechArea20180416/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180416/#triage-duty","text":"This week: Mat Next week: Suchandra 8 (-2) open tickets 17 (-4) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180416/#jira","text":"# of tickets State 113 +6 Open 44 -1 In Progress 3 -6 Ready for Testing 27 +17 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180416/#osg-software-team","text":"IU transition Transition tasks are the highest priority unless stated otherwise We missed quite a few *.grid.iu.edu addresses Site technical transition document Site office hours (Central Time): Monday 4-5pm, Tuesday 1-3pm, Thursday 10-11am Status update on GCT release New Singularity breaks glideinWMS but is fixed in the newest glideinWMS version so they must be released together Next doc focus 2018-04-19 1-5pm Central","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180416/#discussion","text":"Operations support staff will become unavailable starting this Thursday (Action item: Suchandra) Freshdesk accounts are needed for all members of the Technology Area (Action item: BrianL) Send instructions for GGUS account registration (Action item: BrianL) Transition active Footprints tickets","title":"Discussion"},{"location":"meetings/2018/TechArea20180416/#support-update","text":"Purdue (Carl): needs to speak to Derek about their APEL reporting issues Caltech (Marian): Helped LIGO set up their new StashCahce origin LCMAPS VOMS transition (Suchandra): MWT2 and UCR have effectively completed their transitions","title":"Support Update"},{"location":"meetings/2018/TechArea20180416/#osg-release-team","text":"3.3.34 Both 3.4.10 Total Status 0 +0 0 +0 1 -12 1 -12 Open 1 +1 1 +0 9 -15 11 -14 In Progress 0 +0 0 -1 2 -5 2 -6 Ready for Testing 0 +0 3 +2 24 +15 27 +17 Ready for Release 1 +1 4 +1 36 -17 41 -15 Total Ready for Testing Singularity 2.4.6 (waiting on 2.4.7) Adding cigetcert to tarball (waiting on tarball build) Ready for Release HTCondor 8.6.10 xrootd-lcmaps fix crashes on EL6 with HTTPS requests xrootd-hdfs improved write support Upcoming: HTCondor 8.7.7 Release cigetcert in OSG (from fermi) BLAHP Save debugging dir when blahp job submission fails Verify input existance xrootd-hdfs 2.0.1 Fix SELinux configuration for frontier-squid Update Internet2 packages HTCondor CE - Accept InCommon certificates in default mapfile osg-configure - varies small fixes Package maintenance fold osg-gridftp-hdfs into osg-gridftp osg-se-hadoop osg-build - internal tool maintenance","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180416/#discussions","text":"(Action item: TimT) Assign gWMS testing to Edgar. Edgar will try to test it against new/old versions of Singularity (Action item: Derek) S R team members need access to the new repo host. Each S R member will need to send their SSH public keys to Derek for access. (Action item: Carl) Some work still needs to be done in osg-test to fix the EL7 3.3 failures","title":"Discussions"},{"location":"meetings/2018/TechArea20180416/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Ready for testing CVMFS Ready for testing Software Repo Ready for testing OSG Display Ready for testing StashCache Redirector Ready for testing OSG Collector Ready for testing Changes: - OASIS: Development - Testing. Installation was completed and starting to move things over. - CVMFS: Development - Testing. Keys where exchanged, so now we can sign repos.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180416/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180416/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180423/","text":"OSG Technology Area Meeting, 23 April 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements Triage Duty This week: Suchandra Next week: TimT 8 (+0) open tickets 17 (+0) open LCMAPS VOMS tickets JIRA # of tickets State 116 -3 Open 44 -1 In Progress 9 +6 Ready for Testing 3 -24 Ready for Release OSG Software Team Register for a GGUS account IU transition Site technical transition document Site office hours (Central Time): https://unl.zoom.us/j/277958559 or Telephone: US: +1 408 638 0968 or +1 646 876 9923 or +1 669 900 6833 Mondays, 4-5pm Tuesdays, 1-3pm Thursdays, 10-11am OSG 3.4.11 GlideinWMS 3.2.22.2 is ready for testing XRootD 4.8.3 should be ready for testing Discussion None this week Support Update Caltech (Marian) - set up and registered the LIGO StashCahe origin server LIGO (Edgar) - some sites cannot access their frame files FNAL (Derek) - Joel Snow wanted to use rsync against repo for a local mirror but it wasn't working. They're happily using the UNL repo now. OSG Release Team 3.3.34 Both 3.4.11 Total Status 0 +0 0 +0 18 +18 18 +18 Open 0 -1 1 +0 9 +9 10 +8 In Progress 0 +0 0 +0 8 +8 8 +8 Ready for Testing 0 +0 3 +0 0 +0 3 +0 Ready for Release 0 -1 4 +0 35 +35 39 +34 Total Ready for Testing OSG 3.4.11 GlideinWMS 3.2.22 Remove broken dependencies proxy renewal script properly process with the default FQAN Gratia probes Fixed Slurm bugs found at UFL HTCondor probe: projectname case insensitive comparison PBS probe: Handle raw seconds as well as hh:mm:ss for durations Drop GRAM and glexec probe Ready for Release OSG 3.3.34 xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS frontier-squid: fixed startup problem under SELinux xrootd-hdfs 2.0.2: improved write support Discussions Expect a fix for the OSG 3.3 VMU tests this week OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Ready for testing CVMFS Ready for testing Software Repo Deployed OSG Display Ready for testing StashCache Redirector Deployed OSG Collector Ready for testing Changes: StashCache Redirector: Testing - Deployed. Origins are reporting to the new redirector. Software Repo (waiting on DNS): Testing - Deployed. External site is already using it as an rsync mirror (GOC one broke) OIM Projects and VOs are working for GRACC. Topology is untested. Ongoing GRACC Project StashCache Project Discussions None this week","title":"April 23, 2018"},{"location":"meetings/2018/TechArea20180423/#osg-technology-area-meeting-23-april-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 23 April 2018"},{"location":"meetings/2018/TechArea20180423/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180423/#triage-duty","text":"This week: Suchandra Next week: TimT 8 (+0) open tickets 17 (+0) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180423/#jira","text":"# of tickets State 116 -3 Open 44 -1 In Progress 9 +6 Ready for Testing 3 -24 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180423/#osg-software-team","text":"Register for a GGUS account IU transition Site technical transition document Site office hours (Central Time): https://unl.zoom.us/j/277958559 or Telephone: US: +1 408 638 0968 or +1 646 876 9923 or +1 669 900 6833 Mondays, 4-5pm Tuesdays, 1-3pm Thursdays, 10-11am OSG 3.4.11 GlideinWMS 3.2.22.2 is ready for testing XRootD 4.8.3 should be ready for testing","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180423/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180423/#support-update","text":"Caltech (Marian) - set up and registered the LIGO StashCahe origin server LIGO (Edgar) - some sites cannot access their frame files FNAL (Derek) - Joel Snow wanted to use rsync against repo for a local mirror but it wasn't working. They're happily using the UNL repo now.","title":"Support Update"},{"location":"meetings/2018/TechArea20180423/#osg-release-team","text":"3.3.34 Both 3.4.11 Total Status 0 +0 0 +0 18 +18 18 +18 Open 0 -1 1 +0 9 +9 10 +8 In Progress 0 +0 0 +0 8 +8 8 +8 Ready for Testing 0 +0 3 +0 0 +0 3 +0 Ready for Release 0 -1 4 +0 35 +35 39 +34 Total Ready for Testing OSG 3.4.11 GlideinWMS 3.2.22 Remove broken dependencies proxy renewal script properly process with the default FQAN Gratia probes Fixed Slurm bugs found at UFL HTCondor probe: projectname case insensitive comparison PBS probe: Handle raw seconds as well as hh:mm:ss for durations Drop GRAM and glexec probe Ready for Release OSG 3.3.34 xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS frontier-squid: fixed startup problem under SELinux xrootd-hdfs 2.0.2: improved write support","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180423/#discussions","text":"Expect a fix for the OSG 3.3 VMU tests this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180423/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Ready for testing CVMFS Ready for testing Software Repo Deployed OSG Display Ready for testing StashCache Redirector Deployed OSG Collector Ready for testing Changes: StashCache Redirector: Testing - Deployed. Origins are reporting to the new redirector. Software Repo (waiting on DNS): Testing - Deployed. External site is already using it as an rsync mirror (GOC one broke) OIM Projects and VOs are working for GRACC. Topology is untested.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180423/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180423/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180430/","text":"OSG Technology Area Meeting, 30 April 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, Suchandra, TimT Announcements Triage Duty This week: TimT Next week: TimT 14 (+6) open tickets 12 (-5) open LCMAPS VOMS tickets JIRA # of tickets State 115 -1 Open 44 +0 In Progress 14 +5 Ready for Testing 4 +1 Ready for Release OSG Software Team Triage duty Everyone should be subscribed to software-support@opensciencegrid.org plus have GGUS and Freshdesk accounts As we transition support, we need to also watch the new mailing list and Freshdesk using the \"My Group Tickets, all but unresolved and closed\" filter Note: Freshdesk tickets aren't currently public. CC's only see comments after they've been added. Office hours: same times this week but plan to cut down on the frequency next week We are deactivating non-OSG staff JIRA accounts in the new cloud-hosted JIRA instance Discussion Action item (BrianL): Add a banner to ticket to inform users about Freshdesk Action item (Derek): Investigate how mailing lists/other ticket systems work with Freshdesk Action item (Suchandra): Checking to see if you can reverse ticket comment order in Freshdesk Action item (BrianL): Organize another Freshdesk tutorial for how S R should use Freshdesk Support Update Caltech (Marian/Derek) - CVMFS is now indexing the new origin. OSG Release Team 3.3.34 Both 3.4.12 Total Status 0 +0 0 +0 4 -14 4 -14 Open 0 +0 0 -1 2 -7 2 -8 In Progress 0 +0 2 +2 21 +13 23 +15 Ready for Testing 3 +3 0 -3 1 +1 4 +1 Ready for Release 3 +3 2 -2 28 -7 33 -6 Total OSG 3.4.11 - Release Tomorrow High Priority Release: Singularity 2.5.0 OSG 3.4.12 - Testing GlideinWMS 3.2.22 proxy renewal script properly process with the default FQAN Gratia probes Fixed Slurm probe bugs found at UFL HTCondor probe: projectname case insensitive comparison PBS probe: Handle raw seconds as well as hh:mm:ss for durations Drop GRAM and glexec probe XRootD 4.8.2 OSG 3.3.34 - Mostly Ready for Release Eradicate grid.iu.edu references - in testing xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS frontier-squid: fixed startup problem under SELinux xrootd-hdfs 2.0.2: improved write support Discussions Action Item (BrianL): follow up with Zalak on adding Let's Encrypt to the CA bundle OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Ready for testing CVMFS Ready for testing Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Ready for testing Changes: OSG Display: Testing - Deployed. Switched DNS on 4/26. Software Repo: Will switch DNS on 5/1 (Tuesday). Announcement went out 4/25 OASIS: Will make switch on 5/3 (Thursday). Announcement went out 4/27 Need to schedule OSG Collector switch (Consumers are internal and AGIS, so do we announce it?) Ongoing GRACC Project StashCache Project Discussions Action item (Derek): draft an email for the central collector DNS switch Action item (TimT): draft an email for host/service cert renewal","title":"April 30, 2018"},{"location":"meetings/2018/TechArea20180430/#osg-technology-area-meeting-30-april-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 30 April 2018"},{"location":"meetings/2018/TechArea20180430/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180430/#triage-duty","text":"This week: TimT Next week: TimT 14 (+6) open tickets 12 (-5) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180430/#jira","text":"# of tickets State 115 -1 Open 44 +0 In Progress 14 +5 Ready for Testing 4 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180430/#osg-software-team","text":"Triage duty Everyone should be subscribed to software-support@opensciencegrid.org plus have GGUS and Freshdesk accounts As we transition support, we need to also watch the new mailing list and Freshdesk using the \"My Group Tickets, all but unresolved and closed\" filter Note: Freshdesk tickets aren't currently public. CC's only see comments after they've been added. Office hours: same times this week but plan to cut down on the frequency next week We are deactivating non-OSG staff JIRA accounts in the new cloud-hosted JIRA instance","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180430/#discussion","text":"Action item (BrianL): Add a banner to ticket to inform users about Freshdesk Action item (Derek): Investigate how mailing lists/other ticket systems work with Freshdesk Action item (Suchandra): Checking to see if you can reverse ticket comment order in Freshdesk Action item (BrianL): Organize another Freshdesk tutorial for how S R should use Freshdesk","title":"Discussion"},{"location":"meetings/2018/TechArea20180430/#support-update","text":"Caltech (Marian/Derek) - CVMFS is now indexing the new origin.","title":"Support Update"},{"location":"meetings/2018/TechArea20180430/#osg-release-team","text":"3.3.34 Both 3.4.12 Total Status 0 +0 0 +0 4 -14 4 -14 Open 0 +0 0 -1 2 -7 2 -8 In Progress 0 +0 2 +2 21 +13 23 +15 Ready for Testing 3 +3 0 -3 1 +1 4 +1 Ready for Release 3 +3 2 -2 28 -7 33 -6 Total OSG 3.4.11 - Release Tomorrow High Priority Release: Singularity 2.5.0 OSG 3.4.12 - Testing GlideinWMS 3.2.22 proxy renewal script properly process with the default FQAN Gratia probes Fixed Slurm probe bugs found at UFL HTCondor probe: projectname case insensitive comparison PBS probe: Handle raw seconds as well as hh:mm:ss for durations Drop GRAM and glexec probe XRootD 4.8.2 OSG 3.3.34 - Mostly Ready for Release Eradicate grid.iu.edu references - in testing xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS frontier-squid: fixed startup problem under SELinux xrootd-hdfs 2.0.2: improved write support","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180430/#discussions","text":"Action Item (BrianL): follow up with Zalak on adding Let's Encrypt to the CA bundle","title":"Discussions"},{"location":"meetings/2018/TechArea20180430/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Ready for testing CVMFS Ready for testing Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Ready for testing Changes: OSG Display: Testing - Deployed. Switched DNS on 4/26. Software Repo: Will switch DNS on 5/1 (Tuesday). Announcement went out 4/25 OASIS: Will make switch on 5/3 (Thursday). Announcement went out 4/27 Need to schedule OSG Collector switch (Consumers are internal and AGIS, so do we announce it?)","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180430/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180430/#discussions_1","text":"Action item (Derek): draft an email for the central collector DNS switch Action item (TimT): draft an email for host/service cert renewal","title":"Discussions"},{"location":"meetings/2018/TechArea20180507/","text":"OSG Technology Area Meeting, 7 May 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT Announcements BrianL and Derek out Thursday afternoon and Friday Triage Duty This week: TimT Next week: Carl 11 (-3) open tickets 10 (-2) open LCMAPS VOMS tickets JIRA # of tickets State 110 -5 Open 40 -4 In Progress 24 +10 Ready for Testing 11 +7 Ready for Release OSG Software Team Items from last week: Derek: Ticket exchanges with Freshdesk Suchandra: Reversing ticket comment order in Freshdesk BrianL: Spoke with Marina about adding a banner to ticket Freshdesk usage discussion: tentatively Thursday 5/17 JIRA migration: soon (?) but no date set Office hours: Tuesdays 2-3pm CDT and Thursdays 10-11am CDT Discussion Derek will test ticket exchange with HCC RT and Freshdesk Tuesday morning Suchandra: reverse comment order requires plugins that apply across the board and may break other things. Won't pursue it. Action item (BrianL): Look into ticket types for operations/software Freshdesk groups Support Update LIGO (Edgar) - XRootD file transfer failed due to OSG proxies being signed with an expired VOMS certificate UMD (Suchandra) - Not transitioning off of LCMAPS VOMS until the summer. Should be fine as a CMS T3. OSG Release Team OSG 3.4.12 - Mostly Ready for Release GlideinWMS 3.2.22 proxy renewal script properly process with the default FQAN Gratia probes Fixed Slurm probe bugs found at UFL HTCondor probe: projectname case insensitive comparison PBS probe: Handle raw seconds as well as hh:mm:ss for durations Drop GRAM and glexec probe XRootD 4.8.3 - in testing OSG 3.3.34 - Ready for Release Eradicate grid.iu.edu references xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS frontier-squid: fixed startup problem under SELinux Discussions None this week OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Deployed CVMFS Deployed Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Ready for testing Changes: OASIS: Testing - Deployed: Transitioned on 5/3 CVMFS: Testing - Deployed: Transitioned on 5/3 OSG Collector needs discussion. CE's do not transition to the new collector without a restart. Ongoing GRACC Project StashCache Project Discussions None this week","title":"May 7, 2018"},{"location":"meetings/2018/TechArea20180507/#osg-technology-area-meeting-7-may-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT","title":"OSG Technology Area Meeting,  7 May 2018"},{"location":"meetings/2018/TechArea20180507/#announcements","text":"BrianL and Derek out Thursday afternoon and Friday","title":"Announcements"},{"location":"meetings/2018/TechArea20180507/#triage-duty","text":"This week: TimT Next week: Carl 11 (-3) open tickets 10 (-2) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180507/#jira","text":"# of tickets State 110 -5 Open 40 -4 In Progress 24 +10 Ready for Testing 11 +7 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180507/#osg-software-team","text":"Items from last week: Derek: Ticket exchanges with Freshdesk Suchandra: Reversing ticket comment order in Freshdesk BrianL: Spoke with Marina about adding a banner to ticket Freshdesk usage discussion: tentatively Thursday 5/17 JIRA migration: soon (?) but no date set Office hours: Tuesdays 2-3pm CDT and Thursdays 10-11am CDT","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180507/#discussion","text":"Derek will test ticket exchange with HCC RT and Freshdesk Tuesday morning Suchandra: reverse comment order requires plugins that apply across the board and may break other things. Won't pursue it. Action item (BrianL): Look into ticket types for operations/software Freshdesk groups","title":"Discussion"},{"location":"meetings/2018/TechArea20180507/#support-update","text":"LIGO (Edgar) - XRootD file transfer failed due to OSG proxies being signed with an expired VOMS certificate UMD (Suchandra) - Not transitioning off of LCMAPS VOMS until the summer. Should be fine as a CMS T3.","title":"Support Update"},{"location":"meetings/2018/TechArea20180507/#osg-release-team","text":"OSG 3.4.12 - Mostly Ready for Release GlideinWMS 3.2.22 proxy renewal script properly process with the default FQAN Gratia probes Fixed Slurm probe bugs found at UFL HTCondor probe: projectname case insensitive comparison PBS probe: Handle raw seconds as well as hh:mm:ss for durations Drop GRAM and glexec probe XRootD 4.8.3 - in testing OSG 3.3.34 - Ready for Release Eradicate grid.iu.edu references xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS frontier-squid: fixed startup problem under SELinux","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180507/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180507/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Deployed CVMFS Deployed Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Ready for testing Changes: OASIS: Testing - Deployed: Transitioned on 5/3 CVMFS: Testing - Deployed: Transitioned on 5/3 OSG Collector needs discussion. CE's do not transition to the new collector without a restart.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180507/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180507/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180514/","text":"OSG Technology Area Meeting, 14 May 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, TimT Announcements HEPiX this week: Edgar will have limited availability Suchandra driving down to IU to pick up servers today HTCondor week next week: Madison team and Edgar will have limited availability Triage Duty This week: Carl Next week: Edgar 13 (+2) open tickets 0 (+0) open host certificate tickets 11 (+11) open user certificate tickets 8 (-2) open LCMAPS VOMS tickets JIRA # of tickets State 109 -1 Open 40 +0 In Progress 7 -17 Ready for Testing 1 -10 Ready for Release OSG Software Team JIRA migration: This Wednesday 5/16 9am CDT Host certificate document is broken AI (Carl): Review formatting issues and duplicate sub-sections Derek fixed a bad reference to myosg.opensciencegrid.org in osg-display-data AI (Carl): Cut a new build for the GOC repo Triage Duty AI (TimT): revisit unattended tickets from last week Derek: What were the results of the Freshdesk/HCC RT test? Remember to monitor user and host certificate tickets in Footprints AI (everyone): Update all of your Freshdesk tickets appropriately, either respond or set the status to Waiting on Customer Response . AI (everyone): Let's give LCMAPS VOMS tickets one last ping and if we don't get a response, close them next week. Office hours: Tuesdays 2-3pm CDT and Thursdays 10-11am CDT Discussion IU repo host retirement was delayed to May 23 JIRA DNS will be updated but if there are delays in propagation, you can access JIRA via opensciencegrid.atlassian.net A location for JIRA backups need to be found. AFS? AI (BrianL): Put up note about canceling next week's office hours AI (Derek): Test Freshdesk + HCC RT this week Support Update UFL (Derek) - Assisted with April accounting records issuess due to multiple CEs OSG Release Team 3.4.13 Status 11 +11 Open 9 +9 In Progress 4 +4 Ready for Testing 0 +0 Ready for Release 24 +24 Total GOC Testing mash-scripts: grid.iu.edu becomes opensciencegrid.org osg-display: grid.iu.edu becomes opensciencegrid.org repo-update-cadist: RPM packaging Ready for Release Package repo.opensciencegrid.org scripts rev-perfsonar: grid.iu.edu becomes opensciencegrid.org Data IGTF 1.91 OSG 3.4.13 Testing voms-proxy-direct Pegasus 4.8.2 Singularity 2.5.1 OWAMP 3.5.6 Ready for Release Nothing Yet Discussions None this week OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Deployed CVMFS Deployed Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Ready for testing Changes: Will start social compaign to have people restart their CEs to pick up the DNS change for the new collector. Ongoing GRACC Project StashCache Project Discussions None this week","title":"May 14, 2018"},{"location":"meetings/2018/TechArea20180514/#osg-technology-area-meeting-14-may-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Marian, Mat, TimT","title":"OSG Technology Area Meeting, 14 May 2018"},{"location":"meetings/2018/TechArea20180514/#announcements","text":"HEPiX this week: Edgar will have limited availability Suchandra driving down to IU to pick up servers today HTCondor week next week: Madison team and Edgar will have limited availability","title":"Announcements"},{"location":"meetings/2018/TechArea20180514/#triage-duty","text":"This week: Carl Next week: Edgar 13 (+2) open tickets 0 (+0) open host certificate tickets 11 (+11) open user certificate tickets 8 (-2) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180514/#jira","text":"# of tickets State 109 -1 Open 40 +0 In Progress 7 -17 Ready for Testing 1 -10 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180514/#osg-software-team","text":"JIRA migration: This Wednesday 5/16 9am CDT Host certificate document is broken AI (Carl): Review formatting issues and duplicate sub-sections Derek fixed a bad reference to myosg.opensciencegrid.org in osg-display-data AI (Carl): Cut a new build for the GOC repo Triage Duty AI (TimT): revisit unattended tickets from last week Derek: What were the results of the Freshdesk/HCC RT test? Remember to monitor user and host certificate tickets in Footprints AI (everyone): Update all of your Freshdesk tickets appropriately, either respond or set the status to Waiting on Customer Response . AI (everyone): Let's give LCMAPS VOMS tickets one last ping and if we don't get a response, close them next week. Office hours: Tuesdays 2-3pm CDT and Thursdays 10-11am CDT","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180514/#discussion","text":"IU repo host retirement was delayed to May 23 JIRA DNS will be updated but if there are delays in propagation, you can access JIRA via opensciencegrid.atlassian.net A location for JIRA backups need to be found. AFS? AI (BrianL): Put up note about canceling next week's office hours AI (Derek): Test Freshdesk + HCC RT this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180514/#support-update","text":"UFL (Derek) - Assisted with April accounting records issuess due to multiple CEs","title":"Support Update"},{"location":"meetings/2018/TechArea20180514/#osg-release-team","text":"3.4.13 Status 11 +11 Open 9 +9 In Progress 4 +4 Ready for Testing 0 +0 Ready for Release 24 +24 Total GOC Testing mash-scripts: grid.iu.edu becomes opensciencegrid.org osg-display: grid.iu.edu becomes opensciencegrid.org repo-update-cadist: RPM packaging Ready for Release Package repo.opensciencegrid.org scripts rev-perfsonar: grid.iu.edu becomes opensciencegrid.org Data IGTF 1.91 OSG 3.4.13 Testing voms-proxy-direct Pegasus 4.8.2 Singularity 2.5.1 OWAMP 3.5.6 Ready for Release Nothing Yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180514/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180514/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM In Development OASIS Deployed CVMFS Deployed Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Ready for testing Changes: Will start social compaign to have people restart their CEs to pick up the DNS change for the new collector.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180514/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180514/#discussions_1","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180529/","text":"OSG Technology Area Meeting, 29 May 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Mat, Suchandra Announcements Triage Duty This week: Mat Next week: Suchandra 17 (+4) open tickets 2 (+2) open host certificate tickets 21 (+10) open user certificate tickets 4 (-4) open LCMAPS VOMS tickets JIRA # of tickets State 109 -1 Open 31 -9 In Progress 7 +0 Ready for Testing 8 +7 Ready for Release OSG Software Team Support: Footprints host and user cert tickets are a burning priority with the May 31 OSG CA service retirement Derek: What were the results of the Freshdesk/HCC RT test? Creation: Went well. Created on Freshdesk side. No email storm. 1 email on either queue. +1 email to RT because it's a new user, and freshdesk wants it to register as a user. Resolution on FreshDesk: Resolved on freshdesk side worked as expected. The \"resolved\" email created a new ticket in RT, didn't link to old ticket (expected, since RT only looks at it's own injected subject line) Resolution on RT: Created a small email storm. Sent new emails to freshdesk, which created new tickets. Additionally, RT received error mail that it could not send to \"user-support@opensciencegrid.org\" email list. There were at least 3 new tickets in freshdesk from this interaction. And at least 3 more to RT. I got worried, and started deleting emails as fast as I could. OASIS data updater issues last week due to old repo.grid.iu.edu references Koji builds were broken due to CentOS repo issues JIRA migration: Weekly JIRA ticket summary isn't being sent Office hours: Tuesdays 2-3pm CDT Discussion Freshdesk allows you to close tickets without sending an email and may help avoid email storms The OASIS login node is puppeted and we should add a recipe for updating the WN client tarball install Support Update FusionGrid (BrianL): They want to transition off of their GRAM CE. It's not clear why they're part of the OSG as they don't offer cycles or use cycles outside of their own site. OSG Investigations Team GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM Deployed OASIS Deployed CVMFS Deployed Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Deployed Changes: OSG Collector: Social campaign for the OSG collector has started! Look for freshdesk tickets. OIM: New OIM is up and running. DNS change was on Thursday, as scheduled. Done with transitions!?! Ongoing GRACC Project StashCache Project Discussions Derek: There is a command that users can run to verify that they're sending ads to the new central collector","title":"May 29, 2018"},{"location":"meetings/2018/TechArea20180529/#osg-technology-area-meeting-29-may-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Mat, Suchandra","title":"OSG Technology Area Meeting, 29 May 2018"},{"location":"meetings/2018/TechArea20180529/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180529/#triage-duty","text":"This week: Mat Next week: Suchandra 17 (+4) open tickets 2 (+2) open host certificate tickets 21 (+10) open user certificate tickets 4 (-4) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180529/#jira","text":"# of tickets State 109 -1 Open 31 -9 In Progress 7 +0 Ready for Testing 8 +7 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180529/#osg-software-team","text":"Support: Footprints host and user cert tickets are a burning priority with the May 31 OSG CA service retirement Derek: What were the results of the Freshdesk/HCC RT test? Creation: Went well. Created on Freshdesk side. No email storm. 1 email on either queue. +1 email to RT because it's a new user, and freshdesk wants it to register as a user. Resolution on FreshDesk: Resolved on freshdesk side worked as expected. The \"resolved\" email created a new ticket in RT, didn't link to old ticket (expected, since RT only looks at it's own injected subject line) Resolution on RT: Created a small email storm. Sent new emails to freshdesk, which created new tickets. Additionally, RT received error mail that it could not send to \"user-support@opensciencegrid.org\" email list. There were at least 3 new tickets in freshdesk from this interaction. And at least 3 more to RT. I got worried, and started deleting emails as fast as I could. OASIS data updater issues last week due to old repo.grid.iu.edu references Koji builds were broken due to CentOS repo issues JIRA migration: Weekly JIRA ticket summary isn't being sent Office hours: Tuesdays 2-3pm CDT","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180529/#discussion","text":"Freshdesk allows you to close tickets without sending an email and may help avoid email storms The OASIS login node is puppeted and we should add a recipe for updating the WN client tarball install","title":"Discussion"},{"location":"meetings/2018/TechArea20180529/#support-update","text":"FusionGrid (BrianL): They want to transition off of their GRAM CE. It's not clear why they're part of the OSG as they don't offer cycles or use cycles outside of their own site.","title":"Support Update"},{"location":"meetings/2018/TechArea20180529/#osg-investigations-team","text":"GOC Transition is going to dominate the Investigations Team's time over the next couple of weeks. I want to give general status of the transitions we are participating in. So, I will give a couple of states: Have not started In development - Could be provisioning hardware, adapting software, installing software Ready for testing - In a state where it can be tested by the consumers Deployed Service Status OIM Deployed OASIS Deployed CVMFS Deployed Software Repo Deployed OSG Display Deployed StashCache Redirector Deployed OSG Collector Deployed Changes: OSG Collector: Social campaign for the OSG collector has started! Look for freshdesk tickets. OIM: New OIM is up and running. DNS change was on Thursday, as scheduled. Done with transitions!?!","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180529/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180529/#discussions","text":"Derek: There is a command that users can run to verify that they're sending ads to the new central collector","title":"Discussions"},{"location":"meetings/2018/TechArea20180604/","text":"OSG Technology Area Meeting, 4 June 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimT Announcements GitHub acquired by Microsoft! This shouldn't have any effect in the short term. Triage Duty This week: Suchandra Next week: BrianL 16 (-1) open tickets 3 (-1) open LCMAPS VOMS tickets JIRA # of tickets State 113 +4 Open 30 -1 In Progress 6 -1 Ready for Testing 4 -4 Ready for Release OSG Software Team Support: We will discuss Freshdesk workflows during the next doc focus afternoon Ticket publicity is going to be discussed: for the time being if someone other than the ticket owner asks for ticket history, feel free to send it to them Mail not being sent to the OSG software mailing list (mail eventually received, checking with FNAL to see if there were known issues on their end) JIRA + GitHub integration may no longer work (free trial for the paid plugin expired) OASIS login node needs its WN tarball install updated regularly (puppet?) Office hours: Tuesdays 2-3pm CDT Discussion AI (Mat) - Create a freshdesk ticket for OASIS login AI (TimT) - Create a freshdesk ticket for a ticket that wasn't created from mail sent to help@osg.org Support Update User certificates (TimT): handled many renewal requests. Most common issue was that people were not logged in, and we could have addressed this in subsequent emails/documentation. Additionally, next time we should set the \"Reply-To\" to help@osg.org so that TimT doesn't have to field all support requests. UConn (Suchandra) - Richard has allocations on various clusters and wants to use them for GlueX jobs, wants to reduce number of CEs necessary UConn (Edgar): Lowered pilot lifetime since jobs opportunistic jobs were taking over the cluster dedicated to GlueX CHTC (Edgar) - no GLOW pilots running due to pilot certs expiring. Fixed now; certs were requested but not put in the proper location. Central collector (Derek) - FIU TCP issues blocking connections to the central collector. Proven by speed of light. Continuing to poke sites this week OSG Release Team 3.4.13 Status 12 +1 Open 9 +0 In Progress 5 +1 Ready for Testing 4 +4 Ready for Release 30 +6 Total OSG 3.4.13 Testing HTCondor 8.6.11 HTCondor 8.7.8 Drop consider_as_osg form *.repo files OWAMP 3.5.6 Ready for Release voms-proxy-direct Pegasus 4.8.2 Singularity 2.5.1 CMVFS 2.5.0 Data Nothing yet GOC Testing repo-update-cadist: RPM packaging Ready for Release Nothing yet Discussions AI (TimT): update SW release doc for post-service migrations (SOFTWARE-3276) AI (TimT): copy data release doc to create a basic GOC release doc OSG Investigations Team GOC Transitions Done with transitions!?! We are all but done with the transition of services from IU. There are still a few items on our list: (Ongoing) Getting sites to transition to the new HTCondor collector Minor hiccup with OSG-WN tarball in OASIS home directory. Still investigating. And a few longer term goals: Most services are in puppet. Just need to put the finishing touches on it. For example, the changes made in the last week with all of the OIM, Ticket,... redirections. Most services are in check_mk (active monitoring), but need to add specific tests. For example, we check that HTTP(s) is responding to requests, but we would like to make sure it is responding with correct info. Presentations Gave a StashCache presentation at the GPN annual meeting. Went over very well. Highlighted the science drivers for StashCache. Ongoing GRACC Project StashCache Project Discussions AI (BrianL, Derek) - Investigate mailing list outage (possibly related to DNS security changes made last week) and announce outage if necessary","title":"June 4, 2018"},{"location":"meetings/2018/TechArea20180604/#osg-technology-area-meeting-4-june-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting,  4 June 2018"},{"location":"meetings/2018/TechArea20180604/#announcements","text":"GitHub acquired by Microsoft! This shouldn't have any effect in the short term.","title":"Announcements"},{"location":"meetings/2018/TechArea20180604/#triage-duty","text":"This week: Suchandra Next week: BrianL 16 (-1) open tickets 3 (-1) open LCMAPS VOMS tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180604/#jira","text":"# of tickets State 113 +4 Open 30 -1 In Progress 6 -1 Ready for Testing 4 -4 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180604/#osg-software-team","text":"Support: We will discuss Freshdesk workflows during the next doc focus afternoon Ticket publicity is going to be discussed: for the time being if someone other than the ticket owner asks for ticket history, feel free to send it to them Mail not being sent to the OSG software mailing list (mail eventually received, checking with FNAL to see if there were known issues on their end) JIRA + GitHub integration may no longer work (free trial for the paid plugin expired) OASIS login node needs its WN tarball install updated regularly (puppet?) Office hours: Tuesdays 2-3pm CDT","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180604/#discussion","text":"AI (Mat) - Create a freshdesk ticket for OASIS login AI (TimT) - Create a freshdesk ticket for a ticket that wasn't created from mail sent to help@osg.org","title":"Discussion"},{"location":"meetings/2018/TechArea20180604/#support-update","text":"User certificates (TimT): handled many renewal requests. Most common issue was that people were not logged in, and we could have addressed this in subsequent emails/documentation. Additionally, next time we should set the \"Reply-To\" to help@osg.org so that TimT doesn't have to field all support requests. UConn (Suchandra) - Richard has allocations on various clusters and wants to use them for GlueX jobs, wants to reduce number of CEs necessary UConn (Edgar): Lowered pilot lifetime since jobs opportunistic jobs were taking over the cluster dedicated to GlueX CHTC (Edgar) - no GLOW pilots running due to pilot certs expiring. Fixed now; certs were requested but not put in the proper location. Central collector (Derek) - FIU TCP issues blocking connections to the central collector. Proven by speed of light. Continuing to poke sites this week","title":"Support Update"},{"location":"meetings/2018/TechArea20180604/#osg-release-team","text":"3.4.13 Status 12 +1 Open 9 +0 In Progress 5 +1 Ready for Testing 4 +4 Ready for Release 30 +6 Total OSG 3.4.13 Testing HTCondor 8.6.11 HTCondor 8.7.8 Drop consider_as_osg form *.repo files OWAMP 3.5.6 Ready for Release voms-proxy-direct Pegasus 4.8.2 Singularity 2.5.1 CMVFS 2.5.0 Data Nothing yet GOC Testing repo-update-cadist: RPM packaging Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180604/#discussions","text":"AI (TimT): update SW release doc for post-service migrations (SOFTWARE-3276) AI (TimT): copy data release doc to create a basic GOC release doc","title":"Discussions"},{"location":"meetings/2018/TechArea20180604/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180604/#goc-transitions","text":"Done with transitions!?! We are all but done with the transition of services from IU. There are still a few items on our list: (Ongoing) Getting sites to transition to the new HTCondor collector Minor hiccup with OSG-WN tarball in OASIS home directory. Still investigating. And a few longer term goals: Most services are in puppet. Just need to put the finishing touches on it. For example, the changes made in the last week with all of the OIM, Ticket,... redirections. Most services are in check_mk (active monitoring), but need to add specific tests. For example, we check that HTTP(s) is responding to requests, but we would like to make sure it is responding with correct info.","title":"GOC Transitions"},{"location":"meetings/2018/TechArea20180604/#presentations","text":"Gave a StashCache presentation at the GPN annual meeting. Went over very well. Highlighted the science drivers for StashCache.","title":"Presentations"},{"location":"meetings/2018/TechArea20180604/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180604/#discussions_1","text":"AI (BrianL, Derek) - Investigate mailing list outage (possibly related to DNS security changes made last week) and announce outage if necessary","title":"Discussions"},{"location":"meetings/2018/TechArea20180611/","text":"OSG Technology Area Meeting, 11 June 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Suchandra, TimC, TimT Announcements Jeff Dost is the new operations lead Mat OOO until Thursday Triage Duty This week: BrianL Next week: BrianL 14 (-2) open tickets 1 (-2) open LCMAPS VOMS tickets 4 (+4) open VOMS server retirement tickets JIRA # of tickets State 115 +2 Open 27 -3 In Progress 8 +2 Ready for Testing 7 +3 Ready for Release OSG Software Team Doc Focus 6/14 1pm CDT agenda: Freshdesk discussion, doc tickets Nebraska outage over the weekend affected some services, e.g. OASIS data updater, software repository mirror OSG 3.4.14 GlideinWMS 3.4.0 and glideinwms-switchboard-1.0.0 in upcoming-testing both need heavy testing Move HDFS 2.6 to production Discussion OASIS data updater currently runs every 6 hours AI (BrianL): OSG ITB factory is being held up by an HTCondor CNAME bug AI (Edgar/Suchandra): Edgar will update his factory to 3.4; Suchandra will test 3.2/3.3/3.4 frontends against Edgar's factory AI (BrianL): talk to factory ops about relaxing ITB entries on the prod factory policy Support Update Cert requests (TimT): Dealt with some trailing requests Support entry points (TimC): All support entry points now lead to Freshdesk (goc@osg.org moved over last week) Topology (Mat): Investigated mergingin brown-cms; updated some contact entries OSG Release Team 3.4.13 Status 0 -12 Open 0 -9 In Progress 0 -5 Ready for Testing 8 +4 Ready for Release 8 -22 Total OSG 3.4.13 Testing Completed Ready for Release HTCondor 8.6.11 HTCondor 8.7.8 Drop consider-as-osg from *.repo files voms-proxy-direct Pegasus 4.8.2 Singularity 2.5.1 CMVFS 2.5.0 Data Nothing yet GOC Testing repo-update-cadist: RPM packaging Ready for Release Nothing yet Discussion AI (Carl): turn crank on 3.4.13 release AI (TimT): copy data release doc to create a basic GOC release doc OSG Investigations Team Operations TODO: - (Ongoing) Getting sites to transition to the new HTCondor collector - Discussion of SLA's, availability metrics... - Puppetize: Collectors, topology DONE: - Lots of puppetization: Repo, display, redirects... - check_mk of collectors, repo / display / redirects (same host). Investigations Working on releasing Writeable Stashcache as production Beginning work with Internet2 to place StashCache in connection points. Ongoing GRACC Project StashCache Project Discussions In-depth monitoring (e.g., verifying repo contents) unlikely to happen in the short term due to limited Ops time for Marian and Derek, and rest of ops team","title":"June 11, 2018"},{"location":"meetings/2018/TechArea20180611/#osg-technology-area-meeting-11-june-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Suchandra, TimC, TimT","title":"OSG Technology Area Meeting, 11 June 2018"},{"location":"meetings/2018/TechArea20180611/#announcements","text":"Jeff Dost is the new operations lead Mat OOO until Thursday","title":"Announcements"},{"location":"meetings/2018/TechArea20180611/#triage-duty","text":"This week: BrianL Next week: BrianL 14 (-2) open tickets 1 (-2) open LCMAPS VOMS tickets 4 (+4) open VOMS server retirement tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180611/#jira","text":"# of tickets State 115 +2 Open 27 -3 In Progress 8 +2 Ready for Testing 7 +3 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180611/#osg-software-team","text":"Doc Focus 6/14 1pm CDT agenda: Freshdesk discussion, doc tickets Nebraska outage over the weekend affected some services, e.g. OASIS data updater, software repository mirror OSG 3.4.14 GlideinWMS 3.4.0 and glideinwms-switchboard-1.0.0 in upcoming-testing both need heavy testing Move HDFS 2.6 to production","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180611/#discussion","text":"OASIS data updater currently runs every 6 hours AI (BrianL): OSG ITB factory is being held up by an HTCondor CNAME bug AI (Edgar/Suchandra): Edgar will update his factory to 3.4; Suchandra will test 3.2/3.3/3.4 frontends against Edgar's factory AI (BrianL): talk to factory ops about relaxing ITB entries on the prod factory policy","title":"Discussion"},{"location":"meetings/2018/TechArea20180611/#support-update","text":"Cert requests (TimT): Dealt with some trailing requests Support entry points (TimC): All support entry points now lead to Freshdesk (goc@osg.org moved over last week) Topology (Mat): Investigated mergingin brown-cms; updated some contact entries","title":"Support Update"},{"location":"meetings/2018/TechArea20180611/#osg-release-team","text":"3.4.13 Status 0 -12 Open 0 -9 In Progress 0 -5 Ready for Testing 8 +4 Ready for Release 8 -22 Total OSG 3.4.13 Testing Completed Ready for Release HTCondor 8.6.11 HTCondor 8.7.8 Drop consider-as-osg from *.repo files voms-proxy-direct Pegasus 4.8.2 Singularity 2.5.1 CMVFS 2.5.0 Data Nothing yet GOC Testing repo-update-cadist: RPM packaging Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180611/#discussion_1","text":"AI (Carl): turn crank on 3.4.13 release AI (TimT): copy data release doc to create a basic GOC release doc","title":"Discussion"},{"location":"meetings/2018/TechArea20180611/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180611/#operations","text":"TODO: - (Ongoing) Getting sites to transition to the new HTCondor collector - Discussion of SLA's, availability metrics... - Puppetize: Collectors, topology DONE: - Lots of puppetization: Repo, display, redirects... - check_mk of collectors, repo / display / redirects (same host).","title":"Operations"},{"location":"meetings/2018/TechArea20180611/#investigations","text":"Working on releasing Writeable Stashcache as production Beginning work with Internet2 to place StashCache in connection points.","title":"Investigations"},{"location":"meetings/2018/TechArea20180611/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180611/#discussions","text":"In-depth monitoring (e.g., verifying repo contents) unlikely to happen in the short term due to limited Ops time for Marian and Derek, and rest of ops team","title":"Discussions"},{"location":"meetings/2018/TechArea20180618/","text":"OSG Technology Area Meeting, 18 June 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra Announcements OSG Annual Report submitted Triage Duty This week: BrianL Next week: Carl 11 (-3) open tickets 1 (-3) open VOMS server retirement tickets JIRA # of tickets State 115 +0 Open 29 +2 In Progress 9 +1 Ready for Testing 1 -6 Ready for Release OSG Software Team osg-tested-internal tests are failing due to the new Slurm in development OSG 3.4.14 GlideinWMS 3.4.0 and glideinwms-switchboard-1.0.0 : where's testing stand? Move HDFS 2.6 to production PKI tools: replace OIM functionality with the ability to generate CSRs and keys Factory ops will temporarily put a Madison ITB entry into the production factory Discussion Suchandra has a VM up for Slurm and hopes to have it fixed today Edgar will update condor on the factory + gwms switchboard and run tests again Support Update Frontier Squid (Carl): Someone had questions if SELinux + Squid worked, it should with the newest version Topology (BrianL, Mat): Topology and contact updates OSG Investigations Team Operations TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, topology, GRACC Investigations In Progress: Working on releasing Writeable Stashcache as production Beginning work with Internet2 to place StashCache in connection points. Consulting with SLATE for StashCache installtion. Working with Network group on PS data. Moving StashCache docs to OSG docs page: https://github.com/opensciencegrid/docs/pull/388 Ongoing GRACC Project StashCache Project Discussions None this week","title":"June 18, 2018"},{"location":"meetings/2018/TechArea20180618/#osg-technology-area-meeting-18-june-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Mat, Suchandra","title":"OSG Technology Area Meeting, 18 June 2018"},{"location":"meetings/2018/TechArea20180618/#announcements","text":"OSG Annual Report submitted","title":"Announcements"},{"location":"meetings/2018/TechArea20180618/#triage-duty","text":"This week: BrianL Next week: Carl 11 (-3) open tickets 1 (-3) open VOMS server retirement tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180618/#jira","text":"# of tickets State 115 +0 Open 29 +2 In Progress 9 +1 Ready for Testing 1 -6 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180618/#osg-software-team","text":"osg-tested-internal tests are failing due to the new Slurm in development OSG 3.4.14 GlideinWMS 3.4.0 and glideinwms-switchboard-1.0.0 : where's testing stand? Move HDFS 2.6 to production PKI tools: replace OIM functionality with the ability to generate CSRs and keys Factory ops will temporarily put a Madison ITB entry into the production factory","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180618/#discussion","text":"Suchandra has a VM up for Slurm and hopes to have it fixed today Edgar will update condor on the factory + gwms switchboard and run tests again","title":"Discussion"},{"location":"meetings/2018/TechArea20180618/#support-update","text":"Frontier Squid (Carl): Someone had questions if SELinux + Squid worked, it should with the newest version Topology (BrianL, Mat): Topology and contact updates","title":"Support Update"},{"location":"meetings/2018/TechArea20180618/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180618/#operations","text":"TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, topology, GRACC","title":"Operations"},{"location":"meetings/2018/TechArea20180618/#investigations","text":"In Progress: Working on releasing Writeable Stashcache as production Beginning work with Internet2 to place StashCache in connection points. Consulting with SLATE for StashCache installtion. Working with Network group on PS data. Moving StashCache docs to OSG docs page: https://github.com/opensciencegrid/docs/pull/388","title":"Investigations"},{"location":"meetings/2018/TechArea20180618/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180618/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180625/","text":"OSG Technology Area Meeting, 25 June 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements CHEP (July 7 - July 13): Edgar attending (also at CERN the week before) OSG User School (July 9 - July 13): BrianL, Derek, TimT attending Triage Duty This week: Carl Next week: TimT 7 (-4) open tickets JIRA # of tickets State 116 +1 Open 28 -1 In Progress 15 +6 Ready for Testing 2 +1 Ready for Release OSG Software Team OSG 3.4.14 Goal is to release next week, get builds RFT at the beginning of the week Waiting on builds for osg-configure, osg-pki-tools, and RSV Firewall rules for the Madison ITB need to be updated to allow the UCSD factory Remember to update your Y7 effort tracking numbers. I'd also like to start tracking time spent on specific-projects. Discussion Mat is developing a process to upload tarballs to repo.gridcf.org. He will run it by Mattias after we test it with the pre-release tarballs. Carl (AI): update firewall rules on Madison ITB Support Update IceCube (TimT): Users had issues with CILogon OpenID user certs Topology (BrianL, Mat): Miscellaneous topology and contact updates University of Florida (Derek, Carl): Missing jobs in GRACC. Spoke with Carl, may be race condition in how we handle Slurm job with gratia probes. Will recommend the HTCondor-CE gratia probe, if possible. OSG Release Team 3.4.14 Status 6 +6 Open 7 +7 In Progress 13 +13 Ready for Testing 2 +2 Ready for Release 28 +28 Total OSG 3.4.14 Testing glideinWMS switchboard??? RSV with Apache 2.3 gratia twiki links PBS/LSF update last timestamp for empty log file htcondor-ce spam control BLAHP - new version HDFS 2.6 lcmaps-plugins-verify-proxy Ready for Release GlideinWMS 3.4 (Upcoming) OWAMP 3.5.6 Data Nothing yet GOC Testing repo-update-cadist: RPM packaging (Waiting for other changes) Drop mirror.batlab.org from mirror list Ready for Release Nothing yet Discussion Need to update process for generating and uploading src debs for the CA distributions Holding back glideinwms-switchboard TimT (AI): If IGTF releases, Zalak may be unavilable. Contact the security team. OSG Investigations Team Operations Derek's Operations time will be going down by mid-july. Put onto other projects. I will coordinate with ops for any development that is needed, but things like puppetizing and day to day operations, I will not be involved in. TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, ~~topology~~, GRACC One of the OASIS nodes moving to new host, need to send announcement. Investigations In Progress: Working on releasing Writeable Stashcache as production Beginning work with Internet2 to place StashCache in connection points. Ongoing GRACC Project StashCache Project Discussions Anvil Cloud downtime: HCC offered hardware to hold us over and they should be up 1 week before the downtime. The exact downtime dates aren't scheduled yet. Derek (AI): separate actual hosts for ITB host (instead of VHosts). Waiting on puppet modules. Marian (AI): replica needs to be rebuilt, email BrianL, Derek, Jeff, TimC, and TimT about SLA and notification","title":"June 25, 2018"},{"location":"meetings/2018/TechArea20180625/#osg-technology-area-meeting-25-june-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 25 June 2018"},{"location":"meetings/2018/TechArea20180625/#announcements","text":"CHEP (July 7 - July 13): Edgar attending (also at CERN the week before) OSG User School (July 9 - July 13): BrianL, Derek, TimT attending","title":"Announcements"},{"location":"meetings/2018/TechArea20180625/#triage-duty","text":"This week: Carl Next week: TimT 7 (-4) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180625/#jira","text":"# of tickets State 116 +1 Open 28 -1 In Progress 15 +6 Ready for Testing 2 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180625/#osg-software-team","text":"OSG 3.4.14 Goal is to release next week, get builds RFT at the beginning of the week Waiting on builds for osg-configure, osg-pki-tools, and RSV Firewall rules for the Madison ITB need to be updated to allow the UCSD factory Remember to update your Y7 effort tracking numbers. I'd also like to start tracking time spent on specific-projects.","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180625/#discussion","text":"Mat is developing a process to upload tarballs to repo.gridcf.org. He will run it by Mattias after we test it with the pre-release tarballs. Carl (AI): update firewall rules on Madison ITB","title":"Discussion"},{"location":"meetings/2018/TechArea20180625/#support-update","text":"IceCube (TimT): Users had issues with CILogon OpenID user certs Topology (BrianL, Mat): Miscellaneous topology and contact updates University of Florida (Derek, Carl): Missing jobs in GRACC. Spoke with Carl, may be race condition in how we handle Slurm job with gratia probes. Will recommend the HTCondor-CE gratia probe, if possible.","title":"Support Update"},{"location":"meetings/2018/TechArea20180625/#osg-release-team","text":"3.4.14 Status 6 +6 Open 7 +7 In Progress 13 +13 Ready for Testing 2 +2 Ready for Release 28 +28 Total OSG 3.4.14 Testing glideinWMS switchboard??? RSV with Apache 2.3 gratia twiki links PBS/LSF update last timestamp for empty log file htcondor-ce spam control BLAHP - new version HDFS 2.6 lcmaps-plugins-verify-proxy Ready for Release GlideinWMS 3.4 (Upcoming) OWAMP 3.5.6 Data Nothing yet GOC Testing repo-update-cadist: RPM packaging (Waiting for other changes) Drop mirror.batlab.org from mirror list Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180625/#discussion_1","text":"Need to update process for generating and uploading src debs for the CA distributions Holding back glideinwms-switchboard TimT (AI): If IGTF releases, Zalak may be unavilable. Contact the security team.","title":"Discussion"},{"location":"meetings/2018/TechArea20180625/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180625/#operations","text":"Derek's Operations time will be going down by mid-july. Put onto other projects. I will coordinate with ops for any development that is needed, but things like puppetizing and day to day operations, I will not be involved in. TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, ~~topology~~, GRACC One of the OASIS nodes moving to new host, need to send announcement.","title":"Operations"},{"location":"meetings/2018/TechArea20180625/#investigations","text":"In Progress: Working on releasing Writeable Stashcache as production Beginning work with Internet2 to place StashCache in connection points.","title":"Investigations"},{"location":"meetings/2018/TechArea20180625/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180625/#discussions","text":"Anvil Cloud downtime: HCC offered hardware to hold us over and they should be up 1 week before the downtime. The exact downtime dates aren't scheduled yet. Derek (AI): separate actual hosts for ITB host (instead of VHosts). Waiting on puppet modules. Marian (AI): replica needs to be rebuilt, email BrianL, Derek, Jeff, TimC, and TimT about SLA and notification","title":"Discussions"},{"location":"meetings/2018/TechArea20180702/","text":"OSG Technology Area Meeting, 2 July 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl Derek, Marian, Mat, Suchandra, TimT Announcements CHEP (July 7 - July 13): Edgar attending OSG User School (July 9 - July 13): BrianL, Derek, TimT attending Triage Duty This week: TimT Next week: Mat 12 (+5) open tickets JIRA # of tickets State 125 +9 Open 17 -11 In Progress 10 -5 Ready for Testing 17 +15 Ready for Release OSG Software Team HDFS packages need to be untagged from upcoming after the release New version of Singularity needs VMU tests kicked off Planning the next doc focus: expect a doodle poll this week Discussion We will continue to ship Singularity even though Dave/Brian are EPEL maintainers due to yum priorities WBS meeting yet to happen but once the work is done, it'll be shared with the Technology area teams OASIS RPMs in goc-itb will be promoted to release tomorrow Support Update Topology (BrianL, Mat): Miscellaneous topology and downtime updates University of Florida (Derek, Carl): We will modify the Slurm probe instead of trying out the HTCondor-CE probe OSG Release Team 3.4.14 Status 0 -6 Open 0 -7 In Progress 7 -6 Ready for Testing 17 +15 Ready for Release 24 -4 Total OSG 3.4.14 (Release today) Testing RSV fix cacert-verify-probe and crl-freshness-probe An OSG lcmaps-plugins-voms patch breaks its use with llrun Cleanup osg-se-hadoop-gridftp package gratia PBS/LSF update last timestamp for empty log file htcondor-ce spam control Ready for Release GlideinWMS 3.4 (Upcoming) OWAMP 3.5.6 RSV with Apache 2.4 gratia twiki links BLAHP - new version HDFS 2.6 lcmaps-plugins-verify-proxy osg-build osg-configure Data (Thursday release) IGTF 1.92 GOC Testing Drop mirror.batlab.org from mirror list Ready for Release Nothing yet Discussion None this week OSG Investigations Team Operations (Marian) Administration of OASIS Moving OASIS to new hosts TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, GRACC Investigations Released StashCache client in production. In Progress: More StashCaches. Beginning work with Internet2 to place StashCache in connection points. Ongoing GRACC Project StashCache Project Discussions Anvil Cloud downtime: HCC offered hardware to hold us over and they should be up 1 week before the downtime. The exact downtime dates aren't scheduled yet. Derek (AI): separate actual hosts for ITB host (instead of VHosts). Waiting on puppet modules. Marian (AI): replica needs to be rebuilt, email BrianL, Derek, Jeff, TimC, and TimT about SLA and notification","title":"July 2, 2018"},{"location":"meetings/2018/TechArea20180702/#osg-technology-area-meeting-2-july-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl Derek, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting,  2 July 2018"},{"location":"meetings/2018/TechArea20180702/#announcements","text":"CHEP (July 7 - July 13): Edgar attending OSG User School (July 9 - July 13): BrianL, Derek, TimT attending","title":"Announcements"},{"location":"meetings/2018/TechArea20180702/#triage-duty","text":"This week: TimT Next week: Mat 12 (+5) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180702/#jira","text":"# of tickets State 125 +9 Open 17 -11 In Progress 10 -5 Ready for Testing 17 +15 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180702/#osg-software-team","text":"HDFS packages need to be untagged from upcoming after the release New version of Singularity needs VMU tests kicked off Planning the next doc focus: expect a doodle poll this week","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180702/#discussion","text":"We will continue to ship Singularity even though Dave/Brian are EPEL maintainers due to yum priorities WBS meeting yet to happen but once the work is done, it'll be shared with the Technology area teams OASIS RPMs in goc-itb will be promoted to release tomorrow","title":"Discussion"},{"location":"meetings/2018/TechArea20180702/#support-update","text":"Topology (BrianL, Mat): Miscellaneous topology and downtime updates University of Florida (Derek, Carl): We will modify the Slurm probe instead of trying out the HTCondor-CE probe","title":"Support Update"},{"location":"meetings/2018/TechArea20180702/#osg-release-team","text":"3.4.14 Status 0 -6 Open 0 -7 In Progress 7 -6 Ready for Testing 17 +15 Ready for Release 24 -4 Total OSG 3.4.14 (Release today) Testing RSV fix cacert-verify-probe and crl-freshness-probe An OSG lcmaps-plugins-voms patch breaks its use with llrun Cleanup osg-se-hadoop-gridftp package gratia PBS/LSF update last timestamp for empty log file htcondor-ce spam control Ready for Release GlideinWMS 3.4 (Upcoming) OWAMP 3.5.6 RSV with Apache 2.4 gratia twiki links BLAHP - new version HDFS 2.6 lcmaps-plugins-verify-proxy osg-build osg-configure Data (Thursday release) IGTF 1.92 GOC Testing Drop mirror.batlab.org from mirror list Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180702/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180702/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180702/#operations","text":"(Marian) Administration of OASIS Moving OASIS to new hosts TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, GRACC","title":"Operations"},{"location":"meetings/2018/TechArea20180702/#investigations","text":"Released StashCache client in production. In Progress: More StashCaches. Beginning work with Internet2 to place StashCache in connection points.","title":"Investigations"},{"location":"meetings/2018/TechArea20180702/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180702/#discussions","text":"Anvil Cloud downtime: HCC offered hardware to hold us over and they should be up 1 week before the downtime. The exact downtime dates aren't scheduled yet. Derek (AI): separate actual hosts for ITB host (instead of VHosts). Waiting on puppet modules. Marian (AI): replica needs to be rebuilt, email BrianL, Derek, Jeff, TimC, and TimT about SLA and notification","title":"Discussions"},{"location":"meetings/2018/TechArea20180716/","text":"OSG Technology Area Meeting, 16 July 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements TimT at OSCON, returning next Wednesday Triage Duty This week: Suchandra Next week: Edgar 7 (-5) open tickets JIRA # of tickets State 132 +0 Open 19 +0 In Progress 2 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team UNL operations will be moving hosts out of their Anvil Cloud. Assisting them is a priority. OSG 3.4.16: XRootD 4.8.4, Frontier Squid 3.5.27-5.1, and SciTokens (?) Downtime and contact registration interface work Carl: Are the downtime validation tests done? BrianB pointed us at a CILogon2/COManage test instance for contacts Where do we stand with GCT tarballs on repo? Next doc focus 8/2 1PM CDT Discussion If there are errors uploading tarballs to OASIS, contact Suchandra (TimT) AI: update the \"How to cut a release\" document with instructions for sending release announcements (Derek) AI: make Mat's suggested SSHD config changes to support GCT tarballs Support Update None this week OSG Release Team 3.4.16 Status 9 +9 Open 1 +1 In Progress 1 +1 Ready for Testing 0 +0 Ready for Release 11 +11 Total OSG 3.4.16 Testing XRootD 4.8.4 Ready for Release Nothing Yet Data Nothing Yet GOC Testing Drop mirror.batlab.org from mirror list Ready for Release Nothing yet Discussion None this week OSG Investigations Team Operations Moving hosts from Anvil cloud host at Nebraska for Anvil downtime TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, GRACC Investigations Released StashCache client in production. In Progress: Discussions with Internet2 to place StashCache in connection points. Ongoing GRACC Project StashCache Project Discussions (Derek) AI: Will speak with admins about moving IPs out of Anvil so that we don't have central collector downtime (Edgar) AI: gWMS factories would like to generate factory entries based off information in CRIC, which will populate some of its data off of the central collector. Edgar will provide a list of info that he believe may be auto-discoverable on the CE to be advertised back to the central collector","title":"July 16, 2018"},{"location":"meetings/2018/TechArea20180716/#osg-technology-area-meeting-16-july-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 16 July 2018"},{"location":"meetings/2018/TechArea20180716/#announcements","text":"TimT at OSCON, returning next Wednesday","title":"Announcements"},{"location":"meetings/2018/TechArea20180716/#triage-duty","text":"This week: Suchandra Next week: Edgar 7 (-5) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180716/#jira","text":"# of tickets State 132 +0 Open 19 +0 In Progress 2 +1 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180716/#osg-software-team","text":"UNL operations will be moving hosts out of their Anvil Cloud. Assisting them is a priority. OSG 3.4.16: XRootD 4.8.4, Frontier Squid 3.5.27-5.1, and SciTokens (?) Downtime and contact registration interface work Carl: Are the downtime validation tests done? BrianB pointed us at a CILogon2/COManage test instance for contacts Where do we stand with GCT tarballs on repo? Next doc focus 8/2 1PM CDT","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180716/#discussion","text":"If there are errors uploading tarballs to OASIS, contact Suchandra (TimT) AI: update the \"How to cut a release\" document with instructions for sending release announcements (Derek) AI: make Mat's suggested SSHD config changes to support GCT tarballs","title":"Discussion"},{"location":"meetings/2018/TechArea20180716/#support-update","text":"None this week","title":"Support Update"},{"location":"meetings/2018/TechArea20180716/#osg-release-team","text":"3.4.16 Status 9 +9 Open 1 +1 In Progress 1 +1 Ready for Testing 0 +0 Ready for Release 11 +11 Total OSG 3.4.16 Testing XRootD 4.8.4 Ready for Release Nothing Yet Data Nothing Yet GOC Testing Drop mirror.batlab.org from mirror list Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180716/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180716/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180716/#operations","text":"Moving hosts from Anvil cloud host at Nebraska for Anvil downtime TODO: (Ongoing) Getting sites to transition to the new HTCondor collector Discussion of SLA's, availability metrics Puppetize: Collectors, GRACC","title":"Operations"},{"location":"meetings/2018/TechArea20180716/#investigations","text":"Released StashCache client in production. In Progress: Discussions with Internet2 to place StashCache in connection points.","title":"Investigations"},{"location":"meetings/2018/TechArea20180716/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180716/#discussions","text":"(Derek) AI: Will speak with admins about moving IPs out of Anvil so that we don't have central collector downtime (Edgar) AI: gWMS factories would like to generate factory entries based off information in CRIC, which will populate some of its data off of the central collector. Edgar will provide a list of info that he believe may be auto-discoverable on the CE to be advertised back to the central collector","title":"Discussions"},{"location":"meetings/2018/TechArea20180723/","text":"OSG Technology Area Meeting, 23 July 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra Announcements TimT OOO, returning Wednesday BrianL out next Monday Triage Duty This week: Edgar Next week: Mat 7 (+0) open tickets JIRA # of tickets State 135 +3 Open 16 -3 In Progress 5 +3 Ready for Testing 0 +0 Ready for Release OSG Software Team OSG 3.4.16: XRootD 4.8.4, Frontier Squid 3.5.27-5.1, and SciTokens TimC offered his assistance with any web development Review the next triage rotation Let's get the GCT tarballs onto the repo host this week Next doc focus 8/2 1PM CDT Discussion BrianL (AI) - promote SciTokens ticket to testing, assign squid ticket to Suchandra Derek (AI) - Make changes to repo.gridcf.org for the GCT tarballs Support Update LIGO (Edgar) - Some sites don't have secure CVMFS installed (it is difficult for site admins to test) OSU (BrianL, Mat) - inaccurate site topology and still running a GRAM gatekeeper GridUNESP (BrianL) - investigating N/A site records in the GRACC reporting that's suspected to be GridUNESP sites OSG Release Team 3.4.16 Status 4 -5 Open 0 -1 In Progress 4 +3 Ready for Testing 0 +0 Ready for Release 8 -3 Total OSG 3.4.16 Testing XRootD 4.8.4 Frontier Squid Ready for Release Nothing Yet Data Testing cilogon-openid-ca vo-client-dcache Ready for Release Nothing yet GOC Testing Drop mirror.batlab.org from mirror list Ready for Release Nothing yet Discussion None this week OSG Investigations Team Operations Moving hosts from Anvil cloud host at Nebraska for Anvil downtime Lots of hosts moved already. Hopefully, without any noticable downtime GRACC head node is moving today (data nodes were moved 2 weeks ago). May see small downtimes (but will be up at the end of the day) Was able keep the same IPs for the central collectors, CE's automatically picked up the (non)change and reported to the new collectors. Need to start operations meetings again. TODO: Discussion of SLA's, availability metrics Puppetize: ~~Collectors~~, GRACC Investigations In Progress: Discussions with with Internet2 to place StashCache in connection points. Ongoing GRACC Project StashCache Project Discussions None this week","title":"July 23, 2018"},{"location":"meetings/2018/TechArea20180723/#osg-technology-area-meeting-23-july-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Mat, Suchandra","title":"OSG Technology Area Meeting, 23 July 2018"},{"location":"meetings/2018/TechArea20180723/#announcements","text":"TimT OOO, returning Wednesday BrianL out next Monday","title":"Announcements"},{"location":"meetings/2018/TechArea20180723/#triage-duty","text":"This week: Edgar Next week: Mat 7 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180723/#jira","text":"# of tickets State 135 +3 Open 16 -3 In Progress 5 +3 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180723/#osg-software-team","text":"OSG 3.4.16: XRootD 4.8.4, Frontier Squid 3.5.27-5.1, and SciTokens TimC offered his assistance with any web development Review the next triage rotation Let's get the GCT tarballs onto the repo host this week Next doc focus 8/2 1PM CDT","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180723/#discussion","text":"BrianL (AI) - promote SciTokens ticket to testing, assign squid ticket to Suchandra Derek (AI) - Make changes to repo.gridcf.org for the GCT tarballs","title":"Discussion"},{"location":"meetings/2018/TechArea20180723/#support-update","text":"LIGO (Edgar) - Some sites don't have secure CVMFS installed (it is difficult for site admins to test) OSU (BrianL, Mat) - inaccurate site topology and still running a GRAM gatekeeper GridUNESP (BrianL) - investigating N/A site records in the GRACC reporting that's suspected to be GridUNESP sites","title":"Support Update"},{"location":"meetings/2018/TechArea20180723/#osg-release-team","text":"3.4.16 Status 4 -5 Open 0 -1 In Progress 4 +3 Ready for Testing 0 +0 Ready for Release 8 -3 Total OSG 3.4.16 Testing XRootD 4.8.4 Frontier Squid Ready for Release Nothing Yet Data Testing cilogon-openid-ca vo-client-dcache Ready for Release Nothing yet GOC Testing Drop mirror.batlab.org from mirror list Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180723/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180723/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180723/#operations","text":"Moving hosts from Anvil cloud host at Nebraska for Anvil downtime Lots of hosts moved already. Hopefully, without any noticable downtime GRACC head node is moving today (data nodes were moved 2 weeks ago). May see small downtimes (but will be up at the end of the day) Was able keep the same IPs for the central collectors, CE's automatically picked up the (non)change and reported to the new collectors. Need to start operations meetings again. TODO: Discussion of SLA's, availability metrics Puppetize: ~~Collectors~~, GRACC","title":"Operations"},{"location":"meetings/2018/TechArea20180723/#investigations","text":"In Progress: Discussions with with Internet2 to place StashCache in connection points.","title":"Investigations"},{"location":"meetings/2018/TechArea20180723/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180723/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180730/","text":"OSG Technology Area Meeting, 30 July 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Mat, TimT, Carl, Suchandra, Edgar, Derek, Marian, BrianB Announcements Madison folks will be out on Friday Triage Duty This week: TimT Next week: Carl 7 (+0) open tickets JIRA # of tickets State 136 +1 Open 17 +1 In Progress 7 +2 Ready for Testing 2 +2 Ready for Release OSG Software Team OSG 3.4.16: most ready for testing Next doc focus 8/2 (this Thursday) 1PM CDT Review the next triage rotation SSH config is set up for GCT repo host; Mat to handle deploying the tarballs Discussion (none) Support Update Arizona State University wants to get a CE up and running and joining OSG Suchandra pointed them at site reg info and contacting factory ops. Might also need a VO Florida: sent them patched SLURM Gratia probe; will wait a week to see if it fixes their issues OSG Release Team 3.4.16 Status 1 -3 Open 0 +0 In Progress 1 -1 Ready for Testing 2 +2 Ready for Release 6 -2 Total OSG 3.4.16 Testing XRootD 4.8.4 SciTokens 1.2.0 Ready for Release XRootD 4.8.4 Frontier Squid Data Testing cilogon-openid-ca Ready for Release Nothing yet GOC Testing gracc-summary gracc-archive Drop mirror.batlab.org from mirror list Ready for Release Nothing yet Discussion SLURM Gratia probe punted to next release Need to build BLAHP against HTCondor 8.7.9 Madison ITB site not getting GlideIns OSG Investigations Team Operations Anvil is now up; we will be \"moving\" our services back (actually building new machines via Puppet to test Ops meetings start this week Stashcache meetings will resume starting next week (Thursdays at 1 PM) Investigations In Progress: Discussions with with Internet2 to place StashCache in connection points. Starting up with new project called Harvester Ongoing GRACC Project StashCache Project Discussions None this week","title":"July 30, 2018"},{"location":"meetings/2018/TechArea20180730/#osg-technology-area-meeting-30-july-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Mat, TimT, Carl, Suchandra, Edgar, Derek, Marian, BrianB","title":"OSG Technology Area Meeting, 30 July 2018"},{"location":"meetings/2018/TechArea20180730/#announcements","text":"Madison folks will be out on Friday","title":"Announcements"},{"location":"meetings/2018/TechArea20180730/#triage-duty","text":"This week: TimT Next week: Carl 7 (+0) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180730/#jira","text":"# of tickets State 136 +1 Open 17 +1 In Progress 7 +2 Ready for Testing 2 +2 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180730/#osg-software-team","text":"OSG 3.4.16: most ready for testing Next doc focus 8/2 (this Thursday) 1PM CDT Review the next triage rotation SSH config is set up for GCT repo host; Mat to handle deploying the tarballs","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180730/#discussion","text":"(none)","title":"Discussion"},{"location":"meetings/2018/TechArea20180730/#support-update","text":"Arizona State University wants to get a CE up and running and joining OSG Suchandra pointed them at site reg info and contacting factory ops. Might also need a VO Florida: sent them patched SLURM Gratia probe; will wait a week to see if it fixes their issues","title":"Support Update"},{"location":"meetings/2018/TechArea20180730/#osg-release-team","text":"3.4.16 Status 1 -3 Open 0 +0 In Progress 1 -1 Ready for Testing 2 +2 Ready for Release 6 -2 Total OSG 3.4.16 Testing XRootD 4.8.4 SciTokens 1.2.0 Ready for Release XRootD 4.8.4 Frontier Squid Data Testing cilogon-openid-ca Ready for Release Nothing yet GOC Testing gracc-summary gracc-archive Drop mirror.batlab.org from mirror list Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180730/#discussion_1","text":"SLURM Gratia probe punted to next release Need to build BLAHP against HTCondor 8.7.9 Madison ITB site not getting GlideIns","title":"Discussion"},{"location":"meetings/2018/TechArea20180730/#osg-investigations-team","text":"","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180730/#operations","text":"Anvil is now up; we will be \"moving\" our services back (actually building new machines via Puppet to test Ops meetings start this week Stashcache meetings will resume starting next week (Thursdays at 1 PM)","title":"Operations"},{"location":"meetings/2018/TechArea20180730/#investigations","text":"In Progress: Discussions with with Internet2 to place StashCache in connection points. Starting up with new project called Harvester","title":"Investigations"},{"location":"meetings/2018/TechArea20180730/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180730/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180806/","text":"OSG Technology Area Meeting, 6 August 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements Edgar at NRP meeting until Wednesday Marian out of office the rest of the week Triage Duty This week: Carl Next week: Edgar 9 (+2) open tickets JIRA # of tickets State 131 -5 Open 22 +5 In Progress 7 +0 Ready for Testing 0 -2 Ready for Release OSG Software Team OSG 3.4.17 HTCondor 8.6.12 and 8.7.9 ready to be built Singularity 2.6.0 released last Friday Doc focus: quite a few tickets still In Progress, please get PRs in for them this week RSV test failures in the nightlies due to issues with osg-test We need to contact a few admins of unregistered CEs that are reporting to the GRACC Discussion None this week Support Update AGLT2, Purdue, Lehigh (Mat, BrianL): Topology, contact, and downtime updates Central Collector (Derek): There are several reported missing CE's not reporting to the central collectors. WLCG sent a list of collectors they expect to see. None of the CE's on the list where on the transition list, so they didn't report to the old collector, or they were reporting to the new collector already but have since disappeared.transition, either) GridUNESP (BrianL): Registered their second CE to fix GRACC numbers UFL (BrianB): supporting Bockjoo with multi-user XRootD setup OSG Release Team 3.4.17 Status 9 +9 Open 1 +1 In Progress 2 +2 Ready for Testing 0 +0 Ready for Release 12 +12 Total OSG 3.4.17 Testing Gratia SLURM probe cvmfs-x509-helper leaks 2 file descriptors per fetch Ready for Release Nothing yet Data Testing cilogon-openid-ca Ready for Release Nothing yet GOC Testing gracc-summary gracc-archive gracc-request Drop mirror.batlab.org from mirror list Ready for Release Nothing yet Discussion Madison ITB site still not getting GlideIns OSG Investigations Team In Progress: Discussions with with Internet2 to place StashCache in connection points. Shepherding the xrootd plugins through the release process. The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Operations No longer reporting since operations has it's own call now. Ongoing GRACC Project StashCache Project Discussions None this week","title":"August 6, 2018"},{"location":"meetings/2018/TechArea20180806/#osg-technology-area-meeting-6-august-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting,  6 August 2018"},{"location":"meetings/2018/TechArea20180806/#announcements","text":"Edgar at NRP meeting until Wednesday Marian out of office the rest of the week","title":"Announcements"},{"location":"meetings/2018/TechArea20180806/#triage-duty","text":"This week: Carl Next week: Edgar 9 (+2) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180806/#jira","text":"# of tickets State 131 -5 Open 22 +5 In Progress 7 +0 Ready for Testing 0 -2 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180806/#osg-software-team","text":"OSG 3.4.17 HTCondor 8.6.12 and 8.7.9 ready to be built Singularity 2.6.0 released last Friday Doc focus: quite a few tickets still In Progress, please get PRs in for them this week RSV test failures in the nightlies due to issues with osg-test We need to contact a few admins of unregistered CEs that are reporting to the GRACC","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180806/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180806/#support-update","text":"AGLT2, Purdue, Lehigh (Mat, BrianL): Topology, contact, and downtime updates Central Collector (Derek): There are several reported missing CE's not reporting to the central collectors. WLCG sent a list of collectors they expect to see. None of the CE's on the list where on the transition list, so they didn't report to the old collector, or they were reporting to the new collector already but have since disappeared.transition, either) GridUNESP (BrianL): Registered their second CE to fix GRACC numbers UFL (BrianB): supporting Bockjoo with multi-user XRootD setup","title":"Support Update"},{"location":"meetings/2018/TechArea20180806/#osg-release-team","text":"3.4.17 Status 9 +9 Open 1 +1 In Progress 2 +2 Ready for Testing 0 +0 Ready for Release 12 +12 Total OSG 3.4.17 Testing Gratia SLURM probe cvmfs-x509-helper leaks 2 file descriptors per fetch Ready for Release Nothing yet Data Testing cilogon-openid-ca Ready for Release Nothing yet GOC Testing gracc-summary gracc-archive gracc-request Drop mirror.batlab.org from mirror list Ready for Release Nothing yet","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180806/#discussion_1","text":"Madison ITB site still not getting GlideIns","title":"Discussion"},{"location":"meetings/2018/TechArea20180806/#osg-investigations-team","text":"In Progress: Discussions with with Internet2 to place StashCache in connection points. Shepherding the xrootd plugins through the release process. The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180806/#operations","text":"No longer reporting since operations has it's own call now.","title":"Operations"},{"location":"meetings/2018/TechArea20180806/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180806/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180813/","text":"OSG Technology Area Meeting, 13 August 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements Triage Duty This week: Edgar Next week: BrianL 6 (-3) open tickets JIRA # of tickets State 128 -3 Open 21 -1 In Progress 9 +2 Ready for Testing 2 +2 Ready for Release OSG Software Team Update your Y7 effort numbers OSG 3.4.17 packages not yet RFT: htcondor-ce, osg-test, xrootd-multiuser The following hosts are uploading \"Batch\" (i.e. pilot) Gratia records and are not registered in the OSG topology osg-gw-6.t2.ucsd.edu-COMET aragon.cyverse.org fsurf.osgconnect.net (should be uploading Payload records?) sugwg-scitokens.phy.syr.edu login03.osgconnect.net (should be uploading Payload records?) Doc focus remaining tickets: Document usage and examples of topology scripts (TimT) Document topology deployment configuration and instructions (Derek) Enable XRootD over HTTP (Edgar) Review the topology registration doc (BrianL) Review HTCondor-CE submission doc (Suchandra) fetch-crl EL7 failures in the nightlies Discussion BrianL (AI) Tag HTCondor-CE release, Mat to build Talk to Mats about Cyverse Pick a victim for fetch-crl EL7 test failures Derek (AI) Build xrootd-multiuser today, contact Bockjoo for testing Working with Duncan to register the Syracuse CE Suchandra (AI) PR for osg-test packaging changes today PR for HTCondor-CE submission doc review tomorrow TimT (AI): Update topology script documentation along with the release Support Update Arizona State (BrianL, Mat, Suchandra): ironing out the appropriate setup for job submission and organizing a phone call Harvard (BrianL): MCORE jobs aren't getting routed Topology support (BrianL, Carl, Mat): miscellaneous contact and topology updates UConn (Suchandra): ongoing issue with getting pilots New Hosted CEs (Suchandra): New Mexico State University and the University of South Florida with a main cluster is 8k cores. North Dakota State recently came back to test opportunistic usage load on their network. They plan to run tests for a month or two. Long-term they will be setting up a new cluster in December/January to run opportunistically. OSG Release Team 3.4.17 Status 5 -4 Open 2 +2 In Progress 7 +5 Ready for Testing 0 +0 Ready for Release 12 +3 Total OSG 3.4.17 Testing Singularity 2.6.0 Gratia SLURM probe cvmfs-x509-helper leaks 2 file descriptors per fetch HTCondor 8.6.12 HTCondor 8.7.9 Pegasus 4.8.3 xrootd-lcmaps 1.40 Ready for Release Nothing yet Data Testing Nothing yet Ready for Release Nothing yet GOC Testing Drop mirror.batlab.org from mirror list Ready for Release gracc-summary gracc-archive gracc-request Discussion BrianL (AI): Help find cvmfs-x509-helper testers Edgar (AI): Madison ITB site still not getting GlideIns OSG Investigations Team In Progress: XRootD documentation and docker overhaul is ongoing. Organized in StashCache in collaboration with Slate and NRP (PRP) collegues. Discussions with with Internet2 to place StashCache in connection points. Shepherding the xrootd plugins through the release process. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Ongoing GRACC Project StashCache Project Discussions Derek (AI): Verify rsync service functionality for JINR mirror","title":"August 13, 2018"},{"location":"meetings/2018/TechArea20180813/#osg-technology-area-meeting-13-august-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 13 August 2018"},{"location":"meetings/2018/TechArea20180813/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180813/#triage-duty","text":"This week: Edgar Next week: BrianL 6 (-3) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180813/#jira","text":"# of tickets State 128 -3 Open 21 -1 In Progress 9 +2 Ready for Testing 2 +2 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180813/#osg-software-team","text":"Update your Y7 effort numbers OSG 3.4.17 packages not yet RFT: htcondor-ce, osg-test, xrootd-multiuser The following hosts are uploading \"Batch\" (i.e. pilot) Gratia records and are not registered in the OSG topology osg-gw-6.t2.ucsd.edu-COMET aragon.cyverse.org fsurf.osgconnect.net (should be uploading Payload records?) sugwg-scitokens.phy.syr.edu login03.osgconnect.net (should be uploading Payload records?) Doc focus remaining tickets: Document usage and examples of topology scripts (TimT) Document topology deployment configuration and instructions (Derek) Enable XRootD over HTTP (Edgar) Review the topology registration doc (BrianL) Review HTCondor-CE submission doc (Suchandra) fetch-crl EL7 failures in the nightlies","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180813/#discussion","text":"BrianL (AI) Tag HTCondor-CE release, Mat to build Talk to Mats about Cyverse Pick a victim for fetch-crl EL7 test failures Derek (AI) Build xrootd-multiuser today, contact Bockjoo for testing Working with Duncan to register the Syracuse CE Suchandra (AI) PR for osg-test packaging changes today PR for HTCondor-CE submission doc review tomorrow TimT (AI): Update topology script documentation along with the release","title":"Discussion"},{"location":"meetings/2018/TechArea20180813/#support-update","text":"Arizona State (BrianL, Mat, Suchandra): ironing out the appropriate setup for job submission and organizing a phone call Harvard (BrianL): MCORE jobs aren't getting routed Topology support (BrianL, Carl, Mat): miscellaneous contact and topology updates UConn (Suchandra): ongoing issue with getting pilots New Hosted CEs (Suchandra): New Mexico State University and the University of South Florida with a main cluster is 8k cores. North Dakota State recently came back to test opportunistic usage load on their network. They plan to run tests for a month or two. Long-term they will be setting up a new cluster in December/January to run opportunistically.","title":"Support Update"},{"location":"meetings/2018/TechArea20180813/#osg-release-team","text":"3.4.17 Status 5 -4 Open 2 +2 In Progress 7 +5 Ready for Testing 0 +0 Ready for Release 12 +3 Total OSG 3.4.17 Testing Singularity 2.6.0 Gratia SLURM probe cvmfs-x509-helper leaks 2 file descriptors per fetch HTCondor 8.6.12 HTCondor 8.7.9 Pegasus 4.8.3 xrootd-lcmaps 1.40 Ready for Release Nothing yet Data Testing Nothing yet Ready for Release Nothing yet GOC Testing Drop mirror.batlab.org from mirror list Ready for Release gracc-summary gracc-archive gracc-request","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180813/#discussion_1","text":"BrianL (AI): Help find cvmfs-x509-helper testers Edgar (AI): Madison ITB site still not getting GlideIns","title":"Discussion"},{"location":"meetings/2018/TechArea20180813/#osg-investigations-team","text":"In Progress: XRootD documentation and docker overhaul is ongoing. Organized in StashCache in collaboration with Slate and NRP (PRP) collegues. Discussions with with Internet2 to place StashCache in connection points. Shepherding the xrootd plugins through the release process. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180813/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180813/#discussions","text":"Derek (AI): Verify rsync service functionality for JINR mirror","title":"Discussions"},{"location":"meetings/2018/TechArea20180820/","text":"OSG Technology Area Meeting, 21 August 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Suchandra Announcements Triage Duty This week: BrianL Next week: Suchandra 9 (+3) open tickets JIRA # of tickets State 130 +2 Open 16 -5 In Progress 1 -8 Ready for Testing 0 -2 Ready for Release OSG Software Team Update your Y7 effort numbers The following hosts are uploading \"Batch\" (i.e. pilot) Gratia records and are not registered in the OSG topology aragon.cyverse.org (Mats) sugwg-scitokens.phy.syr.edu (Suchandra) Doc focus remaining tickets: Document topology deployment configuration and instructions (Derek) Review the topology registration doc (BrianL) Review HTCondor-CE submission doc (Suchandra) Discussion Repeat security announcements were sent last week due to osg-notify being run on a misconfigured host. Documentation has been updated to clearly state these requirements and additional safeties are to be added to warn users of misconfigured hosts. Support Update Arizona State (BrianL, Suchandra): assisted with CE troubleshooting but we're blocked on reverse DNS issues so we spun up a hosted CE in the meantime Topology support (BrianL, Carl, Mat): miscellaneous contact and topology updates Hosted CEs (Suchandra): USF hosted CE up and running, New Mexico State should be up and running today UConn (Edgar, Suchandra): glidein startds try to report using CCB but are delayed 5 min. Troubleshooting with the HTCondor team. UNL (Marian): xrootd-lcmaps issue discovered, PR submitted, and we're waiting on a new build OSG Release Team OSG 3.4.17 went out last week without a hitch Issues with glidein3 were due to an old HTCondor version and has been resolved Discussion None this week OSG Investigations Team In Progress: XRootD documentation and docker overhaul is ongoing. Organized in StashCache in collaboration with Slate and NRP (PRP) collegues. Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Ongoing GRACC Project StashCache Project Discussions rsync service on the repo hosts have been puppetized but still need to be added to check mk . UNL admins to manually verify rsync functionality after the repo move. Derek (AI): Update the condor mapfile on collector2 to match collector1","title":"August 20, 2018"},{"location":"meetings/2018/TechArea20180820/#osg-technology-area-meeting-21-august-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Derek, Edgar, Marian, Suchandra","title":"OSG Technology Area Meeting, 21 August 2018"},{"location":"meetings/2018/TechArea20180820/#announcements","text":"","title":"Announcements"},{"location":"meetings/2018/TechArea20180820/#triage-duty","text":"This week: BrianL Next week: Suchandra 9 (+3) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180820/#jira","text":"# of tickets State 130 +2 Open 16 -5 In Progress 1 -8 Ready for Testing 0 -2 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180820/#osg-software-team","text":"Update your Y7 effort numbers The following hosts are uploading \"Batch\" (i.e. pilot) Gratia records and are not registered in the OSG topology aragon.cyverse.org (Mats) sugwg-scitokens.phy.syr.edu (Suchandra) Doc focus remaining tickets: Document topology deployment configuration and instructions (Derek) Review the topology registration doc (BrianL) Review HTCondor-CE submission doc (Suchandra)","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180820/#discussion","text":"Repeat security announcements were sent last week due to osg-notify being run on a misconfigured host. Documentation has been updated to clearly state these requirements and additional safeties are to be added to warn users of misconfigured hosts.","title":"Discussion"},{"location":"meetings/2018/TechArea20180820/#support-update","text":"Arizona State (BrianL, Suchandra): assisted with CE troubleshooting but we're blocked on reverse DNS issues so we spun up a hosted CE in the meantime Topology support (BrianL, Carl, Mat): miscellaneous contact and topology updates Hosted CEs (Suchandra): USF hosted CE up and running, New Mexico State should be up and running today UConn (Edgar, Suchandra): glidein startds try to report using CCB but are delayed 5 min. Troubleshooting with the HTCondor team. UNL (Marian): xrootd-lcmaps issue discovered, PR submitted, and we're waiting on a new build","title":"Support Update"},{"location":"meetings/2018/TechArea20180820/#osg-release-team","text":"OSG 3.4.17 went out last week without a hitch Issues with glidein3 were due to an old HTCondor version and has been resolved","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180820/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180820/#osg-investigations-team","text":"In Progress: XRootD documentation and docker overhaul is ongoing. Organized in StashCache in collaboration with Slate and NRP (PRP) collegues. Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180820/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180820/#discussions","text":"rsync service on the repo hosts have been puppetized but still need to be added to check mk . UNL admins to manually verify rsync functionality after the repo move. Derek (AI): Update the condor mapfile on collector2 to match collector1","title":"Discussions"},{"location":"meetings/2018/TechArea20180827/","text":"OSG Technology Area Meeting, 27 August 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Marco Mambelli, Mat Announcements 2018-09-03 meeting canceled (Labor Day) TimT OOO until Thursday Suchandra OOO until Wednesday afternoon Triage Duty This week: Mat Next week: Suchandra 5 (-4) open tickets JIRA # of tickets State 130 +0 Open 19 +3 In Progress 4 +3 Ready for Testing 0 +0 Ready for Release OSG Software Team AI (Suchandra, TimT): Update your Y7 effort numbers AI (Derek): sugwg-scitokens.phy.syr.edu is uploading pilot records is unregistered in the topology A few submit hosts are uploading pilot records. Why? How does the Gratia probe decide what kind of record to upload? User support documentation Discussion AI (Carl, Edgar): Review the user support Gratia configuration AI (Mat): request GlideinWMS 3.4.0 promotion so that Marco can test it Support Update Arizona State (BrianL, Suchandra): consistently running 240 pilots through their hosted CE. Reverse DNS still an issue for their own CE. Cyverse (Mat) - aragon submit host topology registration LIGO (Edgar) - performance diffs between reading lots of small files vs big files of the same total size. They are seeing 10x the number of read failures for the small files case Harvard (BrianL): still having issues with multicore pilots and their CE UC Davis (BrianL): mixed CE and backend HTCondor configuration. Documentation updated to clarify the difference between a CE and backend HTCondor. UFL (BrianL, Edgar): xrootd-lcmaps still doesn't respect the policy option OSG Investigations Team In Progress: XRootD documentation and docker overhaul is ongoing. Organized in StashCache in collaboration with Slate and NRP (PRP) collegues. Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Topology-itb server certificate failed last week. Fixed by Mat, but underlying issue is that puppet is failing on topology-itb, or turned off. Ongoing GRACC Project StashCache Project Discussions rsync service on the repo hosts have been puppetized but still need to be added to check mk . UNL admins to manually verify rsync functionality after the repo move. AI (BrianL): Give BrianB a link to the XRootD use case doc","title":"August 27, 2018"},{"location":"meetings/2018/TechArea20180827/#osg-technology-area-meeting-27-august-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianB, BrianL, Carl, Derek, Edgar, Marian, Marco Mambelli, Mat","title":"OSG Technology Area Meeting, 27 August 2018"},{"location":"meetings/2018/TechArea20180827/#announcements","text":"2018-09-03 meeting canceled (Labor Day) TimT OOO until Thursday Suchandra OOO until Wednesday afternoon","title":"Announcements"},{"location":"meetings/2018/TechArea20180827/#triage-duty","text":"This week: Mat Next week: Suchandra 5 (-4) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180827/#jira","text":"# of tickets State 130 +0 Open 19 +3 In Progress 4 +3 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180827/#osg-software-team","text":"AI (Suchandra, TimT): Update your Y7 effort numbers AI (Derek): sugwg-scitokens.phy.syr.edu is uploading pilot records is unregistered in the topology A few submit hosts are uploading pilot records. Why? How does the Gratia probe decide what kind of record to upload? User support documentation","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180827/#discussion","text":"AI (Carl, Edgar): Review the user support Gratia configuration AI (Mat): request GlideinWMS 3.4.0 promotion so that Marco can test it","title":"Discussion"},{"location":"meetings/2018/TechArea20180827/#support-update","text":"Arizona State (BrianL, Suchandra): consistently running 240 pilots through their hosted CE. Reverse DNS still an issue for their own CE. Cyverse (Mat) - aragon submit host topology registration LIGO (Edgar) - performance diffs between reading lots of small files vs big files of the same total size. They are seeing 10x the number of read failures for the small files case Harvard (BrianL): still having issues with multicore pilots and their CE UC Davis (BrianL): mixed CE and backend HTCondor configuration. Documentation updated to clarify the difference between a CE and backend HTCondor. UFL (BrianL, Edgar): xrootd-lcmaps still doesn't respect the policy option","title":"Support Update"},{"location":"meetings/2018/TechArea20180827/#osg-investigations-team","text":"In Progress: XRootD documentation and docker overhaul is ongoing. Organized in StashCache in collaboration with Slate and NRP (PRP) collegues. Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Topology-itb server certificate failed last week. Fixed by Mat, but underlying issue is that puppet is failing on topology-itb, or turned off.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180827/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180827/#discussions","text":"rsync service on the repo hosts have been puppetized but still need to be added to check mk . UNL admins to manually verify rsync functionality after the repo move. AI (BrianL): Give BrianB a link to the XRootD use case doc","title":"Discussions"},{"location":"meetings/2018/TechArea20180910/","text":"OSG Technology Area Meeting, 10 September 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT Announcements IRIS-HEP officially announced on Tuesday (https://www.nsf.gov/news/news_summ.jsp?cntn_id=296456 org=NSF from=news) Triage Duty This week: BrianL Next week: Mat? 4 (-1) open tickets JIRA # of tickets State 131 +1 Open 27 +4 In Progress 7 +1 Ready for Testing 0 +0 Ready for Release OSG Software Team Remember to add your outages to the OSG Software calendar When replying to Freshdesk tickets, add yourself as a watcher! AI (Suchandra, TimT): Update your Y7 effort numbers OSG 3.4.18 AI (Marian) xrootd-hdfs-2.1.2 build? AI (Carl): xrootd-lcmaps troubleshooting and build AI (Carl): kick off VMU tests against branch that supports xrootd-lcmaps 1.4.0 AI (Suchandra): kick off VMU tests against the new xrootd build AI: blahp, osg-test, htcondor-ce builds AI (Derek): sugwg-scitokens.phy.syr.edu is uploading pilot records is unregistered in the topology Discussion None this week Support Update Harvard (BrianL): still having issues with multicore pilots and their CE. Doesn't seem to be an HTCondor-CE issue at this point. Topology (BrianL, Mat): fixup Purdue downtime, various downtime and contact registrations OSG Release Team 3.4.18 Status 6 +4 Open 9 +6 In Progress 4 +1 Ready for Testing 1 +1 Ready for Release 20 +12 Total OSG 3.4.18 Testing GlideinWMS 3.4 in OSG 3.4 Gratia SLURM probe fails to log exceptions Fix \"GRACC server not responding\" warnings in RSV Gratia Slurm fails to log probe exceptions tarball-client: remove cog-jglobus-axis workaround Ready for Release CVMFS 2.5.1 Data Testing Update LSC and vomses for SuperCDMS and LSST Ready for Release Create vo-client-dcache GOC Testing Nothing Ready for Release Nothing Discussion Probably do a data release this week Probably do a software (3.4.18) release next week AI (TimT): get approval from Alessandra or Wei on the LSST vo-client updates AI (Carl): find out the original reporter of the GRACC-RSV issue AI (Edgar): test GlideinWMS 3.4.0 OSG Investigations Team In Progress: Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Ongoing GRACC Project StashCache Project Discussions None this week","title":"September 10, 2018"},{"location":"meetings/2018/TechArea20180910/#osg-technology-area-meeting-10-september-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT","title":"OSG Technology Area Meeting, 10 September 2018"},{"location":"meetings/2018/TechArea20180910/#announcements","text":"IRIS-HEP officially announced on Tuesday (https://www.nsf.gov/news/news_summ.jsp?cntn_id=296456 org=NSF from=news)","title":"Announcements"},{"location":"meetings/2018/TechArea20180910/#triage-duty","text":"This week: BrianL Next week: Mat? 4 (-1) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180910/#jira","text":"# of tickets State 131 +1 Open 27 +4 In Progress 7 +1 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180910/#osg-software-team","text":"Remember to add your outages to the OSG Software calendar When replying to Freshdesk tickets, add yourself as a watcher! AI (Suchandra, TimT): Update your Y7 effort numbers OSG 3.4.18 AI (Marian) xrootd-hdfs-2.1.2 build? AI (Carl): xrootd-lcmaps troubleshooting and build AI (Carl): kick off VMU tests against branch that supports xrootd-lcmaps 1.4.0 AI (Suchandra): kick off VMU tests against the new xrootd build AI: blahp, osg-test, htcondor-ce builds AI (Derek): sugwg-scitokens.phy.syr.edu is uploading pilot records is unregistered in the topology","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180910/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180910/#support-update","text":"Harvard (BrianL): still having issues with multicore pilots and their CE. Doesn't seem to be an HTCondor-CE issue at this point. Topology (BrianL, Mat): fixup Purdue downtime, various downtime and contact registrations","title":"Support Update"},{"location":"meetings/2018/TechArea20180910/#osg-release-team","text":"3.4.18 Status 6 +4 Open 9 +6 In Progress 4 +1 Ready for Testing 1 +1 Ready for Release 20 +12 Total OSG 3.4.18 Testing GlideinWMS 3.4 in OSG 3.4 Gratia SLURM probe fails to log exceptions Fix \"GRACC server not responding\" warnings in RSV Gratia Slurm fails to log probe exceptions tarball-client: remove cog-jglobus-axis workaround Ready for Release CVMFS 2.5.1 Data Testing Update LSC and vomses for SuperCDMS and LSST Ready for Release Create vo-client-dcache GOC Testing Nothing Ready for Release Nothing","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180910/#discussion_1","text":"Probably do a data release this week Probably do a software (3.4.18) release next week AI (TimT): get approval from Alessandra or Wei on the LSST vo-client updates AI (Carl): find out the original reporter of the GRACC-RSV issue AI (Edgar): test GlideinWMS 3.4.0","title":"Discussion"},{"location":"meetings/2018/TechArea20180910/#osg-investigations-team","text":"In Progress: Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180910/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180910/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180917/","text":"OSG Technology Area Meeting, 17 September 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marco, Marian, Mat, Suchandra Announcements Suchandra's last full day is this Friday, 9/21, and he will be half time until 10/12 Triage Duty This week: Mat Next week: Edgar 6 (+2) open tickets JIRA # of tickets State 134 +3 Open 21 -6 In Progress 11 +4 Ready for Testing 4 +4 Ready for Release OSG Software Team Mat will act as interim software manager between 9/24 and 10/12 AI (Carl, Edgar, TimT): Familiarize yourselves with the topology registration documentation OSG 3.4.18 AI (Suchandra): make an XRootD promotion request AI (Carl): Build xrootd-lcmaps and osg-test AI (TimT): Update your Y7 effort numbers Discussion None this week Support Update GlideinWMS factory entry cleanup (Derek): working on tracking down CEs that aren't reporting to the central collector and removing factory entries if appropriate LIGO (Edgar): non-functional VMs starting on Comet and Edgar worked with Comet to resolve the issue and refund LIGO's allocation Topology (BrianL, Mat): Registered various contact GitHub IDs and downtimes OSG Release Team 3.4.18 Status 1 -5 Open 6 -3 In Progress 11 +7 Ready for Testing 4 +3 Ready for Release 22 +2 Total OSG 3.4.18 Testing HTCondor-CE 3.1.4 (Suchandra) xrootd-hdfs 2.1.3 (Marian/Carl) Pegasus 4.8.4 (Suchandra) GridFTP 12.9-1.1 and GridFTP server control 7.0 (Edgar) Gratia Probe 1.20.7 (Edgar) tarball-client: remove cog-jglobus-axis workaround (TimT) Ready for Release CVMFS 2.5.1 GlideinWMS 3.4 RSV 3.19.8 Data Testing Nothing Ready for Release Nothing GOC Testing Nothing Ready for Release Nothing Discussion None this week OSG Investigations Team In Progress: Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Ongoing GRACC Project StashCache Project Discussions None this week","title":"September 17, 2018"},{"location":"meetings/2018/TechArea20180917/#osg-technology-area-meeting-17-september-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Derek, Edgar, Marco, Marian, Mat, Suchandra","title":"OSG Technology Area Meeting, 17 September 2018"},{"location":"meetings/2018/TechArea20180917/#announcements","text":"Suchandra's last full day is this Friday, 9/21, and he will be half time until 10/12","title":"Announcements"},{"location":"meetings/2018/TechArea20180917/#triage-duty","text":"This week: Mat Next week: Edgar 6 (+2) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180917/#jira","text":"# of tickets State 134 +3 Open 21 -6 In Progress 11 +4 Ready for Testing 4 +4 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180917/#osg-software-team","text":"Mat will act as interim software manager between 9/24 and 10/12 AI (Carl, Edgar, TimT): Familiarize yourselves with the topology registration documentation OSG 3.4.18 AI (Suchandra): make an XRootD promotion request AI (Carl): Build xrootd-lcmaps and osg-test AI (TimT): Update your Y7 effort numbers","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180917/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180917/#support-update","text":"GlideinWMS factory entry cleanup (Derek): working on tracking down CEs that aren't reporting to the central collector and removing factory entries if appropriate LIGO (Edgar): non-functional VMs starting on Comet and Edgar worked with Comet to resolve the issue and refund LIGO's allocation Topology (BrianL, Mat): Registered various contact GitHub IDs and downtimes","title":"Support Update"},{"location":"meetings/2018/TechArea20180917/#osg-release-team","text":"3.4.18 Status 1 -5 Open 6 -3 In Progress 11 +7 Ready for Testing 4 +3 Ready for Release 22 +2 Total OSG 3.4.18 Testing HTCondor-CE 3.1.4 (Suchandra) xrootd-hdfs 2.1.3 (Marian/Carl) Pegasus 4.8.4 (Suchandra) GridFTP 12.9-1.1 and GridFTP server control 7.0 (Edgar) Gratia Probe 1.20.7 (Edgar) tarball-client: remove cog-jglobus-axis workaround (TimT) Ready for Release CVMFS 2.5.1 GlideinWMS 3.4 RSV 3.19.8 Data Testing Nothing Ready for Release Nothing GOC Testing Nothing Ready for Release Nothing","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180917/#discussion_1","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180917/#osg-investigations-team","text":"In Progress: Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180917/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180917/#discussions","text":"None this week","title":"Discussions"},{"location":"meetings/2018/TechArea20180924/","text":"OSG Technology Area Meeting, 24 September 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Mat, TimC, TimT, Edgar, Derek, Marian Announcements Both Brian and Mat out next week; TimT running meeting TimT will handle promotion requests. Triage Duty This week: Edgar Next week: Carl 3 (-3) open tickets JIRA # of tickets State 135 +1 Open 20 -1 In Progress 14 +3 Ready for Testing 5 +1 Ready for Release OSG Software Team Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . Carl is working on automation but is out all this week so it might take a while before it's ready. Discussions none Support Update Edgar working on a HTCondor-CE ticket for Rice but needs help; Marian may be able to help Marian helped MIT with XRootD set up to support HTTPS; hopefully successful; were able to upgrade 3 of 6 data server but they have a mix of OSG 3.3 and 3.4 OASIS Replica production update ran into issues due to Kernel problems so downtime lasted longer than expected. Should be up and running now. OSG Release Team 3.4.18 Status 0 -1 Open 0 -6 In Progress 14 +3 Ready for Testing 5 +1 Ready for Release 19 -3 Total OSG 3.4.18 - Release expected Thursday Testing HTTP patches for XRootD 4.8.4 HTCondor-CE 3.1.4 xrootd-hdfs 2.1.3 Pegasus 4.8.4 GridFTP 12.9-1.1 and GridFTP server control 7.0 Gratia Probe 1.20.7 tarball-client: remove cog-jglobus-axis workaround BLAHP: Proxy renewal in default configuration Fix \"GRACC server not responding\" warnings in RSV Ready for Release CVMFS 2.5.1 GlideinWMS 3.4 RSV 3.19.8 GridFTP 12.9-1.1 and GridFTP server control 7.0 Data IGTF 1.93 Released today, OSG release expected Wednesday Discussion None this week OSG Investigations Team In Progress: Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib StashCache documentation overhaul!!! StashCache packaging is also being modified to include auth, and less copy-paste! We will start sending CMS XRootD traffic to the WLCG file transfer monitoring service (MONIT), in development. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed. Ongoing GRACC Project StashCache Project Last Week FIXED : Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. Should be FIXED with new update!","title":"September 24, 2018"},{"location":"meetings/2018/TechArea20180924/#osg-technology-area-meeting-24-september-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Mat, TimC, TimT, Edgar, Derek, Marian","title":"OSG Technology Area Meeting, 24 September 2018"},{"location":"meetings/2018/TechArea20180924/#announcements","text":"Both Brian and Mat out next week; TimT running meeting TimT will handle promotion requests.","title":"Announcements"},{"location":"meetings/2018/TechArea20180924/#triage-duty","text":"This week: Edgar Next week: Carl 3 (-3) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20180924/#jira","text":"# of tickets State 135 +1 Open 20 -1 In Progress 14 +3 Ready for Testing 5 +1 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20180924/#osg-software-team","text":"Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . Carl is working on automation but is out all this week so it might take a while before it's ready.","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20180924/#discussions","text":"none","title":"Discussions"},{"location":"meetings/2018/TechArea20180924/#support-update","text":"Edgar working on a HTCondor-CE ticket for Rice but needs help; Marian may be able to help Marian helped MIT with XRootD set up to support HTTPS; hopefully successful; were able to upgrade 3 of 6 data server but they have a mix of OSG 3.3 and 3.4 OASIS Replica production update ran into issues due to Kernel problems so downtime lasted longer than expected. Should be up and running now.","title":"Support Update"},{"location":"meetings/2018/TechArea20180924/#osg-release-team","text":"3.4.18 Status 0 -1 Open 0 -6 In Progress 14 +3 Ready for Testing 5 +1 Ready for Release 19 -3 Total OSG 3.4.18 - Release expected Thursday Testing HTTP patches for XRootD 4.8.4 HTCondor-CE 3.1.4 xrootd-hdfs 2.1.3 Pegasus 4.8.4 GridFTP 12.9-1.1 and GridFTP server control 7.0 Gratia Probe 1.20.7 tarball-client: remove cog-jglobus-axis workaround BLAHP: Proxy renewal in default configuration Fix \"GRACC server not responding\" warnings in RSV Ready for Release CVMFS 2.5.1 GlideinWMS 3.4 RSV 3.19.8 GridFTP 12.9-1.1 and GridFTP server control 7.0 Data IGTF 1.93 Released today, OSG release expected Wednesday","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20180924/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20180924/#osg-investigations-team","text":"In Progress: Shepherding the xrootd plugins through the release process. xrootd-TPC - contrib xrootd-macaroons - contrib xrootd-scitokens - release x509-scitokens-issuer - contrib StashCache documentation overhaul!!! StashCache packaging is also being modified to include auth, and less copy-paste! We will start sending CMS XRootD traffic to the WLCG file transfer monitoring service (MONIT), in development. (Still ongoing) The repo server is going to move, once again. But there should be no noticable differences. If there is, then we failed.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20180924/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20180924/#last-week","text":"FIXED : Discussions with with Internet2 to place StashCache in connection points. Waiting on GeoIP issues to be worked out. Should be FIXED with new update!","title":"Last Week"},{"location":"meetings/2018/TechArea20181001/","text":"OSG Technology Area Meeting, 1 October 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, Derek, Marco, Suchandra, TimT Announcements Both Brian and Mat out this week TimT will handle promotion requests. Triage Duty This week: Carl Next week: Tim 6 (+3) open tickets JIRA # of tickets State 138 +3 Open 19 -1 In Progress 0 -14 Ready for Testing 0 -5 Ready for Release OSG Software Team Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . Carl is working on automation and it might take a while before it's ready. Discussions GlideinWWMS Marco thanks us for getting GlideinWMS 3.4 into 3.4 GlideinWMS 3.4.1 is being tested this week GlideinWMS 3.5 should go into upcoming with HTCondor 8.8 Support Update Nothing to report OSG Release Team 3.4.19 Status 5 +5 Open 4 +4 In Progress 0 +0 Ready for Testing 0 +0 Ready for Release 9 +9 Total OSG 3.4.19 Testing Nothing yet Ready for Release Nothing yet Data dteam VO Discussion None this week OSG Investigations Team In Progress: No report Ongoing GRACC Project StashCache Project","title":"October 1, 2018"},{"location":"meetings/2018/TechArea20181001/#osg-technology-area-meeting-1-october-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, Derek, Marco, Suchandra, TimT","title":"OSG Technology Area Meeting, 1 October 2018"},{"location":"meetings/2018/TechArea20181001/#announcements","text":"Both Brian and Mat out this week TimT will handle promotion requests.","title":"Announcements"},{"location":"meetings/2018/TechArea20181001/#triage-duty","text":"This week: Carl Next week: Tim 6 (+3) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20181001/#jira","text":"# of tickets State 138 +3 Open 19 -1 In Progress 0 -14 Ready for Testing 0 -5 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20181001/#osg-software-team","text":"Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . Carl is working on automation and it might take a while before it's ready.","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20181001/#discussions","text":"GlideinWWMS Marco thanks us for getting GlideinWMS 3.4 into 3.4 GlideinWMS 3.4.1 is being tested this week GlideinWMS 3.5 should go into upcoming with HTCondor 8.8","title":"Discussions"},{"location":"meetings/2018/TechArea20181001/#support-update","text":"Nothing to report","title":"Support Update"},{"location":"meetings/2018/TechArea20181001/#osg-release-team","text":"3.4.19 Status 5 +5 Open 4 +4 In Progress 0 +0 Ready for Testing 0 +0 Ready for Release 9 +9 Total OSG 3.4.19 Testing Nothing yet Ready for Release Nothing yet Data dteam VO","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20181001/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20181001/#osg-investigations-team","text":"In Progress: No report","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20181001/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20181008/","text":"OSG Technology Area Meeting, 8 October 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, Edgar, Marian, Suchandra, TimT Announcements Brian Lin returns next week. Mat returns tomorrow. Triage Duty This week: Tim Next week: BrianL 7 (+1) open tickets JIRA # of tickets State 138 +0 Open 17 -2 In Progress 0 +0 Ready for Testing 0 +0 Ready for Release OSG Software Team Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . Carl is working on automation and it might take a while before it's ready. Discussions Marian mentioned that he is working on XRootD 4.8.5 Support Update No update OSG Release Team 3.4.19 Status 5 +0 Open 4 +0 In Progress 0 +0 Ready for Testing 0 +0 Ready for Release 9 +9 Total OSG 3.4.19 Testing Nothing yet Ready for Release Nothing yet Data Nothing on the horizon Discussion None this week OSG Investigations Team No report Ongoing GRACC Project StashCache Project","title":"October 8, 2018"},{"location":"meetings/2018/TechArea20181008/#osg-technology-area-meeting-8-october-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: Carl, Edgar, Marian, Suchandra, TimT","title":"OSG Technology Area Meeting, 8 October 2018"},{"location":"meetings/2018/TechArea20181008/#announcements","text":"Brian Lin returns next week. Mat returns tomorrow.","title":"Announcements"},{"location":"meetings/2018/TechArea20181008/#triage-duty","text":"This week: Tim Next week: BrianL 7 (+1) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20181008/#jira","text":"# of tickets State 138 +0 Open 17 -2 In Progress 0 +0 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20181008/#osg-software-team","text":"Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . Carl is working on automation and it might take a while before it's ready.","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20181008/#discussions","text":"Marian mentioned that he is working on XRootD 4.8.5","title":"Discussions"},{"location":"meetings/2018/TechArea20181008/#support-update","text":"No update","title":"Support Update"},{"location":"meetings/2018/TechArea20181008/#osg-release-team","text":"3.4.19 Status 5 +0 Open 4 +0 In Progress 0 +0 Ready for Testing 0 +0 Ready for Release 9 +9 Total OSG 3.4.19 Testing Nothing yet Ready for Release Nothing yet Data Nothing on the horizon","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20181008/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20181008/#osg-investigations-team","text":"No report","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20181008/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20181015/","text":"OSG Technology Area Meeting, 15 October 2018 Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Mat, TimT Announcements TimT out Wed and Thurs Triage Duty This week: BrianL Next week: Carl 4 (-3) open tickets JIRA # of tickets State 138 +0 Open 14 -3 In Progress 5 +5 Ready for Testing 0 +0 Ready for Release OSG Software Team Next doc focus this Thursday (10/18) afternoon Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . AI (TimT): Update your Y7 effort numbers Discussion None this week Support Update UCSD (Edgar) - worked with the UCSD CIO so that they can now request InCommon IGTF certificates University of Amsterdam (Edgar) - encountered some policy issues with accepting Let's Encrypt certificates. GlideinWMS (Edgar) - 3.4.1 release candidate with built-in Singularity support has been running for a few weeks at GLOW-ITB and GlueX without any issues. UCSD test frontend (Edgar) - OSG and CMS Singularity wrappers are consistently having issues accessing CVMFS at Nebraska, Purdue, and some T3s. Investigating. OSG Release Team 3.4.19 Status 6 +1 Open 3 -1 In Progress 3 +3 Ready for Testing 0 +0 Ready for Release 12 +3 Total OSG 3.4.19 Testing Improve StashCache packaging Release autopyfactory 2.4.9 Release OSG flock submit host packaging Ready for Release Nothing yet Data Nothing on the horizon Operations repo-update-cadist updates Discussion AI (TimT): Coordinate with Edgar about issues with submitting pilots to the Madison ITB OSG Investigations Team Marian out this week. XRootD-scitokens release coming! SciTokens release coming! PyJWT built in osg-devel. Need to run some tests. XRootD-macaroons: Developer testing. Difficult to test (need some infrastructure to setup) XRootD-TPC in osg-contrib Note Edgar, Derek, Brian, Marian all involved in CHEP papers that are due at the end of this month. Ongoing GRACC Project StashCache Project Discussions None this week","title":"October 15, 2018"},{"location":"meetings/2018/TechArea20181015/#osg-technology-area-meeting-15-october-2018","text":"Coordinates: Conference: 719-284-5267, PIN: 57363; https://www.uberconference.com/osgblin Attending: BrianL, Carl, Edgar, Mat, TimT","title":"OSG Technology Area Meeting, 15 October 2018"},{"location":"meetings/2018/TechArea20181015/#announcements","text":"TimT out Wed and Thurs","title":"Announcements"},{"location":"meetings/2018/TechArea20181015/#triage-duty","text":"This week: BrianL Next week: Carl 4 (-3) open tickets","title":"Triage Duty"},{"location":"meetings/2018/TechArea20181015/#jira","text":"# of tickets State 138 +0 Open 14 -3 In Progress 5 +5 Ready for Testing 0 +0 Ready for Release","title":"JIRA"},{"location":"meetings/2018/TechArea20181015/#osg-software-team","text":"Next doc focus this Thursday (10/18) afternoon Reminder: topology and especially downtime updates should be considered high priority. Please watch the topology GitHub repo . AI (TimT): Update your Y7 effort numbers","title":"OSG Software Team"},{"location":"meetings/2018/TechArea20181015/#discussion","text":"None this week","title":"Discussion"},{"location":"meetings/2018/TechArea20181015/#support-update","text":"UCSD (Edgar) - worked with the UCSD CIO so that they can now request InCommon IGTF certificates University of Amsterdam (Edgar) - encountered some policy issues with accepting Let's Encrypt certificates. GlideinWMS (Edgar) - 3.4.1 release candidate with built-in Singularity support has been running for a few weeks at GLOW-ITB and GlueX without any issues. UCSD test frontend (Edgar) - OSG and CMS Singularity wrappers are consistently having issues accessing CVMFS at Nebraska, Purdue, and some T3s. Investigating.","title":"Support Update"},{"location":"meetings/2018/TechArea20181015/#osg-release-team","text":"3.4.19 Status 6 +1 Open 3 -1 In Progress 3 +3 Ready for Testing 0 +0 Ready for Release 12 +3 Total OSG 3.4.19 Testing Improve StashCache packaging Release autopyfactory 2.4.9 Release OSG flock submit host packaging Ready for Release Nothing yet Data Nothing on the horizon Operations repo-update-cadist updates","title":"OSG Release Team"},{"location":"meetings/2018/TechArea20181015/#discussion_1","text":"AI (TimT): Coordinate with Edgar about issues with submitting pilots to the Madison ITB","title":"Discussion"},{"location":"meetings/2018/TechArea20181015/#osg-investigations-team","text":"Marian out this week. XRootD-scitokens release coming! SciTokens release coming! PyJWT built in osg-devel. Need to run some tests. XRootD-macaroons: Developer testing. Difficult to test (need some infrastructure to setup) XRootD-TPC in osg-contrib Note Edgar, Derek, Brian, Marian all involved in CHEP papers that are due at the end of this month.","title":"OSG Investigations Team"},{"location":"meetings/2018/TechArea20181015/#ongoing","text":"GRACC Project StashCache Project","title":"Ongoing"},{"location":"meetings/2018/TechArea20181015/#discussions","text":"None this week","title":"Discussions"},{"location":"policy/bestman2-retire/","text":"BeStMan2 Retirement This document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack. Introduction BeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol. SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\" Additionally, SRM can be used to mediate transfer protocol selection. OSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers. This functionality is redundant to what can be accomplished with GridFTP alone. BeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize. OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents. We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan. Site Transition Plans We have released documentation for a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple GridFTP endpoints. Sites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2. Timeline March 2017 (completed): Release load balanced GridFTP documentation June 2017 (completed): OSG 3.4.0 is released without BeStMan December 2018 (completed): Security-only support for OSG 3.3 series and BeStMan is provided May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.","title":"BeStMan2 Retirement"},{"location":"policy/bestman2-retire/#bestman2-retirement","text":"This document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack.","title":"BeStMan2 Retirement"},{"location":"policy/bestman2-retire/#introduction","text":"BeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol. SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\" Additionally, SRM can be used to mediate transfer protocol selection. OSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers. This functionality is redundant to what can be accomplished with GridFTP alone. BeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize. OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents. We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan.","title":"Introduction"},{"location":"policy/bestman2-retire/#site-transition-plans","text":"We have released documentation for a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple GridFTP endpoints. Sites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2.","title":"Site Transition Plans"},{"location":"policy/bestman2-retire/#timeline","text":"March 2017 (completed): Release load balanced GridFTP documentation June 2017 (completed): OSG 3.4.0 is released without BeStMan December 2018 (completed): Security-only support for OSG 3.3 series and BeStMan is provided May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.","title":"Timeline"},{"location":"policy/external-oasis-repos/","text":"Policy for OSG Mirroring of External CVMFS repositories 12 October 2017 This document provides an overview of the policies and security understanding with regards to OSG mirroring of CVMFS repositories of external organizations. It aims to help external repositories and OSG VOs understand what OSG is attempting to achieve with the mirroring service. This is not a service-level agreement but rather a statement of responsibilities. Note To actually understand the technical procedure for mirroring a repository, see the following page . This document solely covers the policy aspects. Introduction The OSG provides a network of CVMFS Stratum-1 servers for mirroring content of externally-managed repositories. These repositories are often hosted by large HEP or physics VOs for the purpose of distributing software for high-throughput computing jobs. Additionally, OSG provides a repository ( ) for smaller VOs; this is not covered here. OSG will include additional repositories into the content distribution network (CDN) at the request of an OSG-affiliated VO. These repositories are meant to help the OSG-affiliated VO accomplish their domain science. The goal of this mirroring provides an improved quality-of-service for the VO end-users running at OSG sites. OSG does not provide support for use of the software in external repositories, but will help end-users contact the VO for help as necessary. OSG Responsibilities OSG will provide the Stratum-1 server network according to the OASIS SLA OSG will provide a best-effort mirror of the full contents of the external repo. We will attempt to provide best-effort integrity of the object contents, but assume users of the Stratum-1 will do further integrity checking. No SLA is provided covering potential data corruptions. OSG will provide best-effort notification to the mirrored repository in case OSG detects a service outage of the external repo. In the event of a security incident, the operations group will replace the compromised repository with an empty directory, signed by the key managed by them. This will be done in consultation with the security team or, in the unlikely event they cannot be reached, at the discretion of the Operations Coordinator. Once the external repository is approved, OSG will distribute the corresponding repository signing keys in a valid whitelist. The whitelist will be signed by the OSG Stratum-0. This whitelist attests to the authenticity of the key, but not a statement about repository contents. VO Responsibilities The individual responsible on behalf of the VO will be registered with the OASIS Manager role in OIM. The requesting VO should only include targeted repositories they need to support their science. The VO should understand that in the event of a reported security incident, the contents of this repository may be replaced with an empty directory and signed by the OSG repository key. Depending on the OSG Security team's evaluation of the severity and urgency of the incident, the blanking may be done immediately without VO notification or after some notification period. In the case of a security incident, the VO and OSG Security team will need to mutually agree that the incident is resolved before the repository is unblanked. The VO is ultimately responsible for the contents of the repository. OSG provides a mirror. If the external repository is not operated by the VO, OSG may work directly with the external repository maintainers. This is done for ease of operations and may be limited to day-to-day, non-security-related support. Operational Policies To help us provide the best operational setup possible, we have a few additional replication policies: OSG Operations only hosts the shared oasis.opensciencegrid.org repository; VO-dedicated software respositories (such as nova.opensciencegrid.org for the NoVA VO) should be operated by the VO. VOs are asked to either run their own repository or utilize the shared repository, but not both. There is a finite amount of high-performance storage on the CDN. A minimum of 100 GB per repository is guaranteed. Larger limits may be requested. VOs may ask the OSG to replicate their repositories to the European Grid Infrastructure (EGI); however, this can only be done if the repository name ends in .opensciencegrid.org .","title":"OASIS Repository Mirroring"},{"location":"policy/external-oasis-repos/#policy-for-osg-mirroring-of-external-cvmfs-repositories","text":"12 October 2017 This document provides an overview of the policies and security understanding with regards to OSG mirroring of CVMFS repositories of external organizations. It aims to help external repositories and OSG VOs understand what OSG is attempting to achieve with the mirroring service. This is not a service-level agreement but rather a statement of responsibilities. Note To actually understand the technical procedure for mirroring a repository, see the following page . This document solely covers the policy aspects.","title":"Policy for OSG Mirroring of External CVMFS repositories"},{"location":"policy/external-oasis-repos/#introduction","text":"The OSG provides a network of CVMFS Stratum-1 servers for mirroring content of externally-managed repositories. These repositories are often hosted by large HEP or physics VOs for the purpose of distributing software for high-throughput computing jobs. Additionally, OSG provides a repository ( ) for smaller VOs; this is not covered here. OSG will include additional repositories into the content distribution network (CDN) at the request of an OSG-affiliated VO. These repositories are meant to help the OSG-affiliated VO accomplish their domain science. The goal of this mirroring provides an improved quality-of-service for the VO end-users running at OSG sites. OSG does not provide support for use of the software in external repositories, but will help end-users contact the VO for help as necessary.","title":"Introduction"},{"location":"policy/external-oasis-repos/#osg-responsibilities","text":"OSG will provide the Stratum-1 server network according to the OASIS SLA OSG will provide a best-effort mirror of the full contents of the external repo. We will attempt to provide best-effort integrity of the object contents, but assume users of the Stratum-1 will do further integrity checking. No SLA is provided covering potential data corruptions. OSG will provide best-effort notification to the mirrored repository in case OSG detects a service outage of the external repo. In the event of a security incident, the operations group will replace the compromised repository with an empty directory, signed by the key managed by them. This will be done in consultation with the security team or, in the unlikely event they cannot be reached, at the discretion of the Operations Coordinator. Once the external repository is approved, OSG will distribute the corresponding repository signing keys in a valid whitelist. The whitelist will be signed by the OSG Stratum-0. This whitelist attests to the authenticity of the key, but not a statement about repository contents.","title":"OSG Responsibilities"},{"location":"policy/external-oasis-repos/#vo-responsibilities","text":"The individual responsible on behalf of the VO will be registered with the OASIS Manager role in OIM. The requesting VO should only include targeted repositories they need to support their science. The VO should understand that in the event of a reported security incident, the contents of this repository may be replaced with an empty directory and signed by the OSG repository key. Depending on the OSG Security team's evaluation of the severity and urgency of the incident, the blanking may be done immediately without VO notification or after some notification period. In the case of a security incident, the VO and OSG Security team will need to mutually agree that the incident is resolved before the repository is unblanked. The VO is ultimately responsible for the contents of the repository. OSG provides a mirror. If the external repository is not operated by the VO, OSG may work directly with the external repository maintainers. This is done for ease of operations and may be limited to day-to-day, non-security-related support.","title":"VO Responsibilities"},{"location":"policy/external-oasis-repos/#operational-policies","text":"To help us provide the best operational setup possible, we have a few additional replication policies: OSG Operations only hosts the shared oasis.opensciencegrid.org repository; VO-dedicated software respositories (such as nova.opensciencegrid.org for the NoVA VO) should be operated by the VO. VOs are asked to either run their own repository or utilize the shared repository, but not both. There is a finite amount of high-performance storage on the CDN. A minimum of 100 GB per repository is guaranteed. Larger limits may be requested. VOs may ask the OSG to replicate their repositories to the European Grid Infrastructure (EGI); however, this can only be done if the repository name ends in .opensciencegrid.org .","title":"Operational Policies"},{"location":"policy/flexible-release-model/","text":"OSG Software Flexible Release Model Introduction Before November 2017, the OSG software stack was released on the second Tuesday of each month, except in the case of urgent releases, which were infrequent. This schedule had been in place since early 2013. Since then, conditions within and outside of the Software team have changed, and we have adjusted the release schedule and associated processes. The previous release model had the recurring problem of a \"release crunch,\" where it was difficult to find the effort required to test large changes before their deadline had passed. Sometimes the lack of timely effort led to software being pushed to the next release (a month later), because there was insufficient testing time. Based on software support tickets, we noticed that many sites follow a local update schedule that is independent of the OSG Release team schedule; some sites upgrade every few months, skipping interim releases, other sites upgrade individual packages as needed. In addition, upstream software developers do not follow our release schedule either, releasing software on their own development timelines. As a result, some site administrators would prefer to have OSG software updates more often, closer to when they become available, rather than tied to a monthly cycle. For these reasons, we created a new release model. Release Model The OSG Release team releases batches of integrated, tested software on an ad hoc basis, with the process outlined below (changes from the old process are highlighted): Software and Release Team members develop packages and mark them for testing Software and Release Team members test the packages, possibly with help from the community Once adequate testing is complete and successful, the Release Manager approves packages for release Weekly, the Release Manager evaluates packages that are ready for release; when a sufficient number of important packages are ready [1] , the Release Manager schedules a release date and announces it. For urgent changes, the Release Manager evaluates the packages as soon as they are tested The Software and Release Team performs pre-release testing, releases the packages, and announces the release Note: The release dates of parallel release series (e.g., 3.3 and 3.4) do not have to coincide, as they have in the past. [1] The threshold for \u201csufficient number of important packages\u201d is determined by the Release Manager, with input from the other Technology Area leaders.","title":"Flexible Release Model"},{"location":"policy/flexible-release-model/#osg-software-flexible-release-model","text":"","title":"OSG Software Flexible Release Model"},{"location":"policy/flexible-release-model/#introduction","text":"Before November 2017, the OSG software stack was released on the second Tuesday of each month, except in the case of urgent releases, which were infrequent. This schedule had been in place since early 2013. Since then, conditions within and outside of the Software team have changed, and we have adjusted the release schedule and associated processes. The previous release model had the recurring problem of a \"release crunch,\" where it was difficult to find the effort required to test large changes before their deadline had passed. Sometimes the lack of timely effort led to software being pushed to the next release (a month later), because there was insufficient testing time. Based on software support tickets, we noticed that many sites follow a local update schedule that is independent of the OSG Release team schedule; some sites upgrade every few months, skipping interim releases, other sites upgrade individual packages as needed. In addition, upstream software developers do not follow our release schedule either, releasing software on their own development timelines. As a result, some site administrators would prefer to have OSG software updates more often, closer to when they become available, rather than tied to a monthly cycle. For these reasons, we created a new release model.","title":"Introduction"},{"location":"policy/flexible-release-model/#release-model","text":"The OSG Release team releases batches of integrated, tested software on an ad hoc basis, with the process outlined below (changes from the old process are highlighted): Software and Release Team members develop packages and mark them for testing Software and Release Team members test the packages, possibly with help from the community Once adequate testing is complete and successful, the Release Manager approves packages for release Weekly, the Release Manager evaluates packages that are ready for release; when a sufficient number of important packages are ready [1] , the Release Manager schedules a release date and announces it. For urgent changes, the Release Manager evaluates the packages as soon as they are tested The Software and Release Team performs pre-release testing, releases the packages, and announces the release Note: The release dates of parallel release series (e.g., 3.3 and 3.4) do not have to coincide, as they have in the past. [1] The threshold for \u201csufficient number of important packages\u201d is determined by the Release Manager, with input from the other Technology Area leaders.","title":"Release Model"},{"location":"policy/globus-toolkit/","text":"OSG Support of the Globus Toolkit 6 June 2017 Many in the OSG community have heard the news about the end of support for the open-source Globus Toolkit . What does this imply for the OSG Software stack? Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period. Note the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology. During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software. The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software. This model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception. Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus. Over the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI. Please reach out to goc@opensciencegrid.org with your questions, comments, and concerns. A copy of this statement can be found at https://www.opensciencegrid.org/technology/policy/globus-toolkit .","title":"Globus Toolkit Support"},{"location":"policy/globus-toolkit/#osg-support-of-the-globus-toolkit","text":"6 June 2017 Many in the OSG community have heard the news about the end of support for the open-source Globus Toolkit . What does this imply for the OSG Software stack? Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period. Note the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology. During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software. The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software. This model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception. Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus. Over the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI. Please reach out to goc@opensciencegrid.org with your questions, comments, and concerns. A copy of this statement can be found at https://www.opensciencegrid.org/technology/policy/globus-toolkit .","title":"OSG Support of the Globus Toolkit"},{"location":"policy/gums-retire/","text":"GUMS Retirement This document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack. Introduction GUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to local UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from multiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG community, we have found that sites use the following GUMS features: Mapping based on VOMS attributes Host-based mappings Banning users/VOs Supporting pool accounts GUMS is a large Java web application that is more complex than necessary for the subset of features used in the OSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG Software team. OSG's plans to retire GUMS has two major components: Find a suitable replacement for GUMS Provide documentation, tooling, and support to aid in the transition from GUMS to the intended solution Site Transition Plans We have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as user and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and the distributed verification of VOMS extensions makes this feature unnecessary. Pool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific document. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work with any site for which this solution does not work. LCMAPS VOMS plugin installation and configuration documentation can be found here . Timeline April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. July 2017 (completed): OSG 3.4 CEs can be configured with 3.3 GUMS hosts March 2018: Complete transition for sites not using pool accounts May 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.","title":"GUMS Retirement"},{"location":"policy/gums-retire/#gums-retirement","text":"This document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack.","title":"GUMS Retirement"},{"location":"policy/gums-retire/#introduction","text":"GUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to local UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from multiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG community, we have found that sites use the following GUMS features: Mapping based on VOMS attributes Host-based mappings Banning users/VOs Supporting pool accounts GUMS is a large Java web application that is more complex than necessary for the subset of features used in the OSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG Software team. OSG's plans to retire GUMS has two major components: Find a suitable replacement for GUMS Provide documentation, tooling, and support to aid in the transition from GUMS to the intended solution","title":"Introduction"},{"location":"policy/gums-retire/#site-transition-plans","text":"We have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as user and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and the distributed verification of VOMS extensions makes this feature unnecessary. Pool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific document. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work with any site for which this solution does not work. LCMAPS VOMS plugin installation and configuration documentation can be found here .","title":"Site Transition Plans"},{"location":"policy/gums-retire/#timeline","text":"April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. July 2017 (completed): OSG 3.4 CEs can be configured with 3.3 GUMS hosts March 2018: Complete transition for sites not using pool accounts May 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.","title":"Timeline"},{"location":"policy/release-series/","text":"OSG Software Release Series Support Policy This document describes the OSG policy for managing its software releases. Use this policy to help plan when to perform major OSG software updates at your site. OSG software releases are organized into release series , with the intent that software updates within a series do not take long to perform, cause significant downtime, or break dependent software. New series can be more disruptive, allowing OSG to add, substantially change, and remove software components. Releases are assigned versions, such as \"OSG 3.2.1\", where the first two numbers designate the release series and the third number increments within the series. Changes to the first number are infrequent and indicate sweeping changes to the way in which OSG software is distributed. OSG supports at most two concurrent release series, current and previous , where the goal is to begin a new release series about every 18 months. Once a new series starts, OSG will support the previous series for at least 12 months and will announce its end-of-life date at least 6 months in advance. During the first 6 months of a series, OSG will endeavor to apply backward-compatible changes to the previous series as well; afterward, OSG will apply only critical bug and security fixes. When support ends for a release series, it means that OSG no longer updates the software, fixes issues, or troubleshoots installations for releases within the series. The plan is to maintain interoperability between supported series, but there is no guarantee that unsupported series will continue to function in OSG. Files for release series older than current or previous will be removed from the OSG software repositories no earlier than when support ends for the previous release. For example, files for OSG 3.2 will be removed no earlier than when support ends for OSG 3.3 in May 2018. OSG Operations will handle deviations from this policy, in consultation with OSG Technology and stakeholders. Life-cycle Dates Release Series Initial Release End of Regular Support End of Critical Bug/Security Support 3.4 June 2017 Not set Not set 3.3 August 2015 December 2017 May 2018 3.2 November 2013 February 2016 August 2016 3.1 April 2012 October 2014 April 2015","title":"Release Series Support"},{"location":"policy/release-series/#osg-software-release-series-support-policy","text":"This document describes the OSG policy for managing its software releases. Use this policy to help plan when to perform major OSG software updates at your site. OSG software releases are organized into release series , with the intent that software updates within a series do not take long to perform, cause significant downtime, or break dependent software. New series can be more disruptive, allowing OSG to add, substantially change, and remove software components. Releases are assigned versions, such as \"OSG 3.2.1\", where the first two numbers designate the release series and the third number increments within the series. Changes to the first number are infrequent and indicate sweeping changes to the way in which OSG software is distributed. OSG supports at most two concurrent release series, current and previous , where the goal is to begin a new release series about every 18 months. Once a new series starts, OSG will support the previous series for at least 12 months and will announce its end-of-life date at least 6 months in advance. During the first 6 months of a series, OSG will endeavor to apply backward-compatible changes to the previous series as well; afterward, OSG will apply only critical bug and security fixes. When support ends for a release series, it means that OSG no longer updates the software, fixes issues, or troubleshoots installations for releases within the series. The plan is to maintain interoperability between supported series, but there is no guarantee that unsupported series will continue to function in OSG. Files for release series older than current or previous will be removed from the OSG software repositories no earlier than when support ends for the previous release. For example, files for OSG 3.2 will be removed no earlier than when support ends for OSG 3.3 in May 2018. OSG Operations will handle deviations from this policy, in consultation with OSG Technology and stakeholders.","title":"OSG Software Release Series Support Policy"},{"location":"policy/release-series/#life-cycle-dates","text":"Release Series Initial Release End of Regular Support End of Critical Bug/Security Support 3.4 June 2017 Not set Not set 3.3 August 2015 December 2017 May 2018 3.2 November 2013 February 2016 August 2016 3.1 April 2012 October 2014 April 2015","title":"Life-cycle Dates"},{"location":"policy/service-migrations-spring-2018/","text":"Service Migrations - Spring 2018 The Open Science Grid (OSG) has transitioned effort from Indiana, requiring a redistribution of support and services. Some services were retired, most services were migrated to other locations (with minimal expected sites impact), and some services were migrated that resulted in significant impact on sites. This document was intended to guide OSG site administrators through these changes, highlighting where the site administrator action is required. If you have questions or concerns that are not addressed in this document, see the Getting Help section for details. Getting Help If you have questions or concerns that are not addressed in this document, please contact us at the usual locations: help@opensciencegrid.org osg-software@opensciencegrid.org - General discussion amongst team members Slack channel - if you can't create an account, send an e-mail to osg-software@opensciencegrid.org Support Changes The Footprints ticketing system at https://ticket.opensciencegrid.org was used to track support and security issues as well as certificate and membership requests. This service was retired in favor of two different ticketing systems, depending on the VOs you support at your site: If your site primarily supports... Submit new tickets to... LHC VOs GGUS Anyone else Freshdesk If you experience any problems with ticketing, please contact us at help@opensciencegrid.org . Service-specific details OSG CA The OSG CA service offered certificate request, renewal, and revocation through the OIM web interface, the OIM REST API, and the osg-pki-tools command-line tool. This service was retired on May 31 but the OSG CA certificate remains in the IGTF distribution, so any certificates issued by the OSG CA remain valid until they expire. The OSG recommends using the following CA certificate services: For... We plan to use the following Certificate Authorities... Host Certificates InCommon and Let\u2019s Encrypt User Certificates CILogon Basic for non-LHC users LHC users should continue to request their user certificates from CERN. Web-Based services Let's Encrypt Note The semantics of Let's Encrypt certificates are different from those of previous CAs. Please see the security team's position on Let's Encrypt for the security and setup implications of switching to a Let's Encrypt host or service certificate. If you experience any problems acquiring host or service certificates, please contact us at help@opensciencegrid.org . Software Repository The OSG Software repository includes the YUM repositories, client tarballs, and CA tarballs. The physical hosting location changed during the migration but was otherwise unchanged. If you experience any problems with the OSG Software repository, please contact us at help@opensciencegrid.org . MyOSG and OIM The MyOSG service used to provide web and REST interfaces to access information about OSG resource topology, projects, and VOs. The MyOSG web interface was retired but we continue to offer the same REST interface at https://my.opensciencegrid.org . OIM served as the database for the information used by MyOSG with a web interface for data updates. The OIM web interface was retired but its data was migrated to the topology repository . Updates to the aforementioned data can be requested via email or pull request. Note Please see the OSG CA section for information regarding the OIM certificate service. If you experience any problems with MyOSG or the topology repository, please contact us at help@opensciencegrid.org . GRACC Accounting and WLCG Accounting No changes were made to the GRACC accounting service during the service migration. If you experience any problems with GRACC accounting, please contact us at help@opensciencegrid.org . OASIS and CVMFS The OASIS (OSG Application and Software Installation Service) is a service used to distribute common applications and software to OSG sites via CVMFS. The OSG hosts a CVMFS Stratum-0 for keysigning, a repository server, and a CVMFS Stratum-1. The physical hosting location of these services were moved to Nebraska without any other changes. If you experience any problems with OASIS or CVMFS, please contact us at help@opensciencegrid.org . VOMS Admin Server The OSG VOMS service was used to sign VOMS attributes for members of the OSG VO and responded to queries for a list of VO members. VOMS Admin Server is deprecated in the OSG and the OSG VOMS servers were retired as planned. RSV The central RSV service was a monitoring tool that displayed every service status information about OSG sites that elected to provide it. It was retired since there was no longer a need to monitor OSG site status as a whole. If you would like to monitor your OSG services, you can access the status page of your local RSV instance. Collector The central Collector is a central database service that provides details about pilot jobs currently running in the OSG. The physical hosting location of the central Collector was moved but there were no other changes. If you experience any problems with the central Collector, please contact us at help@opensciencegrid.org . Homepage The OSG homepage was a Wordpress instance that has been moved to a static site. If you experience any problems with the homepage, please contact us at help@opensciencegrid.org .","title":"Service Migrations - Spring 2018"},{"location":"policy/service-migrations-spring-2018/#service-migrations-spring-2018","text":"The Open Science Grid (OSG) has transitioned effort from Indiana, requiring a redistribution of support and services. Some services were retired, most services were migrated to other locations (with minimal expected sites impact), and some services were migrated that resulted in significant impact on sites. This document was intended to guide OSG site administrators through these changes, highlighting where the site administrator action is required. If you have questions or concerns that are not addressed in this document, see the Getting Help section for details.","title":"Service Migrations - Spring 2018"},{"location":"policy/service-migrations-spring-2018/#getting-help","text":"If you have questions or concerns that are not addressed in this document, please contact us at the usual locations: help@opensciencegrid.org osg-software@opensciencegrid.org - General discussion amongst team members Slack channel - if you can't create an account, send an e-mail to osg-software@opensciencegrid.org","title":"Getting Help"},{"location":"policy/service-migrations-spring-2018/#support-changes","text":"The Footprints ticketing system at https://ticket.opensciencegrid.org was used to track support and security issues as well as certificate and membership requests. This service was retired in favor of two different ticketing systems, depending on the VOs you support at your site: If your site primarily supports... Submit new tickets to... LHC VOs GGUS Anyone else Freshdesk If you experience any problems with ticketing, please contact us at help@opensciencegrid.org .","title":"Support Changes"},{"location":"policy/service-migrations-spring-2018/#service-specific-details","text":"","title":"Service-specific details"},{"location":"policy/service-migrations-spring-2018/#osg-ca","text":"The OSG CA service offered certificate request, renewal, and revocation through the OIM web interface, the OIM REST API, and the osg-pki-tools command-line tool. This service was retired on May 31 but the OSG CA certificate remains in the IGTF distribution, so any certificates issued by the OSG CA remain valid until they expire. The OSG recommends using the following CA certificate services: For... We plan to use the following Certificate Authorities... Host Certificates InCommon and Let\u2019s Encrypt User Certificates CILogon Basic for non-LHC users LHC users should continue to request their user certificates from CERN. Web-Based services Let's Encrypt Note The semantics of Let's Encrypt certificates are different from those of previous CAs. Please see the security team's position on Let's Encrypt for the security and setup implications of switching to a Let's Encrypt host or service certificate. If you experience any problems acquiring host or service certificates, please contact us at help@opensciencegrid.org .","title":"OSG CA"},{"location":"policy/service-migrations-spring-2018/#software-repository","text":"The OSG Software repository includes the YUM repositories, client tarballs, and CA tarballs. The physical hosting location changed during the migration but was otherwise unchanged. If you experience any problems with the OSG Software repository, please contact us at help@opensciencegrid.org .","title":"Software Repository"},{"location":"policy/service-migrations-spring-2018/#myosg-and-oim","text":"The MyOSG service used to provide web and REST interfaces to access information about OSG resource topology, projects, and VOs. The MyOSG web interface was retired but we continue to offer the same REST interface at https://my.opensciencegrid.org . OIM served as the database for the information used by MyOSG with a web interface for data updates. The OIM web interface was retired but its data was migrated to the topology repository . Updates to the aforementioned data can be requested via email or pull request. Note Please see the OSG CA section for information regarding the OIM certificate service. If you experience any problems with MyOSG or the topology repository, please contact us at help@opensciencegrid.org .","title":"MyOSG and OIM"},{"location":"policy/service-migrations-spring-2018/#gracc-accounting-and-wlcg-accounting","text":"No changes were made to the GRACC accounting service during the service migration. If you experience any problems with GRACC accounting, please contact us at help@opensciencegrid.org .","title":"GRACC Accounting and WLCG Accounting"},{"location":"policy/service-migrations-spring-2018/#oasis-and-cvmfs","text":"The OASIS (OSG Application and Software Installation Service) is a service used to distribute common applications and software to OSG sites via CVMFS. The OSG hosts a CVMFS Stratum-0 for keysigning, a repository server, and a CVMFS Stratum-1. The physical hosting location of these services were moved to Nebraska without any other changes. If you experience any problems with OASIS or CVMFS, please contact us at help@opensciencegrid.org .","title":"OASIS and CVMFS"},{"location":"policy/service-migrations-spring-2018/#voms-admin-server","text":"The OSG VOMS service was used to sign VOMS attributes for members of the OSG VO and responded to queries for a list of VO members. VOMS Admin Server is deprecated in the OSG and the OSG VOMS servers were retired as planned.","title":"VOMS Admin Server"},{"location":"policy/service-migrations-spring-2018/#rsv","text":"The central RSV service was a monitoring tool that displayed every service status information about OSG sites that elected to provide it. It was retired since there was no longer a need to monitor OSG site status as a whole. If you would like to monitor your OSG services, you can access the status page of your local RSV instance.","title":"RSV"},{"location":"policy/service-migrations-spring-2018/#collector","text":"The central Collector is a central database service that provides details about pilot jobs currently running in the OSG. The physical hosting location of the central Collector was moved but there were no other changes. If you experience any problems with the central Collector, please contact us at help@opensciencegrid.org .","title":"Collector"},{"location":"policy/service-migrations-spring-2018/#homepage","text":"The OSG homepage was a Wordpress instance that has been moved to a static site. If you experience any problems with the homepage, please contact us at help@opensciencegrid.org .","title":"Homepage"},{"location":"policy/software-support/","text":"Software Team Support Incoming tickets (Operations staff) When a ticket arrives at the GOC and the Operations staff member decides that the ticket should be assigned to the Software Team, the operations staff member will do two things: The ticket will be assigned to a pseudo-user called \"Software\". The \"Next Action\" field will be set to \"SOFTWARE TRIAGE\". The Software pseudo-user has an email list as its \"personal\" email address. This is: . Triage duty (Technology Area staff) All OSG Technology Area Team members who are at least 50% on the will share triage duty (except Brian Bockelman). Each week (Monday through Friday), during normal work hours, there will be one person on triage duty. If you are on triage duty, this means: Watch the software incoming tickets. If a ticket has not been assigned to a software team member, you assign it appropriately. You are responsible for assigning all incoming tickets that haven't been assigned. This includes tickets that have arrived over the weekend or were not handled by the previous person on triage duty. If you can handle an incoming ticket, assign it to yourself and handle it. Leave the \"software\" user assigned to the ticket. Many tickets are common problems that most team members should be able to solve. If you cannot handle an incoming ticket, collect initial details (versions, logs, etc...), and assign the ticket to the most appropriate software team member. Where appropriate, include people from other teams (i.e. security, operations, glidein...) Leave the \"software\" user assigned to the ticket. Look at assigned tickets. For tickets that are not being handled in a timely fashion, please remind the person that owns the ticket, or, if the ticket is waiting on the user, remind the user. Note Being on triage duty does not mean that you must personally solve all new tickets. It means that you handle the easy tickets and assign the other tickets appropriately. Note that the software pseudo-user has an email address that is a mailing list: . If you find it convenient, you can sign up for the mailing list to see all the incoming tickets. Alain recommends you do this during your triage duty, but you do not need to stay subscribed when you are not on triage duty. All the currently opened tickets assigned to the software team can be seen here: GOC Open Tickets Flow of tickets Note that if you follow the above, we will end up with three assignees to each ticket. This is the overall flow: A ticket arrives at the GOC, either via the ticket creator, or created by Operations in response to an email. ASSIGNMENT #1: The ticket is assigned to an Operations member. They're in charge of ushering the ticket through its whole lifetime, though for software tickets they won't do a whole lot on the technical work. Note that some software tickets may not be assigned to us, because they might assign them to the VO support center. This is good. ASSIGNMENT #2: The Operations member looks at the ticket and decides it's a software ticket. (They might do some upfront work if they can.) They then assign it to \"Software Support (Triage)\". We now have two people assigned to the ticket. When assigned to \"Software Support (Triage)\", all changes to the ticket are sent to , so we leave this pseudo-person on the ticket. Watching the email to this mailing list is a nice (but optional) way for you to see what's happening when you're on triage duty. ASSIGNMENT #3: The person on triage duty assigns it to the right person from the software team. We now have three assignees: GOC member Software Support (Triage) The Technology team member who is responsible for guiding a ticket until it is resolved To avoid any confusion around ticket ownership, assign only one Technology team member per ticket. If the expertise of another Technology team member is needed on the ticket, add them to the CC list. Inasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people. LCMAPS VOMS Transition This section contains the process for helping sites transition from edg-mkgridmap or GUMS to the LCMAPS VOMS plugin as part of the VOMS Admin Server retirement . Ask if the site is using edg-mkgridmap or GUMS. If they're using GUMS, find out what GUMS clients they have at their site. Possibilities include: HTCondor-CE GridFTP XRootD: if an ATLAS site, they use vomsxrd/xrootd-voms-plugin dCache: if an ATLAS site, encourage them to consult US-ATLAS mailing lists BeStMan: encourage the site to transition to GridFTP, as they cannot retire GUMS until BeStMan doesn't depend on it. If CMS/ATLAS, encourage them to consult their US-ATLAS/US-CMS mailing lists for assistance. Ask them to follow the relevant instructions for their authorization solution edg-mkgridmap GUMS After they've completed the above instructions, use one of the following methods (in order of preference) to verify LCMAPS VOMS mappings: If the host is a CE, verify that they are still receiving pilots: Query their CE directly: $ condor_q -name CE HOSTNAME -pool CE HOSTNAME :9619 This may not work if the site has a strict firewall or do not run an HTCondor-CE. Add factory ops or the relevant ATLAS (T2 vs T3) support center to the ticket under OSG Support Centers For non-ATLAS sites, verify that the site's pilot numbers are non-zero. If the host is a GridFTP server, verify file transfer with their VO support center. Updating the triage calendar The calendar is hosted on Tim Cartwright\u2019s Google Calendar account. If you need privileges to edit, ask Brian L. To update Update checkout ( GitHub ) Generate next rotation: ./triage.py --generateNextRotation rotation.txt Check and update assignments according to team member outages Load triage assignments into Google Calendar: ./triage.py --load rotation.txt To subscribe to this calendar in your calendar program, use the iCal URL: https://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics Handling tickets We need to take good care of our users. We are in a small community. Please be friendly and patient even when the user is frustrated or lacking in knowledge. Always sign your ticket with your full name, so people know who is responding. If it's easy for you, include a signature at the bottom of your response. Remember that you can tell people to use the osg-system-profiler to collect information. It can shorten the number of times you ask for information because it collects quite a bit for you. If you run across a problem that has a chance of being hit by other users, consider: Is there a bug we should fix in the software? Or something we could improve in the software? Is there a way to improve our documentation? Can you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users. Direct E-mail vs. Support If someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are: If it's easy to handle and you can definitely do it yourself, leave it in email. If there's a chance that you can't do it in a timely fashion, turn it into a ticket. If there's a chance that you might lose track of the email, turn it into a ticket. If there's a chance that you might need help from others, turn it into a ticket. If it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket. GOC vs JIRA JIRA is for tracking our work. It's meant for internal usage, not for user support. In general, users should not ask for support via JIRA. A single user support ticket might result in zero, one, or multiple JIRA tickets. GOC tickets are for user support. This is where we help users debug, understand their problems, etc. If actionable software team tasks arise from a GOC ticket, JIRA ticket(s) should be created to track that work. Resultant JIRA tickets should: Include a link to the original GOC ticket, a description of the problem, and a proposed solution to the problem. Add the original reporter as a watcher if they have a JIRA account. When all the relevant JIRA tickets are created, ask the user if they would be ok with tracking the issue via JIRA. If they say yes, close the GOC ticket.","title":"Software Support"},{"location":"policy/software-support/#software-team-support","text":"","title":"Software Team Support"},{"location":"policy/software-support/#incoming-tickets-operations-staff","text":"When a ticket arrives at the GOC and the Operations staff member decides that the ticket should be assigned to the Software Team, the operations staff member will do two things: The ticket will be assigned to a pseudo-user called \"Software\". The \"Next Action\" field will be set to \"SOFTWARE TRIAGE\". The Software pseudo-user has an email list as its \"personal\" email address. This is: .","title":"Incoming tickets (Operations staff)"},{"location":"policy/software-support/#triage-duty-technology-area-staff","text":"All OSG Technology Area Team members who are at least 50% on the will share triage duty (except Brian Bockelman). Each week (Monday through Friday), during normal work hours, there will be one person on triage duty. If you are on triage duty, this means: Watch the software incoming tickets. If a ticket has not been assigned to a software team member, you assign it appropriately. You are responsible for assigning all incoming tickets that haven't been assigned. This includes tickets that have arrived over the weekend or were not handled by the previous person on triage duty. If you can handle an incoming ticket, assign it to yourself and handle it. Leave the \"software\" user assigned to the ticket. Many tickets are common problems that most team members should be able to solve. If you cannot handle an incoming ticket, collect initial details (versions, logs, etc...), and assign the ticket to the most appropriate software team member. Where appropriate, include people from other teams (i.e. security, operations, glidein...) Leave the \"software\" user assigned to the ticket. Look at assigned tickets. For tickets that are not being handled in a timely fashion, please remind the person that owns the ticket, or, if the ticket is waiting on the user, remind the user. Note Being on triage duty does not mean that you must personally solve all new tickets. It means that you handle the easy tickets and assign the other tickets appropriately. Note that the software pseudo-user has an email address that is a mailing list: . If you find it convenient, you can sign up for the mailing list to see all the incoming tickets. Alain recommends you do this during your triage duty, but you do not need to stay subscribed when you are not on triage duty. All the currently opened tickets assigned to the software team can be seen here: GOC Open Tickets","title":"Triage duty (Technology Area staff)"},{"location":"policy/software-support/#flow-of-tickets","text":"Note that if you follow the above, we will end up with three assignees to each ticket. This is the overall flow: A ticket arrives at the GOC, either via the ticket creator, or created by Operations in response to an email. ASSIGNMENT #1: The ticket is assigned to an Operations member. They're in charge of ushering the ticket through its whole lifetime, though for software tickets they won't do a whole lot on the technical work. Note that some software tickets may not be assigned to us, because they might assign them to the VO support center. This is good. ASSIGNMENT #2: The Operations member looks at the ticket and decides it's a software ticket. (They might do some upfront work if they can.) They then assign it to \"Software Support (Triage)\". We now have two people assigned to the ticket. When assigned to \"Software Support (Triage)\", all changes to the ticket are sent to , so we leave this pseudo-person on the ticket. Watching the email to this mailing list is a nice (but optional) way for you to see what's happening when you're on triage duty. ASSIGNMENT #3: The person on triage duty assigns it to the right person from the software team. We now have three assignees: GOC member Software Support (Triage) The Technology team member who is responsible for guiding a ticket until it is resolved To avoid any confusion around ticket ownership, assign only one Technology team member per ticket. If the expertise of another Technology team member is needed on the ticket, add them to the CC list. Inasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people.","title":"Flow of tickets"},{"location":"policy/software-support/#lcmaps-voms-transition","text":"This section contains the process for helping sites transition from edg-mkgridmap or GUMS to the LCMAPS VOMS plugin as part of the VOMS Admin Server retirement . Ask if the site is using edg-mkgridmap or GUMS. If they're using GUMS, find out what GUMS clients they have at their site. Possibilities include: HTCondor-CE GridFTP XRootD: if an ATLAS site, they use vomsxrd/xrootd-voms-plugin dCache: if an ATLAS site, encourage them to consult US-ATLAS mailing lists BeStMan: encourage the site to transition to GridFTP, as they cannot retire GUMS until BeStMan doesn't depend on it. If CMS/ATLAS, encourage them to consult their US-ATLAS/US-CMS mailing lists for assistance. Ask them to follow the relevant instructions for their authorization solution edg-mkgridmap GUMS After they've completed the above instructions, use one of the following methods (in order of preference) to verify LCMAPS VOMS mappings: If the host is a CE, verify that they are still receiving pilots: Query their CE directly: $ condor_q -name CE HOSTNAME -pool CE HOSTNAME :9619 This may not work if the site has a strict firewall or do not run an HTCondor-CE. Add factory ops or the relevant ATLAS (T2 vs T3) support center to the ticket under OSG Support Centers For non-ATLAS sites, verify that the site's pilot numbers are non-zero. If the host is a GridFTP server, verify file transfer with their VO support center.","title":"LCMAPS VOMS Transition"},{"location":"policy/software-support/#updating-the-triage-calendar","text":"The calendar is hosted on Tim Cartwright\u2019s Google Calendar account. If you need privileges to edit, ask Brian L. To update Update checkout ( GitHub ) Generate next rotation: ./triage.py --generateNextRotation rotation.txt Check and update assignments according to team member outages Load triage assignments into Google Calendar: ./triage.py --load rotation.txt To subscribe to this calendar in your calendar program, use the iCal URL: https://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics","title":"Updating the triage calendar"},{"location":"policy/software-support/#handling-tickets","text":"We need to take good care of our users. We are in a small community. Please be friendly and patient even when the user is frustrated or lacking in knowledge. Always sign your ticket with your full name, so people know who is responding. If it's easy for you, include a signature at the bottom of your response. Remember that you can tell people to use the osg-system-profiler to collect information. It can shorten the number of times you ask for information because it collects quite a bit for you. If you run across a problem that has a chance of being hit by other users, consider: Is there a bug we should fix in the software? Or something we could improve in the software? Is there a way to improve our documentation? Can you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users.","title":"Handling tickets"},{"location":"policy/software-support/#direct-e-mail-vs-support","text":"If someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are: If it's easy to handle and you can definitely do it yourself, leave it in email. If there's a chance that you can't do it in a timely fashion, turn it into a ticket. If there's a chance that you might lose track of the email, turn it into a ticket. If there's a chance that you might need help from others, turn it into a ticket. If it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket.","title":"Direct E-mail vs. Support"},{"location":"policy/software-support/#goc-vs-jira","text":"JIRA is for tracking our work. It's meant for internal usage, not for user support. In general, users should not ask for support via JIRA. A single user support ticket might result in zero, one, or multiple JIRA tickets. GOC tickets are for user support. This is where we help users debug, understand their problems, etc. If actionable software team tasks arise from a GOC ticket, JIRA ticket(s) should be created to track that work. Resultant JIRA tickets should: Include a link to the original GOC ticket, a description of the problem, and a proposed solution to the problem. Add the original reporter as a watcher if they have a JIRA account. When all the relevant JIRA tickets are created, ask the user if they would be ok with tracking the issue via JIRA. If they say yes, close the GOC ticket.","title":"GOC vs JIRA"},{"location":"policy/voms-admin-retire/","text":"VOMS-Admin Retirement Introduction This document provides an overview of the planned retirement of support for VOMS-Admin in the OSG Software Stack. Support for the VOMS infrastructure has three major components: VOMS-Admin : A web interface for maintaining the list of authorized users in a VO and their various authorizations (group membership, roles, attributes, etc). VOMS-Server : A TCP service which signs a cryptographic extension on an X509 proxy certificate asserting the authorizations available to the authenticated user. VOMS Client : Software for extracting and validating the signed VOMS extension from an X509 proxy. The validation is meant to be distributed: the VOMS client does not need to contact the VOMS-Admin server. However, OSG has historically used software such as GUMS or edg-mkgridmap to cache a list of authorizations from the VOMS-Admin interface, creating a dependency between VOMS client and VOMS-Admin. VOMS-Admin is a large, complex Java web application. Over the last few years, upstream support has tailed off - particularly as OSG has been unable to update to VOMS-Admin version 3. As a result, the maintenance burden has largely fallen on the OSG Software team. Given that VOMS-Admin is deeply tied to X509 security infrastructure - and is maintenance-only from OSG Software - there is no path forward to eliminate the use of X509 certificates in the web browser, a high-priority goal In discussions with the OSG community, we have found very few VOs utilize VOMS-Admin to manage their VO users. Instead, the majority use VOMS-Admin to whitelist a pilot certificate: this can be done without a VOMS-Admin endpoint. OSG's plans to retire VOMS-Admin has three major components: (Sites) Enable distributed validation of VOMS extensions in the VOMS client. (VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing of VOMS proxies. (VOs) Migrate remaining VOs to a central comanage instance for managing user authorizations; maintain a plugin to enable direct callouts from VOMS-Server to comanage for authorization lookups. Site Transition Plans We will release a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions; this verification eludes the need to contact the VOMS-Admin interface for a list of authorizations. In 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when LCMAPS asserts the validation was performed. Hence, when GUMS sites update clients to the latest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer needed. Note : In parallel to the VOMS-Admin transition, OSG Software plans to retire GUMS . There is no need to complete one transition before the other. Sites using edg-mkgridmap will need to use its replacement, lcmaps-plugins-voms (this process is documented here ). VO Transition Plans Based on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain a list of authorized pilots. For these VOs, we will help convert invocations of voms-proxy-init : voms-proxy-init -voms hcc:/hcc/Role=pilot to an equivalent call to voms-proxy-fake : voms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\ -hostkey /etc/grid-security/voms/vomskey.pem \\ -fqan /hcc/Role=pilot/Capability=NULL \\ -voms hcc -uri hcc-voms.unl.edu:15000 The latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate currently on the VOMS-Admin server to be kept on the frontend host. The frontend's account may also need access to the certificate. We plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to comanage . It is not clear there are any such VOs that need support from OSG. If there are, a hosted version of comanage is expected to be available in summer 2017 from the CILogon 2.0 project. If you feel your VO is affected, please contact the OSG and we will build a custom timeline. If there are no such VOs, we will not need to adopt comanage for this use case (other uses of comanage are expected to proceed regardless). Timeline April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. Sites begin transition to validating VOMS extensions. Summer 2017 (completed): As necessary, VOs are given access to a hosted comanage instance. March 2017 (completed): First VOs begin to retire VOMS-Admin. May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.","title":"VOMS Admin Retirement"},{"location":"policy/voms-admin-retire/#voms-admin-retirement","text":"","title":"VOMS-Admin Retirement"},{"location":"policy/voms-admin-retire/#introduction","text":"This document provides an overview of the planned retirement of support for VOMS-Admin in the OSG Software Stack. Support for the VOMS infrastructure has three major components: VOMS-Admin : A web interface for maintaining the list of authorized users in a VO and their various authorizations (group membership, roles, attributes, etc). VOMS-Server : A TCP service which signs a cryptographic extension on an X509 proxy certificate asserting the authorizations available to the authenticated user. VOMS Client : Software for extracting and validating the signed VOMS extension from an X509 proxy. The validation is meant to be distributed: the VOMS client does not need to contact the VOMS-Admin server. However, OSG has historically used software such as GUMS or edg-mkgridmap to cache a list of authorizations from the VOMS-Admin interface, creating a dependency between VOMS client and VOMS-Admin. VOMS-Admin is a large, complex Java web application. Over the last few years, upstream support has tailed off - particularly as OSG has been unable to update to VOMS-Admin version 3. As a result, the maintenance burden has largely fallen on the OSG Software team. Given that VOMS-Admin is deeply tied to X509 security infrastructure - and is maintenance-only from OSG Software - there is no path forward to eliminate the use of X509 certificates in the web browser, a high-priority goal In discussions with the OSG community, we have found very few VOs utilize VOMS-Admin to manage their VO users. Instead, the majority use VOMS-Admin to whitelist a pilot certificate: this can be done without a VOMS-Admin endpoint. OSG's plans to retire VOMS-Admin has three major components: (Sites) Enable distributed validation of VOMS extensions in the VOMS client. (VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing of VOMS proxies. (VOs) Migrate remaining VOs to a central comanage instance for managing user authorizations; maintain a plugin to enable direct callouts from VOMS-Server to comanage for authorization lookups.","title":"Introduction"},{"location":"policy/voms-admin-retire/#site-transition-plans","text":"We will release a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS extensions; this verification eludes the need to contact the VOMS-Admin interface for a list of authorizations. In 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when LCMAPS asserts the validation was performed. Hence, when GUMS sites update clients to the latest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer needed. Note : In parallel to the VOMS-Admin transition, OSG Software plans to retire GUMS . There is no need to complete one transition before the other. Sites using edg-mkgridmap will need to use its replacement, lcmaps-plugins-voms (this process is documented here ).","title":"Site Transition Plans"},{"location":"policy/voms-admin-retire/#vo-transition-plans","text":"Based on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain a list of authorized pilots. For these VOs, we will help convert invocations of voms-proxy-init : voms-proxy-init -voms hcc:/hcc/Role=pilot to an equivalent call to voms-proxy-fake : voms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\ -hostkey /etc/grid-security/voms/vomskey.pem \\ -fqan /hcc/Role=pilot/Capability=NULL \\ -voms hcc -uri hcc-voms.unl.edu:15000 The latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate currently on the VOMS-Admin server to be kept on the frontend host. The frontend's account may also need access to the certificate. We plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to comanage . It is not clear there are any such VOs that need support from OSG. If there are, a hosted version of comanage is expected to be available in summer 2017 from the CILogon 2.0 project. If you feel your VO is affected, please contact the OSG and we will build a custom timeline. If there are no such VOs, we will not need to adopt comanage for this use case (other uses of comanage are expected to proceed regardless).","title":"VO Transition Plans"},{"location":"policy/voms-admin-retire/#timeline","text":"April 2017 (completed): lcmaps-plugins-voms shipped and supported by OSG. May 2017 (completed): osg-configure and documentation necessary for using lcmaps-plugins-voms is shipped. June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, edg-mkgridmap , or GUMS. Sites begin transition to validating VOMS extensions. Summer 2017 (completed): As necessary, VOs are given access to a hosted comanage instance. March 2017 (completed): First VOs begin to retire VOMS-Admin. May 2018 (completed): Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.","title":"Timeline"},{"location":"projects/sha2-support/","text":"SHA-2 Compliance When a certificate authority signs a certificate, it uses one of several possible hash algorithms. Historically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family. SHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by Google researchers . These days, the preferred hash algorithm family is SHA-2. The certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2. The table below denotes indicates the minimum releases necessary to support SHA-2 certificates. Component Version In Release Notes BeStMan 2 bestman2-2.3.0-9.osg 3.1.13 SHA-2 support; also see jGlobus, below dCache SRM client dcache-srmclient-2.2.11.1-2.osg 3.1.22 Major update includes SHA-2 support Globus GRAM globus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg 3.1.9 Critical bug fixes (not SHA-2 specific) GUMS gums-1.3.18.009-15.2.osg 3.1.13 Switched to jGlobus 2 with SHA-2 support; also see jGlobus, below jGlobus (for BeStMan 2) jglobus-2.0.5-3.osg 3.1.18 Fixed CRL refresh bug (not SHA-2 specific) VOMS voms-2.0.8-1.5.osg 3.1.17 SHA-2 fix for voms-proxy-init If a component does not appear in the above table, it already has SHA-2 support.","title":"SHA-2 Support"},{"location":"projects/sha2-support/#sha-2-compliance","text":"When a certificate authority signs a certificate, it uses one of several possible hash algorithms. Historically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family. SHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by Google researchers . These days, the preferred hash algorithm family is SHA-2. The certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2. The table below denotes indicates the minimum releases necessary to support SHA-2 certificates. Component Version In Release Notes BeStMan 2 bestman2-2.3.0-9.osg 3.1.13 SHA-2 support; also see jGlobus, below dCache SRM client dcache-srmclient-2.2.11.1-2.osg 3.1.22 Major update includes SHA-2 support Globus GRAM globus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg 3.1.9 Critical bug fixes (not SHA-2 specific) GUMS gums-1.3.18.009-15.2.osg 3.1.13 Switched to jGlobus 2 with SHA-2 support; also see jGlobus, below jGlobus (for BeStMan 2) jglobus-2.0.5-3.osg 3.1.18 Fixed CRL refresh bug (not SHA-2 specific) VOMS voms-2.0.8-1.5.osg 3.1.17 SHA-2 fix for voms-proxy-init If a component does not appear in the above table, it already has SHA-2 support.","title":"SHA-2 Compliance"},{"location":"release/acceptance-testing/","text":"Acceptance Testing The OSG Release Team collects and maintains testing procedures for major components in the OSG Sofware Stack. These test should verify that basic functionality of the component works in typically deployed configurations. CVMFS Note This acceptance testing recipe was created when access to a machine with sufficient disk space to make a complete replica of OASIS was not available. Creating a CVMFS Repository Server (Stratum 0) Disable SELinux by setting the following in /etc/selinux/config . SELINUX=disabled Check kernel version. uname -a CVMFS for EL7 requires OverlayFS (as of kernel version 4.2.x). If default kernel is = 4.2.x, update kernel. root@host # rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org root@host # rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm root@host # yum install yum-plugin-fastestmirror root@host # yum --enablerepo = elrepo-kernel install kernel-ml Select updated kernel by editing /etc/default/grub . GRUB_DEFAULT=0 and run: root@host # grub2-mkconfig -o /boot/grub2/grub.cfg Reboot system. Check kernel version again and make sure SELinux is disabled. root@host # uname -a root@host # sestatus If kernel = 4.2 and SELinux is disabled, then update system and install CVMFS server and client packages. root@host # yum update root@host # yum install epel-release root@host # yum install yum-plugin-priorities root@host # rpm -Uvh https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm root@host # yum install cvmfs cvmfs-server Configure web server and start it up. Edit /etc/httpd/conf.d/cvmfs.conf : Listen 8000 KeepAlive On and run: root@host # chkconfig httpd on root@host # service httpd start Make new repository. root@host # cvmfs_server mkfs test.cvmfs-stratum-0.novalocal Run transaction on new repository to enable write access. root@host # cvmfs_server transaction test.cvmfs-stratum-0.novalocal Place some sample code in new repository directory and then publish it. root@host # cd /cvmfs/test.cvmfs-stratum-0.novalocal root@host # vi [ bash \\_ pi.sh ]( %ATTACHURL%/bash_pi.sh ) root@host # chmod +x bash \\_ pi.sh root@host # cvmfs \\_ server publish test.cvmfs-stratum-0.novalocal Check repository status after publication. root@host # cvmfs \\_ server check root@host # cvmfs \\_ server tag root@host # wget -qO- http://localhost:8000/cvmfs/test.cvmfs-stratum-0.novalocal/.cvmfswhitelist%7Ccat -v Download a copy of the CVMFS repository's public key e.g., /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub Creating a CVMFS Replica Server (Stratum 1) Repeat steps 1 though 8 in the previous section on \"Creating a CVMFS Repository Server \". However, now also install mod_wsgi . root@host # yum install cvmfs cvmfs-server mod \\_ wsgi Upload a copy of the CVMFS repository's public key and place in /etc/cvmfs/keys directory. Add replica of the repository. root@host # cvmfs_server add-replica -o root http://10.128.3.96:8000/cvmfs/test.cvmfs-stratum-0.novalocal /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub Make a snapshot of the repository. root@host # cvmfs \\_ server snapshot test.cvmfs-stratum-0.novalocal Creating a CVMFS client Update system and install CVMFS client package. root@host # yum update root@host # yum install epel-release root@host # yum install yum-plugin-priorities root@host # rpm -Uvh https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm root@host # yum install cvmfs Upload a copy of the CVMFS repository's public key and place in /etc/cvmfs/keys directory. Edit fuse configuration /etc/fuse.conf . user_allow_other Edit autofs configuration and restart service /etc/auto.master . /cvmfs /etc/auto.cvmfs and run: root@host # service autofs restart Edit cvmfs configuration ( /etc/cvmfs/default.local ) to point to replica server. CVMFS_SERVER_URL = http://10.128.3.97:8000/cvmfs/@fqrn@ CVMFS_REPOSITORIES = test.cvmfs-stratum-0.novalocal CVMFS_HTTP_PROXY = DIRECT Remove OSG CVMFS configuration file. rm /etc/cvmfs/default.d/60-osg.conf Run CVMFS config probe. cvmfs_config probe test.cvmfs-stratum-0.novalocal Check CVMFS config status. cvmfs_config stat -v test.cvmfs-stratum-0.novalocal Execute sample code published to repository from client. /cvmfs/test.cvmfs-stratum-0.novalocal/bash_pi.sh -b 8 -r 5 -s 10000 Creating an OASIS client Update system and install CVMFS client package. yum update yum install epel-release yum install yum-plugin-priorities rpm -Uvh https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm yum install osg-oasis Verify latest versions of cvmfs, cvmfs-config-osg, and cvmfs-x509-helper have been installed. Edit fuse configuration. vi /etc/fuse.conf user_allow_other Edit cvmfs configuration to point to replica server. vi /etc/cvmfs/default.local CVMFS_REPOSITORIES=\"`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\" CVMFS_QUOTA_LIMIT=20000 CVMFS_HTTP_PROXY=DIRECT Edit autofs configuration and restart service. vi /etc/auto.master /cvmfs /etc/auto.cvmfs service autofs restart Run CVMFS config probe. cvmfs_config probe Check CVMFS config status. cvmfs_config stat -v oasis.opensciencegrid.org Additional Documentation CERN's CVMFS Documentation OSG's CVMFS Replica Server OSG's CVMFS Client Documentation OSG's OASIS Documentation bash_pi.sh : A bash script that uses a simple Monte Carlo method to estimate the value of Pi Gratia Probe This section documents the testing procedure to test the gratia probes sufficiently tested to be promoted to the osg-testing repository. The test procedure is the same on both SL6 and SL7. install or update the gratia-probe-condor rpm as appropriate On each VM download the gratia-probe-setup.sh script and run it In /etc/gratia/condor/ProbeConfig , verify the following have been changed: change SiteName to something aside from Generic Site change EnableProbe to 1 change CollectorHost , SSLHost , and SSLRegistrationHost to the an invalid host (E.g. test.com) or the localhost Create /var/lib/osg/ and download the attached user-vo-map file and place it in that directory Edit the user-vo-map file and change the account from sthapa to the account you'll be using to submit the condor jobs in the following step Download and submit the attached condor_submit file (note, on the default fermicloud VM, this takes about 3 hours, so you may want to set NUM_CPUS to 50 so that 50 jobs will run at a time) Run /usr/share/gratia/condor/condor_meter Check /var/lib/gratia/tmp/gratiafiles/ for a subdir.condor_... directory and verify that there are 200 xml jobs and the cpus/wall times are appropriate (either PT0S or PT1M). GSI OpenSSH To test a fresh installation: Spin up two VM's and set up the EPEL/OSG repos on both of them. Choose one of the VM's, it will be the server VM. Consult these instructions to set up the server. From the other VM (client): Install the necessary packages: root@host # yum install globus-proxy-utils gsi-openssh-clients Initialize your proxy. After this, none of the gsi commands should prompt you for your password. Connect to the server: user@host $ gsissh -p 2222 server hostname Copy a test file to the server: user@host $ gsiscp -p 2222 testfile server hostname :/tmp Connect to the server via SFTP and grab files: user@host $ gsisftp -P 2222 server hostname user@host $ cd /tmp user@host $ get testfile HTCondor-CE Collector (WIP) The CE Collector is a stripped-down version of HTCondor-CE that contains mostly just the collector daemon and configs. It was introduced in htcondor-ce-1.6. The production CE Collectors run at the GOC, but you may want to set up your own for testing. Make 2 VMs with the EPEL/OSG repos installed: one for the collector, and one for the CE Setting Up the Collector Install htcondor-ce-collector Create a file called /etc/condor-ce/config.d/99-local.conf that contains this line: COLLECTOR.ALLOW_ADVERTISE_SCHEDD = $(COLLECTOR.ALLOW_ADVERTISE_SCHEDD), your_htcondor_ce_host.example.net /pre (with your_htcondor_ce_host replaced by the hostname the HTCondor-CE VM) Run service condor-ce-collector start Setting Up the CE Install osg-htcondor-ce-condor (replace condor with the batch system of your choice) Ensure osg-configure = 1.0.60-2 is installed Configure your CE using osg-configure You should use the HTCondor-CE Install Docs as a reference, although you can skip several of the steps You can skip setting up Squid: set enabled to True and location to UNAVAILABLE in 01-squid.ini Set htcondor_gateway_enabled to True in 10-gateway.ini You probably don't need GUMS, but if you want it, use the Fermi GUMS server (set gums_host to gums.fnal.gov and authorization_method to xacml in 10-misc.ini) To keep osg-configure from complaining about storage, edit 10-storage.ini : Set se_available to False Set app_dir to /osg/app_dir Set data_dir to /osg/data_dir Do mkdir -p /osg/app_dir/etc; mkdir -p /osg/data_dir; chmod 1777 /osg/app_dir{,/etc} Enable your batch system by setting enabled to True in 20- batch system .ini Set up the site info in 40-siteinfo.ini ; in particular, you'll need to set the resource and resource_group settings \\ (you just need to pick a name; I concatenate my login name with the short host name and use that, e.g. matyasfermicloud001). \\ You can also use the following settings: group=OSG-ITB sponsor=local city=Batavia, IL country=US longitude=-88 latitude=41 Edit the file /etc/osg/config.d/30-infoservices.ini and set ce_collectors to the collector host Run osg-configure -dc Start up your batch system Run service condor-ce start The CE will report to the collector host every five minutes. If you want to force it to send now, run condor_ce_reconfig . You should see your CE if you run condor_ce_status -schedd on the collector host. RSV Testing a fresh installation: make sure the yum repositories required by OSG is installed on your host rpm -Uvh http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm OR rpm -Uvh http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm also make sure epel repo is set up. install the rpm yum --enablerepo=osg-testing install rsv edit /etc/osg/config.d/30-rsv.ini file in my case, I don't have a service cert for testing, so I use my own personal cert to create the proxy, but later on the owner of the proxy should be changed to \"rsv\" user that is created during the rpm install. in the config file, for the ce_hosts and gridftp_hosts, put in a test server, as the result from this test will be uploaded to OSG GOC, which may mess up your production service monitoring if you chose a production server for the test. osg-configure -v osg-configure -c /etc/init.d/condor-cron start /etc/init.d/rsv start rsv-control --list rsv-control --version rsv-control --run --all-enabled 11. make sure the results from the above commands look fine. Testing an upgrade installation: make sure to enable the osg-testing repo, and set its priority higher than the other repos yum --enablerepo=osg-testing upgrade rsv* you can use the old 30-rsv.ini file for configuration repeat steps 4)~11) as mentioned in the last section. Slurm This section goes through the steps needed to set up a slurm install on a VM. This is a necessary prerequisite for testing Slurm related components (CE integration, gratia, etc.). Note that the slurm setup used for this uses weak passwords for mysql. It should be sufficient for a quick setup, testing, and then tear down but should not be used without changes if it will be running for any appreciable length of time. Note need to have a VM with 2+ GB of memory Installation and setup Download scripts and config files: cd /tmp/ git clone https://github.com/sthapa/utilities.git cd utilities/slurm setup and install slurm components export username= USERNAME \\# user that jobs will run as export version= 14.11.7 \\# slurm version to install (e.g. 16.05.2 or 14.11.7) ./slurm-setup.sh After successful completion, slurm and slurm gratia probes should be setup and enabled. Running a job using slurm Generate test.sh with the following: #/bin/bash echo In the directory: `pwd` echo As the user: `whoami` echo \u201cHostname: /bin/hostname sleep 60 /pre run sbatch test.sh the output from the jobs should appear in the current working directory as test.sh.[eo].nnnnn where nnnnn is a job id VO Client This document contains a basic recipe for testing a VO Package release Prerequisites Testing the VO package requires a few components: * X.509 certificate with membership to at least one VO * System with working GUMS installation * System with OSG installation (voms-proxy-init and edg-mkgridmap) Testing voms-proxy-init Login in the system that has voms-proxy-init installed. Make sure that you have the correct vo-client rpms installed and that your X.509 certificate is in your home directory. For each VO that you have membership in, run the following voms-proxy-init -voms [VO] where [VO] is the appropriate VO (e.g. osg, cms, etc.). You should be able to generate a voms-proxy for that VO without errors. Testing edg-mkgridmap Log on to a system with edg-mkgridmap installed. Make sure you have the correct vo-client rpms installed (vo-client-edgmkgridmap). Run edg-mkgridmap and check the log output for errors. There will be some errors so compare your errors with the errors on previous vo-package tickets to make sure no new errors have appeared. Testing GUMS Log on to a system with a working GUMS install. Make sure that you have the correct vo-client rpms (osg-gums-config) installed. Make a backup of /etc/gums/gums.config Copy the mysql database information from /etc/gums/gums.config to /etc/gums/gums.config.template Copy /etc/gums/gums.config.template to /etc/gums/gums.config Start the tomcat6 service Go to the GUMS interface (e.g. https://my.host:8443/gums) Go to the Update VO members page and click on the update VO members button Once completed, there will probably be some errors. Compare errors to errors on prior vo package update tickets and make sure no new errors have occurred. VOMS Admin Server Install and configure voms-admin-server Install and configure voms-admin-server with this voms-install.sh script, entering osg-testing when prompted for the REPO and your own e-mail address when prompted for EMAIL_FROM . Set up TEST_VO and add yourself as an admin To add a test VO (named TEST_VO) and add yourself as an admin, use the following script, replacing the USER_EMAIL , USER_CERT_SUBJECT , and USER_COMMON_NAME variables with your own: #!/bin/bash VO_NAME = TEST_VO TOMCAT_PORT = 8443 USER_EMAIL = YOUR_EMAIL USER_CERT_SUBJECT = YOUR CERT DN USER_CERT_ISSUER = /DC=com/DC=DigiCert-Grid/O=DigiCert Grid/CN=DigiCert Grid CA-1 USER_COMMON_NAME = YOUR CERT CN voms-admin --vo $VO_NAME --host $HOSTNAME --nousercert create-user \\ $USER_CERT_SUBJECT $USER_CERT_ISSUER $USER_COMMON_NAME $USER_EMAIL voms-admin --vo $VO_NAME --host $HOSTNAME --nousercert assign-role \\ / $VO_NAME VO-Admin $USER_CERT_SUBJECT $USER_CERT_ISSUER echo web ui: https:// $HOSTNAME : $TOMCAT_PORT /voms/ $VO_NAME Testing the web UI Add/suspend/restore/delete a user (using your e-mail address for contact) Verify e-mail receipt of suspension message XRootD VOMS Testing This section is intended for OSG Software/Release teams to gather information on testing vomsxrd/xrootd-voms-plugin package. Original plugin named vomsxrd , similar to lcmaps that extracts information for authorization within xrootd of a proxy's voms extension. You need an xrootd server installation In the xrootd server yum install the following packages: xrootd xrootd-voms-plugin vo-client In the xrootd client yum install the following packages: xrootd-client voms-clients vo-client In the xrootd server add this lines to file /etc/xrootd/xrootd-clustered.cfg xrootd.seclib /usr/lib64/libXrdSec.so sec.protparm gsi -vomsfun:/usr/lib64/libXrdSecgsiVOMS.so -vomsfunparms:certfmt=raw|vos=cms|dbg -vomsat:2 sec.protocol /usr/lib64 gsi -ca:1 -crl:3 This configuration will only authorize members of VO cms . You can change it to another VO. Make sure fetch-crl has been run otherwise the xrootd service may fail to start. In the xrootd client get a proxy without voms extension or with another VO extension different that the one used in the configuration: user@host $ voms-proxy-init -voms mis Enter GRID pass phrase: Your identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020 Creating temporary proxy ........................... Done Contacting voms.opensciencegrid.org:15001 [/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=http/voms.opensciencegrid.org] mis Done Creating proxy ............................................... Done user@host $ xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/ [0B/0B][100%][==================================================][0B/s] Run: [FATAL] Auth failed Now get a proxy with cms extension and run it again: user@host $ voms-proxy-init -voms cms Enter GRID pass phrase: Your identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020 Creating temporary proxy ...................................... Done Contacting voms2.cern.ch:15002 [/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch] cms Done Creating proxy .......................................... Done Your proxy is valid until Thu Dec 4 22:53:29 2014 user@host $ xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/ [0B/0B][100%][==================================================][0B/s] XRootD Plugins Hadoop name node installation Use the following script with option 1: #!/bin/bash set -e select NODETYPE in namenode datanode gridftp ; do [[ $NODETYPE ]] break done case $NODETYPE in namenode ) NAMENODE = $HOSTNAME ;; * ) read -p hostname for NAMENODE? NAMENODE ;; esac echo NODETYPE = $NODETYPE echo NAMENODE = $NAMENODE read -p ok? [y/N] ok case $ok in y* | Y* ) ;; # ok * ) exit ;; esac #yum install --enablerepo=osg-minefield osg-se-hadoop-$NODETYPE yum install osg-se-hadoop- $NODETYPE case $NODETYPE in namenode | datanode ) mkdir -p /data/ { hadoop,scratch,checkpoint } chown -R hdfs:hdfs /data sed -i s/NAMENODE/ $NAMENODE / /etc/hadoop/conf.osg/ { core,hdfs } -site.xml cp /etc/hadoop/conf.osg/ { core,hdfs } -site.xml /etc/hadoop/conf/ touch /etc/hosts_exclude ;; gridftp ) ln -snf conf.osg /etc/hadoop/conf sed -i s/NAMENODE/ $NAMENODE / /etc/hadoop/conf.osg/ { core,hdfs } -site.xml echo hadoop-fuse-dfs# /mnt/hadoop fuse server= $NAMENODE ,port=9000,rdbuffer=131072,allow_other 0 0 /etc/fstab mkdir /mnt/hadoop mount /mnt/hadoop cp -v /etc/redhat-release /mnt/hadoop/test-file sed -i /globus_mapping/s/^# *// /etc/grid-security/gsi-authz.conf sed -i s/yourgums.yourdomain/gums.fnal.gov/ /etc/lcmaps.db mkdir /mnt/hadoop/fnalgrid useradd fnalgrid -g fnalgrid chown fnalgrid:fnalgrid /mnt/hadoop/fnalgrid service globus-gridftp-server start if type -t globus-url-copy /dev/null ; then globus-url-copy file:////bin/bash gsiftp:// $HOSTNAME /mnt/hadoop/fnalgrid/first_test else echo globus-url-copy not installed fi ;; esac case $NODETYPE in namenode ) su - hdfs -c hadoop namenode -format ;; esac service hadoop-hdfs- $NODETYPE start Hadoop data node installation Run same script as before but with option number 2. Install xrootd-server: yum install xrootd-server Install xrootd-plugins yum install xrootd-cmstfc xrootd-hdfs GridFTP installation Run same as script but with option number. On the name node [root@fermicloud092 ~]# hdfs dfs -ls /test-file Found 1 items -rw-r--r-- 2 root root 0 2014-07-21 15:57 /test-file This means your hadoop is working. Modify the file /etc/xrootd/xrootd-clustered.cfg to look like this: xrd.port 1094 # The roles this server will play. all.role server all.role manager if xrootd.unl.edu # The known managers all.manager srm.unl.edu:1213 #all.manager xrootd.ultralight.org:1213 # Allow any path to be exported; this is further refined in the authfile. all.export / nostage # Hosts allowed to use this xrootd cluster cms.allow host * ### Standard directives # Simple sites probably don t need to touch these. # Logging verbosity xrootd.trace all -debug ofs.trace all -debug xrd.trace all -debug cms.trace all -debug # Integrate with CMS TFC, placed in /etc/storage.xml oss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop xrootd.seclib /usr/lib64/libXrdSec.so xrootd.fslib /usr/lib64/libXrdOfs.so ofs.osslib /usr/lib64/libXrdHdfs.so all.adminpath /var/run/xrootd all.pidpath /var/run/xrootd cms.delay startup 10 cms.fxhold 60s cms.perf int 30s pgm /usr/bin/XrdOlbMonPerf 30 oss.space default_stage /opt/xrootd_cache Create file /etc/xrootd/storage.xml and place this: storage-mapping lfn-to-pfn protocol= hadoop destination-match= .* path-match= .*/+tmp2/test-file result= /test-file / /storage-mapping For el7 the instrucctions are a little bit different. See: https://jira.opensciencegrid.org/browse/SOFTWARE-2198?focusedCommentId=334667 page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-334667 Now from a node do: xrdcp --debug 3 root://yourdatanode.yourdomain:1094//tmp2/test-file . If it is sucessful it would have tested both cmstfc and hdfs plugins","title":"Acceptance Testing"},{"location":"release/acceptance-testing/#acceptance-testing","text":"The OSG Release Team collects and maintains testing procedures for major components in the OSG Sofware Stack. These test should verify that basic functionality of the component works in typically deployed configurations.","title":"Acceptance Testing"},{"location":"release/acceptance-testing/#cvmfs","text":"Note This acceptance testing recipe was created when access to a machine with sufficient disk space to make a complete replica of OASIS was not available.","title":"CVMFS"},{"location":"release/acceptance-testing/#creating-a-cvmfs-repository-server-stratum-0","text":"Disable SELinux by setting the following in /etc/selinux/config . SELINUX=disabled Check kernel version. uname -a CVMFS for EL7 requires OverlayFS (as of kernel version 4.2.x). If default kernel is = 4.2.x, update kernel. root@host # rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org root@host # rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm root@host # yum install yum-plugin-fastestmirror root@host # yum --enablerepo = elrepo-kernel install kernel-ml Select updated kernel by editing /etc/default/grub . GRUB_DEFAULT=0 and run: root@host # grub2-mkconfig -o /boot/grub2/grub.cfg Reboot system. Check kernel version again and make sure SELinux is disabled. root@host # uname -a root@host # sestatus If kernel = 4.2 and SELinux is disabled, then update system and install CVMFS server and client packages. root@host # yum update root@host # yum install epel-release root@host # yum install yum-plugin-priorities root@host # rpm -Uvh https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm root@host # yum install cvmfs cvmfs-server Configure web server and start it up. Edit /etc/httpd/conf.d/cvmfs.conf : Listen 8000 KeepAlive On and run: root@host # chkconfig httpd on root@host # service httpd start Make new repository. root@host # cvmfs_server mkfs test.cvmfs-stratum-0.novalocal Run transaction on new repository to enable write access. root@host # cvmfs_server transaction test.cvmfs-stratum-0.novalocal Place some sample code in new repository directory and then publish it. root@host # cd /cvmfs/test.cvmfs-stratum-0.novalocal root@host # vi [ bash \\_ pi.sh ]( %ATTACHURL%/bash_pi.sh ) root@host # chmod +x bash \\_ pi.sh root@host # cvmfs \\_ server publish test.cvmfs-stratum-0.novalocal Check repository status after publication. root@host # cvmfs \\_ server check root@host # cvmfs \\_ server tag root@host # wget -qO- http://localhost:8000/cvmfs/test.cvmfs-stratum-0.novalocal/.cvmfswhitelist%7Ccat -v Download a copy of the CVMFS repository's public key e.g., /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub","title":"Creating a CVMFS Repository Server (Stratum 0)"},{"location":"release/acceptance-testing/#creating-a-cvmfs-replica-server-stratum-1","text":"Repeat steps 1 though 8 in the previous section on \"Creating a CVMFS Repository Server \". However, now also install mod_wsgi . root@host # yum install cvmfs cvmfs-server mod \\_ wsgi Upload a copy of the CVMFS repository's public key and place in /etc/cvmfs/keys directory. Add replica of the repository. root@host # cvmfs_server add-replica -o root http://10.128.3.96:8000/cvmfs/test.cvmfs-stratum-0.novalocal /etc/cvmfs/keys/test.cvmfs-stratum-0.novalocal.pub Make a snapshot of the repository. root@host # cvmfs \\_ server snapshot test.cvmfs-stratum-0.novalocal","title":"Creating a CVMFS Replica Server (Stratum 1)"},{"location":"release/acceptance-testing/#creating-a-cvmfs-client","text":"Update system and install CVMFS client package. root@host # yum update root@host # yum install epel-release root@host # yum install yum-plugin-priorities root@host # rpm -Uvh https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm root@host # yum install cvmfs Upload a copy of the CVMFS repository's public key and place in /etc/cvmfs/keys directory. Edit fuse configuration /etc/fuse.conf . user_allow_other Edit autofs configuration and restart service /etc/auto.master . /cvmfs /etc/auto.cvmfs and run: root@host # service autofs restart Edit cvmfs configuration ( /etc/cvmfs/default.local ) to point to replica server. CVMFS_SERVER_URL = http://10.128.3.97:8000/cvmfs/@fqrn@ CVMFS_REPOSITORIES = test.cvmfs-stratum-0.novalocal CVMFS_HTTP_PROXY = DIRECT Remove OSG CVMFS configuration file. rm /etc/cvmfs/default.d/60-osg.conf Run CVMFS config probe. cvmfs_config probe test.cvmfs-stratum-0.novalocal Check CVMFS config status. cvmfs_config stat -v test.cvmfs-stratum-0.novalocal Execute sample code published to repository from client. /cvmfs/test.cvmfs-stratum-0.novalocal/bash_pi.sh -b 8 -r 5 -s 10000","title":"Creating a CVMFS client"},{"location":"release/acceptance-testing/#creating-an-oasis-client","text":"Update system and install CVMFS client package. yum update yum install epel-release yum install yum-plugin-priorities rpm -Uvh https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm yum install osg-oasis Verify latest versions of cvmfs, cvmfs-config-osg, and cvmfs-x509-helper have been installed. Edit fuse configuration. vi /etc/fuse.conf user_allow_other Edit cvmfs configuration to point to replica server. vi /etc/cvmfs/default.local CVMFS_REPOSITORIES=\"`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\" CVMFS_QUOTA_LIMIT=20000 CVMFS_HTTP_PROXY=DIRECT Edit autofs configuration and restart service. vi /etc/auto.master /cvmfs /etc/auto.cvmfs service autofs restart Run CVMFS config probe. cvmfs_config probe Check CVMFS config status. cvmfs_config stat -v oasis.opensciencegrid.org","title":"Creating an OASIS client"},{"location":"release/acceptance-testing/#additional-documentation","text":"CERN's CVMFS Documentation OSG's CVMFS Replica Server OSG's CVMFS Client Documentation OSG's OASIS Documentation bash_pi.sh : A bash script that uses a simple Monte Carlo method to estimate the value of Pi","title":"Additional Documentation"},{"location":"release/acceptance-testing/#gratia-probe","text":"This section documents the testing procedure to test the gratia probes sufficiently tested to be promoted to the osg-testing repository. The test procedure is the same on both SL6 and SL7. install or update the gratia-probe-condor rpm as appropriate On each VM download the gratia-probe-setup.sh script and run it In /etc/gratia/condor/ProbeConfig , verify the following have been changed: change SiteName to something aside from Generic Site change EnableProbe to 1 change CollectorHost , SSLHost , and SSLRegistrationHost to the an invalid host (E.g. test.com) or the localhost Create /var/lib/osg/ and download the attached user-vo-map file and place it in that directory Edit the user-vo-map file and change the account from sthapa to the account you'll be using to submit the condor jobs in the following step Download and submit the attached condor_submit file (note, on the default fermicloud VM, this takes about 3 hours, so you may want to set NUM_CPUS to 50 so that 50 jobs will run at a time) Run /usr/share/gratia/condor/condor_meter Check /var/lib/gratia/tmp/gratiafiles/ for a subdir.condor_... directory and verify that there are 200 xml jobs and the cpus/wall times are appropriate (either PT0S or PT1M).","title":"Gratia Probe"},{"location":"release/acceptance-testing/#gsi-openssh","text":"To test a fresh installation: Spin up two VM's and set up the EPEL/OSG repos on both of them. Choose one of the VM's, it will be the server VM. Consult these instructions to set up the server. From the other VM (client): Install the necessary packages: root@host # yum install globus-proxy-utils gsi-openssh-clients Initialize your proxy. After this, none of the gsi commands should prompt you for your password. Connect to the server: user@host $ gsissh -p 2222 server hostname Copy a test file to the server: user@host $ gsiscp -p 2222 testfile server hostname :/tmp Connect to the server via SFTP and grab files: user@host $ gsisftp -P 2222 server hostname user@host $ cd /tmp user@host $ get testfile","title":"GSI OpenSSH"},{"location":"release/acceptance-testing/#htcondor-ce-collector-wip","text":"The CE Collector is a stripped-down version of HTCondor-CE that contains mostly just the collector daemon and configs. It was introduced in htcondor-ce-1.6. The production CE Collectors run at the GOC, but you may want to set up your own for testing. Make 2 VMs with the EPEL/OSG repos installed: one for the collector, and one for the CE","title":"HTCondor-CE Collector (WIP)"},{"location":"release/acceptance-testing/#setting-up-the-collector","text":"Install htcondor-ce-collector Create a file called /etc/condor-ce/config.d/99-local.conf that contains this line: COLLECTOR.ALLOW_ADVERTISE_SCHEDD = $(COLLECTOR.ALLOW_ADVERTISE_SCHEDD), your_htcondor_ce_host.example.net /pre (with your_htcondor_ce_host replaced by the hostname the HTCondor-CE VM) Run service condor-ce-collector start","title":"Setting Up the Collector"},{"location":"release/acceptance-testing/#setting-up-the-ce","text":"Install osg-htcondor-ce-condor (replace condor with the batch system of your choice) Ensure osg-configure = 1.0.60-2 is installed Configure your CE using osg-configure You should use the HTCondor-CE Install Docs as a reference, although you can skip several of the steps You can skip setting up Squid: set enabled to True and location to UNAVAILABLE in 01-squid.ini Set htcondor_gateway_enabled to True in 10-gateway.ini You probably don't need GUMS, but if you want it, use the Fermi GUMS server (set gums_host to gums.fnal.gov and authorization_method to xacml in 10-misc.ini) To keep osg-configure from complaining about storage, edit 10-storage.ini : Set se_available to False Set app_dir to /osg/app_dir Set data_dir to /osg/data_dir Do mkdir -p /osg/app_dir/etc; mkdir -p /osg/data_dir; chmod 1777 /osg/app_dir{,/etc} Enable your batch system by setting enabled to True in 20- batch system .ini Set up the site info in 40-siteinfo.ini ; in particular, you'll need to set the resource and resource_group settings \\ (you just need to pick a name; I concatenate my login name with the short host name and use that, e.g. matyasfermicloud001). \\ You can also use the following settings: group=OSG-ITB sponsor=local city=Batavia, IL country=US longitude=-88 latitude=41 Edit the file /etc/osg/config.d/30-infoservices.ini and set ce_collectors to the collector host Run osg-configure -dc Start up your batch system Run service condor-ce start The CE will report to the collector host every five minutes. If you want to force it to send now, run condor_ce_reconfig . You should see your CE if you run condor_ce_status -schedd on the collector host.","title":"Setting Up the CE"},{"location":"release/acceptance-testing/#rsv","text":"Testing a fresh installation: make sure the yum repositories required by OSG is installed on your host rpm -Uvh http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm OR rpm -Uvh http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm also make sure epel repo is set up. install the rpm yum --enablerepo=osg-testing install rsv edit /etc/osg/config.d/30-rsv.ini file in my case, I don't have a service cert for testing, so I use my own personal cert to create the proxy, but later on the owner of the proxy should be changed to \"rsv\" user that is created during the rpm install. in the config file, for the ce_hosts and gridftp_hosts, put in a test server, as the result from this test will be uploaded to OSG GOC, which may mess up your production service monitoring if you chose a production server for the test. osg-configure -v osg-configure -c /etc/init.d/condor-cron start /etc/init.d/rsv start rsv-control --list rsv-control --version rsv-control --run --all-enabled 11. make sure the results from the above commands look fine. Testing an upgrade installation: make sure to enable the osg-testing repo, and set its priority higher than the other repos yum --enablerepo=osg-testing upgrade rsv* you can use the old 30-rsv.ini file for configuration repeat steps 4)~11) as mentioned in the last section.","title":"RSV"},{"location":"release/acceptance-testing/#slurm","text":"This section goes through the steps needed to set up a slurm install on a VM. This is a necessary prerequisite for testing Slurm related components (CE integration, gratia, etc.). Note that the slurm setup used for this uses weak passwords for mysql. It should be sufficient for a quick setup, testing, and then tear down but should not be used without changes if it will be running for any appreciable length of time. Note need to have a VM with 2+ GB of memory","title":"Slurm"},{"location":"release/acceptance-testing/#installation-and-setup","text":"Download scripts and config files: cd /tmp/ git clone https://github.com/sthapa/utilities.git cd utilities/slurm setup and install slurm components export username= USERNAME \\# user that jobs will run as export version= 14.11.7 \\# slurm version to install (e.g. 16.05.2 or 14.11.7) ./slurm-setup.sh After successful completion, slurm and slurm gratia probes should be setup and enabled.","title":"Installation and setup"},{"location":"release/acceptance-testing/#running-a-job-using-slurm","text":"Generate test.sh with the following: #/bin/bash echo In the directory: `pwd` echo As the user: `whoami` echo \u201cHostname: /bin/hostname sleep 60 /pre run sbatch test.sh the output from the jobs should appear in the current working directory as test.sh.[eo].nnnnn where nnnnn is a job id","title":"Running a job using slurm"},{"location":"release/acceptance-testing/#vo-client","text":"This document contains a basic recipe for testing a VO Package release","title":"VO Client"},{"location":"release/acceptance-testing/#prerequisites","text":"Testing the VO package requires a few components: * X.509 certificate with membership to at least one VO * System with working GUMS installation * System with OSG installation (voms-proxy-init and edg-mkgridmap)","title":"Prerequisites"},{"location":"release/acceptance-testing/#testing-voms-proxy-init","text":"Login in the system that has voms-proxy-init installed. Make sure that you have the correct vo-client rpms installed and that your X.509 certificate is in your home directory. For each VO that you have membership in, run the following voms-proxy-init -voms [VO] where [VO] is the appropriate VO (e.g. osg, cms, etc.). You should be able to generate a voms-proxy for that VO without errors.","title":"Testing voms-proxy-init"},{"location":"release/acceptance-testing/#testing-edg-mkgridmap","text":"Log on to a system with edg-mkgridmap installed. Make sure you have the correct vo-client rpms installed (vo-client-edgmkgridmap). Run edg-mkgridmap and check the log output for errors. There will be some errors so compare your errors with the errors on previous vo-package tickets to make sure no new errors have appeared.","title":"Testing edg-mkgridmap"},{"location":"release/acceptance-testing/#testing-gums","text":"Log on to a system with a working GUMS install. Make sure that you have the correct vo-client rpms (osg-gums-config) installed. Make a backup of /etc/gums/gums.config Copy the mysql database information from /etc/gums/gums.config to /etc/gums/gums.config.template Copy /etc/gums/gums.config.template to /etc/gums/gums.config Start the tomcat6 service Go to the GUMS interface (e.g. https://my.host:8443/gums) Go to the Update VO members page and click on the update VO members button Once completed, there will probably be some errors. Compare errors to errors on prior vo package update tickets and make sure no new errors have occurred.","title":"Testing GUMS"},{"location":"release/acceptance-testing/#voms-admin-server","text":"","title":"VOMS Admin Server"},{"location":"release/acceptance-testing/#install-and-configure-voms-admin-server","text":"Install and configure voms-admin-server with this voms-install.sh script, entering osg-testing when prompted for the REPO and your own e-mail address when prompted for EMAIL_FROM .","title":"Install and configure voms-admin-server"},{"location":"release/acceptance-testing/#set-up-test_vo-and-add-yourself-as-an-admin","text":"To add a test VO (named TEST_VO) and add yourself as an admin, use the following script, replacing the USER_EMAIL , USER_CERT_SUBJECT , and USER_COMMON_NAME variables with your own: #!/bin/bash VO_NAME = TEST_VO TOMCAT_PORT = 8443 USER_EMAIL = YOUR_EMAIL USER_CERT_SUBJECT = YOUR CERT DN USER_CERT_ISSUER = /DC=com/DC=DigiCert-Grid/O=DigiCert Grid/CN=DigiCert Grid CA-1 USER_COMMON_NAME = YOUR CERT CN voms-admin --vo $VO_NAME --host $HOSTNAME --nousercert create-user \\ $USER_CERT_SUBJECT $USER_CERT_ISSUER $USER_COMMON_NAME $USER_EMAIL voms-admin --vo $VO_NAME --host $HOSTNAME --nousercert assign-role \\ / $VO_NAME VO-Admin $USER_CERT_SUBJECT $USER_CERT_ISSUER echo web ui: https:// $HOSTNAME : $TOMCAT_PORT /voms/ $VO_NAME","title":"Set up TEST_VO and add yourself as an admin"},{"location":"release/acceptance-testing/#testing-the-web-ui","text":"Add/suspend/restore/delete a user (using your e-mail address for contact) Verify e-mail receipt of suspension message","title":"Testing the web UI"},{"location":"release/acceptance-testing/#xrootd-voms-testing","text":"This section is intended for OSG Software/Release teams to gather information on testing vomsxrd/xrootd-voms-plugin package. Original plugin named vomsxrd , similar to lcmaps that extracts information for authorization within xrootd of a proxy's voms extension. You need an xrootd server installation In the xrootd server yum install the following packages: xrootd xrootd-voms-plugin vo-client In the xrootd client yum install the following packages: xrootd-client voms-clients vo-client In the xrootd server add this lines to file /etc/xrootd/xrootd-clustered.cfg xrootd.seclib /usr/lib64/libXrdSec.so sec.protparm gsi -vomsfun:/usr/lib64/libXrdSecgsiVOMS.so -vomsfunparms:certfmt=raw|vos=cms|dbg -vomsat:2 sec.protocol /usr/lib64 gsi -ca:1 -crl:3 This configuration will only authorize members of VO cms . You can change it to another VO. Make sure fetch-crl has been run otherwise the xrootd service may fail to start. In the xrootd client get a proxy without voms extension or with another VO extension different that the one used in the configuration: user@host $ voms-proxy-init -voms mis Enter GRID pass phrase: Your identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020 Creating temporary proxy ........................... Done Contacting voms.opensciencegrid.org:15001 [/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=http/voms.opensciencegrid.org] mis Done Creating proxy ............................................... Done user@host $ xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/ [0B/0B][100%][==================================================][0B/s] Run: [FATAL] Auth failed Now get a proxy with cms extension and run it again: user@host $ voms-proxy-init -voms cms Enter GRID pass phrase: Your identity: /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Edgar Mauricio Fajardo Hernandez 2020 Creating temporary proxy ...................................... Done Contacting voms2.cern.ch:15002 [/DC=ch/DC=cern/OU=computers/CN=voms2.cern.ch] cms Done Creating proxy .......................................... Done Your proxy is valid until Thu Dec 4 22:53:29 2014 user@host $ xrdcp vomsxrdtest root://fermicloud024.fnal.gov:1094//tmp/ [0B/0B][100%][==================================================][0B/s]","title":"XRootD VOMS Testing"},{"location":"release/acceptance-testing/#xrootd-plugins","text":"","title":"XRootD Plugins"},{"location":"release/acceptance-testing/#hadoop-name-node-installation","text":"Use the following script with option 1: #!/bin/bash set -e select NODETYPE in namenode datanode gridftp ; do [[ $NODETYPE ]] break done case $NODETYPE in namenode ) NAMENODE = $HOSTNAME ;; * ) read -p hostname for NAMENODE? NAMENODE ;; esac echo NODETYPE = $NODETYPE echo NAMENODE = $NAMENODE read -p ok? [y/N] ok case $ok in y* | Y* ) ;; # ok * ) exit ;; esac #yum install --enablerepo=osg-minefield osg-se-hadoop-$NODETYPE yum install osg-se-hadoop- $NODETYPE case $NODETYPE in namenode | datanode ) mkdir -p /data/ { hadoop,scratch,checkpoint } chown -R hdfs:hdfs /data sed -i s/NAMENODE/ $NAMENODE / /etc/hadoop/conf.osg/ { core,hdfs } -site.xml cp /etc/hadoop/conf.osg/ { core,hdfs } -site.xml /etc/hadoop/conf/ touch /etc/hosts_exclude ;; gridftp ) ln -snf conf.osg /etc/hadoop/conf sed -i s/NAMENODE/ $NAMENODE / /etc/hadoop/conf.osg/ { core,hdfs } -site.xml echo hadoop-fuse-dfs# /mnt/hadoop fuse server= $NAMENODE ,port=9000,rdbuffer=131072,allow_other 0 0 /etc/fstab mkdir /mnt/hadoop mount /mnt/hadoop cp -v /etc/redhat-release /mnt/hadoop/test-file sed -i /globus_mapping/s/^# *// /etc/grid-security/gsi-authz.conf sed -i s/yourgums.yourdomain/gums.fnal.gov/ /etc/lcmaps.db mkdir /mnt/hadoop/fnalgrid useradd fnalgrid -g fnalgrid chown fnalgrid:fnalgrid /mnt/hadoop/fnalgrid service globus-gridftp-server start if type -t globus-url-copy /dev/null ; then globus-url-copy file:////bin/bash gsiftp:// $HOSTNAME /mnt/hadoop/fnalgrid/first_test else echo globus-url-copy not installed fi ;; esac case $NODETYPE in namenode ) su - hdfs -c hadoop namenode -format ;; esac service hadoop-hdfs- $NODETYPE start","title":"Hadoop name node installation"},{"location":"release/acceptance-testing/#hadoop-data-node-installation","text":"Run same script as before but with option number 2. Install xrootd-server: yum install xrootd-server Install xrootd-plugins yum install xrootd-cmstfc xrootd-hdfs","title":"Hadoop data node installation"},{"location":"release/acceptance-testing/#gridftp-installation","text":"Run same as script but with option number.","title":"GridFTP installation"},{"location":"release/acceptance-testing/#on-the-name-node","text":"[root@fermicloud092 ~]# hdfs dfs -ls /test-file Found 1 items -rw-r--r-- 2 root root 0 2014-07-21 15:57 /test-file This means your hadoop is working. Modify the file /etc/xrootd/xrootd-clustered.cfg to look like this: xrd.port 1094 # The roles this server will play. all.role server all.role manager if xrootd.unl.edu # The known managers all.manager srm.unl.edu:1213 #all.manager xrootd.ultralight.org:1213 # Allow any path to be exported; this is further refined in the authfile. all.export / nostage # Hosts allowed to use this xrootd cluster cms.allow host * ### Standard directives # Simple sites probably don t need to touch these. # Logging verbosity xrootd.trace all -debug ofs.trace all -debug xrd.trace all -debug cms.trace all -debug # Integrate with CMS TFC, placed in /etc/storage.xml oss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop xrootd.seclib /usr/lib64/libXrdSec.so xrootd.fslib /usr/lib64/libXrdOfs.so ofs.osslib /usr/lib64/libXrdHdfs.so all.adminpath /var/run/xrootd all.pidpath /var/run/xrootd cms.delay startup 10 cms.fxhold 60s cms.perf int 30s pgm /usr/bin/XrdOlbMonPerf 30 oss.space default_stage /opt/xrootd_cache Create file /etc/xrootd/storage.xml and place this: storage-mapping lfn-to-pfn protocol= hadoop destination-match= .* path-match= .*/+tmp2/test-file result= /test-file / /storage-mapping For el7 the instrucctions are a little bit different. See: https://jira.opensciencegrid.org/browse/SOFTWARE-2198?focusedCommentId=334667 page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-334667 Now from a node do: xrdcp --debug 3 root://yourdatanode.yourdomain:1094//tmp2/test-file . If it is sucessful it would have tested both cmstfc and hdfs plugins","title":"On the name node"},{"location":"release/cut-data-release/","text":"Note If you are performing a software release, please follow the instructions here How to Cut a Data Release This document details the process for releasing new OSG Data Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found here . Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run. Requirements User certificate registered with OSG's koji with build and release team privileges An account on UW CS machines (e.g. library , ingwe ) to access UW's AFS release-tools scripts in your PATH ( GitHub ) osg-build scripts in your PATH (installed via OSG yum repos or source ) Pick the Version and Revision Numbers The rest of this document makes references to REVISION and VERSION(S) , which refer to the space-delimited list of OSG version(s) and data revision, respectively (e.g. 3.3.28 3.4.3 and 2 , respectively). If you are unsure about either the version or revision, please consult the release manager. Day 0: Generate Preliminary Release List The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = VERSION(S) REVISION = REVISION git clone https://github.com/opensciencegrid/release-tools.git cd release-tools 0 -generate-pkg-list -d $REVISION $VERSIONS Day 1: Verify Pre-Release This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release. Step 1: Generate the release list Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated release-list in git). To do this, run the 1-verify-prerelease script from git: VERSIONS = VERSION(S) REVISION = REVISION 1 -verify-prerelease -d $REVISION $VERSIONS If there are any discrepancies consult the release manager. You may have to tag packages with the osg-koji tool. Step 2: Test the Pre-Release on the Madison ITB site Test the pre-release on the Madison ITB by following the ITB pre-release testing instructions . Day 2: Pushing the Release Step 1: Push from pre-release to release This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. VERSIONS = VERSION(S) REVISION = REVISION 2 -create-release -d $REVISION $VERSIONS *.txt files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS. For each release version, use the *release-note* files to update the relevant sections of the release note pages. Step 2: Update the Docker WN client Update the GitHub repo at opensciencegrid/docker-osg-wn using the update-all script found in opensciencegrid/docker-osg-wn-scripts . This requires push access to the opensciencegrid/docker-osg-wn repo. Instructions for using the script: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git docker-osg-wn-scripts/update-all docker-osg-wn cd docker-osg-wn # Verify everything looks fine and run the git push command # that update-all should have printed Step 3: Verify the VO Package and/or CA certificates Wait for the CA certificates to be updated. It may take a while for the updates to reach the mirror used to update the web site. The repository is checked hourly for updated CA certificates. Once the web page is updated, run the following command to update the VO Package and/or CA certificates in the tarball installations and verify that the version of the VO Package and/or CA certificates match the version that was promoted to release. /p/vdt/workspace/tarball-client/current/amd64_rhel6/osgrun osg-update-data /p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data Step 4: Announce the release The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release. The release manager writes the a release announcement for each version and sends it out. The announcement should mention a handful of the most important updates. Due to downstream formatting issues, each major change should end at column 76 or earlier. Here is a sample, replace BRACKETED TEXT with the appropriate values: If you are only updating certificates or only updated the VO package, delete the corresponding text: Subject : Announcing OSG CA Certificate and VO Package Updates Subject : Announcing OSG CA Certificate Update Subject : Announcing VO Package Update We are pleased to announce a data release for the OSG Software Stack . Data releases do not contain any software changes . This release contains updated CA Certificates based on IGTF VERSION : - Change 1 from IGTF changelog - Change 2 from IGTF changelog This release contains VO Package v VERSION : This release also contains VO Package v VERSION : - Change 1 from VO changelog - Change 2 from VO changelog Release notes and pointers to more documentation can be found at : http :// www . opensciencegrid . org /docs/release/ SERIES.VERSION /release- RELEASE-VERSION / Need help ? Let us know : http :// www . opensciencegrid . org /docs/common/help/ We welcome feedback on this release ! The release manager uses the osg-notify tool on submit-1.chtc.wisc.edu to send the release announcement using the following command: PYTHONPATH=src python bin/osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message message-file \\ --subject EMAIL SUBJECT \\ --recipients osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org vdt-discuss@opensciencegrid.org \\ --oim-recipients resources --oim-contact-type administrative Replace EMAIL SUBJECT with an appropriate subject for your announcement. The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\" Day 3: Update the ITB Now that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following the yum update section of the Madison ITB document.","title":"How to Cut a Data Release"},{"location":"release/cut-data-release/#how-to-cut-a-data-release","text":"This document details the process for releasing new OSG Data Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found here . Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.","title":"How to Cut a Data Release"},{"location":"release/cut-data-release/#requirements","text":"User certificate registered with OSG's koji with build and release team privileges An account on UW CS machines (e.g. library , ingwe ) to access UW's AFS release-tools scripts in your PATH ( GitHub ) osg-build scripts in your PATH (installed via OSG yum repos or source )","title":"Requirements"},{"location":"release/cut-data-release/#pick-the-version-and-revision-numbers","text":"The rest of this document makes references to REVISION and VERSION(S) , which refer to the space-delimited list of OSG version(s) and data revision, respectively (e.g. 3.3.28 3.4.3 and 2 , respectively). If you are unsure about either the version or revision, please consult the release manager.","title":"Pick the Version and Revision Numbers"},{"location":"release/cut-data-release/#day-0-generate-preliminary-release-list","text":"The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = VERSION(S) REVISION = REVISION git clone https://github.com/opensciencegrid/release-tools.git cd release-tools 0 -generate-pkg-list -d $REVISION $VERSIONS","title":"Day 0: Generate Preliminary Release List"},{"location":"release/cut-data-release/#day-1-verify-pre-release","text":"This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release.","title":"Day 1: Verify Pre-Release"},{"location":"release/cut-data-release/#step-1-generate-the-release-list","text":"Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated release-list in git). To do this, run the 1-verify-prerelease script from git: VERSIONS = VERSION(S) REVISION = REVISION 1 -verify-prerelease -d $REVISION $VERSIONS If there are any discrepancies consult the release manager. You may have to tag packages with the osg-koji tool.","title":"Step 1: Generate the release list"},{"location":"release/cut-data-release/#step-2-test-the-pre-release-on-the-madison-itb-site","text":"Test the pre-release on the Madison ITB by following the ITB pre-release testing instructions .","title":"Step 2: Test the Pre-Release on the Madison ITB site"},{"location":"release/cut-data-release/#day-2-pushing-the-release","text":"","title":"Day 2: Pushing the Release"},{"location":"release/cut-data-release/#step-1-push-from-pre-release-to-release","text":"This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. VERSIONS = VERSION(S) REVISION = REVISION 2 -create-release -d $REVISION $VERSIONS *.txt files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS. For each release version, use the *release-note* files to update the relevant sections of the release note pages.","title":"Step 1: Push from pre-release to release"},{"location":"release/cut-data-release/#step-2-update-the-docker-wn-client","text":"Update the GitHub repo at opensciencegrid/docker-osg-wn using the update-all script found in opensciencegrid/docker-osg-wn-scripts . This requires push access to the opensciencegrid/docker-osg-wn repo. Instructions for using the script: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git docker-osg-wn-scripts/update-all docker-osg-wn cd docker-osg-wn # Verify everything looks fine and run the git push command # that update-all should have printed","title":"Step 2: Update the Docker WN client"},{"location":"release/cut-data-release/#step-3-verify-the-vo-package-andor-ca-certificates","text":"Wait for the CA certificates to be updated. It may take a while for the updates to reach the mirror used to update the web site. The repository is checked hourly for updated CA certificates. Once the web page is updated, run the following command to update the VO Package and/or CA certificates in the tarball installations and verify that the version of the VO Package and/or CA certificates match the version that was promoted to release. /p/vdt/workspace/tarball-client/current/amd64_rhel6/osgrun osg-update-data /p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data","title":"Step 3: Verify the VO Package and/or CA certificates"},{"location":"release/cut-data-release/#step-4-announce-the-release","text":"The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release. The release manager writes the a release announcement for each version and sends it out. The announcement should mention a handful of the most important updates. Due to downstream formatting issues, each major change should end at column 76 or earlier. Here is a sample, replace BRACKETED TEXT with the appropriate values: If you are only updating certificates or only updated the VO package, delete the corresponding text: Subject : Announcing OSG CA Certificate and VO Package Updates Subject : Announcing OSG CA Certificate Update Subject : Announcing VO Package Update We are pleased to announce a data release for the OSG Software Stack . Data releases do not contain any software changes . This release contains updated CA Certificates based on IGTF VERSION : - Change 1 from IGTF changelog - Change 2 from IGTF changelog This release contains VO Package v VERSION : This release also contains VO Package v VERSION : - Change 1 from VO changelog - Change 2 from VO changelog Release notes and pointers to more documentation can be found at : http :// www . opensciencegrid . org /docs/release/ SERIES.VERSION /release- RELEASE-VERSION / Need help ? Let us know : http :// www . opensciencegrid . org /docs/common/help/ We welcome feedback on this release ! The release manager uses the osg-notify tool on submit-1.chtc.wisc.edu to send the release announcement using the following command: PYTHONPATH=src python bin/osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message message-file \\ --subject EMAIL SUBJECT \\ --recipients osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org vdt-discuss@opensciencegrid.org \\ --oim-recipients resources --oim-contact-type administrative Replace EMAIL SUBJECT with an appropriate subject for your announcement. The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"","title":"Step 4: Announce the release"},{"location":"release/cut-data-release/#day-3-update-the-itb","text":"Now that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following the yum update section of the Madison ITB document.","title":"Day 3: Update the ITB"},{"location":"release/cut-sw-release/","text":"Note If you are performing a data release, please follow the instructions here How to Cut a Software Release This document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found here . Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run. Requirements User certificate registered with OSG's koji with build and release team privileges An account on UW CS machines (e.g. library , ingwe ) to access UW's AFS release-tools scripts in your PATH ( GitHub ) osg-build scripts in your PATH (installed via OSG yum repos or source ) Access to the tarball repository at UNL (osgcollab@hcc-osg-software.unl.edu) Pick the Version Number The rest of this document makes references to VERSION(S) and NON-UPCOMING VERSIONS(S) , which refer to a space-delimited list of OSG version(s) and that same list minus upcoming (e.g. 3.3.28 3.4.3 upcoming and 3.3.28 3.4.3 ). If you are unsure about either the version or revision, please consult the release manager. Day 0: Generate Preliminary Release List The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Step 1: Update the osg-version RPM For each release (excluding upcoming), update the version number in the osg-version RPM's spec file and build it in koji: # If building for the latest release out of trunk osg-build koji osg-version # If building for an older release out of a branch: MAJOR_VERSION = MAJOR VERSION osg-build koji --repo = $MAJOR_VERSION osg-version Where MAJOR VERSION is of the format x.y (e.g. 3.2 ). Step 2: Promote osg-version and generate the release list Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = VERSION(S) git clone https://github.com/opensciencegrid/release-tools.git cd release-tools 0 -generate-pkg-list $VERSIONS Day 1: Verify Pre-Release and Generate Tarballs This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs. Step 1: Verify Pre-Release Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated release-list in git). To do this, run the 1-verify-prerelease script from git: VERSIONS = VERSION(S) 1 -verify-prerelease $VERSIONS If there are any discrepancies consult the release manager. You may have to tag or untag packages with the osg-koji tool. Note Please verify that the osg-version RPM is in your set of packages for the release! Also verify that if there is a new version of the osg-tested-internal RPM, then it is included in the release as well! Step 2: Test Pre-Release in VM Universe To test pre-release, you will be kicking off a manual VM universe test run from osghost.chtc.wisc.edu . Ensure that you meet the pre-requisites for submitting VM universe test runs Prepare the test suite by running: osg-run-tests -P Testing OSG pre-release cd into the directory specified in the output of the previous command Submit the DAG: condor_submit_dag master-run.dag Note If there are failures, consult the release-manager before proceeding. Step 3: Test Pre-Release on the Madison ITB site Test the pre-release on the Madison ITB by following the ITB pre-release testing instructions . If you not local to Madison, consult the release manager for the designated person to do this testing. Step 4: Regenerate the build repositories To avoid 404 errors when retrieving packages, it's necessary to regenerate the build repositories. Run the following script from a machine with your koji-registered user certificate: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) 1 -regen-repos $NON_UPCOMING_VERSIONS Step 5: Create the client tarballs Create the client tarballs as root on an EL7 fermicloud machine using the relevant script from git: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) git clone https://github.com/opensciencegrid/release-tools.git cd release-tools ./1-client-tarballs $NON_UPCOMING_VERSIONS Step 6: Briefly test the client tarballs As an unprivileged user , extract each tarball into a separate directory. Make sure osg-post-install works. Make sure osgrun osg-version works by running the following tests, replacing NON-UPCOMING VERSION(S) with the appropriate version numbers: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) ./1-verify-tarballs $NON_UPCOMING_VERSIONS If you have time, try some of the binaries, such as grid-proxy-init. Todo We need to automate this and have it run on the proper architectures and version of RHEL. Step 7: Update the UW AFS installation of the tarball client The UW keeps an install of the tarball client in /p/vdt/workspace/tarball-client on the UW's AFS. To update it, run the following commands: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) for ver in $NON_UPCOMING_VERSIONS ; do /p/vdt/workspace/tarball-client/afs-install-tarball-client $ver done Step 8: Wait Wait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day. Day 2: Pushing the Release Step 1: Push from pre-release to release This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. VERSIONS = VERSION(S) 2 -create-release $VERSIONS *.txt files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS. For each release version, use the *release-note* files to update the relevant sections of the release note pages. Step 2: Upload the client tarballs Upload the tarballs to the repository with the following procedure from a UW CS machine (e.g., ingwe ): NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) pushd /p/vdt/public/html/tarball-client for ver in $NON_UPCOMING_VERSIONS ; do major_ver = ${ ver %.* } ssh osgcollab@hcc-osg-software.unl.edu mkdir -p /usr/local/repo/tarball-install/ $major_ver / $ver scp -p $major_ver /*/osg-wn-client- $ver *gz osgcollab@hcc-osg-software.unl.edu:/usr/local/repo/tarball-install/ $major_ver / $ver done popd ssh osgcollab@hcc-osg-software.unl.edu bin/mk-sims.sh for ver in $NON_UPCOMING_VERSIONS ; do major_ver = ${ ver %.* } ssh osgcollab@hcc-osg-software.unl.edu cd /usr/local/repo/tarball-install; ls -l $major_ver /*latest* done # verify the latest symlinks point to the version(s) just installed Step 3: Install the tarballs into OASIS Note You must be an OASIS manager of the mis VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L. Get the uploader script from Git and run it with osgrun from the UW AFS install of the tarball client you made earlier. On a UW CSL machine: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) cd /tmp git clone --depth 1 file:///p/vdt/workspace/git/repo/tarball-client.git for ver in $NON_UPCOMING_VERSIONS ; do /p/vdt/workspace/tarball-client/current/sys/osgrun /tmp/tarball-client/upload-tarballs-to-oasis $ver done The script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process. Step 4: Remove old UW AFS installations of the tarball client To keep space usage down, remove tarball client installations and symlinks under /p/vdt/workspace/tarball-client on UW's AFS that are more than 2 months old. The following command will remove them: find /p/vdt/workspace/tarball-client -maxdepth 1 -mtime +60 -name 3 \\* -ls -exec rm -rf {} \\; Step 5: Update the Docker WN client Update the GitHub repo at opensciencegrid/docker-osg-wn using the update-all script found in opensciencegrid/docker-osg-wn-scripts . This requires push access to the opensciencegrid/docker-osg-wn repo. Instructions for using the script: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git docker-osg-wn-scripts/update-all docker-osg-wn cd docker-osg-wn # Verify everything looks fine and run the git push command # that update-all should have printed Step 6: Announce the release The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release. The release manager writes the a release announcement for each version and sends it out. The announcement should mention a handful of the most important updates. Due to downstream formatting issues, each major change should end at column 76 or earlier. Here is a sample, replace BRACKETED TEXT with the appropriate values: Subject : Announcing OSG Software version VERSION We are pleased to announce OSG Software version VERSION ! Changes to OSG VERSION include : - Major Change 1 - Major Change 2 - Major Change 3 Release notes and pointers to more documentation can be found at : http :// www . opensciencegrid . org /docs/release/ SERIES.VERSION /release- RELEASE-VERSION / Need help ? Let us know : http :// www . opensciencegrid . org /docs/common/help/ We welcome feedback on this release ! The release manager uses the osg-notify tool on submit-1.chtc.wisc.edu to send the release announcement using the following command: PYTHONPATH=src python bin/osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message message-file \\ --subject EMAIL SUBJECT \\ --recipients osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org vdt-discuss@opensciencegrid.org \\ --oim-recipients resources --oim-contact-type administrative Replace EMAIL SUBJECT with an appropriate subject for your announcement. The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Also set the Fix Versions field to the appropriate value(s) and uncheck the box that reads \"Send mail for this update\" Day 3: Update the ITB Now that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following the yum update section of the Madison ITB document. If you are not local to Madison, consult the release manager for the designated person to do the update. Remember to stop the HTCondor and HTCondor-CE daemons according to the HTCondor pre-release testing instructions . Those daemons will need to be restarted after the upgraode.","title":"How to Cut a Release"},{"location":"release/cut-sw-release/#how-to-cut-a-software-release","text":"This document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found here . Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.","title":"How to Cut a Software Release"},{"location":"release/cut-sw-release/#requirements","text":"User certificate registered with OSG's koji with build and release team privileges An account on UW CS machines (e.g. library , ingwe ) to access UW's AFS release-tools scripts in your PATH ( GitHub ) osg-build scripts in your PATH (installed via OSG yum repos or source ) Access to the tarball repository at UNL (osgcollab@hcc-osg-software.unl.edu)","title":"Requirements"},{"location":"release/cut-sw-release/#pick-the-version-number","text":"The rest of this document makes references to VERSION(S) and NON-UPCOMING VERSIONS(S) , which refer to a space-delimited list of OSG version(s) and that same list minus upcoming (e.g. 3.3.28 3.4.3 upcoming and 3.3.28 3.4.3 ). If you are unsure about either the version or revision, please consult the release manager.","title":"Pick the Version Number"},{"location":"release/cut-sw-release/#day-0-generate-preliminary-release-list","text":"The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release.","title":"Day 0: Generate Preliminary Release List"},{"location":"release/cut-sw-release/#step-1-update-the-osg-version-rpm","text":"For each release (excluding upcoming), update the version number in the osg-version RPM's spec file and build it in koji: # If building for the latest release out of trunk osg-build koji osg-version # If building for an older release out of a branch: MAJOR_VERSION = MAJOR VERSION osg-build koji --repo = $MAJOR_VERSION osg-version Where MAJOR VERSION is of the format x.y (e.g. 3.2 ).","title":"Step 1: Update the osg-version RPM"},{"location":"release/cut-sw-release/#step-2-promote-osg-version-and-generate-the-release-list","text":"Run 0-generate-pkg-list from a machine that has your koji-registered user certificate: VERSIONS = VERSION(S) git clone https://github.com/opensciencegrid/release-tools.git cd release-tools 0 -generate-pkg-list $VERSIONS","title":"Step 2: Promote osg-version and generate the release list"},{"location":"release/cut-sw-release/#day-1-verify-pre-release-and-generate-tarballs","text":"This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs.","title":"Day 1: Verify Pre-Release and Generate Tarballs"},{"location":"release/cut-sw-release/#step-1-verify-pre-release","text":"Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated release-list in git). To do this, run the 1-verify-prerelease script from git: VERSIONS = VERSION(S) 1 -verify-prerelease $VERSIONS If there are any discrepancies consult the release manager. You may have to tag or untag packages with the osg-koji tool. Note Please verify that the osg-version RPM is in your set of packages for the release! Also verify that if there is a new version of the osg-tested-internal RPM, then it is included in the release as well!","title":"Step 1: Verify Pre-Release"},{"location":"release/cut-sw-release/#step-2-test-pre-release-in-vm-universe","text":"To test pre-release, you will be kicking off a manual VM universe test run from osghost.chtc.wisc.edu . Ensure that you meet the pre-requisites for submitting VM universe test runs Prepare the test suite by running: osg-run-tests -P Testing OSG pre-release cd into the directory specified in the output of the previous command Submit the DAG: condor_submit_dag master-run.dag Note If there are failures, consult the release-manager before proceeding.","title":"Step 2: Test Pre-Release in VM Universe"},{"location":"release/cut-sw-release/#step-3-test-pre-release-on-the-madison-itb-site","text":"Test the pre-release on the Madison ITB by following the ITB pre-release testing instructions . If you not local to Madison, consult the release manager for the designated person to do this testing.","title":"Step 3: Test Pre-Release on the Madison ITB site"},{"location":"release/cut-sw-release/#step-4-regenerate-the-build-repositories","text":"To avoid 404 errors when retrieving packages, it's necessary to regenerate the build repositories. Run the following script from a machine with your koji-registered user certificate: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) 1 -regen-repos $NON_UPCOMING_VERSIONS","title":"Step 4: Regenerate the build repositories"},{"location":"release/cut-sw-release/#step-5-create-the-client-tarballs","text":"Create the client tarballs as root on an EL7 fermicloud machine using the relevant script from git: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) git clone https://github.com/opensciencegrid/release-tools.git cd release-tools ./1-client-tarballs $NON_UPCOMING_VERSIONS","title":"Step 5: Create the client tarballs"},{"location":"release/cut-sw-release/#step-6-briefly-test-the-client-tarballs","text":"As an unprivileged user , extract each tarball into a separate directory. Make sure osg-post-install works. Make sure osgrun osg-version works by running the following tests, replacing NON-UPCOMING VERSION(S) with the appropriate version numbers: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) ./1-verify-tarballs $NON_UPCOMING_VERSIONS If you have time, try some of the binaries, such as grid-proxy-init. Todo We need to automate this and have it run on the proper architectures and version of RHEL.","title":"Step 6: Briefly test the client tarballs"},{"location":"release/cut-sw-release/#step-7-update-the-uw-afs-installation-of-the-tarball-client","text":"The UW keeps an install of the tarball client in /p/vdt/workspace/tarball-client on the UW's AFS. To update it, run the following commands: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) for ver in $NON_UPCOMING_VERSIONS ; do /p/vdt/workspace/tarball-client/afs-install-tarball-client $ver done","title":"Step 7: Update the UW AFS installation of the tarball client"},{"location":"release/cut-sw-release/#step-8-wait","text":"Wait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day.","title":"Step 8: Wait"},{"location":"release/cut-sw-release/#day-2-pushing-the-release","text":"","title":"Day 2: Pushing the Release"},{"location":"release/cut-sw-release/#step-1-push-from-pre-release-to-release","text":"This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. VERSIONS = VERSION(S) 2 -create-release $VERSIONS *.txt files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS. For each release version, use the *release-note* files to update the relevant sections of the release note pages.","title":"Step 1: Push from pre-release to release"},{"location":"release/cut-sw-release/#step-2-upload-the-client-tarballs","text":"Upload the tarballs to the repository with the following procedure from a UW CS machine (e.g., ingwe ): NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) pushd /p/vdt/public/html/tarball-client for ver in $NON_UPCOMING_VERSIONS ; do major_ver = ${ ver %.* } ssh osgcollab@hcc-osg-software.unl.edu mkdir -p /usr/local/repo/tarball-install/ $major_ver / $ver scp -p $major_ver /*/osg-wn-client- $ver *gz osgcollab@hcc-osg-software.unl.edu:/usr/local/repo/tarball-install/ $major_ver / $ver done popd ssh osgcollab@hcc-osg-software.unl.edu bin/mk-sims.sh for ver in $NON_UPCOMING_VERSIONS ; do major_ver = ${ ver %.* } ssh osgcollab@hcc-osg-software.unl.edu cd /usr/local/repo/tarball-install; ls -l $major_ver /*latest* done # verify the latest symlinks point to the version(s) just installed","title":"Step 2: Upload the client tarballs"},{"location":"release/cut-sw-release/#step-3-install-the-tarballs-into-oasis","text":"Note You must be an OASIS manager of the mis VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L. Get the uploader script from Git and run it with osgrun from the UW AFS install of the tarball client you made earlier. On a UW CSL machine: NON_UPCOMING_VERSIONS = NON-UPCOMING VERSION(S) cd /tmp git clone --depth 1 file:///p/vdt/workspace/git/repo/tarball-client.git for ver in $NON_UPCOMING_VERSIONS ; do /p/vdt/workspace/tarball-client/current/sys/osgrun /tmp/tarball-client/upload-tarballs-to-oasis $ver done The script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process.","title":"Step 3: Install the tarballs into OASIS"},{"location":"release/cut-sw-release/#step-4-remove-old-uw-afs-installations-of-the-tarball-client","text":"To keep space usage down, remove tarball client installations and symlinks under /p/vdt/workspace/tarball-client on UW's AFS that are more than 2 months old. The following command will remove them: find /p/vdt/workspace/tarball-client -maxdepth 1 -mtime +60 -name 3 \\* -ls -exec rm -rf {} \\;","title":"Step 4: Remove old UW AFS installations of the tarball client"},{"location":"release/cut-sw-release/#step-5-update-the-docker-wn-client","text":"Update the GitHub repo at opensciencegrid/docker-osg-wn using the update-all script found in opensciencegrid/docker-osg-wn-scripts . This requires push access to the opensciencegrid/docker-osg-wn repo. Instructions for using the script: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git docker-osg-wn-scripts/update-all docker-osg-wn cd docker-osg-wn # Verify everything looks fine and run the git push command # that update-all should have printed","title":"Step 5: Update the Docker WN client"},{"location":"release/cut-sw-release/#step-6-announce-the-release","text":"The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release. The release manager writes the a release announcement for each version and sends it out. The announcement should mention a handful of the most important updates. Due to downstream formatting issues, each major change should end at column 76 or earlier. Here is a sample, replace BRACKETED TEXT with the appropriate values: Subject : Announcing OSG Software version VERSION We are pleased to announce OSG Software version VERSION ! Changes to OSG VERSION include : - Major Change 1 - Major Change 2 - Major Change 3 Release notes and pointers to more documentation can be found at : http :// www . opensciencegrid . org /docs/release/ SERIES.VERSION /release- RELEASE-VERSION / Need help ? Let us know : http :// www . opensciencegrid . org /docs/common/help/ We welcome feedback on this release ! The release manager uses the osg-notify tool on submit-1.chtc.wisc.edu to send the release announcement using the following command: PYTHONPATH=src python bin/osg-notify --cert your-cert.pem --key your-key.pem \\ --no-sign --type production --message message-file \\ --subject EMAIL SUBJECT \\ --recipients osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org vdt-discuss@opensciencegrid.org \\ --oim-recipients resources --oim-contact-type administrative Replace EMAIL SUBJECT with an appropriate subject for your announcement. The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Also set the Fix Versions field to the appropriate value(s) and uncheck the box that reads \"Send mail for this update\"","title":"Step 6: Announce the release"},{"location":"release/cut-sw-release/#day-3-update-the-itb","text":"Now that the release has had a chance to propogate to all the mirrors, update the Madison ITB site by following the yum update section of the Madison ITB document. If you are not local to Madison, consult the release manager for the designated person to do the update. Remember to stop the HTCondor and HTCondor-CE daemons according to the HTCondor pre-release testing instructions . Those daemons will need to be restarted after the upgraode.","title":"Day 3: Update the ITB"},{"location":"release/empty-pkgs/","text":"Procedure for updating empty-* packages Background The empty-* packages were introduced a workaround for sites that install certain software (for example HTCondor or CA certs) from tarballs or other means that do not involve Yum/RPM. The packages contain no files, and exist merely to satisfy RPM dependencies so that other packages can be installed. It is the admin's responsibility to make sure that whatever component they installed the empty package for is functional. The empty packages are kept in a separate repository to prevent them from being accidentally installed instead of the component they claim to provide. Because of this, they do not go through the normal release process of development to testing to prerelease to release, but are moved straight from osg-development into osg-empty after developer testing. Warning It is important to untag the packages from osg-development immediately after promotion to osg-empty Procedure Prepare the package update, but do not build yet. Coordinate with the Software and Release Managers to set aside a good time to update the package. An empty package should not remain in the development repos for longer than a few hours. Build into development. Test out of development. Be thorough , as there is no separate acceptance testing for empty packages. In the JIRA ticket, document your testing procedure and request permission from both the Software and the Release Managers. (Since there is no acceptance testing, both of them have to sign off on the new build). After receiving permission, tag the builds into the osg-empty tags, and untag them from the osg-development tags. Then regenerate the osg-empty repos. osg-koji move-pkg osg-3.3-el6-development osg-3.3-el6-empty EL6_BUILD_NVR osg-koji move-pkg osg-3.3-el7-development osg-3.3-el7-empty EL7_BUILD_NVR osg-koji regen-repo --nowait osg-3.3-el6-empty osg-koji regen-repo --nowait osg-3.3-el7-empty","title":"Empty Packages"},{"location":"release/empty-pkgs/#procedure-for-updating-empty-42-packages","text":"","title":"Procedure for updating empty-* packages"},{"location":"release/empty-pkgs/#background","text":"The empty-* packages were introduced a workaround for sites that install certain software (for example HTCondor or CA certs) from tarballs or other means that do not involve Yum/RPM. The packages contain no files, and exist merely to satisfy RPM dependencies so that other packages can be installed. It is the admin's responsibility to make sure that whatever component they installed the empty package for is functional. The empty packages are kept in a separate repository to prevent them from being accidentally installed instead of the component they claim to provide. Because of this, they do not go through the normal release process of development to testing to prerelease to release, but are moved straight from osg-development into osg-empty after developer testing. Warning It is important to untag the packages from osg-development immediately after promotion to osg-empty","title":"Background"},{"location":"release/empty-pkgs/#procedure","text":"Prepare the package update, but do not build yet. Coordinate with the Software and Release Managers to set aside a good time to update the package. An empty package should not remain in the development repos for longer than a few hours. Build into development. Test out of development. Be thorough , as there is no separate acceptance testing for empty packages. In the JIRA ticket, document your testing procedure and request permission from both the Software and the Release Managers. (Since there is no acceptance testing, both of them have to sign off on the new build). After receiving permission, tag the builds into the osg-empty tags, and untag them from the osg-development tags. Then regenerate the osg-empty repos. osg-koji move-pkg osg-3.3-el6-development osg-3.3-el6-empty EL6_BUILD_NVR osg-koji move-pkg osg-3.3-el7-development osg-3.3-el7-empty EL7_BUILD_NVR osg-koji regen-repo --nowait osg-3.3-el6-empty osg-koji regen-repo --nowait osg-3.3-el7-empty","title":"Procedure"},{"location":"release/itb-testing/","text":"Testing OSG Software Prereleases on the Madison ITB Site This document contains basic recipes for testing a OSG software prereleases on the Madison ITB site, which includes HTCondor prerelease builds and full OSG software stack prereleases from Yum. Prerequisites The following items are known prerequisites to using this recipe. If you are not running the Ansible commands from osghost, there are almost certainly other prerequisites that are not listed below. And even using osghost for Ansible and itb-submit for the submissions, there may be other prerequisites missing. Please improve this document by adding other prerequisites as they are identified! A checkout of the osgitb directory from our local git instance (not GitHub) Your X.509 DN in the osgitb/unmanaged/htcondor-ce/grid-mapfile file and (via Ansible) on itb-ce1 and itb-ce2 Gathering Information Technically skippable, this section is about checking on the state of the ITB machines before making changes. The plan is to keep the ITB machines generally up-to-date independently, so those steps are not listed here. And honestly, the steps below are just some ideas; do whatever makes sense for the given update. The commands can be run as-is from within the osgitb directory from git. Check OS versions for all current ITB hosts: ansible current -i inventory -f 20 -o -m command -a cat /etc/redhat-release Check the date and time on all hosts (in case NTP stops working): ansible current -i inventory -f 20 -o -m command -a date Check software versions for certain hosts (e.g., for the condor package on hosts in the workers group): ansible workers -i inventory -f 20 -o -m command -a rpm -q condor Installing HTCondor Prerelease Use this section to install a new version of HTCondor, specifically a prerelease build from the development or upcoming-development repository, on the test hosts. Obtain the NVR of the HTCondor prerelease build from OSG to test. Do this by talking to Tim T. and checking Koji. Shut down HTCondor and HTCondor-CE on prerelease machines: ansible testing: ces -i inventory -bK -f 20 -m service -a name=condor-ce state=stopped ansible testing: condor -i inventory -bK -f 20 -m service -a name=condor state=stopped Install new version of HTCondor on prerelease machines: ansible testing: condor -i inventory -bK -f 10 -m command -a yum --enablerepo=osg-development --assumeyes update condor or, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed: ansible testing: condor -i inventory -bK -f 10 -m command -a yum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp Verify correct RPM versions across the site: ansible condor -i inventory -f 20 -o -m command -a rpm -q condor Restart HTCondor and HTCondor-CE on prerelease machines: ansible testing: condor -i inventory -bK -f 20 -m service -a name=condor state=started ansible testing: ces -i inventory -bK -f 20 -m service -a name=condor-ce state=started Installing a Prerelease of the OSG Software Stack Use this section to install new versions of all OSG software from a prerelease repository in Yum. Check with the Release Manager to make sure that the prerelease repository has been populated with the desired package versions. Make sure that software is generally up-to-date on the hosts \u2014 see the MadisonITB page for more details It may be desirable to update only non-OSG software at this stage, in which case one could simply disable the OSG repositories by adding command-line options to the yum update commands. Install new software on prerelease hosts: ansible testing -i inventory -bK -f 20 -m command -a yum --enablerepo=osg-prerelease --assumeyes update Read the Yum output carefully, and follow up on any warnings, etc. If the osg-configure package was updated on any host(s), run the osg-configure command on the host(s): ansible testing -i inventory -bK -f 20 -m command -a osg-configure -v -l [HOST(S)] ansible testing -i inventory -bK -f 20 -m command -a osg-configure -c -l [HOST(S)] Verify OSG software updates by inspecting the Yum output carefully or examining specific package versions: ansible current -i inventory -f 20 -o -m command -a rpm -q osg-wn-client Use an inventory group and package names that best fit the situation. Running Tests For the first two test workflows, use your personal space on itb-submit . Copy or checkout the osgitb/htcondor-tests directory to get the test directories. Part \u2160: Submitting jobs directly Change into the 1-direct-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in a few minutes. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt You should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per machine and the fact that HTCondor can and will reuse claims to process many jobs on a single host. Especially watch out for a case in which no jobs run on the newly updated hosts (at the time of writing: itb-data[456] ). (Optional) Clean up, using the make clean or make distclean commands. Use the clean target to remove intermediate result and log files generated by a workflow run but preserve the final output file; use the distclean target to remove all workflow-generated files (plus Emacs backup files). Part \u2161: Submitting jobs using HTCondor-C If direct submissions fail, there is probably no point to doing this step. Change into the 2-htcondor-c-jobs subdirectory If there are old result files in the directory, remove them: make distclean Get a proxy for your X.509 credentials voms-proxy-init Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the direct submission test, because of the layers of indirection. Also, status updates from the CEs back to the submit host are infrequent. For direct information about the CEs, log in to itb-ce1 and itb-ce2 to check status; don\u2019t forget to check both condor_ce_q and condor_q on the CEs, probably in that order. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt Again, look for a reasonable distribution of jobs by hostname. (Optional) Clean up, using the make clean or make distclean commands. Part \u2162: Submitting jobs from a GlideinWMS VO Frontend For this workflow, use your personal space on glidein3.chtc.wisc.edu . Copy or checkout the osgitb/htcondor-tests directory to get the test directories. Again, if previous steps fail, do not bother with this step. Change into the 3-frontend-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck This workflow could take much longer than the first two, maybe an hour or so. Also, unless there are active glideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs. Thus it is helpful to monitor condor_q -totals until all of the jobs are submitted (there should be 2001), then switch to monitoring condor_status until glideins start appearing. After the first jobs start running and finishing, it is probably safe to ignore the rest of the run. If the jobs do not appear in the local queue, if glideins do not appear, or if jobs do not start running on the glideins, it is time to start troubleshooting. Check the final output file: cat count-by-hostnames.txt The distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in which pilots ramp up over time and how HTCondor allocates jobs to slots. (Optional) Clean up, using the make clean or make distclean commands.","title":"ITB Prerelease Testing"},{"location":"release/itb-testing/#testing-osg-software-prereleases-on-the-madison-itb-site","text":"This document contains basic recipes for testing a OSG software prereleases on the Madison ITB site, which includes HTCondor prerelease builds and full OSG software stack prereleases from Yum.","title":"Testing OSG Software Prereleases on the Madison ITB Site"},{"location":"release/itb-testing/#prerequisites","text":"The following items are known prerequisites to using this recipe. If you are not running the Ansible commands from osghost, there are almost certainly other prerequisites that are not listed below. And even using osghost for Ansible and itb-submit for the submissions, there may be other prerequisites missing. Please improve this document by adding other prerequisites as they are identified! A checkout of the osgitb directory from our local git instance (not GitHub) Your X.509 DN in the osgitb/unmanaged/htcondor-ce/grid-mapfile file and (via Ansible) on itb-ce1 and itb-ce2","title":"Prerequisites"},{"location":"release/itb-testing/#gathering-information","text":"Technically skippable, this section is about checking on the state of the ITB machines before making changes. The plan is to keep the ITB machines generally up-to-date independently, so those steps are not listed here. And honestly, the steps below are just some ideas; do whatever makes sense for the given update. The commands can be run as-is from within the osgitb directory from git. Check OS versions for all current ITB hosts: ansible current -i inventory -f 20 -o -m command -a cat /etc/redhat-release Check the date and time on all hosts (in case NTP stops working): ansible current -i inventory -f 20 -o -m command -a date Check software versions for certain hosts (e.g., for the condor package on hosts in the workers group): ansible workers -i inventory -f 20 -o -m command -a rpm -q condor","title":"Gathering Information"},{"location":"release/itb-testing/#installing-htcondor-prerelease","text":"Use this section to install a new version of HTCondor, specifically a prerelease build from the development or upcoming-development repository, on the test hosts. Obtain the NVR of the HTCondor prerelease build from OSG to test. Do this by talking to Tim T. and checking Koji. Shut down HTCondor and HTCondor-CE on prerelease machines: ansible testing: ces -i inventory -bK -f 20 -m service -a name=condor-ce state=stopped ansible testing: condor -i inventory -bK -f 20 -m service -a name=condor state=stopped Install new version of HTCondor on prerelease machines: ansible testing: condor -i inventory -bK -f 10 -m command -a yum --enablerepo=osg-development --assumeyes update condor or, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed: ansible testing: condor -i inventory -bK -f 10 -m command -a yum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp Verify correct RPM versions across the site: ansible condor -i inventory -f 20 -o -m command -a rpm -q condor Restart HTCondor and HTCondor-CE on prerelease machines: ansible testing: condor -i inventory -bK -f 20 -m service -a name=condor state=started ansible testing: ces -i inventory -bK -f 20 -m service -a name=condor-ce state=started","title":"Installing HTCondor Prerelease"},{"location":"release/itb-testing/#installing-a-prerelease-of-the-osg-software-stack","text":"Use this section to install new versions of all OSG software from a prerelease repository in Yum. Check with the Release Manager to make sure that the prerelease repository has been populated with the desired package versions. Make sure that software is generally up-to-date on the hosts \u2014 see the MadisonITB page for more details It may be desirable to update only non-OSG software at this stage, in which case one could simply disable the OSG repositories by adding command-line options to the yum update commands. Install new software on prerelease hosts: ansible testing -i inventory -bK -f 20 -m command -a yum --enablerepo=osg-prerelease --assumeyes update Read the Yum output carefully, and follow up on any warnings, etc. If the osg-configure package was updated on any host(s), run the osg-configure command on the host(s): ansible testing -i inventory -bK -f 20 -m command -a osg-configure -v -l [HOST(S)] ansible testing -i inventory -bK -f 20 -m command -a osg-configure -c -l [HOST(S)] Verify OSG software updates by inspecting the Yum output carefully or examining specific package versions: ansible current -i inventory -f 20 -o -m command -a rpm -q osg-wn-client Use an inventory group and package names that best fit the situation.","title":"Installing a Prerelease of the OSG Software Stack"},{"location":"release/itb-testing/#running-tests","text":"For the first two test workflows, use your personal space on itb-submit . Copy or checkout the osgitb/htcondor-tests directory to get the test directories.","title":"Running Tests"},{"location":"release/itb-testing/#part-i-submitting-jobs-directly","text":"Change into the 1-direct-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in a few minutes. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt You should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per machine and the fact that HTCondor can and will reuse claims to process many jobs on a single host. Especially watch out for a case in which no jobs run on the newly updated hosts (at the time of writing: itb-data[456] ). (Optional) Clean up, using the make clean or make distclean commands. Use the clean target to remove intermediate result and log files generated by a workflow run but preserve the final output file; use the distclean target to remove all workflow-generated files (plus Emacs backup files).","title":"Part \u2160: Submitting jobs directly"},{"location":"release/itb-testing/#part-ii-submitting-jobs-using-htcondor-c","text":"If direct submissions fail, there is probably no point to doing this step. Change into the 2-htcondor-c-jobs subdirectory If there are old result files in the directory, remove them: make distclean Get a proxy for your X.509 credentials voms-proxy-init Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck In the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the direct submission test, because of the layers of indirection. Also, status updates from the CEs back to the submit host are infrequent. For direct information about the CEs, log in to itb-ce1 and itb-ce2 to check status; don\u2019t forget to check both condor_ce_q and condor_q on the CEs, probably in that order. If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do! Keep trying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets). Check the final output file: cat count-by-hostnames.txt Again, look for a reasonable distribution of jobs by hostname. (Optional) Clean up, using the make clean or make distclean commands.","title":"Part \u2161: Submitting jobs using HTCondor-C"},{"location":"release/itb-testing/#part-iii-submitting-jobs-from-a-glideinwms-vo-frontend","text":"For this workflow, use your personal space on glidein3.chtc.wisc.edu . Copy or checkout the osgitb/htcondor-tests directory to get the test directories. Again, if previous steps fail, do not bother with this step. Change into the 3-frontend-jobs subdirectory If there are old result files in the directory, remove them: make distclean Submit the test workflow condor_submit_dag test.dag Monitor the jobs until they are complete or stuck This workflow could take much longer than the first two, maybe an hour or so. Also, unless there are active glideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs. Thus it is helpful to monitor condor_q -totals until all of the jobs are submitted (there should be 2001), then switch to monitoring condor_status until glideins start appearing. After the first jobs start running and finishing, it is probably safe to ignore the rest of the run. If the jobs do not appear in the local queue, if glideins do not appear, or if jobs do not start running on the glideins, it is time to start troubleshooting. Check the final output file: cat count-by-hostnames.txt The distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in which pilots ramp up over time and how HTCondor allocates jobs to slots. (Optional) Clean up, using the make clean or make distclean commands.","title":"Part \u2162: Submitting jobs from a GlideinWMS VO Frontend"},{"location":"release/new-release-series/","text":"How to Prepare a New Release Series Warning This document was specifically written for OSG 3.4 and will need to be adapted to work for 3.5, 3.6, etc. The information was taken from SOFTWARE-2622 and Matyas's notes for the OSG 3.4 release . Do first, anytime before the month of the release Add 3.4 koji tags and targets Modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/osg34/koji/create-new-koji-osg34-tags-etc At first, upcoming-build should continue to inherit from 3.3-devel Create osg-3.4-elX-bootstrap and have the build tag inherit from it Add Koji package signing Edit /etc/koji-hub/plugins/sign.conf; copy and modify one of the other osg entries Ensure the permissions are 0600 apache:apache Save the result with etckeeper commit Update osg-build to use the new koji tags and targets (not by default of course) See the Git commits on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this You will be using this version of osg-build for some tasks, even if it hasn't been released Do afterward, anytime before the month of the release Create a blank osg-3.4 SVN branch and add buildsys-macros package svn copy buildsys-macros from trunk and hand-edit it to hardcode all the new values For EL 6: Edit the spec file and set dver to 6 Run the following commands (adjust the NVR as necessary): $ osg-build rpmbuild --el6 $ osg-koji import _build_results/buildsys-macros-*.el6.src.rpm $ osg-koji import _build_results/buildsys-macros-*.el6.noarch.rpm $ osg-koji tag-pkg osg-3.4-el6-development buildsys-macros-7-5.osg34.el6 Then do the same for EL 7: Edit the spec file and set dver to 7 Run the following commands (adjust the NVR as necessary): $ osg-build rpmbuild --el7 $ osg-koji import _build_results/buildsys-macros-*.el7.src.rpm $ osg-koji import _build_results/buildsys-macros-*.el7.noarch.rpm $ osg-koji tag-pkg osg-3.4-el7-development buildsys-macros-7-5.osg34.el7 Do a 'real' build of buildsys-macros Bump the revision in the buildsys-macros spec file and edit the %changelog . Again, you will need a version of osg-build with 3.4 support. Set dver to 6. Commit $ osg-build koji --repo = 3 .4 --el6 Set dver to 7. Commit $ osg-build koji --repo = 3 .4 --el7 Update tarball-client scripts bundles.ini patches/ upload-tarballs-to-oasis (for 3.4, foreach_dver_arch will need to be updated because we're dropping i386 support) Populate the bootstrap tags Need to have them inherit from the 3.3 development tags, but only packages, not builds (hence the --noconfig ; yes, the name is weird) $ for el in el6 el7 ; do \\ for repo in 3 .3 upcoming ; do \\ osg-koji add-tag-inheritance --noconfig --priority = 2 \\ osg-3.4- $el -bootstrap osg- $repo - $el -development ; \\ done ; \\ done Get the actual NVRs to tag I put Brian's spreadsheet into Excel and used its filtering feature to separate out: the packages going into 3.4.0 the el6 versus el7 packages and copied the column containing the NVRs into Vim, and did a search replace of the dver to the appropriate version saved two text files (pkgtotag-el6.txt and pkgtotag-el7.txt) Tagging: $ for el in el6 el7 ; do \\ xargs -a pkgtotag- $el osg-koji tag-pkg osg-3.4- $el -bootstrap ; \\ done (btw, xargs -a doesn't work on a Mac) Do on the month of the 3.4.0 release Populate SVN branch and tags (as in fill it with the packages we're going to release for 3.4) Mass rebuild Don't forget to update the empty and contrib tags with the appropriate packages; remove the empty* packages from the development tags after they've been tagged into the empty tags Update mash (coordinate with steige) On repo-itb On repo1/repo2 Update documentation here Update osg-test / vmu-test-runs They're only going to test from minefield (and eventurally testing) until the release Do immediately after the 3.4.0 release Update tag inheritance on the upcoming-build tags to inherit from 3.4-devel instead of 3.3-devel Drop the osg-3.4-elX-bootstrap koji tags Do sometime after the 3.4.0 release Do these three at the same time: Move the SVN trunk to branches/osg-3.3 and move branches/osg-3.4 to trunk Update the koji osg-elX build targets to build from and to 3.4 instead of 3.3 Notify the software list of this change Update osg-test / vmu-test-runs again to add release and release - testing tests Update the docker-osg-wn-client scripts to build from 3.4 (need direct push access) Update the constants in the genbranches script in the docker-osg-wn-scripts repo Update the branches in docker-osg-wn-client ; a script like this ought to work: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git cd docker-osg-wn-scripts ./genbranches cd ../docker-osg-wn for bpath in ../docker-osg-wn-scripts/branches/* ; do b = ${ bpath ##*/ } git checkout -b $b master \\ mv $bpath Dockerfile.in \\ git add Dockerfile.in \\ git commit -m Add branch $b done and then run a similar script to update the existing branches Check the results before pushing, and then run git push --all Update the arrays in update-all and osg-wn-nightly-build in docker-osg-wn-scripts Update the default promotion route aliases in osg-promote Update documentation again to reflect that 3.4 is now the main branch and 3.3 is the maintenance branch:","title":"New Release Series"},{"location":"release/new-release-series/#how-to-prepare-a-new-release-series","text":"Warning This document was specifically written for OSG 3.4 and will need to be adapted to work for 3.5, 3.6, etc. The information was taken from SOFTWARE-2622 and Matyas's notes for the OSG 3.4 release .","title":"How to Prepare a New Release Series"},{"location":"release/new-release-series/#do-first-anytime-before-the-month-of-the-release","text":"Add 3.4 koji tags and targets Modify this script as appropriate and run: https://github.com/opensciencegrid/osg-next-tools/blob/osg34/koji/create-new-koji-osg34-tags-etc At first, upcoming-build should continue to inherit from 3.3-devel Create osg-3.4-elX-bootstrap and have the build tag inherit from it Add Koji package signing Edit /etc/koji-hub/plugins/sign.conf; copy and modify one of the other osg entries Ensure the permissions are 0600 apache:apache Save the result with etckeeper commit Update osg-build to use the new koji tags and targets (not by default of course) See the Git commits on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this You will be using this version of osg-build for some tasks, even if it hasn't been released","title":"Do first, anytime before the month of the release"},{"location":"release/new-release-series/#do-afterward-anytime-before-the-month-of-the-release","text":"Create a blank osg-3.4 SVN branch and add buildsys-macros package svn copy buildsys-macros from trunk and hand-edit it to hardcode all the new values For EL 6: Edit the spec file and set dver to 6 Run the following commands (adjust the NVR as necessary): $ osg-build rpmbuild --el6 $ osg-koji import _build_results/buildsys-macros-*.el6.src.rpm $ osg-koji import _build_results/buildsys-macros-*.el6.noarch.rpm $ osg-koji tag-pkg osg-3.4-el6-development buildsys-macros-7-5.osg34.el6 Then do the same for EL 7: Edit the spec file and set dver to 7 Run the following commands (adjust the NVR as necessary): $ osg-build rpmbuild --el7 $ osg-koji import _build_results/buildsys-macros-*.el7.src.rpm $ osg-koji import _build_results/buildsys-macros-*.el7.noarch.rpm $ osg-koji tag-pkg osg-3.4-el7-development buildsys-macros-7-5.osg34.el7 Do a 'real' build of buildsys-macros Bump the revision in the buildsys-macros spec file and edit the %changelog . Again, you will need a version of osg-build with 3.4 support. Set dver to 6. Commit $ osg-build koji --repo = 3 .4 --el6 Set dver to 7. Commit $ osg-build koji --repo = 3 .4 --el7 Update tarball-client scripts bundles.ini patches/ upload-tarballs-to-oasis (for 3.4, foreach_dver_arch will need to be updated because we're dropping i386 support) Populate the bootstrap tags Need to have them inherit from the 3.3 development tags, but only packages, not builds (hence the --noconfig ; yes, the name is weird) $ for el in el6 el7 ; do \\ for repo in 3 .3 upcoming ; do \\ osg-koji add-tag-inheritance --noconfig --priority = 2 \\ osg-3.4- $el -bootstrap osg- $repo - $el -development ; \\ done ; \\ done Get the actual NVRs to tag I put Brian's spreadsheet into Excel and used its filtering feature to separate out: the packages going into 3.4.0 the el6 versus el7 packages and copied the column containing the NVRs into Vim, and did a search replace of the dver to the appropriate version saved two text files (pkgtotag-el6.txt and pkgtotag-el7.txt) Tagging: $ for el in el6 el7 ; do \\ xargs -a pkgtotag- $el osg-koji tag-pkg osg-3.4- $el -bootstrap ; \\ done (btw, xargs -a doesn't work on a Mac)","title":"Do afterward, anytime before the month of the release"},{"location":"release/new-release-series/#do-on-the-month-of-the-340-release","text":"Populate SVN branch and tags (as in fill it with the packages we're going to release for 3.4) Mass rebuild Don't forget to update the empty and contrib tags with the appropriate packages; remove the empty* packages from the development tags after they've been tagged into the empty tags Update mash (coordinate with steige) On repo-itb On repo1/repo2 Update documentation here Update osg-test / vmu-test-runs They're only going to test from minefield (and eventurally testing) until the release","title":"Do on the month of the 3.4.0 release"},{"location":"release/new-release-series/#do-immediately-after-the-340-release","text":"Update tag inheritance on the upcoming-build tags to inherit from 3.4-devel instead of 3.3-devel Drop the osg-3.4-elX-bootstrap koji tags","title":"Do immediately after the 3.4.0 release"},{"location":"release/new-release-series/#do-sometime-after-the-340-release","text":"Do these three at the same time: Move the SVN trunk to branches/osg-3.3 and move branches/osg-3.4 to trunk Update the koji osg-elX build targets to build from and to 3.4 instead of 3.3 Notify the software list of this change Update osg-test / vmu-test-runs again to add release and release - testing tests Update the docker-osg-wn-client scripts to build from 3.4 (need direct push access) Update the constants in the genbranches script in the docker-osg-wn-scripts repo Update the branches in docker-osg-wn-client ; a script like this ought to work: git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git git clone git@github.com:opensciencegrid/docker-osg-wn.git cd docker-osg-wn-scripts ./genbranches cd ../docker-osg-wn for bpath in ../docker-osg-wn-scripts/branches/* ; do b = ${ bpath ##*/ } git checkout -b $b master \\ mv $bpath Dockerfile.in \\ git add Dockerfile.in \\ git commit -m Add branch $b done and then run a similar script to update the existing branches Check the results before pushing, and then run git push --all Update the arrays in update-all and osg-wn-nightly-build in docker-osg-wn-scripts Update the default promotion route aliases in osg-promote Update documentation again to reflect that 3.4 is now the main branch and 3.3 is the maintenance branch:","title":"Do sometime after the 3.4.0 release"},{"location":"release/old-release-removal/","text":"Old Release Series Removal Plan In order to reduce clutter and disk usage on our repositories and build system, we will remove older OSG Software release series. This will result in packages from those series becoming unavailable, so we will remove a release series when its packages are no longer needed. We will remove a release series no earlier than when the following series is completely out of support. For example, OSG 3.1 will be removed when OSG 3.2 is out of support, and OSG 3.2 will be removed when OSG 3.3 is out of support. Tasks Removing a release series requires work from both Operations and Software Release. The first step is to create a JIRA ticket in the SOFTWARE project to track the work. Second, Software Release will enumerate the directories for Operations to remove. Operations tasks should be completed before Software Release tasks. Operations These tasks should be completed in order. Two weeks in advance, notify sites (including mirror sites) that the release series is going away. See the template email below. Remove the series from the mash configs on the repo.opensciencegrid.org machines: Add the koji tags for the old series to the /usr/local/osg-tags.excluded file: # cd /usr/local # fgrep osg-3.1 osg-tags osg-3.1-el5-contrib osg-3.1-el5-development osg-3.1-el5-release osg-3.1-el5-testing osg-3.1-el6-contrib osg-3.1-el6-development osg-3.1-el6-release osg-3.1-el6-testing # fgrep osg-3.1 osg-tags osg-tags.exclude Re-run update_mashfiles.sh to update the mash config files: # ./update_mashfiles.sh Remove the appropriate repo directories from /usr/local/repo/osg . # rm -rf repo*/osg/3.1/ Reclaim space from any cached rpms in the mash cache which are no longer linked elsewhere: # find mash/cache/ -name \\* .rpm -type f -links 1 -delete Wait for mash to run and verify that the repos are no longer getting updated: Look at the mash logs in /var/log/repo . Verify that mash did not recreate the repo directory under /usr/local/repo/osg corresponding to the old release series. Software Release These tasks can be completed in any order. Tag and remove the SVN branch corresponding to the release series. Edit vm-test-runs and remove any \"long tail\" tests that reference the series. Edit tarball-client : Remove bundles from bundles.ini . Remove patch and other files that were used only by those bundles. Test that the current bundles didn't get broken by your changes. Edit osg-build : Remove the promotion routes from promoter.ini . Remove references in constants.py . Test your changes; also run the unit tests. Remove things from Koji: All targets referencing the series. All tags referencing the series. Remove references to the series from opensciencegrid/docker-osg-wn-scripts on GitHub, including the genbranches and update-all scripts. Remove branches from opensciencegrid/docker-osg-wn on GitHub. Move files in /p/vdt/public/html/release-info to its attic subdirectory. Undoing If we really need RPMs from a removed release series, we can look at the text files in /p/vdt/public/html/release-info/attic to determine the exact NVRs we need, and download them from Koji. Template Email On DAYNAME, MONTH DAY , the OSG will be removing the OSG 3.X release series from our repositories. This includes both RPMs and tarballs hosted on repo.opensciencegrid.org. All support for OSG 3.X had been discontinued at the end of MONTH YEAR . Any sites running OSG 3.X should upgrade to the current release series, OSG 3.Y . If you need assistance upgrading, please contact us at help@opensciencegrid.org. If we're dropping support for a distro (e.g. EL 5 when we drop OSG 3.2), add the following after the first paragraph: Note that OSG 3.X was the last release to support Enterprise Linux Z distributions.","title":"Old Release Series Removal"},{"location":"release/old-release-removal/#old-release-series-removal-plan","text":"In order to reduce clutter and disk usage on our repositories and build system, we will remove older OSG Software release series. This will result in packages from those series becoming unavailable, so we will remove a release series when its packages are no longer needed. We will remove a release series no earlier than when the following series is completely out of support. For example, OSG 3.1 will be removed when OSG 3.2 is out of support, and OSG 3.2 will be removed when OSG 3.3 is out of support.","title":"Old Release Series Removal Plan"},{"location":"release/old-release-removal/#tasks","text":"Removing a release series requires work from both Operations and Software Release. The first step is to create a JIRA ticket in the SOFTWARE project to track the work. Second, Software Release will enumerate the directories for Operations to remove. Operations tasks should be completed before Software Release tasks.","title":"Tasks"},{"location":"release/old-release-removal/#operations","text":"These tasks should be completed in order. Two weeks in advance, notify sites (including mirror sites) that the release series is going away. See the template email below. Remove the series from the mash configs on the repo.opensciencegrid.org machines: Add the koji tags for the old series to the /usr/local/osg-tags.excluded file: # cd /usr/local # fgrep osg-3.1 osg-tags osg-3.1-el5-contrib osg-3.1-el5-development osg-3.1-el5-release osg-3.1-el5-testing osg-3.1-el6-contrib osg-3.1-el6-development osg-3.1-el6-release osg-3.1-el6-testing # fgrep osg-3.1 osg-tags osg-tags.exclude Re-run update_mashfiles.sh to update the mash config files: # ./update_mashfiles.sh Remove the appropriate repo directories from /usr/local/repo/osg . # rm -rf repo*/osg/3.1/ Reclaim space from any cached rpms in the mash cache which are no longer linked elsewhere: # find mash/cache/ -name \\* .rpm -type f -links 1 -delete Wait for mash to run and verify that the repos are no longer getting updated: Look at the mash logs in /var/log/repo . Verify that mash did not recreate the repo directory under /usr/local/repo/osg corresponding to the old release series.","title":"Operations"},{"location":"release/old-release-removal/#software-release","text":"These tasks can be completed in any order. Tag and remove the SVN branch corresponding to the release series. Edit vm-test-runs and remove any \"long tail\" tests that reference the series. Edit tarball-client : Remove bundles from bundles.ini . Remove patch and other files that were used only by those bundles. Test that the current bundles didn't get broken by your changes. Edit osg-build : Remove the promotion routes from promoter.ini . Remove references in constants.py . Test your changes; also run the unit tests. Remove things from Koji: All targets referencing the series. All tags referencing the series. Remove references to the series from opensciencegrid/docker-osg-wn-scripts on GitHub, including the genbranches and update-all scripts. Remove branches from opensciencegrid/docker-osg-wn on GitHub. Move files in /p/vdt/public/html/release-info to its attic subdirectory.","title":"Software &amp; Release"},{"location":"release/old-release-removal/#undoing","text":"If we really need RPMs from a removed release series, we can look at the text files in /p/vdt/public/html/release-info/attic to determine the exact NVRs we need, and download them from Koji.","title":"Undoing"},{"location":"release/old-release-removal/#template-email","text":"On DAYNAME, MONTH DAY , the OSG will be removing the OSG 3.X release series from our repositories. This includes both RPMs and tarballs hosted on repo.opensciencegrid.org. All support for OSG 3.X had been discontinued at the end of MONTH YEAR . Any sites running OSG 3.X should upgrade to the current release series, OSG 3.Y . If you need assistance upgrading, please contact us at help@opensciencegrid.org. If we're dropping support for a distro (e.g. EL 5 when we drop OSG 3.2), add the following after the first paragraph: Note that OSG 3.X was the last release to support Enterprise Linux Z distributions.","title":"Template Email"},{"location":"release/release-policy/","text":"Software Release Policy This document doesn't talk about technical details of how to do a release. The release process is discussed elsewhere . Software Repositories The Software Team maintains five primary software repositories osg-development : This is the \"wild west\", the place where software goes while it is being worked on by the software team. osg-testing : This is where software goes when it is ready for wide-spread testing. osg-prerelease : This is where software goes just before being released, for final verification. osg-release : This is the official, production release of the software stack. This is the main repository for end-users. osg-contrib : This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful. We also create a repository per release, called osg-release-VERSION (such as osg-release-3.0.4). This is intended mostly for testing purposes, though users may occasionally find it useful. Occasionally there may be other repositories for specific short-term purposes. Version Numbers There is a single version number that is used to summarize the contents of the osg-release repository. Having a single version number is very useful for a variety of reasons, including: Every time changes are made to the osg-release repository, we update the version number and write release notes. We have a shorthand for referring to the state of the repository; we can talk about specific releases. However, there are important caveats about the version number: Even if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered. The version number is only meaningful in the osg-release repository, though for technical reasons it's present (as an RPM) in other repositories. The version number is communicated in two ways: Every time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number. There is an osg-version RPM that reports the version of the release. Major metapackages (osg-ce, osg-client, etc...) depend on this RPM. The RPM itself has the version number in it. It also provide a program that reports the version, and a text file that contains the version number. The version number will be of the form X.Y.Z. As of this writing, version numbers are 3.0.Z, where Z indicates a minor revision. Progression of repositories This figure shows the progression of repositories that packages will go through: osg-development - osg-testing - osg-prerelease - osg-release \\ - osg-contrib Release policies Adding packages to osg-development New packages will only be added to osg-development with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager. Moving packages to osg-testing A package may be moved from osg-development to osg-testing when the individual maintainer of that package decides that it is ready for widespread testing and when the package is eventually intended for a production release, and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team. Moving packages to osg-prerelease; Readying the release When we are ready to make a production release, we first move the correct subset of packages from osg-testing into osg-prerelease . This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of osg-prerelease is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the osg-testing repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to osg-prerelease , the team member doing the release will: Update the osg-version RPM to reflect the new version. Push this RPM through osg-development , osg-testing , and into osg-prerelease . Find the correct set of packages to push from osg-testing into osg-prerelease . At a minimum, run the automated test suite on the contents of osg-prerelease . In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.) We expect that in most cases, this process of updating and testing the osg-prelease repository will be less than one day. If there are urgent security updates to release, this process may be shortened. Note that, except in exceptional circumstances, we release software on Tuesdays. Therefore the osg-prerelease cache is probably updated and readied on a Monday (or perhaps late the previous week). Moving packages to osg-release Note that, except in exceptional circumstances, we release software on Tuesdays, so this process will only happen on Tuesdays. When the osg-prerelease repository has been updated and verified, all of the changed software can be moved into the osg-release repository. As part of this move, two important tasks must be done: Record the complete set of packages in the new release repository. Update the Release Notes . Note that each release has a separate page to describe the release, and it's linked from the main page. The individual page lists the changes at a high level (i.e. Updated package X to version Y) and the complete set of RPMs that changed. Create a ticket on ticket.opensciencegrid.org with a release announcement. Operations will distribute it to the right places. In addition, we will make another Koji tag/yum repository called osg-release-VERSION . All of the latest packages in osg-release will be tagged to be in this repository, and the tag will be locked. This will give us a reproducible way to install any given OSG Software release. Moving packages to osg-release-VERSION When we make a specific release, we copy the osg-release repository to a versioned osg-release-VERSION repository. This allows us to do testing with specific versions and in rare cases allows users to use a specific release. Moving packages to osg-contrib The osg-contrib repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in osg-contrib . Contrib packages should be tested in osg-development first. Timing of releases Normally, releases happen on Tuesdays. Code freezes happen two business days in advance of the release (normally Friday). Specifically: RPM updates intended to be included in the next release (that is, pushed to the osg-release yum repo) must be in the osg-testing yum repo by noon Central Time two business days in advance of the release. This will allow time for final testing, discussions, reverts, etc. We will make exceptions for urgent situations; consult with the release manager when needed. CA Certificates and VO Client packages Packages that contain only data are not part of the usual release cycle. Currently, these are the CA certificate packages and the VO Client packages. Updates to these packages come from the Security Team and Operations Team, respectively. They still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level. However, the actual releases of these packages do not increment the version number of the software stack. The release process for data packages is discussed here.","title":"Release Policy"},{"location":"release/release-policy/#software-release-policy","text":"This document doesn't talk about technical details of how to do a release. The release process is discussed elsewhere .","title":"Software Release Policy"},{"location":"release/release-policy/#software-repositories","text":"The Software Team maintains five primary software repositories osg-development : This is the \"wild west\", the place where software goes while it is being worked on by the software team. osg-testing : This is where software goes when it is ready for wide-spread testing. osg-prerelease : This is where software goes just before being released, for final verification. osg-release : This is the official, production release of the software stack. This is the main repository for end-users. osg-contrib : This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful. We also create a repository per release, called osg-release-VERSION (such as osg-release-3.0.4). This is intended mostly for testing purposes, though users may occasionally find it useful. Occasionally there may be other repositories for specific short-term purposes.","title":"Software Repositories"},{"location":"release/release-policy/#version-numbers","text":"There is a single version number that is used to summarize the contents of the osg-release repository. Having a single version number is very useful for a variety of reasons, including: Every time changes are made to the osg-release repository, we update the version number and write release notes. We have a shorthand for referring to the state of the repository; we can talk about specific releases. However, there are important caveats about the version number: Even if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered. The version number is only meaningful in the osg-release repository, though for technical reasons it's present (as an RPM) in other repositories. The version number is communicated in two ways: Every time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number. There is an osg-version RPM that reports the version of the release. Major metapackages (osg-ce, osg-client, etc...) depend on this RPM. The RPM itself has the version number in it. It also provide a program that reports the version, and a text file that contains the version number. The version number will be of the form X.Y.Z. As of this writing, version numbers are 3.0.Z, where Z indicates a minor revision.","title":"Version Numbers"},{"location":"release/release-policy/#progression-of-repositories","text":"This figure shows the progression of repositories that packages will go through: osg-development - osg-testing - osg-prerelease - osg-release \\ - osg-contrib","title":"Progression of repositories"},{"location":"release/release-policy/#release-policies","text":"","title":"Release policies"},{"location":"release/release-policy/#adding-packages-to-osg-development","text":"New packages will only be added to osg-development with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager.","title":"Adding packages to osg-development"},{"location":"release/release-policy/#moving-packages-to-osg-testing","text":"A package may be moved from osg-development to osg-testing when the individual maintainer of that package decides that it is ready for widespread testing and when the package is eventually intended for a production release, and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team.","title":"Moving packages to osg-testing"},{"location":"release/release-policy/#moving-packages-to-osg-prerelease-readying-the-release","text":"When we are ready to make a production release, we first move the correct subset of packages from osg-testing into osg-prerelease . This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of osg-prerelease is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the osg-testing repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to osg-prerelease , the team member doing the release will: Update the osg-version RPM to reflect the new version. Push this RPM through osg-development , osg-testing , and into osg-prerelease . Find the correct set of packages to push from osg-testing into osg-prerelease . At a minimum, run the automated test suite on the contents of osg-prerelease . In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.) We expect that in most cases, this process of updating and testing the osg-prelease repository will be less than one day. If there are urgent security updates to release, this process may be shortened. Note that, except in exceptional circumstances, we release software on Tuesdays. Therefore the osg-prerelease cache is probably updated and readied on a Monday (or perhaps late the previous week).","title":"Moving packages to osg-prerelease; Readying the release"},{"location":"release/release-policy/#moving-packages-to-osg-release","text":"Note that, except in exceptional circumstances, we release software on Tuesdays, so this process will only happen on Tuesdays. When the osg-prerelease repository has been updated and verified, all of the changed software can be moved into the osg-release repository. As part of this move, two important tasks must be done: Record the complete set of packages in the new release repository. Update the Release Notes . Note that each release has a separate page to describe the release, and it's linked from the main page. The individual page lists the changes at a high level (i.e. Updated package X to version Y) and the complete set of RPMs that changed. Create a ticket on ticket.opensciencegrid.org with a release announcement. Operations will distribute it to the right places. In addition, we will make another Koji tag/yum repository called osg-release-VERSION . All of the latest packages in osg-release will be tagged to be in this repository, and the tag will be locked. This will give us a reproducible way to install any given OSG Software release.","title":"Moving packages to osg-release"},{"location":"release/release-policy/#moving-packages-to-osg-release-version","text":"When we make a specific release, we copy the osg-release repository to a versioned osg-release-VERSION repository. This allows us to do testing with specific versions and in rare cases allows users to use a specific release.","title":"Moving packages to osg-release-VERSION"},{"location":"release/release-policy/#moving-packages-to-osg-contrib","text":"The osg-contrib repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in osg-contrib . Contrib packages should be tested in osg-development first.","title":"Moving packages to osg-contrib"},{"location":"release/release-policy/#timing-of-releases","text":"Normally, releases happen on Tuesdays. Code freezes happen two business days in advance of the release (normally Friday). Specifically: RPM updates intended to be included in the next release (that is, pushed to the osg-release yum repo) must be in the osg-testing yum repo by noon Central Time two business days in advance of the release. This will allow time for final testing, discussions, reverts, etc. We will make exceptions for urgent situations; consult with the release manager when needed.","title":"Timing of releases"},{"location":"release/release-policy/#ca-certificates-and-vo-client-packages","text":"Packages that contain only data are not part of the usual release cycle. Currently, these are the CA certificate packages and the VO Client packages. Updates to these packages come from the Security Team and Operations Team, respectively. They still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level. However, the actual releases of these packages do not increment the version number of the software stack. The release process for data packages is discussed here.","title":"CA Certificates and VO Client packages"},{"location":"software/ce-test-scaling/","text":"How to Run Scalability Tests on a CE Introduction This document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an HTCondor-CE , but they should be applicable for other CE software. The focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued. Sleeper Pool With the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs. When large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work. Setting Up a Sleeper Pool A sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine: START = ( \\ (SlotID = 1) \\ (SlotID 33) \\ (RequiresWholeMachine =!= TRUE ) \\ (SleepSlot =!= TRUE) \\ (distro =?= RHEL6 ) \\ (CPU_Only == TRUE ) \\ ) || \\ ( (SlotID = 33) (distro =?= RHEL6 ) (SleepSlot == TRUE) ) Usual Topology of the Tests A brief introduction to the topology involved in the tests. Batch System and Sleeper Pool This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator. CE This is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications. Submitter An HTCondor submit host. It can be a virtual machine for most test submissions. Monitoring tools To monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example: condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const JobStatus=?=2 | wc -l gmetric --name RunningJobsCE Generating Load Location The load_generators are found in the OSgscal github repo . The binary of interest here is loadtest_condor Use Just untar it or check it out from mas on the HTCondor submit node (see above): git checkout https://github.com/efajardo/osgscal cd load_generators/loadtest_condor/trunk/bin Keep in mind that you also need a valid proxy for grid submissions. For example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command: ./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50","title":"CE Scale Testing"},{"location":"software/ce-test-scaling/#how-to-run-scalability-tests-on-a-ce","text":"","title":"How to Run Scalability Tests on a CE"},{"location":"software/ce-test-scaling/#introduction","text":"This document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an HTCondor-CE , but they should be applicable for other CE software. The focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued.","title":"Introduction"},{"location":"software/ce-test-scaling/#sleeper-pool","text":"With the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs. When large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work.","title":"Sleeper Pool"},{"location":"software/ce-test-scaling/#setting-up-a-sleeper-pool","text":"A sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine: START = ( \\ (SlotID = 1) \\ (SlotID 33) \\ (RequiresWholeMachine =!= TRUE ) \\ (SleepSlot =!= TRUE) \\ (distro =?= RHEL6 ) \\ (CPU_Only == TRUE ) \\ ) || \\ ( (SlotID = 33) (distro =?= RHEL6 ) (SleepSlot == TRUE) )","title":"Setting Up a Sleeper Pool"},{"location":"software/ce-test-scaling/#usual-topology-of-the-tests","text":"A brief introduction to the topology involved in the tests.","title":"Usual Topology of the Tests"},{"location":"software/ce-test-scaling/#batch-system-and-sleeper-pool","text":"This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator.","title":"Batch System and Sleeper Pool"},{"location":"software/ce-test-scaling/#ce","text":"This is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications.","title":"CE"},{"location":"software/ce-test-scaling/#submitter","text":"An HTCondor submit host. It can be a virtual machine for most test submissions.","title":"Submitter"},{"location":"software/ce-test-scaling/#monitoring-tools","text":"To monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example: condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const JobStatus=?=2 | wc -l gmetric --name RunningJobsCE","title":"Monitoring tools"},{"location":"software/ce-test-scaling/#generating-load","text":"","title":"Generating Load"},{"location":"software/ce-test-scaling/#location","text":"The load_generators are found in the OSgscal github repo . The binary of interest here is loadtest_condor","title":"Location"},{"location":"software/ce-test-scaling/#use","text":"Just untar it or check it out from mas on the HTCondor submit node (see above): git checkout https://github.com/efajardo/osgscal cd load_generators/loadtest_condor/trunk/bin Keep in mind that you also need a valid proxy for grid submissions. For example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command: ./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50","title":"Use"},{"location":"software/create-vo-client/","text":"Creating the VO Client package Overview The VO Client package is just a conversion of the tarball created by GOC into the RPM format. In order to build the RPM, one needs: The tarball containing the: vomses file edg-mkgridmap.conf file gums.config.template file vomsdir directory tree, containing the lsc files. The RPM spec file Making the tarball To make the tarball: start with a clean directory copy in the vomses , gums.template , and edg-mkgridmap.conf files and rename the edg-mkgridmap file: wget http://repo.opensciencgrid.org/pacman/tarballs/vo-package/vomses wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/edg-mkgridmap.osg mv edg-mkgridmap.osg edg-mkgridmap.conf wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/gums.template # TODO UNSURE Create the vomsdir directory by downloading the .lsc files wget --recursive --no-host-directories --cut-dirs=3 -A *.lsc http://repo.opensciencegrid.org/pacman/tarballs/vo-package/vomsdir In a separate directory, unpack the old vo-client tarball (from the upstream source cache) diff the two directories, and compare the changes to the expected changes listed in the JIRA ticket for this VO Client package release Follow the instructions in the attached gums-template-conversion.txt file to convert it from GUMS 1.1 (1.2?) format to GUMS 1.3 format. Name the result gums.config.template . See also the Automated GUMS Conversion section below for a scripted version of this step. Move the files into a subdirectory to include in the tarball: VERSION=44 # set appropriately mkdir vo-client-$VERSION mv vomses gums.config.template edg-mkgridmap.conf vomsdir vo-client-$VERSION/ tar -czf vo-client-$VERSION-osg.tar.gz vo-client-$VERSION/ Upload the tarball into the upstream source cache , in the vo-client/VERSION/ directory. Automated GUMS Conversion The above instructions outline a procedure for converting the osg gums.config template from GUMS 1.1 format to 1.3 format. Because setting up a GUMS instance for this can be time consuming and tricky to get right, a script was written to automate the procedure on a Fermi VM. The script lives in svn under: $SVN/software/tools/convert-osg-gums-template-for-vo-client.sh . To use it: Create a new Fermi VM (el5 or el6) Copy the script and the new gums.template to be converted to the /root homedir on the VM. Log into the VM as root, make sure the script is executable, and run against the gums template: $ ssh root@el6-vo-client # wget https://vdt.cs.wisc.edu/svn/software/tools/convert-osg-gums-template-for-vo-client.sh # chmod +x convert-osg-gums-template-for-vo-client.sh # ./convert-osg-gums-template-for-vo-client.sh gums.template It takes a little while to install and set up gums and related packages, but if it succeeds, you should see a message that says \"User group has been saved.\", and a file gums.config.template should be written in the current directory. The newly converted gums.config.template should be compared to the old version of that file (from the previous vo-client package) to ensure that the only the differences are the changes for this release. (I have had to manually strip the extra test account stuff.) The 'meld' program is a nice graphical diff tool that I use for comparing them. RPM spec file maintenance The OSG RPM spec file is maintained in Subversion . The VO Client package is located in native/redhat/trunk/vo-client There are two files that need to be maintained: osg/vo-client.spec - This is the RPM spec file proper. One needs to update the version (and/or the release number) every time a new RPM is created. upstream/release_tarball.source - This file contains the relative path of the tarball within the upstream source cache . Since the tarball file name will change with every new RPM version, this file has to be changed accordingly. RPM building After installing the osg-build tools , check out a clean copy from svn, then: osg-build prebuild . Once there are no errors, run osg-build koji . --scratch This can be done without making any permanent change. Once that builds successfully, run osg-build koji . This is permanent, unlike when you ran with --scratch . You cannot rebuild this version of the RPM again - you must bump the release number and edit the changelog. This will push the RPMs into the OSG development repository. Koji requires additional setup compared to rpmbuild; see the documentation here . Promotion to testing and release: Policies Read Release Policy . These should be synchronized internally with other GOC update activities.","title":"Creating the VO Client Package"},{"location":"software/create-vo-client/#creating-the-vo-client-package","text":"","title":"Creating the VO Client package"},{"location":"software/create-vo-client/#overview","text":"The VO Client package is just a conversion of the tarball created by GOC into the RPM format. In order to build the RPM, one needs: The tarball containing the: vomses file edg-mkgridmap.conf file gums.config.template file vomsdir directory tree, containing the lsc files. The RPM spec file","title":"Overview"},{"location":"software/create-vo-client/#making-the-tarball","text":"To make the tarball: start with a clean directory copy in the vomses , gums.template , and edg-mkgridmap.conf files and rename the edg-mkgridmap file: wget http://repo.opensciencgrid.org/pacman/tarballs/vo-package/vomses wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/edg-mkgridmap.osg mv edg-mkgridmap.osg edg-mkgridmap.conf wget http://repo.opensciencegrid.org/pacman/tarballs/vo-package/gums.template # TODO UNSURE Create the vomsdir directory by downloading the .lsc files wget --recursive --no-host-directories --cut-dirs=3 -A *.lsc http://repo.opensciencegrid.org/pacman/tarballs/vo-package/vomsdir In a separate directory, unpack the old vo-client tarball (from the upstream source cache) diff the two directories, and compare the changes to the expected changes listed in the JIRA ticket for this VO Client package release Follow the instructions in the attached gums-template-conversion.txt file to convert it from GUMS 1.1 (1.2?) format to GUMS 1.3 format. Name the result gums.config.template . See also the Automated GUMS Conversion section below for a scripted version of this step. Move the files into a subdirectory to include in the tarball: VERSION=44 # set appropriately mkdir vo-client-$VERSION mv vomses gums.config.template edg-mkgridmap.conf vomsdir vo-client-$VERSION/ tar -czf vo-client-$VERSION-osg.tar.gz vo-client-$VERSION/ Upload the tarball into the upstream source cache , in the vo-client/VERSION/ directory.","title":"Making the tarball"},{"location":"software/create-vo-client/#automated-gums-conversion","text":"The above instructions outline a procedure for converting the osg gums.config template from GUMS 1.1 format to 1.3 format. Because setting up a GUMS instance for this can be time consuming and tricky to get right, a script was written to automate the procedure on a Fermi VM. The script lives in svn under: $SVN/software/tools/convert-osg-gums-template-for-vo-client.sh . To use it: Create a new Fermi VM (el5 or el6) Copy the script and the new gums.template to be converted to the /root homedir on the VM. Log into the VM as root, make sure the script is executable, and run against the gums template: $ ssh root@el6-vo-client # wget https://vdt.cs.wisc.edu/svn/software/tools/convert-osg-gums-template-for-vo-client.sh # chmod +x convert-osg-gums-template-for-vo-client.sh # ./convert-osg-gums-template-for-vo-client.sh gums.template It takes a little while to install and set up gums and related packages, but if it succeeds, you should see a message that says \"User group has been saved.\", and a file gums.config.template should be written in the current directory. The newly converted gums.config.template should be compared to the old version of that file (from the previous vo-client package) to ensure that the only the differences are the changes for this release. (I have had to manually strip the extra test account stuff.) The 'meld' program is a nice graphical diff tool that I use for comparing them.","title":"Automated GUMS Conversion"},{"location":"software/create-vo-client/#rpm-spec-file-maintenance","text":"The OSG RPM spec file is maintained in Subversion . The VO Client package is located in native/redhat/trunk/vo-client There are two files that need to be maintained: osg/vo-client.spec - This is the RPM spec file proper. One needs to update the version (and/or the release number) every time a new RPM is created. upstream/release_tarball.source - This file contains the relative path of the tarball within the upstream source cache . Since the tarball file name will change with every new RPM version, this file has to be changed accordingly.","title":"RPM spec file maintenance"},{"location":"software/create-vo-client/#rpm-building","text":"After installing the osg-build tools , check out a clean copy from svn, then: osg-build prebuild . Once there are no errors, run osg-build koji . --scratch This can be done without making any permanent change. Once that builds successfully, run osg-build koji . This is permanent, unlike when you ran with --scratch . You cannot rebuild this version of the RPM again - you must bump the release number and edit the changelog. This will push the RPMs into the OSG development repository. Koji requires additional setup compared to rpmbuild; see the documentation here .","title":"RPM building"},{"location":"software/create-vo-client/#promotion-to-testing-and-release","text":"","title":"Promotion to testing and release:"},{"location":"software/create-vo-client/#policies","text":"Read Release Policy . These should be synchronized internally with other GOC update activities.","title":"Policies"},{"location":"software/development-process/","text":"Software Development Process This page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.) Overall Development Cycle For a typical update to an existing package, the overall development cycle is roughly as follows: Download the new upstream source (tarball, source RPM, checkout) into the UW AFS upstream area In a checkout of our packaging code , update the reference to the upstream file and, as needed, the RPM spec file Use osg-build to perform a scratch build of the updated package Verify that the build succeeded; if not, redo previous steps until success Optionally, lightly test the new RPM(s); if there are problems, redo previous steps until success Use osg-build to perform an official build of the updated package (which will go into the development repos) Perform standard developer testing of the new RPM(s) \u2014 see below for details Obtain permission from the Software manager to promote the package Promote the package to testing \u2014 see below for details Versioning Guidelines OSG-owned software should contain three digits, X.Y.Z, where X represents the major version, Y the minor version, and Z the maintenance version. New releases of software should increment one of the major, minor, or maintenance according to the following guidelines: Major: Major new software, typically (but not limited to) full rewrites, new architectures, major new features; can certainly break backward compatibility (but should provide a smooth upgrade path). Worthy of introduction into Upcoming. Minor: Notable changes to the software, including significant feature changes, API changes, etc.; may break compatibility, but must provide an upgrade path from other versions within the same Major series. Maintenance: Bug fixes, minor feature tweaks, etc.; must not break compatibility with other versions within the same Major.Minor series. If you are unsure about which version number to increment in a software update, consult the Software Manager. Build Procedures Building packages for multiple OSG release series The OSG Software team supports multiple release series, independent but in parallel to a large degree. In many cases, a single package is the same across release series, and therefore we want to build the package once and share it among the series. The procedure below suggests a way to accomplish this task. Current definitions: maintenance: None current: OSG 3.4 ( trunk ) Procedure: Make changes to trunk Optionally, make and test a scratch build from trunk Commit the changes Make an official build from trunk (e.g.: osg-build koji PACKAGE ) Perform the standard 4 tests for the current series (see below) Merge the relevant commits from trunk into the maintenance branch (see below for tips) Optionally, make and test a scratch build from the maintenance branch Commit the merge Make an official build from the maintenance branch (e.g.: osg-build koji --repo=3.3 PACKAGE ) Perform the standard 4 tests for the maintenance series (see below) As needed (or directed by the Software manager), perform the cross-series tests (see below) Note Do not change the RPM Release number in the maintenance branch before rebuilding; the %dist tag will differ automatically, and hence the maintenance and current NVRs will not conflict. Merging changes from one release series to another These instructions assume that you are merging from trunk to branches/osg-3.3 . They also assume that the current directory you are in is a checkout of branches/osg-3.3 . I will use $pkg to refer to the name of your package. First, you will need the commit numbers for your changes: svn log \\^/native/redhat/trunk/$pkg | less Write down the commits you want to merge. If you only have one commit, merge that commit with -c as follows: svn merge -c $commit_num \\^/native/redhat/trunk/$pkg $pkg Where $commit_num is the SVN revision number of that commit (e.g. 17000). Merging an individual change like this is referred to as \"cherry-picking\". If you have a range of commits and you wish to merge all commits within that range, then do the following: svn merge -r $start_num:$end_num \\^/native/redhat/trunk/$pkg $pkg Where $start_num is the SVN revision of the commit BEFORE your first commit, and $end_num is the SVN revision of your last commit in that range. Note: Be very careful when merging a range from trunk into the maintenance branch so that you do not introduce more changes to the maintenance branch than are necessary. If you have multiple commits but they are not contiguous (i.e. there are commits made by you or someone else in that range that you do not want to merge), you will need to cherry-pick each individual commit. svn merge -c $commit1 \\^/native/redhat/trunk/$pkg $pkg svn merge -c $commit2 \\^/native/redhat/trunk/$pkg $pkg ... Where $commit1 , $commit2 are the commit numbers of the individual changes. Note that merge tracking in recent versions of SVN (1.5 or newer) should prevent commits from accidentally being merged multiple times. You should still look out for conflicts and examine the changes via svn diff before committing the merge. Testing Procedures Before promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing. If you are not doing your own development testing for a package , contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released. The \"Standard 4\" tests, defined In most cases, the Software manager will ask a developer to perform the \u201cstandard 4\u201d tests on an updated package in a release series before promotion. This is a shorthand description for a standard set of 4 test runs: Fresh install on el6 Fresh install on el7 Update install on el6 Update install on el7 An \u201cupdate install\u201d is a fresh install of the relevant package (or better yet, metapackage that includes it) from the production repository , followed by an update to the new build from the development repository . For each test run, the amount of functional testing required will vary. For very simple changes, it may be sufficient to verify that each installation succeeds and that the expected files are in place For some changes, it may be sufficient to run osg-test on the resulting installation For some changes, it will be necessary to perform careful functional tests of the affected component(s) If you have questions, check with the Software Manager to determine the amount of testing that is required per test run. The \"Cross-Series\" test, defined The cross-series test may need to be run for packages that have been built for multiple release series of the OSG software stack (i.e. 3.3 and 3.4): On el6, install from the 3.3 repositories, then update from the 3.4 repositories On el7, install from the 3.3 repositories, then update from the 3.4 repositories Viewed another way, this test is similar to the update installs, above, except from 3.3-release to 3.4-development. The \"Long Tail\" tests, defined These tests may need to be run when updating a package that's also in the old, unsupported (3.2) branch. They will consist of: Install from 3.2-release and update to 3.4-development (on el6 only) The \"full set of tests\", defined All of the tests mentioned above. Running the tests in VM Universe In the case that the package you're testing is covered by osg-tested-internal, you can run the full set of tests in a manual VM universe test run. Make sure you meet the pre-requisites required to submit VM Universe jobs on osghost.chtc.wisc.edu . After that's done, you can prepare the test suite by running: osg-run-tests Testing change x After you cd into the directory specified in the output of the previous command, you will need to edit the *.yaml files in parameters.d to reflect the tests that you will want to run i.e. clean installs, upgrade installs and upgrade installs between OSG versions. Once you're satisfied with your list of parameters, submit the dag: condor_submit_dag master-run.dag Promoting a Package to Testing Once development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle. Preparing a Good Promotion Request Developers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected JIRA ticket and will be answered there as well. Below are some tips for writing a good promotion request: Make sure that relevant information about goals, history, and resolution is in the associated ticket(s) Include globs for the NVRs to be promoted (or a detailed list, if it is that complicated, which it almost never is) If you ran automated tests: Link to the results page(s) Verify that relevant tests ran successfully (as opposed to being skipped or failing) \u2013 briefly summarize your findings Note whether the automated tests are just regression tests or actually test the current change(s) If there are any failures, explain why they are not important to the promotion request If you ran manual tests: Summarize your tests and findings If there were failures, explain why they are not important to the promotion request If there are critical build dependencies that we typically check, include reports from the built-against-pkgs tool Note: This step is really just for known, specific cases, like the {HTCondor, HTCondor-CE, BLAHP} set and the {BeStMan, GUMS, VOMS Admin, etc.} Java set Occasionally, the OSG Software manager will request the tool to be run for other cases If other packages depend on the to-be-promoted package, explain whether the dependent packages must be rebuilt or, if not, why not For example (hypothetical promotion request for HTCondor-CE): May I promote htcondor-ce-2.3.4-2.osg3*.el* ? I ran a complete set of automated tests LINK THE PRECEDING TEXT OR SEPARATELY HERE> ; the HTCondor-CE tests ran and passed in all cases. There were some spurious failures of RSV in the All condition for RHEL 6, but this is a known failure case that is independent of HTCondor-CE. I also did a few spot checks manually (one VM each for SL 6 and SL 7), and in each case setting use_frobnosticator = true in the configuration resulted in the expected behavior as defined in the description field above. The built-against-pkgs tool shows that I built against all the latest HTCondor and BLAHP builds, see below. JIRA-formatted table comes after> Promoting Follow these steps to request promotion, promote a package, and note the promotion in JIRA: Make sure the package update has at least one associated JIRA ticket; if there is no ticket, at least create one for releasing the package(s) Obtain permission to promote the package(s) from the Software manager (see above) Use osg-promote to promote the package(s) from development to testing Comment on the associated JIRA ticket(s) with osg-promote\u2019s JIRA-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing Mark each associated JIRA ticket as \u201cReady For Testing\u201d and (done automatically for you:) set the Assignee to \u201cUnassigned\u201d","title":"Development Process"},{"location":"software/development-process/#software-development-process","text":"This page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.)","title":"Software Development Process"},{"location":"software/development-process/#overall-development-cycle","text":"For a typical update to an existing package, the overall development cycle is roughly as follows: Download the new upstream source (tarball, source RPM, checkout) into the UW AFS upstream area In a checkout of our packaging code , update the reference to the upstream file and, as needed, the RPM spec file Use osg-build to perform a scratch build of the updated package Verify that the build succeeded; if not, redo previous steps until success Optionally, lightly test the new RPM(s); if there are problems, redo previous steps until success Use osg-build to perform an official build of the updated package (which will go into the development repos) Perform standard developer testing of the new RPM(s) \u2014 see below for details Obtain permission from the Software manager to promote the package Promote the package to testing \u2014 see below for details","title":"Overall Development Cycle"},{"location":"software/development-process/#versioning-guidelines","text":"OSG-owned software should contain three digits, X.Y.Z, where X represents the major version, Y the minor version, and Z the maintenance version. New releases of software should increment one of the major, minor, or maintenance according to the following guidelines: Major: Major new software, typically (but not limited to) full rewrites, new architectures, major new features; can certainly break backward compatibility (but should provide a smooth upgrade path). Worthy of introduction into Upcoming. Minor: Notable changes to the software, including significant feature changes, API changes, etc.; may break compatibility, but must provide an upgrade path from other versions within the same Major series. Maintenance: Bug fixes, minor feature tweaks, etc.; must not break compatibility with other versions within the same Major.Minor series. If you are unsure about which version number to increment in a software update, consult the Software Manager.","title":"Versioning Guidelines"},{"location":"software/development-process/#build-procedures","text":"","title":"Build Procedures"},{"location":"software/development-process/#building-packages-for-multiple-osg-release-series","text":"The OSG Software team supports multiple release series, independent but in parallel to a large degree. In many cases, a single package is the same across release series, and therefore we want to build the package once and share it among the series. The procedure below suggests a way to accomplish this task. Current definitions: maintenance: None current: OSG 3.4 ( trunk ) Procedure: Make changes to trunk Optionally, make and test a scratch build from trunk Commit the changes Make an official build from trunk (e.g.: osg-build koji PACKAGE ) Perform the standard 4 tests for the current series (see below) Merge the relevant commits from trunk into the maintenance branch (see below for tips) Optionally, make and test a scratch build from the maintenance branch Commit the merge Make an official build from the maintenance branch (e.g.: osg-build koji --repo=3.3 PACKAGE ) Perform the standard 4 tests for the maintenance series (see below) As needed (or directed by the Software manager), perform the cross-series tests (see below) Note Do not change the RPM Release number in the maintenance branch before rebuilding; the %dist tag will differ automatically, and hence the maintenance and current NVRs will not conflict.","title":"Building packages for multiple OSG release series"},{"location":"software/development-process/#merging-changes-from-one-release-series-to-another","text":"These instructions assume that you are merging from trunk to branches/osg-3.3 . They also assume that the current directory you are in is a checkout of branches/osg-3.3 . I will use $pkg to refer to the name of your package. First, you will need the commit numbers for your changes: svn log \\^/native/redhat/trunk/$pkg | less Write down the commits you want to merge. If you only have one commit, merge that commit with -c as follows: svn merge -c $commit_num \\^/native/redhat/trunk/$pkg $pkg Where $commit_num is the SVN revision number of that commit (e.g. 17000). Merging an individual change like this is referred to as \"cherry-picking\". If you have a range of commits and you wish to merge all commits within that range, then do the following: svn merge -r $start_num:$end_num \\^/native/redhat/trunk/$pkg $pkg Where $start_num is the SVN revision of the commit BEFORE your first commit, and $end_num is the SVN revision of your last commit in that range. Note: Be very careful when merging a range from trunk into the maintenance branch so that you do not introduce more changes to the maintenance branch than are necessary. If you have multiple commits but they are not contiguous (i.e. there are commits made by you or someone else in that range that you do not want to merge), you will need to cherry-pick each individual commit. svn merge -c $commit1 \\^/native/redhat/trunk/$pkg $pkg svn merge -c $commit2 \\^/native/redhat/trunk/$pkg $pkg ... Where $commit1 , $commit2 are the commit numbers of the individual changes. Note that merge tracking in recent versions of SVN (1.5 or newer) should prevent commits from accidentally being merged multiple times. You should still look out for conflicts and examine the changes via svn diff before committing the merge.","title":"Merging changes from one release series to another"},{"location":"software/development-process/#testing-procedures","text":"Before promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing. If you are not doing your own development testing for a package , contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released.","title":"Testing Procedures"},{"location":"software/development-process/#the-standard-4-tests-defined","text":"In most cases, the Software manager will ask a developer to perform the \u201cstandard 4\u201d tests on an updated package in a release series before promotion. This is a shorthand description for a standard set of 4 test runs: Fresh install on el6 Fresh install on el7 Update install on el6 Update install on el7 An \u201cupdate install\u201d is a fresh install of the relevant package (or better yet, metapackage that includes it) from the production repository , followed by an update to the new build from the development repository . For each test run, the amount of functional testing required will vary. For very simple changes, it may be sufficient to verify that each installation succeeds and that the expected files are in place For some changes, it may be sufficient to run osg-test on the resulting installation For some changes, it will be necessary to perform careful functional tests of the affected component(s) If you have questions, check with the Software Manager to determine the amount of testing that is required per test run.","title":"The \"Standard 4\" tests, defined"},{"location":"software/development-process/#the-cross-series-test-defined","text":"The cross-series test may need to be run for packages that have been built for multiple release series of the OSG software stack (i.e. 3.3 and 3.4): On el6, install from the 3.3 repositories, then update from the 3.4 repositories On el7, install from the 3.3 repositories, then update from the 3.4 repositories Viewed another way, this test is similar to the update installs, above, except from 3.3-release to 3.4-development.","title":"The \"Cross-Series\" test, defined"},{"location":"software/development-process/#the-long-tail-tests-defined","text":"These tests may need to be run when updating a package that's also in the old, unsupported (3.2) branch. They will consist of: Install from 3.2-release and update to 3.4-development (on el6 only)","title":"The \"Long Tail\" tests, defined"},{"location":"software/development-process/#the-full-set-of-tests-defined","text":"All of the tests mentioned above.","title":"The \"full set of tests\", defined"},{"location":"software/development-process/#running-the-tests-in-vm-universe","text":"In the case that the package you're testing is covered by osg-tested-internal, you can run the full set of tests in a manual VM universe test run. Make sure you meet the pre-requisites required to submit VM Universe jobs on osghost.chtc.wisc.edu . After that's done, you can prepare the test suite by running: osg-run-tests Testing change x After you cd into the directory specified in the output of the previous command, you will need to edit the *.yaml files in parameters.d to reflect the tests that you will want to run i.e. clean installs, upgrade installs and upgrade installs between OSG versions. Once you're satisfied with your list of parameters, submit the dag: condor_submit_dag master-run.dag","title":"Running the tests in VM Universe"},{"location":"software/development-process/#promoting-a-package-to-testing","text":"Once development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle.","title":"Promoting a Package to Testing"},{"location":"software/development-process/#preparing-a-good-promotion-request","text":"Developers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected JIRA ticket and will be answered there as well. Below are some tips for writing a good promotion request: Make sure that relevant information about goals, history, and resolution is in the associated ticket(s) Include globs for the NVRs to be promoted (or a detailed list, if it is that complicated, which it almost never is) If you ran automated tests: Link to the results page(s) Verify that relevant tests ran successfully (as opposed to being skipped or failing) \u2013 briefly summarize your findings Note whether the automated tests are just regression tests or actually test the current change(s) If there are any failures, explain why they are not important to the promotion request If you ran manual tests: Summarize your tests and findings If there were failures, explain why they are not important to the promotion request If there are critical build dependencies that we typically check, include reports from the built-against-pkgs tool Note: This step is really just for known, specific cases, like the {HTCondor, HTCondor-CE, BLAHP} set and the {BeStMan, GUMS, VOMS Admin, etc.} Java set Occasionally, the OSG Software manager will request the tool to be run for other cases If other packages depend on the to-be-promoted package, explain whether the dependent packages must be rebuilt or, if not, why not For example (hypothetical promotion request for HTCondor-CE): May I promote htcondor-ce-2.3.4-2.osg3*.el* ? I ran a complete set of automated tests LINK THE PRECEDING TEXT OR SEPARATELY HERE> ; the HTCondor-CE tests ran and passed in all cases. There were some spurious failures of RSV in the All condition for RHEL 6, but this is a known failure case that is independent of HTCondor-CE. I also did a few spot checks manually (one VM each for SL 6 and SL 7), and in each case setting use_frobnosticator = true in the configuration resulted in the expected behavior as defined in the description field above. The built-against-pkgs tool shows that I built against all the latest HTCondor and BLAHP builds, see below. JIRA-formatted table comes after>","title":"Preparing a Good Promotion Request"},{"location":"software/development-process/#promoting","text":"Follow these steps to request promotion, promote a package, and note the promotion in JIRA: Make sure the package update has at least one associated JIRA ticket; if there is no ticket, at least create one for releasing the package(s) Obtain permission to promote the package(s) from the Software manager (see above) Use osg-promote to promote the package(s) from development to testing Comment on the associated JIRA ticket(s) with osg-promote\u2019s JIRA-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing Mark each associated JIRA ticket as \u201cReady For Testing\u201d and (done automatically for you:) set the Assignee to \u201cUnassigned\u201d","title":"Promoting"},{"location":"software/effort-tracking/","text":"Effort Tracking This page describes a simple plan for tracking effort in the OSG Technology teams. Basic Ideas At its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking. There are just a few simple ideas to keep in mind: Each week, report your effort on OSG Technology activities Update your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example: EFFORT External development: 63% Support: 12% Leave: 20% Outside: 5 Follow standard federal regulations for calculating effort (e.g., OMB Circular A-21) The main idea is that all of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%. Report 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.) If you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see. Effort is reported as integer percentages, no less accurate than 5% intervals So please do not report percentages like 43.21% and please do not round to the nearest 10%. Effort Categories Here are the categories in which to track effort: Investigations Work on the Investigations team External Software work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components Internal Software work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances Documentation Work on our TWiki or Markdown documentation Release Release team activities, primarily acceptance testing and cutting releases Support User support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support. Management This is mainly for team leads; e.g., managing team activities and tickets (generally); hiring; leading (not just attending) meetings Education Not for general learning or training activities The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities. Admin General administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014 use sparingly!! Outside For all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL) Leave This is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours A few thoughts about tricky situations: Meetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time. Administrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between Admin and Outside according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities. Outside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not. Learning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into Internal . But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as Admin , and maybe add a comment explaining the activity. Ultimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.","title":"Effort Tracking"},{"location":"software/effort-tracking/#effort-tracking","text":"This page describes a simple plan for tracking effort in the OSG Technology teams.","title":"Effort Tracking"},{"location":"software/effort-tracking/#basic-ideas","text":"At its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking. There are just a few simple ideas to keep in mind: Each week, report your effort on OSG Technology activities Update your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example: EFFORT External development: 63% Support: 12% Leave: 20% Outside: 5 Follow standard federal regulations for calculating effort (e.g., OMB Circular A-21) The main idea is that all of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%. Report 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.) If you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see. Effort is reported as integer percentages, no less accurate than 5% intervals So please do not report percentages like 43.21% and please do not round to the nearest 10%.","title":"Basic Ideas"},{"location":"software/effort-tracking/#effort-categories","text":"Here are the categories in which to track effort: Investigations Work on the Investigations team External Software work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components Internal Software work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances Documentation Work on our TWiki or Markdown documentation Release Release team activities, primarily acceptance testing and cutting releases Support User support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support. Management This is mainly for team leads; e.g., managing team activities and tickets (generally); hiring; leading (not just attending) meetings Education Not for general learning or training activities The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities. Admin General administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014 use sparingly!! Outside For all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL) Leave This is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours A few thoughts about tricky situations: Meetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time. Administrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between Admin and Outside according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities. Outside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not. Learning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into Internal . But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as Admin , and maybe add a comment explaining the activity. Ultimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.","title":"Effort Categories"},{"location":"software/git-software-development/","text":"Git software development workflow This document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software. Git and GitHub basics If you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at https://help.github.com/categories/bootcamp/ Getting shell access to GitHub There are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience. The instructions here are derived from GitHub's own instructions on using SSH keys . Creating a new SSH key (optional but recommended) If you already have an SSH keypair in your ~/.ssh directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub. The instructions below will create an SSH public/private key pair with the private key stored in ~/.ssh/id_github and public key stored in ~/.ssh/id_github.pub . Generating the key Use ssh-keygen to generate the SSH keypair. For EMAIL_ADDRESS , use the email address associated with your GitHub account. [user@client ~ ] $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_github -C EMAIL_ADDRESS Configuring SSH to use the key for GitHub Make sure SSH uses the new key by default to access GitHub. Create or edit ~/.ssh/config and append the following lines: Host github.com IdentityFile YOUR_HOME_DIR /.ssh/id_github Adding the SSH public key to GitHub Using the GitHub web interface: On the upper right of the screen, click on your profile picture In the menu that pops up, click \"Settings\" On the left-hand sidebar, click \"SSH and GPG keys\" In the top right of the \"SSH keys\" box, click \"New SSH key\" In the \"Title\" field of the dialog that pops up, enter a descriptive name for the key Open the public key file (e.g. ~/.ssh/id_github.pub (don't forget the .pub )) in a text editor and copy its full contents to the clipboard In the \"Key\" field, paste the public key Below the \"Key\" field, click \"Add SSH key\" You should see your new key in the \"SSH keys\" list. Testing that shell access works To verify you can authenticate to GitHub using SSH, SSH to git@github.com . You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.' Contribution workflow We use the standard GitHub pull request workflow for making contributions to OSG software. If you've never contributed to this project on GitHub before, do the following steps first: Using the GitHub web interface, fork the repo you wish to contribute to. Make a clone of your forked repo on your local machine. [user@client ~ ] $ git clone git@github.com : USERNAME/PROJECT Note If you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above. If you get some other error, the GitHub page on SSH may contain useful information on troubleshooting. Once you have your local repo, do the following: Create a branch to hold changes that are related to the issue you are working on. Give the branch a name that will remind you of its purpose, such as sw2345-pathchange [user@client ~ ] $ git checkout -b BRANCH Make your commits to this branch, then push the branch to your repo on GitHub. [user@client ~ ] $ git push origin BRANCH Select your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a JIRA ticket number in the message to automatically link the pull request to the JIRA ticket. Request a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member. If the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from. If your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request. Release workflow This section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by opensciencegrid . (If you can approve pull requests, you have write access). A release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by opensciencegrid (the OSG repo): If you haven't already, add the OSG repo as a \"remote\" to your repo: [user@client ~ ] $ git remote add upstream git@github.com:opensciencegrid/ PROJECT Fetch changes from the OSG repo: [user@client ~ ] $ git fetch upstream Compare your branch you are releasing from (probably master ) to its copy in the OSG repo: [user@client ~ ] $ git checkout master ; git diff upstream/master There should be no differences. Once this is done, release the software as you usually do. This process varies from one project to another, but often it involves running make upstream or similar. Check your project's README file for instructions. Test your software. Tag the commit that you made the release from. Git release tags are conventionally called VERSION , where VERSION is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the tag v1.3.0 . Note Once a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from. Create the tag in your local repo: [user@client ~ ] $ git tag TAG Push the tag to your own GitHub repo: [user@client ~ ] $ git push origin TAG Push the tag to the OSG repo: [user@client ~ ] $ git push upstream TAG","title":"Git Software Development Process"},{"location":"software/git-software-development/#git-software-development-workflow","text":"This document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software.","title":"Git software development workflow"},{"location":"software/git-software-development/#git-and-github-basics","text":"If you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at https://help.github.com/categories/bootcamp/","title":"Git and GitHub basics"},{"location":"software/git-software-development/#getting-shell-access-to-github","text":"There are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience. The instructions here are derived from GitHub's own instructions on using SSH keys .","title":"Getting shell access to GitHub"},{"location":"software/git-software-development/#creating-a-new-ssh-key-optional-but-recommended","text":"If you already have an SSH keypair in your ~/.ssh directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub. The instructions below will create an SSH public/private key pair with the private key stored in ~/.ssh/id_github and public key stored in ~/.ssh/id_github.pub .","title":"Creating a new SSH key (optional but recommended)"},{"location":"software/git-software-development/#generating-the-key","text":"Use ssh-keygen to generate the SSH keypair. For EMAIL_ADDRESS , use the email address associated with your GitHub account. [user@client ~ ] $ ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_github -C EMAIL_ADDRESS","title":"Generating the key"},{"location":"software/git-software-development/#configuring-ssh-to-use-the-key-for-github","text":"Make sure SSH uses the new key by default to access GitHub. Create or edit ~/.ssh/config and append the following lines: Host github.com IdentityFile YOUR_HOME_DIR /.ssh/id_github","title":"Configuring SSH to use the key for GitHub"},{"location":"software/git-software-development/#adding-the-ssh-public-key-to-github","text":"Using the GitHub web interface: On the upper right of the screen, click on your profile picture In the menu that pops up, click \"Settings\" On the left-hand sidebar, click \"SSH and GPG keys\" In the top right of the \"SSH keys\" box, click \"New SSH key\" In the \"Title\" field of the dialog that pops up, enter a descriptive name for the key Open the public key file (e.g. ~/.ssh/id_github.pub (don't forget the .pub )) in a text editor and copy its full contents to the clipboard In the \"Key\" field, paste the public key Below the \"Key\" field, click \"Add SSH key\" You should see your new key in the \"SSH keys\" list.","title":"Adding the SSH public key to GitHub"},{"location":"software/git-software-development/#testing-that-shell-access-works","text":"To verify you can authenticate to GitHub using SSH, SSH to git@github.com . You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.'","title":"Testing that shell access works"},{"location":"software/git-software-development/#contribution-workflow","text":"We use the standard GitHub pull request workflow for making contributions to OSG software. If you've never contributed to this project on GitHub before, do the following steps first: Using the GitHub web interface, fork the repo you wish to contribute to. Make a clone of your forked repo on your local machine. [user@client ~ ] $ git clone git@github.com : USERNAME/PROJECT Note If you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above. If you get some other error, the GitHub page on SSH may contain useful information on troubleshooting. Once you have your local repo, do the following: Create a branch to hold changes that are related to the issue you are working on. Give the branch a name that will remind you of its purpose, such as sw2345-pathchange [user@client ~ ] $ git checkout -b BRANCH Make your commits to this branch, then push the branch to your repo on GitHub. [user@client ~ ] $ git push origin BRANCH Select your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a JIRA ticket number in the message to automatically link the pull request to the JIRA ticket. Request a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member. If the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from. If your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request.","title":"Contribution workflow"},{"location":"software/git-software-development/#release-workflow","text":"This section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by opensciencegrid . (If you can approve pull requests, you have write access). A release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by opensciencegrid (the OSG repo): If you haven't already, add the OSG repo as a \"remote\" to your repo: [user@client ~ ] $ git remote add upstream git@github.com:opensciencegrid/ PROJECT Fetch changes from the OSG repo: [user@client ~ ] $ git fetch upstream Compare your branch you are releasing from (probably master ) to its copy in the OSG repo: [user@client ~ ] $ git checkout master ; git diff upstream/master There should be no differences. Once this is done, release the software as you usually do. This process varies from one project to another, but often it involves running make upstream or similar. Check your project's README file for instructions. Test your software. Tag the commit that you made the release from. Git release tags are conventionally called VERSION , where VERSION is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the tag v1.3.0 . Note Once a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from. Create the tag in your local repo: [user@client ~ ] $ git tag TAG Push the tag to your own GitHub repo: [user@client ~ ] $ git push origin TAG Push the tag to the OSG repo: [user@client ~ ] $ git push upstream TAG","title":"Release workflow"},{"location":"software/globus-mass-update-procedure/","text":"Globus mass update procedure Globus consists of many packages, which we tend to update at the same time. This requires extra work, primarily to prevent dependency issues. Prep work Docs Create a spreadsheet or table of the builds. Table should have NVR, perhaps URL, status (not started, imported, built, tested), and comments (mostly to record if it was a simple pass-through or not). Get packages to update, using osg-outdated-epel-pkgs from opensciencegrid/tools . To get in N-V-R format: [you@host]$ ./osg-outdated-epel-pkgs | \\ egrep ^(globus|myproxy|gsi) | \\ awk BEGIN {OFS= } {print $1, - , $3} or to split up N and V-R in a comma-separated way (which you can feed into a Google Sheet to turn it into two columns): [you@host]$ ./osg-outdated-epel-pkgs | \\ egrep ^(globus|myproxy|gsi) | \\ awk BEGIN {OFS= } {print $1, , , $3} SVN Create a separate SVN branch and populate it with all the packages you will update. (Get the list from the doc created above). [you@uw]$ svn mkdir file:///p/vdt/workspace/svn/native/redhat/branches/globus # ## From a checkout, in native/redhat [you@uw]$ for x in PACKAGES ; do \\ svn copy $x branches/globus/ ${ x #trunk/ } ; \\ done Koji (Mat/Carl) This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist. Ensure Koji tags exist: a destination tag, and a build tag, one for each dver, e.g.: el6-globus el6-globus-build el7-globus el7-globus-build Set up tag inheritence: base the build tags off of the corresponding dist-el?-build tag. This is because we don't want old osg packages interfering with the new versions we're building. These may already exist -- check the el?-globus-build tags in the web interface. [you@host]$ for el in el6 el7 ; do \\ osg-koji add-tag --parent = dist- $el -build \\ --arches = x86_64 $el -globus-build ; \\ done Tag buildsys-macros for the OSG release into the build tags: [you@host]$ for el in el6 el7 ; do \\ buildsys_macros_nvr = $( osg-koji -q list-tagged osg-3.4- $el -development \\ buildsys-macros --latest | awk {print $1} ) ; \\ osg-koji tag-pkg $el -globus-build $buildsys_macros_nvr ; \\ done Ensure Koji targets exist, one for each dver, e.g.: el6-globus (el6-globus-build el6-globus) el7-globus (el7-globus-build el7-globus) kojira-fake-el6-globus (el6-globus kojira-fake) kojira-fake-el7-globus (el7-globus kojira-fake) [you@host]$ for el in el6 el7 ; do \\ osg-koji add-target $el -globus $el -globus-build $el -globus ; \\ osg-koji add-target kojira-fake- $el -globus $el -globus kojira-fake ; \\ done If basing the packages off of the Globus repos, add the Globus repos as external repos, and add them to the build tags (but not the dest tags). Edit /etc/koji-hub/plugins/sign.conf and set up the GPG signing for the RPMs. Run /etc/koji-hub/plugins/fix-permissions after editing the file. Per-package work cd into branches/globus Download packages from http://dl.fedoraproject.org/pub/epel/6/SRPMS/ A useful alias: [you@host]$ alias osg-build-globus = osg-build koji --ktt el6-globus --ktt el7-globus Strict pass-through (no osg/ directory) Run: [you@uw]$ osg-import-srpm URL [you@uw]$ osg-build-globus --scratch PKG Commit - use a message like \"Update to 3.12-1 from EPEL (SOFTWARE-2197)\" Do a non-scratch build. Non-strict pass-through Run: [you@uw]$ osg-import-srpm --diff3 URL Fix merge conflicts in the spec file. If not already there, put a .1 after the Release number to mark the changes as ours. Run: [you@uw]$ osg-build quilt PKG Fix patches if necessary. Run: [you@uw ]$ osg-build-globus --scratch PKG Commit - use a message like \"Update to 8.29-1 from EPEL and merge OSG changes (SOFTWARE-2197)\" Do a non-scratch build. Testing Create a yum .repo file similar to osg-minefield that installs from the el?-globus repos. Enable this and osg-minefield . EL7 example: [globus] name = globus baseurl = http://koji.chtc.wisc.edu/mnt/koji/repos/el7-globus/latest/$basearch/ failovermethod = priority priority = 98 enabled = 1 gpgcheck = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG consider_as_osg = yes Merge Koji (Mat/Carl) This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist. Untag broken versions that we don't want to ship. Use move-pkg : [you@host ]$ for el in el6 el7 ; do \\ osg-koji -q list-tagged ${ el } -globus | \\ awk {print $1} ${ el } -tagged.txt ; \\ done # ## Check the txts if they look sane [you@host ]$ for el in el6 el7 ; do \\ xargs -a ${ el } -tagged.txt \\ osg-koji move-pkg ${ el } -globus \\ osg-3.3- ${ el } -development ; \\ done SVN Merge from trunk to branches/globus first, to pick up any globus changes that may have happened in trunk. Merge from branches/globus to trunk . Move branches/globus to tags/globus- DATE .","title":"Globus Mass Update Procedure"},{"location":"software/globus-mass-update-procedure/#globus-mass-update-procedure","text":"Globus consists of many packages, which we tend to update at the same time. This requires extra work, primarily to prevent dependency issues.","title":"Globus mass update procedure"},{"location":"software/globus-mass-update-procedure/#prep-work","text":"","title":"Prep work"},{"location":"software/globus-mass-update-procedure/#docs","text":"Create a spreadsheet or table of the builds. Table should have NVR, perhaps URL, status (not started, imported, built, tested), and comments (mostly to record if it was a simple pass-through or not). Get packages to update, using osg-outdated-epel-pkgs from opensciencegrid/tools . To get in N-V-R format: [you@host]$ ./osg-outdated-epel-pkgs | \\ egrep ^(globus|myproxy|gsi) | \\ awk BEGIN {OFS= } {print $1, - , $3} or to split up N and V-R in a comma-separated way (which you can feed into a Google Sheet to turn it into two columns): [you@host]$ ./osg-outdated-epel-pkgs | \\ egrep ^(globus|myproxy|gsi) | \\ awk BEGIN {OFS= } {print $1, , , $3}","title":"Docs"},{"location":"software/globus-mass-update-procedure/#svn","text":"Create a separate SVN branch and populate it with all the packages you will update. (Get the list from the doc created above). [you@uw]$ svn mkdir file:///p/vdt/workspace/svn/native/redhat/branches/globus # ## From a checkout, in native/redhat [you@uw]$ for x in PACKAGES ; do \\ svn copy $x branches/globus/ ${ x #trunk/ } ; \\ done","title":"SVN"},{"location":"software/globus-mass-update-procedure/#koji-matcarl","text":"This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist. Ensure Koji tags exist: a destination tag, and a build tag, one for each dver, e.g.: el6-globus el6-globus-build el7-globus el7-globus-build Set up tag inheritence: base the build tags off of the corresponding dist-el?-build tag. This is because we don't want old osg packages interfering with the new versions we're building. These may already exist -- check the el?-globus-build tags in the web interface. [you@host]$ for el in el6 el7 ; do \\ osg-koji add-tag --parent = dist- $el -build \\ --arches = x86_64 $el -globus-build ; \\ done Tag buildsys-macros for the OSG release into the build tags: [you@host]$ for el in el6 el7 ; do \\ buildsys_macros_nvr = $( osg-koji -q list-tagged osg-3.4- $el -development \\ buildsys-macros --latest | awk {print $1} ) ; \\ osg-koji tag-pkg $el -globus-build $buildsys_macros_nvr ; \\ done Ensure Koji targets exist, one for each dver, e.g.: el6-globus (el6-globus-build el6-globus) el7-globus (el7-globus-build el7-globus) kojira-fake-el6-globus (el6-globus kojira-fake) kojira-fake-el7-globus (el7-globus kojira-fake) [you@host]$ for el in el6 el7 ; do \\ osg-koji add-target $el -globus $el -globus-build $el -globus ; \\ osg-koji add-target kojira-fake- $el -globus $el -globus kojira-fake ; \\ done If basing the packages off of the Globus repos, add the Globus repos as external repos, and add them to the build tags (but not the dest tags). Edit /etc/koji-hub/plugins/sign.conf and set up the GPG signing for the RPMs. Run /etc/koji-hub/plugins/fix-permissions after editing the file.","title":"Koji (Mat/Carl)"},{"location":"software/globus-mass-update-procedure/#per-package-work","text":"cd into branches/globus Download packages from http://dl.fedoraproject.org/pub/epel/6/SRPMS/ A useful alias: [you@host]$ alias osg-build-globus = osg-build koji --ktt el6-globus --ktt el7-globus","title":"Per-package work"},{"location":"software/globus-mass-update-procedure/#strict-pass-through-no-osg-directory","text":"Run: [you@uw]$ osg-import-srpm URL [you@uw]$ osg-build-globus --scratch PKG Commit - use a message like \"Update to 3.12-1 from EPEL (SOFTWARE-2197)\" Do a non-scratch build.","title":"Strict pass-through (no osg/ directory)"},{"location":"software/globus-mass-update-procedure/#non-strict-pass-through","text":"Run: [you@uw]$ osg-import-srpm --diff3 URL Fix merge conflicts in the spec file. If not already there, put a .1 after the Release number to mark the changes as ours. Run: [you@uw]$ osg-build quilt PKG Fix patches if necessary. Run: [you@uw ]$ osg-build-globus --scratch PKG Commit - use a message like \"Update to 8.29-1 from EPEL and merge OSG changes (SOFTWARE-2197)\" Do a non-scratch build.","title":"Non-strict pass-through"},{"location":"software/globus-mass-update-procedure/#testing","text":"Create a yum .repo file similar to osg-minefield that installs from the el?-globus repos. Enable this and osg-minefield . EL7 example: [globus] name = globus baseurl = http://koji.chtc.wisc.edu/mnt/koji/repos/el7-globus/latest/$basearch/ failovermethod = priority priority = 98 enabled = 1 gpgcheck = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG consider_as_osg = yes","title":"Testing"},{"location":"software/globus-mass-update-procedure/#merge","text":"","title":"Merge"},{"location":"software/globus-mass-update-procedure/#koji-matcarl_1","text":"This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist. Untag broken versions that we don't want to ship. Use move-pkg : [you@host ]$ for el in el6 el7 ; do \\ osg-koji -q list-tagged ${ el } -globus | \\ awk {print $1} ${ el } -tagged.txt ; \\ done # ## Check the txts if they look sane [you@host ]$ for el in el6 el7 ; do \\ xargs -a ${ el } -tagged.txt \\ osg-koji move-pkg ${ el } -globus \\ osg-3.3- ${ el } -development ; \\ done","title":"Koji (Mat/Carl)"},{"location":"software/globus-mass-update-procedure/#svn_1","text":"Merge from trunk to branches/globus first, to pick up any globus changes that may have happened in trunk. Merge from branches/globus to trunk . Move branches/globus to tags/globus- DATE .","title":"SVN"},{"location":"software/ipv6-testing/","text":"Testing Software with IPv6 About this Document This document provides instructions on setting up a host with an IPv6 address for testing the OSG software stack. The plan is to be able to spin up special Fermicloud VM\u2019s that have corresponding public IPv6 addresses meaning that there will be a limit of ~15 VM\u2019s at one time. For more information on IPv6, consult Wikipedia . Requirements Be familiar with your institute's network policy and firewall configuration 1 Root access is required to configure iptables Enabling IPV6 Determine the public IPv6 address of your host (highlighted in red ): user@host $ nslookup -type = aaaa hostname Server: 132.239.0.252 Address: 132.239.0.252#53 Non-authoritative answer: ipv6vm001.fnal.gov has AAAA address 2001:400:2410:29::182 Replacing hostname with your machine's hostname. Ask your network administrator for your IPv6 default gateway Modify /etc/sysconfig/network-scripts/ifcfg-eth0 and be sure these lines exist, and : IPV6INIT=yes IPV6_AUTOCONF=no IPV6ADDR= IPv6 address IPV6_DEFAULTGW= The IPV6 Default Gateway Replacing IPv6 address with the address found in step 1. Restart the network devices: root@host # service network restart Shutting down interface eth0: [ OK ] Shutting down loopback interface: [ OK ] Bringing up loopback interface: [ OK ] Bringing up interface eth0: [ OK ] Testing IPv6 connectivity To verify that the VM is capable of IPv6 we will be using the ping6 command between the test VM and another IPv6 capable machine From another IPv6 capable machine, ping your VM: user@host $ ping6 ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov(ipv6vm001.fnal.gov) 56 data bytes 64 bytes from ipv6vm001.fnal.gov: icmp_seq=1 ttl=51 time=68.1 ms 64 bytes from ipv6vm001.fnal.gov: icmp_seq=2 ttl=51 time=57.6 ms From your test VM, ping another IPv6 capable machine (a list of IPv6 machines can be found here ): [efajardo@ipv6vm001 ~]# ping6 uaf-4.t2.ucsd.edu PING uaf-4.t2.ucsd.edu(uaf-4.t2.ucsd.edu) 56 data bytes 64 bytes from uaf-4.t2.ucsd.edu: icmp_seq=1 ttl=51 time=57.6 ms Verifying SSH over IPv6 Make sure you can login to your VM over IPv6. Currently, Fermilab's kerberos does not support SSH over IPv6. Add your ssh_key to your machine and make sure /etc/ssh/sshd_config has the following lines: RSAAuthentication yes PubkeyAuthentication yes Try connecting to you IPv6 enabled machine over SSH: efajardo@uaf-4 ~$ ssh -6 root@ipv6vm001.fnal.gov Last login: Wed Jun 11 14:51:47 2014 from 2607:f720:1700:1b30:21f:c6ff:feeb:2631 [root@ipv6vm001 ~]# Disabling IPv4 If you were able to log into your VM over IPv6, you can disable IPv4 and try to communicate exclusively over IPv6. Comment the IPADDR line in /etc/sysconfig/network-scripts/ifcfg-eth0 : #IPADDR=131.225.41.182 IPV6ADDR= 2607:f720:1700:1b30::9b Note Ensure that your IPV6ADDR is uncommented otherwise you will not be able to connect to the host again Restart the network services: root@host # service network restart The ping command should no longer work: root@host # ping ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov (131.225.41.182): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Disabling IPv6 In your testing, you may find the need to disable IPv6. root@host # sysctl -w net.ipv6.conf.all.disable_ipv6 = 1 root@host # service network restart The ping6 command should no longer work: root@host # ping6 ipv6vm001.fnal.gov Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Testing in mixed mode To test IPv6 in mixed mode, you can use the ntop tool to monitor traffic over IPv6. ntop is installed on all the test machines and you can access the web interface at hostname:3000 . To see a table that displays network traffic between your VM and another host by going to All Protocols - Traffic and looking at the IPv6 column. Enforcing communication over IPv6 If you want to enforce IPv6 over mixed mode you can try using the IPv6 address for whatever software that you are testing. For example with xrdcp: root@host # xrdcp -d 1 /tmp/first_test root:// [ 2607 :f720:1700:1b30::a4 ] :1094//tmp/first_test_8 [19B/19B][100%][==================================================][0B/s] Notice that the IPv6 address follows RFC2732 .","title":"IPv6 Testing"},{"location":"software/ipv6-testing/#testing-software-with-ipv6","text":"","title":"Testing Software with IPv6"},{"location":"software/ipv6-testing/#about-this-document","text":"This document provides instructions on setting up a host with an IPv6 address for testing the OSG software stack. The plan is to be able to spin up special Fermicloud VM\u2019s that have corresponding public IPv6 addresses meaning that there will be a limit of ~15 VM\u2019s at one time. For more information on IPv6, consult Wikipedia .","title":"About this Document"},{"location":"software/ipv6-testing/#requirements","text":"Be familiar with your institute's network policy and firewall configuration 1 Root access is required to configure iptables","title":"Requirements"},{"location":"software/ipv6-testing/#enabling-ipv6","text":"Determine the public IPv6 address of your host (highlighted in red ): user@host $ nslookup -type = aaaa hostname Server: 132.239.0.252 Address: 132.239.0.252#53 Non-authoritative answer: ipv6vm001.fnal.gov has AAAA address 2001:400:2410:29::182 Replacing hostname with your machine's hostname. Ask your network administrator for your IPv6 default gateway Modify /etc/sysconfig/network-scripts/ifcfg-eth0 and be sure these lines exist, and : IPV6INIT=yes IPV6_AUTOCONF=no IPV6ADDR= IPv6 address IPV6_DEFAULTGW= The IPV6 Default Gateway Replacing IPv6 address with the address found in step 1. Restart the network devices: root@host # service network restart Shutting down interface eth0: [ OK ] Shutting down loopback interface: [ OK ] Bringing up loopback interface: [ OK ] Bringing up interface eth0: [ OK ]","title":"Enabling IPV6"},{"location":"software/ipv6-testing/#testing-ipv6-connectivity","text":"To verify that the VM is capable of IPv6 we will be using the ping6 command between the test VM and another IPv6 capable machine From another IPv6 capable machine, ping your VM: user@host $ ping6 ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov(ipv6vm001.fnal.gov) 56 data bytes 64 bytes from ipv6vm001.fnal.gov: icmp_seq=1 ttl=51 time=68.1 ms 64 bytes from ipv6vm001.fnal.gov: icmp_seq=2 ttl=51 time=57.6 ms From your test VM, ping another IPv6 capable machine (a list of IPv6 machines can be found here ): [efajardo@ipv6vm001 ~]# ping6 uaf-4.t2.ucsd.edu PING uaf-4.t2.ucsd.edu(uaf-4.t2.ucsd.edu) 56 data bytes 64 bytes from uaf-4.t2.ucsd.edu: icmp_seq=1 ttl=51 time=57.6 ms","title":"Testing IPv6 connectivity"},{"location":"software/ipv6-testing/#verifying-ssh-over-ipv6","text":"Make sure you can login to your VM over IPv6. Currently, Fermilab's kerberos does not support SSH over IPv6. Add your ssh_key to your machine and make sure /etc/ssh/sshd_config has the following lines: RSAAuthentication yes PubkeyAuthentication yes Try connecting to you IPv6 enabled machine over SSH: efajardo@uaf-4 ~$ ssh -6 root@ipv6vm001.fnal.gov Last login: Wed Jun 11 14:51:47 2014 from 2607:f720:1700:1b30:21f:c6ff:feeb:2631 [root@ipv6vm001 ~]#","title":"Verifying SSH over IPv6"},{"location":"software/ipv6-testing/#disabling-ipv4","text":"If you were able to log into your VM over IPv6, you can disable IPv4 and try to communicate exclusively over IPv6. Comment the IPADDR line in /etc/sysconfig/network-scripts/ifcfg-eth0 : #IPADDR=131.225.41.182 IPV6ADDR= 2607:f720:1700:1b30::9b Note Ensure that your IPV6ADDR is uncommented otherwise you will not be able to connect to the host again Restart the network services: root@host # service network restart The ping command should no longer work: root@host # ping ipv6vm001.fnal.gov PING ipv6vm001.fnal.gov (131.225.41.182): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1","title":"Disabling IPv4"},{"location":"software/ipv6-testing/#disabling-ipv6","text":"In your testing, you may find the need to disable IPv6. root@host # sysctl -w net.ipv6.conf.all.disable_ipv6 = 1 root@host # service network restart The ping6 command should no longer work: root@host # ping6 ipv6vm001.fnal.gov Request timeout for icmp_seq 0 Request timeout for icmp_seq 1","title":"Disabling IPv6"},{"location":"software/ipv6-testing/#testing-in-mixed-mode","text":"To test IPv6 in mixed mode, you can use the ntop tool to monitor traffic over IPv6. ntop is installed on all the test machines and you can access the web interface at hostname:3000 . To see a table that displays network traffic between your VM and another host by going to All Protocols - Traffic and looking at the IPv6 column.","title":"Testing in mixed mode"},{"location":"software/ipv6-testing/#enforcing-communication-over-ipv6","text":"If you want to enforce IPv6 over mixed mode you can try using the IPv6 address for whatever software that you are testing. For example with xrdcp: root@host # xrdcp -d 1 /tmp/first_test root:// [ 2607 :f720:1700:1b30::a4 ] :1094//tmp/first_test_8 [19B/19B][100%][==================================================][0B/s] Notice that the IPv6 address follows RFC2732 .","title":"Enforcing communication over IPv6"},{"location":"software/koji-mass-rebuilds/","text":"Mass RPM Rebuilds for a new Build Target in Koji Whenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner. This document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds. New RHEL version vs new OSG series New RHEL version For a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later. New OSG series For a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series. A prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *). After some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort. Doing scratch builds of everything first Before doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target. Doing mass scratch rebuilds for an existing series is easy, as they can all be done at once. Relatedly, doing a round of scratch builds after successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues. Options for calculating build dependencies We can get dependency information from a number of places: scraping .spec files for Requires/BuildRequires/Provides and %package names querying existing rpms directly on koji-hub and our OS/EPEL mirrors ( rpm -q ) querying srpms from osg-build prebuild directly for build requirements inspecting previous buildroots to determine resolved build dependencies use repoquery to determine whatrequires/whatprovides for packages use yum-builddep to find packages with all build requirements available using the repodata (primary+filelists) from rpm repositories, including: upcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR osg-upcoming-elX-build, which includes them all One important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build. That is, (A BuildRequires B) and (B Requires C) implies A BuildRequires C . Combined with the fact that runtime requirements are transitive, that is, (A Requires B) and (B Requires C) implies A Requires C , computing build requirements is a recursive operation, which can be many levels deep. Another question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz = 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.) Note also that the dependencies/capabilities for a given package often varies between different rhel versions. Pre-computing (predictive) vs just-in-time Two different approaches to determining dependency order for building are: pre compute all dependencies based on an existing series/rhel version, OR compute which remaining packages have all build reqs satisfied now The first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target. The second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets. It may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first. Package list closure, pruning At some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts). But if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target. Proposal / Recommendations As mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version. For a new OSG series: update native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages tag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development) build all packages in new packaging area into new build target at once for all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development) For a new RHEL version: pull the repodata from the relevant *-build repo from koji: for pre-computing, use a build repo from an existing rhel version: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/ for just-in-time, use the new build repo: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/ the primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.) generate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo. the primary (sqlite) file can be used to get build-requires. use sql to resolve direct dependencies at the package name level: src-pkg: bin-pkg (BuildRequires) bin-pkg: bin-pkg (Requires) bin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies) resolve this list into a full list of recursive build dependencies. Since this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python. build ready-to-be-built packages update our copy of the repodata from the regen'ed *-build repo, as often as new versions become available update our dependency lists repeat until all packages are built","title":"Koji Mass Rebuilds"},{"location":"software/koji-mass-rebuilds/#mass-rpm-rebuilds-for-a-new-build-target-in-koji","text":"Whenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner. This document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds.","title":"Mass RPM Rebuilds for a new Build Target in Koji"},{"location":"software/koji-mass-rebuilds/#new-rhel-version-vs-new-osg-series","text":"","title":"New RHEL version vs new OSG series"},{"location":"software/koji-mass-rebuilds/#new-rhel-version","text":"For a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later.","title":"New RHEL version"},{"location":"software/koji-mass-rebuilds/#new-osg-series","text":"For a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series. A prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *). After some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort.","title":"New OSG series"},{"location":"software/koji-mass-rebuilds/#doing-scratch-builds-of-everything-first","text":"Before doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target. Doing mass scratch rebuilds for an existing series is easy, as they can all be done at once. Relatedly, doing a round of scratch builds after successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues.","title":"Doing scratch builds of everything first"},{"location":"software/koji-mass-rebuilds/#options-for-calculating-build-dependencies","text":"We can get dependency information from a number of places: scraping .spec files for Requires/BuildRequires/Provides and %package names querying existing rpms directly on koji-hub and our OS/EPEL mirrors ( rpm -q ) querying srpms from osg-build prebuild directly for build requirements inspecting previous buildroots to determine resolved build dependencies use repoquery to determine whatrequires/whatprovides for packages use yum-builddep to find packages with all build requirements available using the repodata (primary+filelists) from rpm repositories, including: upcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR osg-upcoming-elX-build, which includes them all One important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build. That is, (A BuildRequires B) and (B Requires C) implies A BuildRequires C . Combined with the fact that runtime requirements are transitive, that is, (A Requires B) and (B Requires C) implies A Requires C , computing build requirements is a recursive operation, which can be many levels deep. Another question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz = 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.) Note also that the dependencies/capabilities for a given package often varies between different rhel versions.","title":"Options for calculating build dependencies"},{"location":"software/koji-mass-rebuilds/#pre-computing-predictive-vs-just-in-time","text":"Two different approaches to determining dependency order for building are: pre compute all dependencies based on an existing series/rhel version, OR compute which remaining packages have all build reqs satisfied now The first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target. The second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets. It may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first.","title":"Pre-computing (predictive) vs just-in-time"},{"location":"software/koji-mass-rebuilds/#package-list-closure-pruning","text":"At some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts). But if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target.","title":"Package list closure, pruning"},{"location":"software/koji-mass-rebuilds/#proposal-recommendations","text":"As mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version.","title":"Proposal / Recommendations"},{"location":"software/koji-mass-rebuilds/#for-a-new-osg-series","text":"update native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages tag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development) build all packages in new packaging area into new build target at once for all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development)","title":"For a new OSG series:"},{"location":"software/koji-mass-rebuilds/#for-a-new-rhel-version","text":"pull the repodata from the relevant *-build repo from koji: for pre-computing, use a build repo from an existing rhel version: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/ for just-in-time, use the new build repo: https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/ the primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.) generate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo. the primary (sqlite) file can be used to get build-requires. use sql to resolve direct dependencies at the package name level: src-pkg: bin-pkg (BuildRequires) bin-pkg: bin-pkg (Requires) bin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies) resolve this list into a full list of recursive build dependencies. Since this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python. build ready-to-be-built packages update our copy of the repodata from the regen'ed *-build repo, as often as new versions become available update our dependency lists repeat until all packages are built","title":"For a new RHEL version:"},{"location":"software/koji-workflow/","text":"Koji Workflow This covers the basics of using and understanding the OSG Koji instance. It is meant primarily for OSG Software team members who need to interact with the service. Terminology Using and understanding the following terminology correctly will help in the reading of this document: Package This refers to a named piece of software in the Koji database. An example would be \"lcmaps\". Build A specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package. Tag A named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-3.3-el6-development\" tag, which contains (among others) the \"lcmaps\" package and the \"lcmaps-1.6.6-1.1.osg33.el6\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include CentOS base, EPEL, or JPackage. Note A tag is NOT a yum repository. Target A target consists of a build tag and a destination tag. An example is \"osg-3.3-el6\", where the build tag is \"osg-3.3-el6-build\" and the destination tag is \"osg-3.3-el6\". A target is used by the build task to know what repository to build from and tag to build into. Task A unit of work for Koji. Several common tasks are: build This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask. Note If the build task is marked as \"scratch\", then it won't result in a saved Build. buildArch This task takes a SRPM, architecture name, and a Koji repository as an input, and runs mock to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful. tagBuild This adds a successful build to a given tag. newRepo This creates a new repository from a given tag. Build artifacts The results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs. Repository A yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag. Using Koji Required Software Using Koji requires: osg-build version 1.6.3 or later. koji 1.6.0-2.osg or later. Note that you want a koji build from osg; the output of rpm -q koji should end in \".osg\". Both pieces of software are available from the osg repositories. osg-build may also be obtained from GitHub by cloning out https://github.com/opensciencegrid/osg-build Special instructions for UW-Madison CSL machines: Clone the osg-build GitHub repo: [you@host]$ git clone https://github.com/opensciencegrid/osg-build Add this directory to your $PATH Run [you@host]$ osg-koji setup to set up the koji configuration and certificates in ~/.osg-koji Obtaining a login You will be using your grid certificate to log in. Email a Koji admin the DN of your certificate, and we will set up a Koji account with the appropriate permissions. If you are switching certificate providers, you will need to email a Koji admin with your new DN. You will also need to clear your browser cookies and cache for https://koji.chtc.wisc.edu before trying to use the Koji web interface again. If your CN has changed, you will not be able to use your old certificate. Current Koji admins are Mat Selmeci and Carl Edquist. Configuring certificate authentication You must also configure certificate authentication for the command-line tools on your build host: Run [you@host]$ osg-koji setup to set up the appropriate configuration and certificates in ~/.osg-koji After this, you will also be able to run koji commands manually by using the osg-koji wrapper script. You might need to rerun osg-koji setup if you renew or change your cert. Creating a new build We create a new build in Koji from the package's directory in OSG Software subversion. If a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You can do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji. To do a build, execute the following command from within the OSG Software subversion checkout: [you@host]$ osg-build koji PACKAGE NAME To do a scratch build, simply add the --scratch command line flag. Each invocation of osg-build will ask for the password once or twice; if you get asked more like 20 times, then you may not be running the OSG-patched version of Koji; try switching to the one from the osg-development repository. When you do a non-scratch build, it will build with the osg-el6 and osg-el7 targets. This will assign your build the osg-3.4-el6-development and osg-3.4-el7-development tags (and your package will be assigned the osg-el6 and osg-el7 tags). If successful, your build will end up in the Koji osg-minefield yum repos and will eventually show up in the osg-development yum repos. This is a high latency process. Build task Results How to find build results The most recent build results are always shown on the home page of Koji: https://koji.chtc.wisc.edu/koji/index Clicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM. If your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a wildcard), and it will bring up a list of all builds for that package. You can find your build from there. For example, the \"lcmaps\" package page is here: https://koji.chtc.wisc.edu/koji/packageinfo?packageID=56 And the lcmaps-1.6.6-1.1.osg33.el6 build is here: https://koji.chtc.wisc.edu/koji/buildinfo?buildID=7427 Trying our your build Because it takes a while for your build to get into one of the regular repositories, it's simplest to download your RPM directly (see the previous section on How to find build results), and install it with: [root@host]# yum localinstall RPM How to get the resulting RPM into a repository Once a package has been built, it is added to a tag. We then must turn the tag into a yum repository. This is normally done automatically and you do not need to deal with it yourself. Three notes: The kojira daemon creates a repository automatically post-build on the koji-hub host. Eventually, the development repository will be the one hosted by koji-hub. The koji-hub repository can be created manually by running [you@host]$ osg-koji regen-repo TAG NAME For example, the tag name for osg-development in 3.4 on el6 is \"osg-3.4-el6-development\". Likely, you won't need to do this when kojira is working. - Repositories are created on external hosts with the mash tool. These are usually triggered by cron jobs, but may be run by hand too. Documentation for running mash is on the TODO list. - You can create your own personal repository using mash . Debugging build issues Failed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include: root.log This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here. build.log The output of the rpmbuild executable. If your package fails to compile, the reason will show up here. One input to the buildArch task is a repository, which is based on a Koji tag. If the repository hasn't been recreated for a dependency you need (for example, when kojira isn't working), you may not have the right RPMs available in your build root. One common issue is building a chain of dependencies. For example, suppose build B depends on the results of build A. If you build A then build B immediately, B will likely fail. This is because A is not in the repository that B uses. The proper string of events building A, starting the regeneration of the destination and build repo (which should happen in a few minutes of the build A task completing), then submitting build task B. Note if you submit build task B while the build repository task is open, it will not start until the build task has finished. Other errors package PACKAGE NAME not in list for tag TAG This happens when the name of the directory your package is in does not match the name of the package. You must rename one or the other and commit your changes before trying again. Promoting Builds from Development - Testing Software contributors can promote any package to testing. Members of the security team can promote ca-cert packages to testing. To promote from development to testing: Using osg-promote If you want to promote the latest version: [you@host]$ osg-promote -r OSGVER -testing PACKAGE NAME PACKAGE NAME is the bare package name without version, e.g. gratia-probe . If you want to promote a specific version: [you@host]$ osg-promote -r OSGVER -testing BUILD NAME BUILD NAME is a full name-version-revision.disttag such as gratia-probe-1.17.0-2.osg33.el6 . OSGVER is the OSG major version that you are promoting for (e.g. 3.4 ). osg-promote will promote both the el6 and el7 builds of a package. After promoting, copy and paste the JIRA code osg-promote produces into the JIRA ticket that you are working on. For osg-promote , you may omit the .osg34.el6 or .osg34.el7 ; the script will add the appropriate disttag on. See OSG Building Tools for full details on osg-promote . Creating custom koji areas Occasionally you may want to make builds of a package (or packages) which you do not yet want to go into the main development repos. In this case, you can create a set of custom koji tags and build targets for these builds. We have a script in our osg-next-tools repo called new-koji-area that facilitates this set up. Further reading Official Koji documentation: https://docs.pagure.org/koji/ Fedora's koji documentation: https://fedoraproject.org/wiki/Koji Fedora's \"Using Koji\" page: https://fedoraproject.org/wiki/Using_the_Koji_build_system Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.","title":"Koji Workflow"},{"location":"software/koji-workflow/#koji-workflow","text":"This covers the basics of using and understanding the OSG Koji instance. It is meant primarily for OSG Software team members who need to interact with the service.","title":"Koji Workflow"},{"location":"software/koji-workflow/#terminology","text":"Using and understanding the following terminology correctly will help in the reading of this document: Package This refers to a named piece of software in the Koji database. An example would be \"lcmaps\". Build A specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package. Tag A named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-3.3-el6-development\" tag, which contains (among others) the \"lcmaps\" package and the \"lcmaps-1.6.6-1.1.osg33.el6\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include CentOS base, EPEL, or JPackage. Note A tag is NOT a yum repository. Target A target consists of a build tag and a destination tag. An example is \"osg-3.3-el6\", where the build tag is \"osg-3.3-el6-build\" and the destination tag is \"osg-3.3-el6\". A target is used by the build task to know what repository to build from and tag to build into. Task A unit of work for Koji. Several common tasks are: build This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask. Note If the build task is marked as \"scratch\", then it won't result in a saved Build. buildArch This task takes a SRPM, architecture name, and a Koji repository as an input, and runs mock to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful. tagBuild This adds a successful build to a given tag. newRepo This creates a new repository from a given tag. Build artifacts The results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs. Repository A yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag.","title":"Terminology"},{"location":"software/koji-workflow/#using-koji","text":"","title":"Using Koji"},{"location":"software/koji-workflow/#required-software","text":"Using Koji requires: osg-build version 1.6.3 or later. koji 1.6.0-2.osg or later. Note that you want a koji build from osg; the output of rpm -q koji should end in \".osg\". Both pieces of software are available from the osg repositories. osg-build may also be obtained from GitHub by cloning out https://github.com/opensciencegrid/osg-build","title":"Required Software"},{"location":"software/koji-workflow/#special-instructions-for-uw-madison-csl-machines","text":"Clone the osg-build GitHub repo: [you@host]$ git clone https://github.com/opensciencegrid/osg-build Add this directory to your $PATH Run [you@host]$ osg-koji setup to set up the koji configuration and certificates in ~/.osg-koji","title":"Special instructions for UW-Madison CSL machines:"},{"location":"software/koji-workflow/#obtaining-a-login","text":"You will be using your grid certificate to log in. Email a Koji admin the DN of your certificate, and we will set up a Koji account with the appropriate permissions. If you are switching certificate providers, you will need to email a Koji admin with your new DN. You will also need to clear your browser cookies and cache for https://koji.chtc.wisc.edu before trying to use the Koji web interface again. If your CN has changed, you will not be able to use your old certificate. Current Koji admins are Mat Selmeci and Carl Edquist.","title":"Obtaining a login"},{"location":"software/koji-workflow/#configuring-certificate-authentication","text":"You must also configure certificate authentication for the command-line tools on your build host: Run [you@host]$ osg-koji setup to set up the appropriate configuration and certificates in ~/.osg-koji After this, you will also be able to run koji commands manually by using the osg-koji wrapper script. You might need to rerun osg-koji setup if you renew or change your cert.","title":"Configuring certificate authentication"},{"location":"software/koji-workflow/#creating-a-new-build","text":"We create a new build in Koji from the package's directory in OSG Software subversion. If a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You can do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji. To do a build, execute the following command from within the OSG Software subversion checkout: [you@host]$ osg-build koji PACKAGE NAME To do a scratch build, simply add the --scratch command line flag. Each invocation of osg-build will ask for the password once or twice; if you get asked more like 20 times, then you may not be running the OSG-patched version of Koji; try switching to the one from the osg-development repository. When you do a non-scratch build, it will build with the osg-el6 and osg-el7 targets. This will assign your build the osg-3.4-el6-development and osg-3.4-el7-development tags (and your package will be assigned the osg-el6 and osg-el7 tags). If successful, your build will end up in the Koji osg-minefield yum repos and will eventually show up in the osg-development yum repos. This is a high latency process.","title":"Creating a new build"},{"location":"software/koji-workflow/#build-task-results","text":"","title":"Build task Results"},{"location":"software/koji-workflow/#how-to-find-build-results","text":"The most recent build results are always shown on the home page of Koji: https://koji.chtc.wisc.edu/koji/index Clicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM. If your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a wildcard), and it will bring up a list of all builds for that package. You can find your build from there. For example, the \"lcmaps\" package page is here: https://koji.chtc.wisc.edu/koji/packageinfo?packageID=56 And the lcmaps-1.6.6-1.1.osg33.el6 build is here: https://koji.chtc.wisc.edu/koji/buildinfo?buildID=7427","title":"How to find build results"},{"location":"software/koji-workflow/#trying-our-your-build","text":"Because it takes a while for your build to get into one of the regular repositories, it's simplest to download your RPM directly (see the previous section on How to find build results), and install it with: [root@host]# yum localinstall RPM","title":"Trying our your build"},{"location":"software/koji-workflow/#how-to-get-the-resulting-rpm-into-a-repository","text":"Once a package has been built, it is added to a tag. We then must turn the tag into a yum repository. This is normally done automatically and you do not need to deal with it yourself. Three notes: The kojira daemon creates a repository automatically post-build on the koji-hub host. Eventually, the development repository will be the one hosted by koji-hub. The koji-hub repository can be created manually by running [you@host]$ osg-koji regen-repo TAG NAME For example, the tag name for osg-development in 3.4 on el6 is \"osg-3.4-el6-development\". Likely, you won't need to do this when kojira is working. - Repositories are created on external hosts with the mash tool. These are usually triggered by cron jobs, but may be run by hand too. Documentation for running mash is on the TODO list. - You can create your own personal repository using mash .","title":"How to get the resulting RPM into a repository"},{"location":"software/koji-workflow/#debugging-build-issues","text":"Failed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include: root.log This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here. build.log The output of the rpmbuild executable. If your package fails to compile, the reason will show up here. One input to the buildArch task is a repository, which is based on a Koji tag. If the repository hasn't been recreated for a dependency you need (for example, when kojira isn't working), you may not have the right RPMs available in your build root. One common issue is building a chain of dependencies. For example, suppose build B depends on the results of build A. If you build A then build B immediately, B will likely fail. This is because A is not in the repository that B uses. The proper string of events building A, starting the regeneration of the destination and build repo (which should happen in a few minutes of the build A task completing), then submitting build task B. Note if you submit build task B while the build repository task is open, it will not start until the build task has finished. Other errors package PACKAGE NAME not in list for tag TAG This happens when the name of the directory your package is in does not match the name of the package. You must rename one or the other and commit your changes before trying again.","title":"Debugging build issues"},{"location":"software/koji-workflow/#promoting-builds-from-development-testing","text":"Software contributors can promote any package to testing. Members of the security team can promote ca-cert packages to testing. To promote from development to testing:","title":"Promoting Builds from Development -&gt; Testing"},{"location":"software/koji-workflow/#using-osg-promote","text":"If you want to promote the latest version: [you@host]$ osg-promote -r OSGVER -testing PACKAGE NAME PACKAGE NAME is the bare package name without version, e.g. gratia-probe . If you want to promote a specific version: [you@host]$ osg-promote -r OSGVER -testing BUILD NAME BUILD NAME is a full name-version-revision.disttag such as gratia-probe-1.17.0-2.osg33.el6 . OSGVER is the OSG major version that you are promoting for (e.g. 3.4 ). osg-promote will promote both the el6 and el7 builds of a package. After promoting, copy and paste the JIRA code osg-promote produces into the JIRA ticket that you are working on. For osg-promote , you may omit the .osg34.el6 or .osg34.el7 ; the script will add the appropriate disttag on. See OSG Building Tools for full details on osg-promote .","title":"Using osg-promote"},{"location":"software/koji-workflow/#creating-custom-koji-areas","text":"Occasionally you may want to make builds of a package (or packages) which you do not yet want to go into the main development repos. In this case, you can create a set of custom koji tags and build targets for these builds. We have a script in our osg-next-tools repo called new-koji-area that facilitates this set up.","title":"Creating custom koji areas"},{"location":"software/koji-workflow/#further-reading","text":"Official Koji documentation: https://docs.pagure.org/koji/ Fedora's koji documentation: https://fedoraproject.org/wiki/Koji Fedora's \"Using Koji\" page: https://fedoraproject.org/wiki/Using_the_Koji_build_system Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.","title":"Further reading"},{"location":"software/new-team-member/","text":"Setup Instructions for New Team Members Computing account at FNAL To get this, follow the instructions at https://fermi.service-now.com/kb_view.do?sysparm_article=KB0010797 ssh access to a UW CompSci account, including AFS access Send email to Tim C with top 3 requested usernames Read/write access to the UW Subversion repository; Send email to Mat or Tim C User certificate Obtain a user certificate here: https://oim.opensciencegrid.org/oim/certificate Import the certificate into your browser of choice Access to FermiCloud http://fclweb.fnal.gov/ Register for a GGUS account with the following information: Your certificate's subject DN Select none from the \"Virtual Organization\" drop-down Select yes for \"Do you want to have support access?\" and answer \"Why?\" with the following: Yes, I need to comment on tickets as a member of the OSG Software Release Team (https://www.opensciencegrid.org/technology/#the-team) Jira ticket system Send email to and request access to JIRA Access to Koji Follow the instructions on the Koji user management page Sign up for mailing lists osg-software@opensciencegrid.org osg-general@opensciencegrid.org osg-sites@opensciencegrid.org osg-commits@cs.wisc.edu GitHub team membership https://github.com/orgs/opensciencegrid/teams/software-and-release/members UNL repository access Send SSH public key to Tim T, Derek, or Brian L to gain access to the UNL repository (osgcollab@hcc-osg-software.unl.edu) If 50% S R, add them to the triage schedule","title":"New Team Member"},{"location":"software/new-team-member/#setup-instructions-for-new-team-members","text":"Computing account at FNAL To get this, follow the instructions at https://fermi.service-now.com/kb_view.do?sysparm_article=KB0010797 ssh access to a UW CompSci account, including AFS access Send email to Tim C with top 3 requested usernames Read/write access to the UW Subversion repository; Send email to Mat or Tim C User certificate Obtain a user certificate here: https://oim.opensciencegrid.org/oim/certificate Import the certificate into your browser of choice Access to FermiCloud http://fclweb.fnal.gov/ Register for a GGUS account with the following information: Your certificate's subject DN Select none from the \"Virtual Organization\" drop-down Select yes for \"Do you want to have support access?\" and answer \"Why?\" with the following: Yes, I need to comment on tickets as a member of the OSG Software Release Team (https://www.opensciencegrid.org/technology/#the-team) Jira ticket system Send email to and request access to JIRA Access to Koji Follow the instructions on the Koji user management page Sign up for mailing lists osg-software@opensciencegrid.org osg-general@opensciencegrid.org osg-sites@opensciencegrid.org osg-commits@cs.wisc.edu GitHub team membership https://github.com/orgs/opensciencegrid/teams/software-and-release/members UNL repository access Send SSH public key to Tim T, Derek, or Brian L to gain access to the UNL repository (osgcollab@hcc-osg-software.unl.edu) If 50% S R, add them to the triage schedule","title":"Setup Instructions for New Team Members"},{"location":"software/osg-build-tools/","text":"OSG Build Tools This page documents the tools used for RPM development for the OSG Software Stack. See the RPM development guide for the principles on which these tools are based. The tools are distributed in the osg-build RPM in our repositories, but can also be used from a Git clone of opensciencegrid/osg-build on GitHub . This page is up-to-date as of osg-build version 1.10.1. The tools osg-build Overview This is the primary tool used in building source and binary RPMs. osg-build TASK [options] PACKAGE DIRECTORY [...] package_directory is a directory containing an osg/ and/or an upstream/ subdirectory. See the RPM development guide for how these directories are organized. Tasks koji Prebuilds the final source package, then builds it remotely using the Koji instance hosted at UW-Madison. https://koji.chtc.wisc.edu By default, the resulting RPMs will end up in the osg-minefield repositories based on the most recent OSG major version (e.g. 3.4). You may specify a different set of repos with --repo , described later. RPMs from the osg-minefield repositories are regularly pulled to the osg-development repositories hosted by the GOC at http://repo.opensciencegrid.org Unless you specify otherwise (by passing --el6 , --el7 or specifying a different koji tag/target), the package will be built for both el6 and el7. This is the method used to build final versions of packages you expect to ship. lint Prebuilds the final source package, then runs rpmlint on it to check for various problems. You will need to have rpmlint installed. People on UW CSL machines should add /p/vdt/workspace/rpmlint to their $PATH. mock Prebuilds the final source package, then builds it locally using mock , and stores the resulting source and binary RPMs in the package-specific _build_results directory. prebuild Prebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the _upstream_srpm_contents and _final_srpm_contents directories. prepare Prebuilds the final source package, then calls rpmbuild -bp on the result, extracting and patching the source files (and performing any other steps defined in the %prep section of the spec file. The resulting sources will be under _final_srpm_contents . rpmbuild Prebuilds the final source package, then builds it locally using rpmbuild , and stores the resulting source and binary RPMs in the package-specific _build_results directory. quilt Collects the upstream local sources and spec file, then calls quilt setup on the spec file, extracting the source files and adding the patches to a quilt series file. See Quilt documentation (PDF link) for more information on quilt; also look at the example in the Usage Patterns section below. Similar to prepare (in fact, quilt calls rpmbuild -bp behind the scenes), but the source tree is in pre-patch state, and various quilt commands can be used to apply and modify patches. Unpacks into _quilt as of osg-build-1.2.2 or _final_srpm_contents in previous versions. Requires quilt . People on UW CSL machines should add /p/vdt/workspace/quilt/bin to their $PATH , and /p/vdt/workspace/quilt/share/man to their $MANPATH . Options This section lists the command-line options. --help Prints the built-in usage information and exits without doing anything else. --version Prints the version of osg-build and exits without doing anything else. Common Options -a, --autoclean, --no-autoclean Before each build, clean out the contents of the underscore directories (_build_results, _final_srpm_contents, _upstream_srpm_contents, _upstream_tarball_contents). If the directories are not cleaned up, earlier builds of a package may interfere with later ones. --no-autoclean will disable this. Default is true . Has no effect with the --vcs flag. -c, --cache-prefix prefix Sets the prefix for upstream source cache references. The prefix must be a valid URI starting with either http , https , or file , or one of the following special values: AFS (corresponds to file:///p/vdt/public/html/upstream , which is the location of the VDT cache using AFS from a UW CS machine). VDT (corresponds to http://vdt.cs.wisc.edu/upstream , which is the location of the VDT cache from off-site). AUTO (AFS if available, VDT if not) The upstream source cache must be organized as described above. All files referenced by .source files in the affected packages must exist in the cache, or a runtime error will occur. Default is AUTO . Has no effect with the --vcs flag. --el6, --el7, --redhat-release version (Config: redhat_release) Sets the distro version to build for. This affects the %dist tag, the mock config, and the default koji tag and target (unless otherwise specified). --el6 is equivalent to --redhat-release 6 --el7 is equivalent to --redhat-release 7 --loglevel loglevel Sets the verbosity of the script. Valid values are: debug , info , warning , error and critical . Default is info . -q, --quiet Do not display as much information. Equivalent to --loglevel warning -v, --verbose Display more information. Equivalent to --loglevel debug -w, --working-directory path Use path as the root directory of the files created by the script. For example, if path is $HOME/working , and the package being built is ndt , the following tree will be created: $HOME/working/ndt/_upstream_srpm_contents $HOME/working/ndt/_upstream_tarball_contents $HOME/working/ndt/_final_srpm_contents $HOME/working/ndt/_build_results If path is TEMP , a randomly named directory under /tmp is used as the working directory. The default setting is to use the package directory as the working directory. Has no effect with the --vcs flag. Options specific to prebuild task --full-extract If set, all upstream tarballs will be extracted into _upstream_tarball_contents/ during the prebuild step. This flag is now mostly redundant with the prepare and quilt tasks. Options specific to rpmbuild and mock tasks --distro-tag dist Sets the distribution tag added on to the end of the release in the RPM ( rpmbuild and mock tasks only ). Default is .osg.el6 or .osg.el7 -t, --target-arch arch Specify an architecture to build packages for ( rpmbuild and mock tasks only ). Default is unspecified, which builds for the current machine architecture. Options specific to mock task --mock-clean, --no-mock-clean Enable/disable deletion of the mock buildroot after a successful build. Default is true . -m, --mock-config path Specifies the mock configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag %dist ). See also --mock-config-from-koji . --mock-config-from-koji build tag Creates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is: osg-3.4-el6-build Also requires the Koji command-line tools (package koji ), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely: --koji-backend , --koji-login , and --koji-wrapper . Options specific to koji task --dry-run Do not actually run koji, merely show the command(s) that will be run. For debugging purposes. --getfiles, --get-files For scratch builds without --vcs only. Download the resulting RPMs and logs from the build into the _build_results directory. -k, --kojilogin, --koji-login login Sets the login to use for the koji task. This should most likely be your CN. If not specified, will extract it from your client cert ( ~/.osg-koji/client.crt or ~/.koji/client.crt ). --koji-target target The koji target to use for building. Default is osg-el6 for el6 and osg-el7 for el7. --koji-tag tag The koji tag to add packages to. See the Koji Workflow guide for more information on the terminology. The special value TARGET uses the destination tag defined in the koji target. Default is osg-el6 or osg-el7 . --ktt, --koji-tag-and-target arg Shorthand for setting both --koji-tag and --koji-target to arg . --koji-wrapper, --no-koji-wrapper Enable/disable use of the osg-koji wrapper script around koji. See below for a description of osg-koji . Default is true . --koji-backend backend Specifies the method osg-build will use to interface with Koji. This can be shell or kojilib . --wait, --no-wait, --nowait Wait for koji tasks to finish. Bad for running multiple builds in a single command, since you will have to type in your passphrase for the first one, wait for it to complete, then type in your passphrase for the second one, wait for it to complete, etc. If you want to wait for multiple tasks to finish, use the koji watch-task command or look at the website https://koji.chtc.wisc.edu . --wait used to be the default until osg-build-1.1.3 --regen-repos Start a regen-repo koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with --no-wait , since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete. --scratch, --no-scratch Perform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient. --vcs, --no-vcs, --svn, --no-svn Have Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. Currently, SVN and Git are supported. If this flag is specified, you may use SVN URLs or URL@Revision pairs to specify the packages to build. You may continue specify package directories from an SVN checkout, in which case osg-build will use svn info to find the right URL@Revision pair to use and warn you about uncommitted changes. osg-build will also warn you about an outdated working directory. --vcs defaults to true for non-scratch builds, and false for scratch builds. --repo= destination repository , --upcoming Selects the repositories (osg-3.3, upcoming, etc.) to build packages for. Currently valid repositories are: Repository Description osg OSG Software development repos for trunk (this is the default) osg-3.3 (or just 3.3) OSG Software development repos for 3.2 branch upcoming OSG Software development repos for upcoming branch internal OSG Software internal branch hcc Holland Computing Center (Nebraska) testing repos --upcoming is an alias for --repo=upcoming Note that the repo selection affects which VCS paths you are allowed to build from. For example, you are not allowed to build from branches/osg-3.3 (from the OSG SVN) into the 'osg' repo, or from HCC's git repositories into the 'upcoming' repo. koji-tag-diff This script displays the differences between the latest packages in two koji tags. Example invocation: koji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing This prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa. osg-build-test This script runs automated tests for osg-build . Only a few tests have been implemented so far. osg-import-srpm This is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an upstream/*.source file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass --upstream-root= UPSTREAM DIR to put them somewhere else. If called with the --extract-spec or -e argument, it will extract the spec file from the SRPM and place it into the osg subdir in SVN. If called with the --diff-spec or -d argument, it will extract the spec file and compare it to the existing spec file in the osg subdir. The script hasn't been touched in a while and needs a good deal of cleanup. A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications. osg-koji This is a wrapper script around the koji command line tool. It automatically specifies parameters to access the OSG's koji instance, and forces SSL authentication. It takes the same parameters as koji and passes them on. An additional command, osg-koji setup exists, which performs the following tasks: Create a koji configuration in ~/.osg-koji Create a CA bundle for verifying the server. Use either files in /etc/grid-security/certificates , or (if those are not found), from files downloaded from the DOEGrids and DigiCert sites. Create a client cert file. This can be a symlink to your grid proxy, or it can be a file created from your grid public and private key files. The location of those files can be specified by the --usercert and --userkey arguments. If unspecified, usercert defaults to ~/.globus/usercert.pem , and userkey defaults to ~/.globus/userkey.pem . osg-promote Overview Run this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines. Once the packages are promoted, the script will generate code to cut and paste into a JIRA comment. Synopsis osg-promote [-r|--route ROUTE ]... [options] PACKAGE OR BUILD [...] Examples Promote the latest build of osg-version to testing for the current release series osg-promote -r testing osg-version Promote the latest builds of osg-ce to testing for the 3.3 and 3.4 release series osg-promote -r 3.3-testing -r 3.4-testing osg-ce Promote osg-build-1.5.0-1 to testing for the current release series osg-promote -r testing osg-build-1.5.0-1 Arguments -h Display help and a list of valid routes. package or build A package (e.g. osg-version ) or build (e.g. osg-version-3.3.0-1.osg33.el6 ) to promote. You may omit the dist tag (the .osg33.el6 part). If a package is specified, the most recent version of that package will be promoted. If a build is specified, that build and the build that has the same version - release for the other distro version(s) will be promoted. That is, if you specify the route 3.3-testing and the build foo-1-1 , then foo-1-1.osg33.el6 and foo-1-1.osg33.el7 will be promoted. This may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package must match. -r ROUTE , --route ROUTE The promotion route to use. Use osg-promote -h to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass -r 3.4-testing -r 3.3-testing . If not specified, the testing route is used, which corresponds to the testing route for the latest release series. -n, --dry-run Do not promote, just show what would be done. --el6-only / --el7-only Only promote packages for el6 / el7. --no-el6 / --no-el7 Do not promote packages for el6 / el7. --ignore-rejects Ignore rejections due to version mismatch between dvers or missing package for one dver. --regen Regenerate the destination repos after promoting. -y, --assume-yes Do not prompt before promotion. Common Usage Patterns Verify that all files necessary to build the package are in the right place Run osg-build prebuild PACKAGEDIR . Fetch and extract all source files for examination Run osg-build prebuild --full-extract PACKAGEDIR . Look inside the _upstream_tarball_contents directory. Get a post-patch version of the upstream sources for examination Run osg-build prepare PACKAGEDIR . Look inside the _build_results directory. See which patches work with a new version of a package, update or remove them Place the new source tarball into the upstream cache, edit the version in the spec file and *.sources files as necessary Run osg-build quilt PACKAGEDIR . Enter the extracted sources inside the _final_srpm_contents directory. You should see a file called series and a symlink called patches . Type quilt series to get a list of patches in order of application. Type quilt push to apply the next patch. If the patch applies cleanly, continue. If the patch applies with some fuzz, type quilt refresh to update the offsets in the patch. If the patch does not apply and you wish to remove it, type quilt delete PATCH NAME (delete only removes it from the series file, not the disk) If the patch does not apply and you wish to fix it, either type quilt push -f to interactively apply the patch, or quilt delete PATCH NAME the patch and use quilt new / quilt edit / quilt refresh to edit files and make a new patch from your changes. Consult the quilt(1) manpage for more info. If you have a new patch, run quilt import PATCHFILE to add the patch to the series file, and run quilt push to apply it. If you have changes to make to the source code that you want to save as a patch, type quilt new PATCHNAME , edit the files, type quilt add FILE on each file you edited, then type quilt refresh to recreate the patch. Once you're all done, copy the patches in the patches/ directory to the osg/ dir in SVN, run quilt series to get the application order and update the spec file accordingly. See if a package builds successfully for OSG 3.4 If you have all the build dependencies of the package installed, run osg-build rpmbuild PACKAGEDIR . The resulting RPMs will be in the _build_results directory. If you do not have all the build dependencies installed, or want to make sure you specified all of the necessary ones and the package builds from a clean environment, run osg-build mock --mock-config-from-koji osg-3.4-el6-build PACKAGEDIR . The resulting RPMs will be in the _build_results directory. If you do not have mock installed, or want to exactly replicate the build environment in Koji, run osg-build koji --scratch PACKAGEDIR . You may download the resulting RPMs from kojiweb https://koji.chtc.wisc.edu/koji or pass --getfiles to osg-build koji and they will get downloaded to the _build_results directory. Check for potential errors in a package Run osg-build lint PACKAGEDIR . Create and test a final build of a package for all platforms for upcoming svn commit your changes in branches/upcoming . Type osg-build koji --repo=upcoming PACKAGEDIR Wait for the osg-upcoming-minefield repos to be regenerated containing the new version of your package. You can run osg-koji wait-repo osg-upcoming-el X -development --build= PACKAGENAME-VERSION-RELEASE and wait for that process to finish (substitute 6 or 7 for X ). Or, you can just check kojiweb https://koji.chtc.wisc.edu/koji/tasks . On your test machine, make sure the osg-upcoming-minefield repo is enabled (edit /etc/yum.repos.d/osg-upcoming-minefield.repo or /etc/yum.repos.d/osg-el6-upcoming-minefield.repo ). Clean your cache ( yum clean all; yum clean expire-cache ). Install your software, see if it works. Promote the latest build of a package to testing for the current OSG release series Run osg-promote -r testing PACKAGE Promote the latest build of a package to testing for the 3.3 and 3.4 release series Run osg-promote -r 3.3-testing -r 3.4-testing PACKAGE","title":"OSG Build Tools"},{"location":"software/osg-build-tools/#osg-build-tools","text":"This page documents the tools used for RPM development for the OSG Software Stack. See the RPM development guide for the principles on which these tools are based. The tools are distributed in the osg-build RPM in our repositories, but can also be used from a Git clone of opensciencegrid/osg-build on GitHub . This page is up-to-date as of osg-build version 1.10.1.","title":"OSG Build Tools"},{"location":"software/osg-build-tools/#the-tools","text":"","title":"The tools"},{"location":"software/osg-build-tools/#osg-build","text":"","title":"osg-build"},{"location":"software/osg-build-tools/#overview","text":"This is the primary tool used in building source and binary RPMs. osg-build TASK [options] PACKAGE DIRECTORY [...] package_directory is a directory containing an osg/ and/or an upstream/ subdirectory. See the RPM development guide for how these directories are organized.","title":"Overview"},{"location":"software/osg-build-tools/#tasks","text":"","title":"Tasks"},{"location":"software/osg-build-tools/#koji","text":"Prebuilds the final source package, then builds it remotely using the Koji instance hosted at UW-Madison. https://koji.chtc.wisc.edu By default, the resulting RPMs will end up in the osg-minefield repositories based on the most recent OSG major version (e.g. 3.4). You may specify a different set of repos with --repo , described later. RPMs from the osg-minefield repositories are regularly pulled to the osg-development repositories hosted by the GOC at http://repo.opensciencegrid.org Unless you specify otherwise (by passing --el6 , --el7 or specifying a different koji tag/target), the package will be built for both el6 and el7. This is the method used to build final versions of packages you expect to ship.","title":"koji"},{"location":"software/osg-build-tools/#lint","text":"Prebuilds the final source package, then runs rpmlint on it to check for various problems. You will need to have rpmlint installed. People on UW CSL machines should add /p/vdt/workspace/rpmlint to their $PATH.","title":"lint"},{"location":"software/osg-build-tools/#mock","text":"Prebuilds the final source package, then builds it locally using mock , and stores the resulting source and binary RPMs in the package-specific _build_results directory.","title":"mock"},{"location":"software/osg-build-tools/#prebuild","text":"Prebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the _upstream_srpm_contents and _final_srpm_contents directories.","title":"prebuild"},{"location":"software/osg-build-tools/#prepare","text":"Prebuilds the final source package, then calls rpmbuild -bp on the result, extracting and patching the source files (and performing any other steps defined in the %prep section of the spec file. The resulting sources will be under _final_srpm_contents .","title":"prepare"},{"location":"software/osg-build-tools/#rpmbuild","text":"Prebuilds the final source package, then builds it locally using rpmbuild , and stores the resulting source and binary RPMs in the package-specific _build_results directory.","title":"rpmbuild"},{"location":"software/osg-build-tools/#quilt","text":"Collects the upstream local sources and spec file, then calls quilt setup on the spec file, extracting the source files and adding the patches to a quilt series file. See Quilt documentation (PDF link) for more information on quilt; also look at the example in the Usage Patterns section below. Similar to prepare (in fact, quilt calls rpmbuild -bp behind the scenes), but the source tree is in pre-patch state, and various quilt commands can be used to apply and modify patches. Unpacks into _quilt as of osg-build-1.2.2 or _final_srpm_contents in previous versions. Requires quilt . People on UW CSL machines should add /p/vdt/workspace/quilt/bin to their $PATH , and /p/vdt/workspace/quilt/share/man to their $MANPATH .","title":"quilt"},{"location":"software/osg-build-tools/#options","text":"This section lists the command-line options.","title":"Options"},{"location":"software/osg-build-tools/#-help","text":"Prints the built-in usage information and exits without doing anything else.","title":"--help"},{"location":"software/osg-build-tools/#-version","text":"Prints the version of osg-build and exits without doing anything else.","title":"--version"},{"location":"software/osg-build-tools/#common-options","text":"","title":"Common Options"},{"location":"software/osg-build-tools/#-a-autoclean-no-autoclean","text":"Before each build, clean out the contents of the underscore directories (_build_results, _final_srpm_contents, _upstream_srpm_contents, _upstream_tarball_contents). If the directories are not cleaned up, earlier builds of a package may interfere with later ones. --no-autoclean will disable this. Default is true . Has no effect with the --vcs flag.","title":"-a, --autoclean, --no-autoclean"},{"location":"software/osg-build-tools/#-c-cache-prefix-prefix","text":"Sets the prefix for upstream source cache references. The prefix must be a valid URI starting with either http , https , or file , or one of the following special values: AFS (corresponds to file:///p/vdt/public/html/upstream , which is the location of the VDT cache using AFS from a UW CS machine). VDT (corresponds to http://vdt.cs.wisc.edu/upstream , which is the location of the VDT cache from off-site). AUTO (AFS if available, VDT if not) The upstream source cache must be organized as described above. All files referenced by .source files in the affected packages must exist in the cache, or a runtime error will occur. Default is AUTO . Has no effect with the --vcs flag.","title":"-c, --cache-prefix prefix"},{"location":"software/osg-build-tools/#-el6-el7-redhat-release-version-config-redhat95release","text":"Sets the distro version to build for. This affects the %dist tag, the mock config, and the default koji tag and target (unless otherwise specified). --el6 is equivalent to --redhat-release 6 --el7 is equivalent to --redhat-release 7","title":"--el6, --el7, --redhat-release version (Config: redhat_release)"},{"location":"software/osg-build-tools/#-loglevel-loglevel","text":"Sets the verbosity of the script. Valid values are: debug , info , warning , error and critical . Default is info .","title":"--loglevel loglevel"},{"location":"software/osg-build-tools/#-q-quiet","text":"Do not display as much information. Equivalent to --loglevel warning","title":"-q, --quiet"},{"location":"software/osg-build-tools/#-v-verbose","text":"Display more information. Equivalent to --loglevel debug","title":"-v, --verbose"},{"location":"software/osg-build-tools/#-w-working-directory-path","text":"Use path as the root directory of the files created by the script. For example, if path is $HOME/working , and the package being built is ndt , the following tree will be created: $HOME/working/ndt/_upstream_srpm_contents $HOME/working/ndt/_upstream_tarball_contents $HOME/working/ndt/_final_srpm_contents $HOME/working/ndt/_build_results If path is TEMP , a randomly named directory under /tmp is used as the working directory. The default setting is to use the package directory as the working directory. Has no effect with the --vcs flag.","title":"-w, --working-directory path"},{"location":"software/osg-build-tools/#options-specific-to-prebuild-task","text":"","title":"Options specific to prebuild task"},{"location":"software/osg-build-tools/#-full-extract","text":"If set, all upstream tarballs will be extracted into _upstream_tarball_contents/ during the prebuild step. This flag is now mostly redundant with the prepare and quilt tasks.","title":"--full-extract"},{"location":"software/osg-build-tools/#options-specific-to-rpmbuild-and-mock-tasks","text":"","title":"Options specific to rpmbuild and mock tasks"},{"location":"software/osg-build-tools/#-distro-tag-dist","text":"Sets the distribution tag added on to the end of the release in the RPM ( rpmbuild and mock tasks only ). Default is .osg.el6 or .osg.el7","title":"--distro-tag dist"},{"location":"software/osg-build-tools/#-t-target-arch-arch","text":"Specify an architecture to build packages for ( rpmbuild and mock tasks only ). Default is unspecified, which builds for the current machine architecture.","title":"-t, --target-arch arch"},{"location":"software/osg-build-tools/#options-specific-to-mock-task","text":"","title":"Options specific to mock task"},{"location":"software/osg-build-tools/#-mock-clean-no-mock-clean","text":"Enable/disable deletion of the mock buildroot after a successful build. Default is true .","title":"--mock-clean, --no-mock-clean"},{"location":"software/osg-build-tools/#-m-mock-config-path","text":"Specifies the mock configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag %dist ). See also --mock-config-from-koji .","title":"-m, --mock-config path"},{"location":"software/osg-build-tools/#-mock-config-from-koji-build-tag","text":"Creates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is: osg-3.4-el6-build Also requires the Koji command-line tools (package koji ), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely: --koji-backend , --koji-login , and --koji-wrapper .","title":"--mock-config-from-koji build tag"},{"location":"software/osg-build-tools/#options-specific-to-koji-task","text":"","title":"Options specific to koji task"},{"location":"software/osg-build-tools/#-dry-run","text":"Do not actually run koji, merely show the command(s) that will be run. For debugging purposes.","title":"--dry-run"},{"location":"software/osg-build-tools/#-getfiles-get-files","text":"For scratch builds without --vcs only. Download the resulting RPMs and logs from the build into the _build_results directory.","title":"--getfiles, --get-files"},{"location":"software/osg-build-tools/#-k-kojilogin-koji-login-login","text":"Sets the login to use for the koji task. This should most likely be your CN. If not specified, will extract it from your client cert ( ~/.osg-koji/client.crt or ~/.koji/client.crt ).","title":"-k, --kojilogin, --koji-login login"},{"location":"software/osg-build-tools/#-koji-target-target","text":"The koji target to use for building. Default is osg-el6 for el6 and osg-el7 for el7.","title":"--koji-target target"},{"location":"software/osg-build-tools/#-koji-tag-tag","text":"The koji tag to add packages to. See the Koji Workflow guide for more information on the terminology. The special value TARGET uses the destination tag defined in the koji target. Default is osg-el6 or osg-el7 .","title":"--koji-tag tag"},{"location":"software/osg-build-tools/#-ktt-koji-tag-and-target-arg","text":"Shorthand for setting both --koji-tag and --koji-target to arg .","title":"--ktt, --koji-tag-and-target arg"},{"location":"software/osg-build-tools/#-koji-wrapper-no-koji-wrapper","text":"Enable/disable use of the osg-koji wrapper script around koji. See below for a description of osg-koji . Default is true .","title":"--koji-wrapper, --no-koji-wrapper"},{"location":"software/osg-build-tools/#-koji-backend-backend","text":"Specifies the method osg-build will use to interface with Koji. This can be shell or kojilib .","title":"--koji-backend backend"},{"location":"software/osg-build-tools/#-wait-no-wait-nowait","text":"Wait for koji tasks to finish. Bad for running multiple builds in a single command, since you will have to type in your passphrase for the first one, wait for it to complete, then type in your passphrase for the second one, wait for it to complete, etc. If you want to wait for multiple tasks to finish, use the koji watch-task command or look at the website https://koji.chtc.wisc.edu . --wait used to be the default until osg-build-1.1.3","title":"--wait, --no-wait, --nowait"},{"location":"software/osg-build-tools/#-regen-repos","text":"Start a regen-repo koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with --no-wait , since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete.","title":"--regen-repos"},{"location":"software/osg-build-tools/#-scratch-no-scratch","text":"Perform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient.","title":"--scratch, --no-scratch"},{"location":"software/osg-build-tools/#-vcs-no-vcs-svn-no-svn","text":"Have Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. Currently, SVN and Git are supported. If this flag is specified, you may use SVN URLs or URL@Revision pairs to specify the packages to build. You may continue specify package directories from an SVN checkout, in which case osg-build will use svn info to find the right URL@Revision pair to use and warn you about uncommitted changes. osg-build will also warn you about an outdated working directory. --vcs defaults to true for non-scratch builds, and false for scratch builds.","title":"--vcs, --no-vcs, --svn, --no-svn"},{"location":"software/osg-build-tools/#-repodestination-repository-upcoming","text":"Selects the repositories (osg-3.3, upcoming, etc.) to build packages for. Currently valid repositories are: Repository Description osg OSG Software development repos for trunk (this is the default) osg-3.3 (or just 3.3) OSG Software development repos for 3.2 branch upcoming OSG Software development repos for upcoming branch internal OSG Software internal branch hcc Holland Computing Center (Nebraska) testing repos --upcoming is an alias for --repo=upcoming Note that the repo selection affects which VCS paths you are allowed to build from. For example, you are not allowed to build from branches/osg-3.3 (from the OSG SVN) into the 'osg' repo, or from HCC's git repositories into the 'upcoming' repo.","title":"--repo=destination repository, --upcoming"},{"location":"software/osg-build-tools/#koji-tag-diff","text":"This script displays the differences between the latest packages in two koji tags. Example invocation: koji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing This prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa.","title":"koji-tag-diff"},{"location":"software/osg-build-tools/#osg-build-test","text":"This script runs automated tests for osg-build . Only a few tests have been implemented so far.","title":"osg-build-test"},{"location":"software/osg-build-tools/#osg-import-srpm","text":"This is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an upstream/*.source file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass --upstream-root= UPSTREAM DIR to put them somewhere else. If called with the --extract-spec or -e argument, it will extract the spec file from the SRPM and place it into the osg subdir in SVN. If called with the --diff-spec or -d argument, it will extract the spec file and compare it to the existing spec file in the osg subdir. The script hasn't been touched in a while and needs a good deal of cleanup. A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications.","title":"osg-import-srpm"},{"location":"software/osg-build-tools/#osg-koji","text":"This is a wrapper script around the koji command line tool. It automatically specifies parameters to access the OSG's koji instance, and forces SSL authentication. It takes the same parameters as koji and passes them on. An additional command, osg-koji setup exists, which performs the following tasks: Create a koji configuration in ~/.osg-koji Create a CA bundle for verifying the server. Use either files in /etc/grid-security/certificates , or (if those are not found), from files downloaded from the DOEGrids and DigiCert sites. Create a client cert file. This can be a symlink to your grid proxy, or it can be a file created from your grid public and private key files. The location of those files can be specified by the --usercert and --userkey arguments. If unspecified, usercert defaults to ~/.globus/usercert.pem , and userkey defaults to ~/.globus/userkey.pem .","title":"osg-koji"},{"location":"software/osg-build-tools/#osg-promote","text":"","title":"osg-promote"},{"location":"software/osg-build-tools/#overview_1","text":"Run this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines. Once the packages are promoted, the script will generate code to cut and paste into a JIRA comment.","title":"Overview"},{"location":"software/osg-build-tools/#synopsis","text":"osg-promote [-r|--route ROUTE ]... [options] PACKAGE OR BUILD [...]","title":"Synopsis"},{"location":"software/osg-build-tools/#examples","text":"Promote the latest build of osg-version to testing for the current release series osg-promote -r testing osg-version Promote the latest builds of osg-ce to testing for the 3.3 and 3.4 release series osg-promote -r 3.3-testing -r 3.4-testing osg-ce Promote osg-build-1.5.0-1 to testing for the current release series osg-promote -r testing osg-build-1.5.0-1","title":"Examples"},{"location":"software/osg-build-tools/#arguments","text":"","title":"Arguments"},{"location":"software/osg-build-tools/#-h","text":"Display help and a list of valid routes.","title":"-h"},{"location":"software/osg-build-tools/#package-or-build","text":"A package (e.g. osg-version ) or build (e.g. osg-version-3.3.0-1.osg33.el6 ) to promote. You may omit the dist tag (the .osg33.el6 part). If a package is specified, the most recent version of that package will be promoted. If a build is specified, that build and the build that has the same version - release for the other distro version(s) will be promoted. That is, if you specify the route 3.3-testing and the build foo-1-1 , then foo-1-1.osg33.el6 and foo-1-1.osg33.el7 will be promoted. This may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package must match.","title":"package or build"},{"location":"software/osg-build-tools/#-r-route-route-route","text":"The promotion route to use. Use osg-promote -h to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass -r 3.4-testing -r 3.3-testing . If not specified, the testing route is used, which corresponds to the testing route for the latest release series.","title":"-r ROUTE, --route ROUTE"},{"location":"software/osg-build-tools/#-n-dry-run","text":"Do not promote, just show what would be done.","title":"-n, --dry-run"},{"location":"software/osg-build-tools/#-el6-only-el7-only","text":"Only promote packages for el6 / el7.","title":"--el6-only / --el7-only"},{"location":"software/osg-build-tools/#-no-el6-no-el7","text":"Do not promote packages for el6 / el7.","title":"--no-el6 / --no-el7"},{"location":"software/osg-build-tools/#-ignore-rejects","text":"Ignore rejections due to version mismatch between dvers or missing package for one dver.","title":"--ignore-rejects"},{"location":"software/osg-build-tools/#-regen","text":"Regenerate the destination repos after promoting.","title":"--regen"},{"location":"software/osg-build-tools/#-y-assume-yes","text":"Do not prompt before promotion.","title":"-y, --assume-yes"},{"location":"software/osg-build-tools/#common-usage-patterns","text":"","title":"Common Usage Patterns"},{"location":"software/osg-build-tools/#verify-that-all-files-necessary-to-build-the-package-are-in-the-right-place","text":"Run osg-build prebuild PACKAGEDIR .","title":"Verify that all files necessary to build the package are in the right place"},{"location":"software/osg-build-tools/#fetch-and-extract-all-source-files-for-examination","text":"Run osg-build prebuild --full-extract PACKAGEDIR . Look inside the _upstream_tarball_contents directory.","title":"Fetch and extract all source files for examination"},{"location":"software/osg-build-tools/#get-a-post-patch-version-of-the-upstream-sources-for-examination","text":"Run osg-build prepare PACKAGEDIR . Look inside the _build_results directory.","title":"Get a post-patch version of the upstream sources for examination"},{"location":"software/osg-build-tools/#see-which-patches-work-with-a-new-version-of-a-package-update-or-remove-them","text":"Place the new source tarball into the upstream cache, edit the version in the spec file and *.sources files as necessary Run osg-build quilt PACKAGEDIR . Enter the extracted sources inside the _final_srpm_contents directory. You should see a file called series and a symlink called patches . Type quilt series to get a list of patches in order of application. Type quilt push to apply the next patch. If the patch applies cleanly, continue. If the patch applies with some fuzz, type quilt refresh to update the offsets in the patch. If the patch does not apply and you wish to remove it, type quilt delete PATCH NAME (delete only removes it from the series file, not the disk) If the patch does not apply and you wish to fix it, either type quilt push -f to interactively apply the patch, or quilt delete PATCH NAME the patch and use quilt new / quilt edit / quilt refresh to edit files and make a new patch from your changes. Consult the quilt(1) manpage for more info. If you have a new patch, run quilt import PATCHFILE to add the patch to the series file, and run quilt push to apply it. If you have changes to make to the source code that you want to save as a patch, type quilt new PATCHNAME , edit the files, type quilt add FILE on each file you edited, then type quilt refresh to recreate the patch. Once you're all done, copy the patches in the patches/ directory to the osg/ dir in SVN, run quilt series to get the application order and update the spec file accordingly.","title":"See which patches work with a new version of a package, update or remove them"},{"location":"software/osg-build-tools/#see-if-a-package-builds-successfully-for-osg-34","text":"If you have all the build dependencies of the package installed, run osg-build rpmbuild PACKAGEDIR . The resulting RPMs will be in the _build_results directory. If you do not have all the build dependencies installed, or want to make sure you specified all of the necessary ones and the package builds from a clean environment, run osg-build mock --mock-config-from-koji osg-3.4-el6-build PACKAGEDIR . The resulting RPMs will be in the _build_results directory. If you do not have mock installed, or want to exactly replicate the build environment in Koji, run osg-build koji --scratch PACKAGEDIR . You may download the resulting RPMs from kojiweb https://koji.chtc.wisc.edu/koji or pass --getfiles to osg-build koji and they will get downloaded to the _build_results directory.","title":"See if a package builds successfully for OSG 3.4"},{"location":"software/osg-build-tools/#check-for-potential-errors-in-a-package","text":"Run osg-build lint PACKAGEDIR .","title":"Check for potential errors in a package"},{"location":"software/osg-build-tools/#create-and-test-a-final-build-of-a-package-for-all-platforms-for-upcoming","text":"svn commit your changes in branches/upcoming . Type osg-build koji --repo=upcoming PACKAGEDIR Wait for the osg-upcoming-minefield repos to be regenerated containing the new version of your package. You can run osg-koji wait-repo osg-upcoming-el X -development --build= PACKAGENAME-VERSION-RELEASE and wait for that process to finish (substitute 6 or 7 for X ). Or, you can just check kojiweb https://koji.chtc.wisc.edu/koji/tasks . On your test machine, make sure the osg-upcoming-minefield repo is enabled (edit /etc/yum.repos.d/osg-upcoming-minefield.repo or /etc/yum.repos.d/osg-el6-upcoming-minefield.repo ). Clean your cache ( yum clean all; yum clean expire-cache ). Install your software, see if it works.","title":"Create and test a final build of a package for all platforms for upcoming"},{"location":"software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-current-osg-release-series","text":"Run osg-promote -r testing PACKAGE","title":"Promote the latest build of a package to testing for the current OSG release series"},{"location":"software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-33-and-34-release-series","text":"Run osg-promote -r 3.3-testing -r 3.4-testing PACKAGE","title":"Promote the latest build of a package to testing for the 3.3 and 3.4 release series"},{"location":"software/quilt/","text":"How to Write a Patch You create one or more .patch files with diff and stick them in the osg directory. Then you declare the patch files in the header of the spec file with a line like Patch0: py24compat.patch and in the %prep section, just after %setup , you add a %patch line to actually apply the patch, like this: %patch0 -p1 (where the -p1 indicates that it should strip off the first leading component of the path in each file mentioned in the .patch file) Look at the mash package for an example. The easiest way to actually create the patch in the first place is to use a utility called quilt. First you run osg-build quilt on the package directory and it will create a _quilt subdirectory that has the expanded sources and patches. cd into _quilt/pegasus-source-2.3.0 , then run quilt push -a to apply any patches that already exist (there are none for pegasus but there might be for other packages). run quilt new py24compat.patch to name your new patch file. run quilt add filename for each file you want to make changes to (you must run this before making any changes). actually make the changes. run quilt refresh -p1 to have quilt add those changes into the .patch file. (The -p1 option to quilt refresh must be the same as the -p1 option to %patch0 in your spec file). copy patches/py24compat.patch into the pegasus/osg directory and edit the spec file as above. Don't forget to git add your new patch file before committing. Once you've tested your patch successfully, you should make that bug report and send them the patch. A bug report is looked on more favorably if it includes a patch to fix the problem.","title":"Using Quilt"},{"location":"software/quilt/#how-to-write-a-patch","text":"You create one or more .patch files with diff and stick them in the osg directory. Then you declare the patch files in the header of the spec file with a line like Patch0: py24compat.patch and in the %prep section, just after %setup , you add a %patch line to actually apply the patch, like this: %patch0 -p1 (where the -p1 indicates that it should strip off the first leading component of the path in each file mentioned in the .patch file) Look at the mash package for an example. The easiest way to actually create the patch in the first place is to use a utility called quilt. First you run osg-build quilt on the package directory and it will create a _quilt subdirectory that has the expanded sources and patches. cd into _quilt/pegasus-source-2.3.0 , then run quilt push -a to apply any patches that already exist (there are none for pegasus but there might be for other packages). run quilt new py24compat.patch to name your new patch file. run quilt add filename for each file you want to make changes to (you must run this before making any changes). actually make the changes. run quilt refresh -p1 to have quilt add those changes into the .patch file. (The -p1 option to quilt refresh must be the same as the -p1 option to %patch0 in your spec file). copy patches/py24compat.patch into the pegasus/osg directory and edit the spec file as above. Don't forget to git add your new patch file before committing. Once you've tested your patch successfully, you should make that bug report and send them the patch. A bug report is looked on more favorably if it includes a patch to fix the problem.","title":"How to Write a Patch"},{"location":"software/release-planning/","text":"Plans for Future Releases This informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see this page . Unless explicitly noted, bullet points refer to software in the release repo. This page is not meant to track minor bugfixes or updates -- rather, its focus should be new features. OSG 3.4 (May 2017) Package(s) Change in osg-release Notes BeStMan2 Drop Retirement policy edg-mkgridmap Drop SOFTWARE-2600 frontier-squid Modify Version 3 glexec Drop SOFTWARE-2620 GRAM Drop SOFTWARE-2530 GUMS Drop Retirement policy , SOFTWARE-2600 jglobus Drop SOFTWARE-2606 netlogger Drop osg-ce Modify Drop GridFTP , gums-client osg-info-services Drop osg-version Drop singularity Add voms-admin-server Drop Retirement policy Track OSG 3.4 updates through its JIRA epic . Support Policy for OSG 3.3 According to our release support policy and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018. Previous Releases 12 November 2013 OSG 3.1 HTCondor-CE with PBS osg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change) OSG 3.2 Initial release ( how to create ) HDFS 2.0.0 (already done in Upcoming) HTCondor 8.0.4 glideinWMS 3.2.0 osg-info-services (Note: ReSS will likely be retired around year-end) OSG 3.1 updates Upcoming HTCondor 8.1 with unified RPM BOSCO 10 December 2013 OSG 3.2 RSV-for-VOs Squid must be present on OSG-CE (??? what does this mean?)","title":"Release Planning"},{"location":"software/release-planning/#plans-for-future-releases","text":"This informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see this page . Unless explicitly noted, bullet points refer to software in the release repo. This page is not meant to track minor bugfixes or updates -- rather, its focus should be new features.","title":"Plans for Future Releases"},{"location":"software/release-planning/#osg-34-may-2017","text":"Package(s) Change in osg-release Notes BeStMan2 Drop Retirement policy edg-mkgridmap Drop SOFTWARE-2600 frontier-squid Modify Version 3 glexec Drop SOFTWARE-2620 GRAM Drop SOFTWARE-2530 GUMS Drop Retirement policy , SOFTWARE-2600 jglobus Drop SOFTWARE-2606 netlogger Drop osg-ce Modify Drop GridFTP , gums-client osg-info-services Drop osg-version Drop singularity Add voms-admin-server Drop Retirement policy Track OSG 3.4 updates through its JIRA epic .","title":"OSG 3.4 (May 2017)"},{"location":"software/release-planning/#support-policy-for-osg-33","text":"According to our release support policy and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018.","title":"Support Policy for OSG 3.3"},{"location":"software/release-planning/#previous-releases","text":"","title":"Previous Releases"},{"location":"software/release-planning/#12-november-2013","text":"OSG 3.1 HTCondor-CE with PBS osg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change) OSG 3.2 Initial release ( how to create ) HDFS 2.0.0 (already done in Upcoming) HTCondor 8.0.4 glideinWMS 3.2.0 osg-info-services (Note: ReSS will likely be retired around year-end) OSG 3.1 updates Upcoming HTCondor 8.1 with unified RPM BOSCO","title":"12 November 2013"},{"location":"software/release-planning/#10-december-2013","text":"OSG 3.2 RSV-for-VOs Squid must be present on OSG-CE (??? what does this mean?)","title":"10 December 2013"},{"location":"software/repository-management/","text":"Repository Management This document attempts to record everything there is to know about repository management for the OSG. Public repositories We host four public-facing repositories at repo.opensciencegrid.org : development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage. testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison. release : This repository contains software that we are willing to support and can be used by the general community. contrib : RPMs contributed from outside the OSG. These repos are updated by the mash script running on repo.opensciencegrid.org . Internal repositories In addition to the public repositories above, we host two repositories on koji.chtc.wisc.edu . These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them. minefield : This repository is a copy of development above. prerelease : This repository is a staging area for software that is slated to be in the next release. These repos are updated by the kojira daemon running on koji.chtc.wisc.edu . Build repositories The koji task in osg-build uses the osg-3.4-el6-build / osg-3.4-el7-build repo, which is the union of the following repositories: Minefield a.k.a. osg-3.4-el6-development / osg-3.4-el7-development The osg-el6-internal / osg-el7-internal tag (containing build dependencies we do not want to make public) The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist) CentOS and EPEL Koji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies. The tarball creation scripts use the osg-3.4-el6-release-build / osg-3.4-el7-release-build repo, which is the union of the following repositories: Release a.k.a. osg-3.4-el6-release / osg-3.4-el7-release The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist ) CentOS and EPEL","title":"Repository Management"},{"location":"software/repository-management/#repository-management","text":"This document attempts to record everything there is to know about repository management for the OSG.","title":"Repository Management"},{"location":"software/repository-management/#public-repositories","text":"We host four public-facing repositories at repo.opensciencegrid.org : development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage. testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison. release : This repository contains software that we are willing to support and can be used by the general community. contrib : RPMs contributed from outside the OSG. These repos are updated by the mash script running on repo.opensciencegrid.org .","title":"Public repositories"},{"location":"software/repository-management/#internal-repositories","text":"In addition to the public repositories above, we host two repositories on koji.chtc.wisc.edu . These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them. minefield : This repository is a copy of development above. prerelease : This repository is a staging area for software that is slated to be in the next release. These repos are updated by the kojira daemon running on koji.chtc.wisc.edu .","title":"Internal repositories"},{"location":"software/repository-management/#build-repositories","text":"The koji task in osg-build uses the osg-3.4-el6-build / osg-3.4-el7-build repo, which is the union of the following repositories: Minefield a.k.a. osg-3.4-el6-development / osg-3.4-el7-development The osg-el6-internal / osg-el7-internal tag (containing build dependencies we do not want to make public) The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist) CentOS and EPEL Koji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies. The tarball creation scripts use the osg-3.4-el6-release-build / osg-3.4-el7-release-build repo, which is the union of the following repositories: Release a.k.a. osg-3.4-el6-release / osg-3.4-el7-release The dist-el6-build / dist-el7-build tag (consisting of the appropriate macros for %dist ) CentOS and EPEL","title":"Build repositories"},{"location":"software/resurrecting-epel-packages/","text":"Resurrecting EPEL RPMs You will need to be a Koji admin to do these steps. [user@client ~] $ osg-koji --list-permissions --mine Will tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman. EPEL version EPEL Koji tag Our Koji tag 5 dist-5E-epel epelrescue-el5 6 dist-6E-epel epelrescue-el6 7 epel7 epelrescue-el7 Determine the NVR of the build containing the RPM of the package you want. Use the Fedora/EPEL Koji web interface ( https://koji.fedoraproject.org ) to search for it. You can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards. EPEL builds have .el5, .el6, or .el7 in the dist tag. Download all RPMs for all architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms. You have three options for the downloads: Use the links in the web interface Use the koji command-line interface against the Fedora koji: Download fedora-koji.conf , attached to this page Run koji --noauth -c fedora-koji.conf download-build --debuginfo BUILD_NVR Delete RPMs for architectures we do not care about (see list above) Dig around in https://kojipkgs.fedoraproject.org/packages/ On your development machine: Important: Verify that all of the RPMs are signed: [root@client ~] # rpm -K *.rpm | grep -iv gpg should be empty If not, STOP and sign them using the OSG RPM key -- talk to Mat Import the RPMs themselves into the Koji system [user@client ~] $ osg-koji import RPM_DIRECTORY /*.rpm They will not be in any tags at this point Add the package to the whitelist for our koji tag: [user@client ~] $ osg-koji add-pkg OUR_KOJI_TAG PACKAGE --owner = YOUR_KOJI_USERNAME Actually tag the builds: [user@client ~] $ osg-koji tag-pkg OUR_KOJI_TAG BUILD Check the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in. If it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them): for repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do osg-koji regen-repo --nowait $repo done Make a test VM and install the package from minefield to test that it is actually present. Update the epelrescue RPMs table below Removing resurrected RPMs In case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version: Find out the NVR of the build: [user@client ~] $ osg-koji list-tagged OUR_KOJI_TAG PACKAGE Untag the packages: [user@client ~] $ osg-koji untag-pkg OUR_KOJI_TAG BUILD Why you should not use block-pkg EPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case: EPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5. Later, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5. I used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead. The block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build. I unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again. Policy for epelrescue tags https://jira.opensciencegrid.org/browse/SOFTWARE-2046 Table of epelrescue RPMs Package Distro version Date added Reason added Date removed python-six-1.7.3-1.el6 6 2015-08-12 Dep of osg-build (via mock) 2015-10-14 python-argparse-1.2.1-2.el6 6 2015-09-23 Dep of osg-wn-client (via gfal2) 2015-10-14 python-backports-ssl_match_hostname-3.4.0.2-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-requests-1.1.0-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-urllib3-1.5-7.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 Finding out if a package is still needed in epelrescue Set $pkg to the name of a package to test (e.g. python-six ), and $rhel set to the RHEL version you're testing for (e.g. 5 , 6 , or 7 ). Using Carl's centos-srpms , scientific-srpms , slf-srpms scripts: for script in centos-srpms scientific-srpms slf-srpms; do echo -n $script : $script -$rhel $pkg | grep . || echo none done A dry run of removing the package: osg-koji untag-pkg -n --all epelrescue-el$rhel $pkg Remove the -n when the output of that looks fine.","title":"Resurrecting Epel Packages"},{"location":"software/resurrecting-epel-packages/#resurrecting-epel-rpms","text":"You will need to be a Koji admin to do these steps. [user@client ~] $ osg-koji --list-permissions --mine Will tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman. EPEL version EPEL Koji tag Our Koji tag 5 dist-5E-epel epelrescue-el5 6 dist-6E-epel epelrescue-el6 7 epel7 epelrescue-el7 Determine the NVR of the build containing the RPM of the package you want. Use the Fedora/EPEL Koji web interface ( https://koji.fedoraproject.org ) to search for it. You can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards. EPEL builds have .el5, .el6, or .el7 in the dist tag. Download all RPMs for all architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms. You have three options for the downloads: Use the links in the web interface Use the koji command-line interface against the Fedora koji: Download fedora-koji.conf , attached to this page Run koji --noauth -c fedora-koji.conf download-build --debuginfo BUILD_NVR Delete RPMs for architectures we do not care about (see list above) Dig around in https://kojipkgs.fedoraproject.org/packages/ On your development machine: Important: Verify that all of the RPMs are signed: [root@client ~] # rpm -K *.rpm | grep -iv gpg should be empty If not, STOP and sign them using the OSG RPM key -- talk to Mat Import the RPMs themselves into the Koji system [user@client ~] $ osg-koji import RPM_DIRECTORY /*.rpm They will not be in any tags at this point Add the package to the whitelist for our koji tag: [user@client ~] $ osg-koji add-pkg OUR_KOJI_TAG PACKAGE --owner = YOUR_KOJI_USERNAME Actually tag the builds: [user@client ~] $ osg-koji tag-pkg OUR_KOJI_TAG BUILD Check the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in. If it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them): for repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do osg-koji regen-repo --nowait $repo done Make a test VM and install the package from minefield to test that it is actually present. Update the epelrescue RPMs table below","title":"Resurrecting EPEL RPMs"},{"location":"software/resurrecting-epel-packages/#removing-resurrected-rpms","text":"In case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version: Find out the NVR of the build: [user@client ~] $ osg-koji list-tagged OUR_KOJI_TAG PACKAGE Untag the packages: [user@client ~] $ osg-koji untag-pkg OUR_KOJI_TAG BUILD","title":"Removing resurrected RPMs"},{"location":"software/resurrecting-epel-packages/#why-you-should-not-use-block-pkg","text":"EPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case: EPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5. Later, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5. I used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead. The block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build. I unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again.","title":"Why you should not use block-pkg"},{"location":"software/resurrecting-epel-packages/#policy-for-epelrescue-tags","text":"https://jira.opensciencegrid.org/browse/SOFTWARE-2046","title":"Policy for epelrescue tags"},{"location":"software/resurrecting-epel-packages/#table-of-epelrescue-rpms","text":"Package Distro version Date added Reason added Date removed python-six-1.7.3-1.el6 6 2015-08-12 Dep of osg-build (via mock) 2015-10-14 python-argparse-1.2.1-2.el6 6 2015-09-23 Dep of osg-wn-client (via gfal2) 2015-10-14 python-backports-ssl_match_hostname-3.4.0.2-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-requests-1.1.0-4.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14 python-urllib3-1.5-7.el6 6 2015-09-23 Dep of osg-build (via mock) 2015-10-14","title":"Table of epelrescue RPMs"},{"location":"software/resurrecting-epel-packages/#finding-out-if-a-package-is-still-needed-in-epelrescue","text":"Set $pkg to the name of a package to test (e.g. python-six ), and $rhel set to the RHEL version you're testing for (e.g. 5 , 6 , or 7 ). Using Carl's centos-srpms , scientific-srpms , slf-srpms scripts: for script in centos-srpms scientific-srpms slf-srpms; do echo -n $script : $script -$rhel $pkg | grep . || echo none done A dry run of removing the package: osg-koji untag-pkg -n --all epelrescue-el$rhel $pkg Remove the -n when the output of that looks fine.","title":"Finding out if a package is still needed in epelrescue"},{"location":"software/rpm-development-guide/","text":"RPM Development Guide This page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack. Principles The principles below guide the design and implementation of the technical details that follow. Packages should adhere to community standards (e.g., Fedora Packaging Guidelines ) when possible, and significant deviations must be documented Every released package must be reproducible from data stored in our system Source code for software should be clearly separable from the packaging of that software Upstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team Building source and binary packages from our system should be easy and efficient External developers should have a clear and effective system for developing and contributing packages We should use standard tools from relevant packaging and development communities when appropriate Contributing Packages We encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors. To participate in the packaging community: You must subscribe to the email list. Subscribing to an OSG email list is described here . To create and edit packages: Obtain access to VDT SVN . To upload new source tarballs: You must have a cs.wisc.edu account with write access to the VDT source tarball directory. Email the osg-software list and request permission. To build using the OSG's Koji build system: You must have a valid grid certificate and a Koji account. Email the osg-software list with your cert's DN and request permission. Development Infrastructure This section documents most of what a developer needs to know about our RPM infrastructure: Upstream Source Cache \u2014 a filesystem scheme for caching upstream source files Revision Control System \u2014 where to get and store development files, and how they are organized Build System \u2014 how to build packages from the revision control system Yum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it Upstream Source Cache One of our principles (every released package must be reproducible from data stored in our system) creates a potential issue: If we keep all historical source data, especially upstream files like source tarballs and source RPMs, in our revision control system, we may face large checkouts and consequently long checkout and update times. Our solution is to cache all upstream source files in a separate filesystem area, retaining historical files indefinitely. To avoid tainting upstream files, our policy is to leave them unmodified after download. Locating Files in the Cache Upstream source files are stored in the filesystem as follows: /p/vdt/public/html/upstream/ PACKAGE / VERSION / FILE where: Symbol Definition Example PACKAGE Upstream name of the source package, or some widely accepted form thereof ndt VERSION Upstream version string used to identify the release 3.6.4 FILE Upstream filename itself ndt-3.6.4.tar.gz The authoritative cache is the VDT webserver, which is fully backed up. The Koji build system uses this cache. Upstream source files are referenced from within the revision control system; see below for details. Contributing Upstream Files You must make sure that any new upstream source files are cached on the VDT webserver before building the package via Koji. You have two options: If you have access to a UW\u2013Madison CSL machine, you can scp the source files directly into the AFS locations using that machine If you do not have such access, write to the osg-software list to find someone who will post the files for you Git/GitHub Hosted Upstream Files It is also possible to pull sources and spec files from remote Git or GitHub repos instead of our source cache. See the upstream dir info for more information. Revision Control System All packages that the OSG Software Team releases are checked into our Subversion repository. Subversion Access Our Subversion repository is located at: https://vdt.cs.wisc.edu/svn Procedure for offsite users obtaining access to Subversion Or, from a UW\u2013Madison Computer Sciences machine: file:///p/condor/workspaces/vdt/svn The current SVN directory housing our native package work is $SVN/native/redhat (where $SVN is one of the ways of accessing our SVN repository above). For example, to check out the current package repository via HTTPS, do: [you@host]$ svn co https://vdt.cs.wisc.edu/svn/native/redhat OSG-Owned Software OSG-owned software goes into GitHub under the opensciencegrid organization. Files are organized as the developer sees fit. It is strongly recommended that each software package include a top-level Makefile with at least the following targets: Symbol Purpose install Install the software into final FHS locations rooted at DESTDIR dist Create a distribution source tarball (in the current section directory) for a release upstream Install the distribution source tarball into the upstream source cache Packaging Top-Level Directory Organization The top levels of our Subversion directory hierarchy for packaging are as follows: native/redhat/ SECTION / PACKAGE where: Symbol Definition Example SECTION Development section Standard Subversion sections like trunk and branches/* PACKAGE Our standardized name for a source package ndt Package Directory Organization Within a source package directory, the following files (detailed in separate sections below) may exist: README text file package notes, by and for developers upstream/ directory references to the upstream source cache and other kinds of upstream files osg/ directory overrides and patches of upstream files, plus new files, which contribute to the final OSG source package README This is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the why , not just the what . upstream Within the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents. A reference file is named: DESCRIPTION . TYPE .source where: Symbol Definition Example DESCRIPTION Descriptive label of the source of the referenced file developer , epel , emi TYPE Type of referenced file tarball , srpm and contain references to cached files, Git repos, and comments. which start with # and continue until the end of the line. It is useful to add the source of the upstream file into a comment. Cached files To reference files in the upstream source cache, use the upstream source cache path defined above, without the prefix component: PACKAGE / VERSION / FILE Example The reference file for globus-common 's source tarball is named epel.srpm.source and contains: globus-common/16.4/globus-common-16.4-1.el6.src.rpm # Downloaded from http://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm Git repos Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to . Note This feature requires OSG-Build 1.11.2 or later. Note You can use a shorter syntax for GitHub repos -- see below. See also advanced features for Git and GitHub repos . To reference tags in Git repos, use the following syntax (all on one line): type=git url= URL name= NAME tag= TAG hash= HASH where: Symbol Definition Example URL Location of the Git repo https://github.com/opensciencegrid/osg-build.git NAME Name of the software (optional) osg-build TAG Git tag to use v1.11.2 HASH Full 40-char Git hash of the tag 5bcf48c442d21b1e8c93a468d884f84122f7cc9e Note NAME is optional; if not present, OSG-Build will use the last component of the URL, without the .git suffix. The tarball will be called NAME - VERSION .tar.gz where VERSION is TAG without the v prefix (if there is one). Example The reference file for osg-build 's repo is named osg.github.source and contains: type=git url=https://github.com/opensciencegrid/osg-build.git name=osg-build tag=v1.11.2 hash=5bcf48c442d21b1e8c93a468d884f84122f7cc9e This results in a tarball named osg-build-1.11.2.tar.gz . In addition, if the repository contains a file called rpm/ NAME .spec , it will be used as the spec file for the build (unless overridden in the osg directory). GitHub repos Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to . Note This feature requires OSG-Build 1.12.2 or later. Note See also advanced features for Git and GitHub repos . To reference tags in GitHub repos, use the following syntax (all on one line): type=github repo= OWNER / PROJECT tag= TAG hash= HASH where: Symbol Definition Example OWNER Owner of the GitHub repo opensciencegrid PROJECT Name of the project osg-build TAG Git tag to use v1.12.2 HASH Full 40-char Git hash of the tag cff50ffe812282552cedae81f3809d3cf7087a3e Note The tarball will be called PROJECT - VERSION .tar.gz where VERSION is TAG without the v prefix (if there is one). Example You can refer to the 1.12.2 release of osg-build with this line: type=github repo=opensciencegrid/osg-build tag=v1.12.2 hash=cff50ffe812282552cedae81f3809d3cf7087a3e This results in a tarball named osg-build-1.12.2.tar.gz . In addition, if the repository contains a file called rpm/ PROJECT .spec , it will be used as the spec file for the build (unless overridden in the osg directory). Advanced features for Git and GitHub repos Note These features require OSG-Build 1.12.2 or later. The following features make software development in Git and GitHub more convenient: Support for RPM release numbers in Git tags: If the tag for the software contains a dash, as in v1.12.2-1 , it is assumed that the text after the dash is the RPM release instead of the software version. The RPM release is not included in the tarball. That is, the project osg-build with the tag v1.12.2-1 will result in a tarball named osg-build-1.12.1.tar.gz , not osg-build-1.12.1-1.tar.gz . Can specify tarball name in the .source file: The new tarball attribute allows you to specify the name of the tarball and directory that the repo contents will be put into. The syntax is tarball= NAME .tar.gz . The extension must be .tar.gz , no other archive formats are supported. The directory inside the tarball will then be NAME / . Can specify hash and tarball instead of tag (scratch and local builds only): For local builds (rpmbuild and mock tasks) and Koji scratch builds, you can omit the tag attribute and use only the hash to determine what to check out. You must specify the tarball attribute in this case. Non-scratch Koji builds will still require a tag. Can ignore hash mismatch (scratch and local builds only): For local builds (rpmbuild and mock tasks) and Koji scratch builds, a hash mismatch will result in a warning. Non-scratch Koji builds will still consider it an error. Can use a branch as the tag: The tag attribute can refer to a branch instead of a tag, e.g. tag=master . Combining the last two features can really speed up package development. For example, you can use this to make scratch builds of the current master: type=github repo= OWNER / PROJECT tarball= PROJECT - VERSION .tar.gz tag=master hash=0 This might also be useful as part of a continuous integration scheme (e.g. Travis-CI). osg The osg directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files: An RPM .spec file, which overrides any spec file from a referenced source Patch ( .patch ) or replacement files, which override any same-named file from the top-level directory of a referenced source Other files, which must be explicitly placed into the package by the spec file Generated directories The following directories may be generated by our build tool, OSG-Build . They are not under revision control. _upstream_srpm_contents/ expanded contents of a cached upstream source package _upstream_tarball_contents/ expanded contents of all cached upstream source tarballs _final_srpm_contents/ the final contents of the OSG source package _build_results/ OSG source and binary packages resulting from a build _quilt/ expanded, patched contents of the upstream sources, as generated by the quilt tool _upstream_srpm_contents The _upstream_srpm_contents directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use. _upstream_tarball_contents The _upstream_tarball_contents directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the --full-extract option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.). _final_srpm_contents The _final_srpm_contents directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use. _build_results The _build_results directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use. _quilt The _quilt directory contains the unpacked sources after they have been patched using the quilt utility. This allows easier patch development. Packaging Organization Examples Use Case 1: Packaging an Upstream Source Tarball When the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains: rsv/ osg/ rsv.spec upstream/ developer.tarball.source Use Case 2: Passing Through a Source RPM When the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the globus-common source RPM from EPEL. Its package directory contains: globus-common/ upstream/ epel.srpm.source Use Case 3: Modifying a Source RPM When the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the osg directory. For example, we use this mechanism for the globus-ftp-client package, originally obtained from EPEL. Its package directory contains: globus-ftp-client/ osg/ globus-ftp-client.spec 1853-ssh-bin.patch upstream/ epel.srpm.source Build Process All necessary information to create the package will be committed to the VDT source code repository (see below) The OSG build tools will take those files, create a source RPM, and submit it to our Koji build system Developers may use rpmbuild and mock for faster iterative development before submitting the package to Koji. osg-build may be used as a wrapper script around rpmbuild and mock . OSG Software Repository OSG Operations maintains the Yum repositories that contain our source and binary RPMs at https://repo.opensciencegrid.org/osg/ and are mirrored at other institutions as well. Release Levels Every package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level ( osg-development ). The members of the OSG Software and Release teams may promote packages through the release levels, as per our Release Policy page . Packaging Conventions In addition to adhering to the Fedora Packaging Guidelines (FPG), we have a few rules and guidelines of our own: When we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows: The version of the original RPM remains unchanged The release is composed of three parts: ORIGINALRELEASE.OSGRELEASE We add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use %{?dist} in the Release field) Example: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes foobar-3.0.5-1.1.osg33.el6 Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like: Release : 1.2 %{?dist} Packaging for Multiple Distro Versions Conditionalizing spec files Some packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this. The following macros are defined: Name Value (EL6) Value (EL7) %rhel 6 7 %el6 1 undefined or 0 %el7 undefined or 0 1 Here's how to use them: %if 0%{?el6} # this code will be executed on EL 6 only %endif %if 0%{?el7} # this code will be executed on EL 7 only %endif %if 0%{?rhel} = 7 # this code will be executed on EL 7 and newer %endif (There does not seem to be an %elseif ). The syntax %{?el6} expands to the value of the %el6 macro if it is defined, and to the empty string if not; the 0 is there to keep the condition from being empty in the %if statement if the macro is not defined. Renaming or Removing Packages Occasionally we want to cause a package to be removed on update, or replaced by a package with a different name. For the most part, the Fedora Packaging Guidelines page on renames shows how to do that. The exception is that we do not have the equivalent of a fedora-obsolete-packages package, so in order to force the removal of an entire package (not a subpackage), we have to dummy out the package instead -- see below. (This should be a rare situation.) Note After doing a rename or a removal, you must update all the packages and subpackages that require the package being removed or renamed, and change or remove the requirements as appropriate. To find packages that require the old package at run time, set up a host with the OSG repos and install the yum-utils RPM. Then, run: $ repoquery --plugins --whatrequires $OLDPACKAGE To find packages that require the old package at build time, install osg-build , and do this from a checkout of the OSG repos: $ osg-build prebuild * $ for srpm in */_final_srpm_contents/*.src.rpm ; do \\ echo ***** $srpm ***** ; \\ rpm -q --requires -p $srpm | grep -w $OLDPACKAGE ; \\ done (examine the output to avoid false matches) Note Carefully test these changes, including places where the old package may be brought in indirectly. Dummying out a package In order to forcibly remove an entire package with no replacement, you have to replace the package with one that does nothing. This is because there is no package that will \"obsolete\" the old package. Do the following for the main package and any subpackages it may have: Change the Summary to \"Dummy package\" Change the %description to: This is an empty package created for $REASONS . It may safely be removed. Remove all Requires and Obsoletes lines Do not remove Provides lines Remove %pre and %post scriptlets Unless there is a good reason not to, remove %preun and %postun scriptlets Empty the %files section","title":"RPM Development Guide"},{"location":"software/rpm-development-guide/#rpm-development-guide","text":"This page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack.","title":"RPM Development Guide"},{"location":"software/rpm-development-guide/#principles","text":"The principles below guide the design and implementation of the technical details that follow. Packages should adhere to community standards (e.g., Fedora Packaging Guidelines ) when possible, and significant deviations must be documented Every released package must be reproducible from data stored in our system Source code for software should be clearly separable from the packaging of that software Upstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team Building source and binary packages from our system should be easy and efficient External developers should have a clear and effective system for developing and contributing packages We should use standard tools from relevant packaging and development communities when appropriate","title":"Principles"},{"location":"software/rpm-development-guide/#contributing-packages","text":"We encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors. To participate in the packaging community: You must subscribe to the email list. Subscribing to an OSG email list is described here . To create and edit packages: Obtain access to VDT SVN . To upload new source tarballs: You must have a cs.wisc.edu account with write access to the VDT source tarball directory. Email the osg-software list and request permission. To build using the OSG's Koji build system: You must have a valid grid certificate and a Koji account. Email the osg-software list with your cert's DN and request permission.","title":"Contributing Packages"},{"location":"software/rpm-development-guide/#development-infrastructure","text":"This section documents most of what a developer needs to know about our RPM infrastructure: Upstream Source Cache \u2014 a filesystem scheme for caching upstream source files Revision Control System \u2014 where to get and store development files, and how they are organized Build System \u2014 how to build packages from the revision control system Yum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it","title":"Development Infrastructure"},{"location":"software/rpm-development-guide/#upstream-source-cache","text":"One of our principles (every released package must be reproducible from data stored in our system) creates a potential issue: If we keep all historical source data, especially upstream files like source tarballs and source RPMs, in our revision control system, we may face large checkouts and consequently long checkout and update times. Our solution is to cache all upstream source files in a separate filesystem area, retaining historical files indefinitely. To avoid tainting upstream files, our policy is to leave them unmodified after download.","title":"Upstream Source Cache"},{"location":"software/rpm-development-guide/#locating-files-in-the-cache","text":"Upstream source files are stored in the filesystem as follows: /p/vdt/public/html/upstream/ PACKAGE / VERSION / FILE where: Symbol Definition Example PACKAGE Upstream name of the source package, or some widely accepted form thereof ndt VERSION Upstream version string used to identify the release 3.6.4 FILE Upstream filename itself ndt-3.6.4.tar.gz The authoritative cache is the VDT webserver, which is fully backed up. The Koji build system uses this cache. Upstream source files are referenced from within the revision control system; see below for details.","title":"Locating Files in the Cache"},{"location":"software/rpm-development-guide/#contributing-upstream-files","text":"You must make sure that any new upstream source files are cached on the VDT webserver before building the package via Koji. You have two options: If you have access to a UW\u2013Madison CSL machine, you can scp the source files directly into the AFS locations using that machine If you do not have such access, write to the osg-software list to find someone who will post the files for you","title":"Contributing Upstream Files"},{"location":"software/rpm-development-guide/#gitgithub-hosted-upstream-files","text":"It is also possible to pull sources and spec files from remote Git or GitHub repos instead of our source cache. See the upstream dir info for more information.","title":"Git/GitHub Hosted Upstream Files"},{"location":"software/rpm-development-guide/#revision-control-system","text":"All packages that the OSG Software Team releases are checked into our Subversion repository.","title":"Revision Control System"},{"location":"software/rpm-development-guide/#subversion-access","text":"Our Subversion repository is located at: https://vdt.cs.wisc.edu/svn Procedure for offsite users obtaining access to Subversion Or, from a UW\u2013Madison Computer Sciences machine: file:///p/condor/workspaces/vdt/svn The current SVN directory housing our native package work is $SVN/native/redhat (where $SVN is one of the ways of accessing our SVN repository above). For example, to check out the current package repository via HTTPS, do: [you@host]$ svn co https://vdt.cs.wisc.edu/svn/native/redhat","title":"Subversion Access"},{"location":"software/rpm-development-guide/#osg-owned-software","text":"OSG-owned software goes into GitHub under the opensciencegrid organization. Files are organized as the developer sees fit. It is strongly recommended that each software package include a top-level Makefile with at least the following targets: Symbol Purpose install Install the software into final FHS locations rooted at DESTDIR dist Create a distribution source tarball (in the current section directory) for a release upstream Install the distribution source tarball into the upstream source cache","title":"OSG-Owned Software"},{"location":"software/rpm-development-guide/#packaging-top-level-directory-organization","text":"The top levels of our Subversion directory hierarchy for packaging are as follows: native/redhat/ SECTION / PACKAGE where: Symbol Definition Example SECTION Development section Standard Subversion sections like trunk and branches/* PACKAGE Our standardized name for a source package ndt","title":"Packaging Top-Level Directory Organization"},{"location":"software/rpm-development-guide/#package-directory-organization","text":"Within a source package directory, the following files (detailed in separate sections below) may exist: README text file package notes, by and for developers upstream/ directory references to the upstream source cache and other kinds of upstream files osg/ directory overrides and patches of upstream files, plus new files, which contribute to the final OSG source package","title":"Package Directory Organization"},{"location":"software/rpm-development-guide/#readme","text":"This is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the why , not just the what .","title":"README"},{"location":"software/rpm-development-guide/#upstream","text":"Within the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents. A reference file is named: DESCRIPTION . TYPE .source where: Symbol Definition Example DESCRIPTION Descriptive label of the source of the referenced file developer , epel , emi TYPE Type of referenced file tarball , srpm and contain references to cached files, Git repos, and comments. which start with # and continue until the end of the line. It is useful to add the source of the upstream file into a comment.","title":"upstream"},{"location":"software/rpm-development-guide/#cached-files","text":"To reference files in the upstream source cache, use the upstream source cache path defined above, without the prefix component: PACKAGE / VERSION / FILE Example The reference file for globus-common 's source tarball is named epel.srpm.source and contains: globus-common/16.4/globus-common-16.4-1.el6.src.rpm # Downloaded from http://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm","title":"Cached files"},{"location":"software/rpm-development-guide/#git-repos","text":"Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to . Note This feature requires OSG-Build 1.11.2 or later. Note You can use a shorter syntax for GitHub repos -- see below. See also advanced features for Git and GitHub repos . To reference tags in Git repos, use the following syntax (all on one line): type=git url= URL name= NAME tag= TAG hash= HASH where: Symbol Definition Example URL Location of the Git repo https://github.com/opensciencegrid/osg-build.git NAME Name of the software (optional) osg-build TAG Git tag to use v1.11.2 HASH Full 40-char Git hash of the tag 5bcf48c442d21b1e8c93a468d884f84122f7cc9e Note NAME is optional; if not present, OSG-Build will use the last component of the URL, without the .git suffix. The tarball will be called NAME - VERSION .tar.gz where VERSION is TAG without the v prefix (if there is one). Example The reference file for osg-build 's repo is named osg.github.source and contains: type=git url=https://github.com/opensciencegrid/osg-build.git name=osg-build tag=v1.11.2 hash=5bcf48c442d21b1e8c93a468d884f84122f7cc9e This results in a tarball named osg-build-1.11.2.tar.gz . In addition, if the repository contains a file called rpm/ NAME .spec , it will be used as the spec file for the build (unless overridden in the osg directory).","title":"Git repos"},{"location":"software/rpm-development-guide/#github-repos","text":"Warning OSG software policy requires that all Git and GitHub repos used for building software have mirrors at the UW. Many software repos under the opensciencegrid GitHub organization are already mirrored. If you are uncertain, or have a new project that you want mirrored, send email to . Note This feature requires OSG-Build 1.12.2 or later. Note See also advanced features for Git and GitHub repos . To reference tags in GitHub repos, use the following syntax (all on one line): type=github repo= OWNER / PROJECT tag= TAG hash= HASH where: Symbol Definition Example OWNER Owner of the GitHub repo opensciencegrid PROJECT Name of the project osg-build TAG Git tag to use v1.12.2 HASH Full 40-char Git hash of the tag cff50ffe812282552cedae81f3809d3cf7087a3e Note The tarball will be called PROJECT - VERSION .tar.gz where VERSION is TAG without the v prefix (if there is one). Example You can refer to the 1.12.2 release of osg-build with this line: type=github repo=opensciencegrid/osg-build tag=v1.12.2 hash=cff50ffe812282552cedae81f3809d3cf7087a3e This results in a tarball named osg-build-1.12.2.tar.gz . In addition, if the repository contains a file called rpm/ PROJECT .spec , it will be used as the spec file for the build (unless overridden in the osg directory).","title":"GitHub repos"},{"location":"software/rpm-development-guide/#advanced-features-for-git-and-github-repos","text":"Note These features require OSG-Build 1.12.2 or later. The following features make software development in Git and GitHub more convenient: Support for RPM release numbers in Git tags: If the tag for the software contains a dash, as in v1.12.2-1 , it is assumed that the text after the dash is the RPM release instead of the software version. The RPM release is not included in the tarball. That is, the project osg-build with the tag v1.12.2-1 will result in a tarball named osg-build-1.12.1.tar.gz , not osg-build-1.12.1-1.tar.gz . Can specify tarball name in the .source file: The new tarball attribute allows you to specify the name of the tarball and directory that the repo contents will be put into. The syntax is tarball= NAME .tar.gz . The extension must be .tar.gz , no other archive formats are supported. The directory inside the tarball will then be NAME / . Can specify hash and tarball instead of tag (scratch and local builds only): For local builds (rpmbuild and mock tasks) and Koji scratch builds, you can omit the tag attribute and use only the hash to determine what to check out. You must specify the tarball attribute in this case. Non-scratch Koji builds will still require a tag. Can ignore hash mismatch (scratch and local builds only): For local builds (rpmbuild and mock tasks) and Koji scratch builds, a hash mismatch will result in a warning. Non-scratch Koji builds will still consider it an error. Can use a branch as the tag: The tag attribute can refer to a branch instead of a tag, e.g. tag=master . Combining the last two features can really speed up package development. For example, you can use this to make scratch builds of the current master: type=github repo= OWNER / PROJECT tarball= PROJECT - VERSION .tar.gz tag=master hash=0 This might also be useful as part of a continuous integration scheme (e.g. Travis-CI).","title":"Advanced features for Git and GitHub repos"},{"location":"software/rpm-development-guide/#osg","text":"The osg directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files: An RPM .spec file, which overrides any spec file from a referenced source Patch ( .patch ) or replacement files, which override any same-named file from the top-level directory of a referenced source Other files, which must be explicitly placed into the package by the spec file","title":"osg"},{"location":"software/rpm-development-guide/#generated-directories","text":"The following directories may be generated by our build tool, OSG-Build . They are not under revision control. _upstream_srpm_contents/ expanded contents of a cached upstream source package _upstream_tarball_contents/ expanded contents of all cached upstream source tarballs _final_srpm_contents/ the final contents of the OSG source package _build_results/ OSG source and binary packages resulting from a build _quilt/ expanded, patched contents of the upstream sources, as generated by the quilt tool","title":"Generated directories"},{"location":"software/rpm-development-guide/#95upstream95srpm95contents","text":"The _upstream_srpm_contents directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use.","title":"_upstream_srpm_contents"},{"location":"software/rpm-development-guide/#95upstream95tarball95contents","text":"The _upstream_tarball_contents directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the --full-extract option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.).","title":"_upstream_tarball_contents"},{"location":"software/rpm-development-guide/#95final95srpm95contents","text":"The _final_srpm_contents directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use.","title":"_final_srpm_contents"},{"location":"software/rpm-development-guide/#95build95results","text":"The _build_results directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use.","title":"_build_results"},{"location":"software/rpm-development-guide/#95quilt","text":"The _quilt directory contains the unpacked sources after they have been patched using the quilt utility. This allows easier patch development.","title":"_quilt"},{"location":"software/rpm-development-guide/#packaging-organization-examples","text":"","title":"Packaging Organization Examples"},{"location":"software/rpm-development-guide/#use-case-1-packaging-an-upstream-source-tarball","text":"When the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains: rsv/ osg/ rsv.spec upstream/ developer.tarball.source","title":"Use Case 1: Packaging an Upstream Source Tarball"},{"location":"software/rpm-development-guide/#use-case-2-passing-through-a-source-rpm","text":"When the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the globus-common source RPM from EPEL. Its package directory contains: globus-common/ upstream/ epel.srpm.source","title":"Use Case 2: Passing Through a Source RPM"},{"location":"software/rpm-development-guide/#use-case-3-modifying-a-source-rpm","text":"When the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the osg directory. For example, we use this mechanism for the globus-ftp-client package, originally obtained from EPEL. Its package directory contains: globus-ftp-client/ osg/ globus-ftp-client.spec 1853-ssh-bin.patch upstream/ epel.srpm.source","title":"Use Case 3: Modifying a Source RPM"},{"location":"software/rpm-development-guide/#build-process","text":"All necessary information to create the package will be committed to the VDT source code repository (see below) The OSG build tools will take those files, create a source RPM, and submit it to our Koji build system Developers may use rpmbuild and mock for faster iterative development before submitting the package to Koji. osg-build may be used as a wrapper script around rpmbuild and mock .","title":"Build Process"},{"location":"software/rpm-development-guide/#osg-software-repository","text":"OSG Operations maintains the Yum repositories that contain our source and binary RPMs at https://repo.opensciencegrid.org/osg/ and are mirrored at other institutions as well.","title":"OSG Software Repository"},{"location":"software/rpm-development-guide/#release-levels","text":"Every package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level ( osg-development ). The members of the OSG Software and Release teams may promote packages through the release levels, as per our Release Policy page .","title":"Release Levels"},{"location":"software/rpm-development-guide/#packaging-conventions","text":"In addition to adhering to the Fedora Packaging Guidelines (FPG), we have a few rules and guidelines of our own: When we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows: The version of the original RPM remains unchanged The release is composed of three parts: ORIGINALRELEASE.OSGRELEASE We add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use %{?dist} in the Release field) Example: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes foobar-3.0.5-1.1.osg33.el6 Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like: Release : 1.2 %{?dist}","title":"Packaging Conventions"},{"location":"software/rpm-development-guide/#packaging-for-multiple-distro-versions","text":"","title":"Packaging for Multiple Distro Versions"},{"location":"software/rpm-development-guide/#conditionalizing-spec-files","text":"Some packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this. The following macros are defined: Name Value (EL6) Value (EL7) %rhel 6 7 %el6 1 undefined or 0 %el7 undefined or 0 1 Here's how to use them: %if 0%{?el6} # this code will be executed on EL 6 only %endif %if 0%{?el7} # this code will be executed on EL 7 only %endif %if 0%{?rhel} = 7 # this code will be executed on EL 7 and newer %endif (There does not seem to be an %elseif ). The syntax %{?el6} expands to the value of the %el6 macro if it is defined, and to the empty string if not; the 0 is there to keep the condition from being empty in the %if statement if the macro is not defined.","title":"Conditionalizing spec files"},{"location":"software/rpm-development-guide/#renaming-or-removing-packages","text":"Occasionally we want to cause a package to be removed on update, or replaced by a package with a different name. For the most part, the Fedora Packaging Guidelines page on renames shows how to do that. The exception is that we do not have the equivalent of a fedora-obsolete-packages package, so in order to force the removal of an entire package (not a subpackage), we have to dummy out the package instead -- see below. (This should be a rare situation.) Note After doing a rename or a removal, you must update all the packages and subpackages that require the package being removed or renamed, and change or remove the requirements as appropriate. To find packages that require the old package at run time, set up a host with the OSG repos and install the yum-utils RPM. Then, run: $ repoquery --plugins --whatrequires $OLDPACKAGE To find packages that require the old package at build time, install osg-build , and do this from a checkout of the OSG repos: $ osg-build prebuild * $ for srpm in */_final_srpm_contents/*.src.rpm ; do \\ echo ***** $srpm ***** ; \\ rpm -q --requires -p $srpm | grep -w $OLDPACKAGE ; \\ done (examine the output to avoid false matches) Note Carefully test these changes, including places where the old package may be brought in indirectly.","title":"Renaming or Removing Packages"},{"location":"software/rpm-development-guide/#dummying-out-a-package","text":"In order to forcibly remove an entire package with no replacement, you have to replace the package with one that does nothing. This is because there is no package that will \"obsolete\" the old package. Do the following for the main package and any subpackages it may have: Change the Summary to \"Dummy package\" Change the %description to: This is an empty package created for $REASONS . It may safely be removed. Remove all Requires and Obsoletes lines Do not remove Provides lines Remove %pre and %post scriptlets Unless there is a good reason not to, remove %preun and %postun scriptlets Empty the %files section","title":"Dummying out a package"},{"location":"software/upcoming-to-main/","text":"Promoting Packages from Upcoming to Main Sometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move: If needed, move the software from upcoming to trunk and release using the usual process: Merge changes to the package in SVN from branches/upcoming to trunk. Build the package from trunk. Follow the normal process to prepare a build for release (including development testing, promotion, etc.). On release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos: Untag from all upcoming repos the version of the package corresponding to the version that was released in main. (Do NOT untag from the osg-upcoming-elN-release-X.Y.Z tags) Also, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos. If you do not have the privileges to untag from upcoming-release, someone on the Release Team can help. (These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos). Unless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.","title":"Upcoming to Main"},{"location":"software/upcoming-to-main/#promoting-packages-from-upcoming-to-main","text":"Sometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move: If needed, move the software from upcoming to trunk and release using the usual process: Merge changes to the package in SVN from branches/upcoming to trunk. Build the package from trunk. Follow the normal process to prepare a build for release (including development testing, promotion, etc.). On release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos: Untag from all upcoming repos the version of the package corresponding to the version that was released in main. (Do NOT untag from the osg-upcoming-elN-release-X.Y.Z tags) Also, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos. If you do not have the privileges to untag from upcoming-release, someone on the Release Team can help. (These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos). Unless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.","title":"Promoting Packages from Upcoming to Main"}]}